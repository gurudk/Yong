<?xml version="1.0" encoding="UTF-8"?>
<TEI
        xmlns="http://www.tei-c.org/ns/1.0"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve"
        xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
    <teiHeader xml:lang="en">
        <fileDesc>
            <titleStmt>
                <title level="a"
                       type="main">Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</title>
            </titleStmt>
            <publicationStmt>
                <publisher/>
                <availability status="unknown">
                    <licence/>
                </availability>
            </publicationStmt>
            <sourceDesc>
                <biblStruct>
                    <analytic>
                        <author>
                            <persName>
                                <forename type="first">Li</forename>
                                <surname>Yuan</surname>
                            </persName>
                            <email>yuanli@u.nus.edu</email>
                        </author>
                        <author>
                            <persName>
                                <forename type="first">Yunpeng</forename>
                                <surname>Chen</surname>
                            </persName>
                            <email>yunpeng.chen@yitu-inc.com</email>
                        </author>
                        <author>
                            <persName>
                                <forename type="first">Tao</forename>
                                <surname>Wang</surname>
                            </persName>
                        </author>
                        <author>
                            <persName>
                                <forename type="first">Weihao</forename>
                                <surname>Yu</surname>
                            </persName>
                        </author>
                        <author>
                            <persName>
                                <forename type="first">Yujun</forename>
                                <surname>Shi</surname>
                            </persName>
                        </author>
                        <author>
                            <persName>
                                <forename type="first">Zihang</forename>
                                <surname>Jiang</surname>
                            </persName>
                        </author>
                        <author>
                            <persName>
                                <forename type="first">Francis</forename>
                                <forename type="middle">E H</forename>
                                <surname>Tay</surname>
                            </persName>
                        </author>
                        <author>
                            <persName>
                                <forename type="first">Jiashi</forename>
                                <surname>Feng</surname>
                            </persName>
                        </author>
                        <author>
                            <persName>
                                <forename type="first">Shuicheng</forename>
                                <surname>Yan</surname>
                            </persName>
                            <email>shuicheng.yan@gmail.com</email>
                        </author>
                        <title level="a" type="main">Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</title>
                    </analytic>
                    <monogr>
                        <imprint>
                            <date/>
                        </imprint>
                    </monogr>
                    <idno type="MD5">503AA0283727D7647C29CF3C385DC824</idno>
                </biblStruct>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-07-08T16:18+0000">
                    <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
                    <ref target="https://github.com/kermitt2/grobid"/>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <abstract>
                <div
                        xmlns="http://www.tei-c.org/ns/1.0">
                    <p>
                        <s>Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classification.</s>
                        <s>The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification.</s>
                        <s>However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet.</s>
                        <s>We find it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness for fixed computation budgets and limited training samples.</s>
                        <s>To overcome such limitations, we propose a new Tokens-To-Token Vision Transformer (T2T-ViT), which incorporates 1) a layerwise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study.</s>
                        <s>Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0% improvement when trained from scratch on ImageNet.</s>
                        <s>It also outperforms ResNets and achieves comparable performance with MobileNets by directly training on ImageNet.</s>
                        <s>For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3% top1 accuracy in image resolution 384×384 on ImageNet. 1</s>
                    </p>
                </div>
            </abstract>
        </profileDesc>
    </teiHeader>
    <text xml:lang="en">
        <body>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="1.">Introduction</head>
                <p>
                    <s>Self-attention models for language modeling like Transformers
                        <ref type="bibr" target="#b37">[37]</ref>
                        have been recently applied to vision tasks, including image classification
                        <ref type="bibr" target="#b5">[5,</ref>
                        <ref type="bibr" target="#b12">12,</ref>
                        <ref type="bibr" target="#b43">43]</ref>, object detec- tion
                        <ref type="bibr" target="#b3">[3,</ref>
                        <ref type="bibr" target="#b61">61]</ref>
                        and image processing like denoising, superresolution and deraining
                        <ref type="bibr" target="#b4">[4]</ref>.
                    </s>
                    <s>Among them, the Vision Transformer (ViT)
                        <ref type="bibr" target="#b12">[12]</ref>
                        is the first full-transformer model that can be directly applied for image classification.
                    </s>
                    <s>In particular, ViT splits each image into 14×14 or 16×16 patches (a.k.a., tokens) with fixed length; then following practice of the transformer for language modeling, ViT applies transformer layers to model the global relation among these tokens for classification.</s>
                </p>
                <p>
                    <s>Though ViT proves the full-transformer architecture is promising for vision tasks, its performance is still inferior to that of similar-sized CNN counterparts (e.g.</s>
                    <s>ResNets) when trained from scratch on a midsize dataset (e.g., Im-ageNet).</s>
                    <s>We hypothesize that such performance gap roots in two main limitations of ViT: 1) the straightforward tokenization of input images by hard split makes ViT unable to model the image local structure like edges and lines, and thus it requires significantly more training samples (like JFT-300M for pretraining) than CNNs for achieving similar performance; 2) the attention backbone of ViT is not welldesigned as CNNs for vision tasks, which contains redundancy and leads to limited feature richness and difficulties in model training.</s>
                </p>
                <p>
                    <s>To verify our hypotheses, we conduct a pilot study to investigate the difference in the learned features of ViT-L/16
                        <ref type="bibr" target="#b12">[12]</ref>
                        and ResNet50
                        <ref type="bibr" target="#b15">[15]</ref>
                        through visualization in Fig.
                        <ref type="figure">2</ref>. We observe the features of ResNet capture the desired local Figure
                        <ref type="figure">2</ref>. Feature visualization of ResNet50, ViT-L/16
                        <ref type="bibr" target="#b12">[12]</ref>
                        and our proposed T2T-ViT-24 trained on ImageNet.
                    </s>
                    <s>Green boxes highlight learned low-level structure features such as edges and lines; red boxes highlight invalid feature maps with zero or too large values.</s>
                    <s>Note the feature maps visualized here for ViT and T2T-ViT are not attention maps, but image features reshaped from tokens.</s>
                    <s>For better visualization, we scale the input image to size 1024 × 1024 or 2048 × 2048.</s>
                </p>
                <p>
                    <s>structure (edges, lines, textures, etc.) progressively from the bottom layer (conv1) to the middle layer (conv25).</s>
                    <s>However, the features of ViT are quite different: the structure information is poorly modeled while the global relations (e.g., the whole dog) are captured by all the attention blocks.</s>
                    <s>These observations indicate that the vanilla ViT ignores the local structure when directly splitting images to tokens with fixed length.</s>
                    <s>Besides, we find many channels in ViT have zero value (highlighted in red in Fig.
                        <ref type="figure">2</ref>), implying the backbone of ViT is not efficient as ResNets and offers
                        limited feature richness when training samples are not enough.
                    </s>
                </p>
                <p>
                    <s>We are then motivated to design a new full-transformer vision model to overcome above limitations.</s>
                    <s>1) Instead of the naive tokenization used in ViT
                        <ref type="bibr" target="#b12">[12]</ref>, we propose a progressive tokenization module to
                        aggregate neighboring Tokens to one Token (named Tokens-to-Token module), which can model the
                        local structure information of surrounding tokens and reduce the length of tokens iteratively.
                    </s>
                    <s>Specifically, in each Token-to-Token (T2T) step, the tokens output by a transformer layer are reconstructed as an image (restructurization) which is then split into tokens with overlapping (soft split) and finally the surrounding tokens are aggregated together by flattening the split patches.</s>
                    <s>Thus the local structure from surrounding patches is embedded into the tokens to be input into the next transformer layer.</s>
                    <s>By conducting T2T iteratively, the local structure is aggregated into tokens and the length of tokens can be reduced by the aggregation process.</s>
                    <s>2) To find an efficient backbone for vision transformers, we explore borrowing some architecture designs from CNNs to build transformer layers for improving the feature richness, and we find "deepnarrow" architecture design with fewer channels but more layers in ViT brings much better performance at comparable model size and MACs (Multi-Adds).</s>
                    <s>Specifically, we investigate Wide-ResNets (shallow-wide vs deep-narrow structure)
                        <ref type="bibr" target="#b52">[52]</ref>, DenseNet (dense connection)
                        <ref type="bibr" target="#b21">[21]</ref>, ResneXt structure
                        <ref type="bibr" target="#b44">[44]</ref>, Ghost operation
                        <ref type="bibr" target="#b14">[14,</ref>
                        <ref type="bibr" target="#b59">59]</ref>
                        and channel attention
                        <ref type="bibr" target="#b20">[20]</ref>.
                    </s>
                    <s>We find among them, deep-narrow structure
                        <ref type="bibr" target="#b52">[52]</ref>
                        is the most efficient and effective for ViT, reducing the parameter count and MACs significantly
                        with nearly no degradation in performance.
                    </s>
                    <s>This also indicates the architecture engineering of CNNs can benefit the backbone design of vision transformers.</s>
                </p>
                <p>
                    <s>Based on the T2T module and deep-narrow backbone architecture, we develop the Tokens-to-Token Vision Transformer (T2T-ViT), which significantly boosts the performance when trained from scratch on ImageNet (Fig.
                        <ref type="figure" target="#fig_0">1</ref>), and is more lightweight than the vanilla ViT.
                    </s>
                    <s>As shown in Fig.
                        <ref type="figure" target="#fig_0">1</ref>, our T2T-ViT with 21.5M parameters and 4.8G MACs can
                        achieve 81.5% top-1 accuracy on ImageNet, much higher than that of ViT
                        <ref type="bibr" target="#b12">[12]</ref>
                        with 48.6M parameters and 10.1G MACs (78.1%).
                    </s>
                    <s>This result is also higher than the popular CNNs of similar size, like ResNet50 with 25.5M parameters (76%-79%).</s>
                    <s>Besides, we also design lite variants of T2T-ViT by simply adopting fewer layers, which achieve comparable results with MobileNets
                        <ref type="bibr" target="#b17">[17,</ref>
                        <ref type="bibr" target="#b32">32]</ref>
                        (Fig.
                        <ref type="figure" target="#fig_0">1</ref>).
                    </s>
                </p>
                <p>
                    <s>To sum up, our contributions are three-fold:</s>
                </p>
                <p>
                    <s>• For the first time, we show by carefully designing transformers architecture (T2T module and efficient backbone), visual transformers can outperform CNNs at different complexities on ImageNet without pretraining on JFT-300M.</s>
                </p>
                <p>
                    <s>• We develop a novel progressive tokenization for ViT and demonstrate its advantage over the simple tokenization approach by ViT, and we propose a T2T module that can encode the important local structure for each token.</s>
                </p>
                <p>
                    <s>• We show the architecture engineering of CNNs can benefit the backbone design of ViT to improve the feature richness and reduce redundancy.</s>
                    <s>Through extensive experiments, we find deep-narrow architecture design works best for ViT.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="2.">Related Work</head>
                <p>
                    <s>Transformers in Vision Transformers
                        <ref type="bibr" target="#b37">[37]</ref>
                        are the models that entirely rely on the self-attention mechanism to draw global dependencies
                        between input and output, and currently they have dominated natural language modelling
                        <ref type="bibr" target="#b10">[10,</ref>
                        <ref type="bibr" target="#b30">30,</ref>
                        <ref type="bibr" target="#b2">2,</ref>
                        <ref type="bibr" target="#b46">46,</ref>
                        <ref type="bibr" target="#b29">29,</ref>
                        <ref type="bibr" target="#b23">23]</ref>.
                    </s>
                    <s>A transformer layer usually consists of a multi-head self-attention layer (MSA) and an MLP block.</s>
                    <s>Layernorm (LN) is applied before each layer and residual connections in both the self-attention layer and MLP block.</s>
                    <s>Recent works have explored applying transformers to various vision tasks: image classification
                        <ref type="bibr" target="#b5">[5,</ref>
                        <ref type="bibr" target="#b12">12]</ref>, object detection
                        <ref type="bibr" target="#b3">[3,</ref>
                        <ref type="bibr" target="#b61">61,</ref>
                        <ref type="bibr" target="#b58">58,</ref>
                        <ref type="bibr" target="#b8">8,</ref>
                        <ref type="bibr" target="#b34">34]</ref>, segmentation
                        <ref type="bibr" target="#b4">[4,</ref>
                        <ref type="bibr" target="#b40">40]</ref>, image enhancement
                        <ref type="bibr" target="#b4">[4,</ref>
                        <ref type="bibr" target="#b45">45]</ref>, image generation
                        <ref type="bibr" target="#b27">[27]</ref>, video processing
                        <ref type="bibr" target="#b60">[60,</ref>
                        <ref type="bibr" target="#b53">53]</ref>, and 3D point cloud processing
                        <ref type="bibr" target="#b56">[56]</ref>.
                    </s>
                    <s>Among them, the Vision Transformer (ViT) proves that a pure Transformer architecture can also attain state-of-the-art performance on image classification.</s>
                    <s>However, ViT heavily relies on large-scale datasets such as ImageNet-21k and JFT-300M (which is not publically available) for model pretraining, requiring huge computation resources.</s>
                    <s>In contrast, our proposed T2T-ViT is more efficient and can be trained on ImageNet without using those largescale datasets.</s>
                    <s>A recent concurrent work DeiT
                        <ref type="bibr" target="#b36">[36]</ref>
                        applies Knowledge Distillation
                        <ref type="bibr" target="#b16">[16,</ref>
                        <ref type="bibr" target="#b49">49]</ref>
                        to improve the original ViT by adding a KD token along with the class token, which is orthogonal
                        to our work, as our T2T-ViT focuses on the architecture design, and our T2T-ViT can achieve
                        higher performance than DeiT without CNN as teacher model.
                    </s>
                </p>
                <p>
                    <s>Self-attention in CNNs Self-attention mechanism has been widely applied to CNNs in vision task
                        <ref type="bibr" target="#b38">[38,</ref>
                        <ref type="bibr" target="#b57">57,</ref>
                        <ref type="bibr" target="#b19">19,</ref>
                        <ref type="bibr" target="#b47">47,</ref>
                        <ref type="bibr" target="#b20">20,</ref>
                        <ref type="bibr" target="#b39">39,</ref>
                        <ref type="bibr" target="#b1">1,</ref>
                        <ref type="bibr" target="#b6">6,</ref>
                        <ref type="bibr" target="#b18">18,</ref>
                        <ref type="bibr" target="#b31">31,</ref>
                        <ref type="bibr" target="#b42">42,</ref>
                        <ref type="bibr" target="#b13">13,</ref>
                        <ref type="bibr" target="#b50">50,</ref>
                        <ref type="bibr" target="#b48">48]</ref>.
                    </s>
                    <s>Among these works, the SE block
                        <ref type="bibr" target="#b20">[20]</ref>
                        applies attention to channel dimensions and non-local networks
                        <ref type="bibr" target="#b39">[39]</ref>
                        are designed for capturing long-range dependencies via global attention.
                    </s>
                    <s>Compared with most of the works exploring global attention on images
                        <ref type="bibr" target="#b1">[1,</ref>
                        <ref type="bibr" target="#b42">42,</ref>
                        <ref type="bibr" target="#b13">13,</ref>
                        <ref type="bibr" target="#b39">39]</ref>, some works
                        <ref type="bibr" target="#b18">[18,</ref>
                        <ref type="bibr" target="#b31">31]</ref>
                        also explore self-attention in a local patch to reduce the memory and computation cost.
                    </s>
                    <s>More recently, SAN
                        <ref type="bibr" target="#b55">[55]</ref>
                        investigates both pairwise and patchwise self-attention for image recognition, where the
                        patchwise self-attention is a generalization of convolution.
                    </s>
                    <s>In this work, we also replace the T2T module with multiple convolution layers in experiments and find the convolution layers do not perform better than our designed T2T module.</s>
                    <s>then Ii is split with overlapping to tokens Ti+1 again.</s>
                    <s>Specifically, as shown in the pink panel, the four tokens (1,2,4,5) of the input Ii are concatenated to form one token in Ti+1.</s>
                    <s>The T2T transformer can be a normal Transformer layer
                        <ref type="bibr" target="#b37">[37]</ref>
                        or other efficient transformers like Performer layer
                        <ref type="bibr" target="#b34">[34]</ref>
                        at limited GPU memory.
                    </s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="3.">Tokens-to-Token ViT</head>
                <p>
                    <s>To overcome the limitations of simple tokenization and inefficient backbone of ViT, we propose Tokens-to-Token Vision Transformer (T2T-ViT) which can progressively tokenize the image to tokens and has an efficient backbone.</s>
                    <s>Hence, T2T-ViT consists of two main components (Fig.
                        <ref type="figure" target="#fig_2">4</ref>): 1) a layer-wise "Tokens-to-Token module" (T2T
                        module) to model the local structure information of the image and reduce the length of tokens
                        progressively; 2) an efficient "T2T-ViT backbone" to draw the global attention relation on
                        tokens from the T2T module.
                    </s>
                    <s>We adopt a deep-narrow structure for the backbone to reduce redundancy and improve the feature richness after exploring several CNNbased architecture designs.</s>
                    <s>We now explain these components one by one.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="3.1.">Tokens-to-Token: Progressive Tokenization</head>
                <p>
                    <s>The Token-to-Token (T2T) module aims to overcome the limitation of simple tokenization in ViT.</s>
                    <s>It progressively structurizes an image to tokens and models the local structure information, and in this way the length of tokens can be reduced iteratively.</s>
                    <s>Each T2T process has two steps: Restructurization and Soft Split (SS) (Fig.
                        <ref type="figure" target="#fig_1">3</ref>).
                    </s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head>Re-structurization</head>
                <p>
                    <s>As shown in Fig.
                        <ref type="figure" target="#fig_1">3</ref>, given a sequence of tokens T from the preceding
                        transformer layer, it will be transformed by the self-attention block (the T2T transformer in
                        Fig.
                        <ref type="figure" target="#fig_1">3</ref>):
                    </s>
                </p>
                <formula xml:id="formula_0">T = MLP(MSA(T )),
                    <label>(1)</label>
                </formula>
                <p>
                    <s>where MSA denotes the multihead self-attention operation with layer normalization and "MLP" is the multilayer per-ceptron with layer normalization in the standard Transformer
                        <ref type="bibr" target="#b12">[12]</ref>.
                    </s>
                    <s>Then the tokens T will be reshaped as an image in the spatial dimension,</s>
                </p>
                <formula xml:id="formula_1">I = Reshape(T ).
                    <label>(2)</label>
                </formula>
                <p>
                    <s>Here "Reshape" re-organizes tokens T ∈ R l×c to I ∈ R h×w×c , where l is the length of T , h, w, c are height, width and channel respectively, and l = h × w.</s>
                </p>
                <p>
                    <s>Soft Split As shown in Fig.
                        <ref type="figure" target="#fig_1">3</ref>, after obtaining the restructurized image I, we apply
                        the soft split on it to model local structure information and reduce length of tokens.
                    </s>
                    <s>Specifically, to avoid information loss in generating tokens from the re-structurizated image, we split it into patches with overlapping.</s>
                    <s>As such, each patch is correlated with surrounding patches to establish a prior that there should be stronger correlations between surrounding tokens.</s>
                    <s>The tokens in each split patch are concatenated as one token (Tokens-to-Token, Fig.
                        <ref type="figure" target="#fig_1">3</ref>), and thus the local information can be aggregated
                        from surrounding pixels and patches.
                    </s>
                    <s>When conducting the soft split, the size of each patch is k ×k with s overlapping and p padding on the image, where k -s is similar to the stride in convolution operation.</s>
                    <s>So for the reconstructed image I ∈ R h×w×c , the length of output tokens T o after soft split is</s>
                </p>
                <formula xml:id="formula_2">l o = h + 2p -k k -s + 1 × w + 2p -k k -s + 1 .
                    <label>(3)</label>
                </formula>
                <p>
                    <s>Each split patch has size k × k × c.</s>
                    <s>We flatten all patches in spatial dimensions to tokens T o ∈ R lo×ck 2 .</s>
                    <s>After the soft split, the output tokens are fed for the next T2T process.</s>
                </p>
                <p>
                    <s>T2T module By conducting the above Re-structurization and Soft Split iteratively, the T2T module can progressively reduce the length of tokens and transform the spatial structure of the image.</s>
                    <s>The iterative process in T2T module can be formulated as</s>
                </p>
                <formula xml:id="formula_3">T i = MLP(MSA(T i ), I i = Reshape(T i ), T i+1 = SS(I i ), i = 1...(n -1).
                    <label>(4)</label>
                </formula>
                <p>
                    <s>For the input image I 0 , we apply a soft split at first to split it to tokens: T 1 = SS(I 0 ).</s>
                    <s>After the final iteration, the output tokens T f of the T2T module has fixed length, so the backbone of T2T-ViT can model the global relation on T f .</s>
                    <s>Additionally, as the length of tokens in the T2T module is larger than the normal case
                        <ref type="bibr">(16 × 16)</ref>
                        in ViT, the MACs and memory usage are huge.
                    </s>
                    <s>To address the limitations, in our T2T module, we set the channel dimension of the T2T layer small (32 or 64) to reduce MACs, and optionally adopt an efficient Transformer such as Performer
                        <ref type="bibr" target="#b7">[7]</ref>
                        layer to reduce memory usage at limited GPU memory.
                    </s>
                    <s>We provide an ablation study on the difference between adopting standard Transformer layer and Performer layer in our experiments.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="3.2.">T2T-ViT Backbone</head>
                <p>
                    <s>As many channels in the backbone of vanilla ViT are invalid (Fig.
                        <ref type="figure">2</ref>), we plan to find an efficient backbone for our T2T-ViT to reduce the
                        redundancy and improve the feature richness.
                    </s>
                    <s>Thus we explore different architecture designs for ViT and borrow some designs from CNNs to improve the backbone efficiency and enhance the richness of the learned features.</s>
                    <s>As each transformer layer has skip connection as ResNets, a straightforward idea is to apply dense connection as DenseNet
                        <ref type="bibr" target="#b21">[21]</ref>
                        to increase the connectivity and feature richness, or apply Wide-ResNets or ResNeXt structure to
                        change the channel dimension and head number in the backbone of ViT.
                    </s>
                    <s>We explore five architecture designs from CNNs to ViT:</s>
                </p>
                <p>
                    <s>1. Dense connection as DenseNet
                        <ref type="bibr" target="#b21">[21]</ref>; 2. Deep-narrow vs. shallow-wide structure as in
                        Wide-ResNets
                        <ref type="bibr" target="#b52">[52]</ref>;
                    </s>
                </p>
                <p>
                    <s>3. Channel attention as Squeeze-an-Excitation (SE) Networks
                        <ref type="bibr" target="#b20">[20]</ref>;
                    </s>
                </p>
                <p>
                    <s>4. More split heads in multi-head attention layer as ResNeXt
                        <ref type="bibr" target="#b44">[44]</ref>;
                    </s>
                </p>
                <p>
                    <s>5. Ghost operations as GhostNet
                        <ref type="bibr" target="#b14">[14]</ref>.
                    </s>
                </p>
                <p>
                    <s>The details of these structure designs in ViT are given in the appendix.</s>
                    <s>We conduct extensive experiments on the structures transferring in Sec.</s>
                    <s>4.2.</s>
                    <s>We empirically find that 1) by adopting a deep-narrow structure that simply decreases channel dimensions to reduce the redundancy in channels and increase layer depth to improve feature richness in ViT, both the model size and MACs are decreased but performance is improved; 2) the channel attention as SE block also improves ViT but is less effective than using the deepnarrow structure.</s>
                </p>
                <p>
                    <s>Based on these findings, we design a deep-narrow architecture for our T2T-ViT backbone.</s>
                    <s>Specifically, it has a small channel number and a hidden dimension d but more layers b.</s>
                    <s>For tokens with fixed length T f from the last layer of T2T module, we concatenate a class token to it and then add Sinusoidal Position Embedding (PE) to it, the same as ViT to do classification:</s>
                </p>
                <formula xml:id="formula_4">T f0 = [t cls ; T f ] + E, E ∈ R (l+1)×d T fi = MLP(MSA(T fi-1 )), i = 1...b y = fc(LN(T f b ))
                    <label>(5)</label>
                </formula>
                <p>
                    <s>where E is Sinusoidal Position Embedding, LN is layer normalization, fc is one fully-connected layer for classification and y is the output prediction.</s>
                    <s>In the T2T module, the input image is first soft split as patches, and then unfolded as a sequence of tokens T0.</s>
                    <s>The length of tokens is reduced progressively in the T2T module (we use two iterations here and output T f ).</s>
                    <s>Then the T2T-ViT backbone takes the fixed tokens as input and outputs the predictions.</s>
                    <s>The two T2T blocks are the same as Fig.
                        <ref type="figure" target="#fig_1">3</ref>
                        and PE is Position Embedding.
                    </s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="3.3.">T2T-ViT Architecture</head>
                <p>
                    <s>The T2T-ViT has two parts: the Tokens-to-Token (T2T) module and the T2T-ViT backbone (Fig.
                        <ref type="figure" target="#fig_2">4</ref>).
                    </s>
                    <s>There are various possible design choices for the T2T module.</s>
                    <s>Here, we set n = 2 as shown in Fig.
                        <ref type="figure" target="#fig_2">4</ref>, which means there is n+1 = 3 soft split and n = 2
                        re-structurization in T2T module.
                    </s>
                    <s>The patch size for the three soft splits is P = [7, 3, 3], and the overlapping is S = [3, 1, 1], which reduces size of the input image from 224 × 224 to 14 × 14 according to Eqn.
                        <ref type="bibr" target="#b3">(3)</ref>.
                    </s>
                </p>
                <p>
                    <s>The T2T-ViT backbone takes tokens with fixed length from the T2T module as input, the same as ViT; but has a deep-narrow architecture design with smaller hidden dimensions (256-512) and MLP size (512-1536) than ViT.</s>
                    <s>For example, T2T-ViT-14 has 14 transformer layers in T2T-ViT backbone with 384 hidden dimensions, while ViT-B/16 has 12 transformer layers and 768 hidden dimensions, which is 3x larger than T2T-ViT-14 in parameters and MACs.</s>
                </p>
                <p>
                    <s>To fairly compare with common hand-designed CNNs, we make T2T-ViT models have comparable size with ResNets and MobileNets.</s>
                    <s>Specifically, we design three models: T2T-ViT-14, T2T-ViT-19 and T2T-ViT-24 of comparable parameters with ResNet50, ResNet101 and ResNet152 respectively.</s>
                    <s>To compare with small models like MobileNets, we design two lite models: T2T-ViT-7, T2T-ViT-12 with comparable model size with MibileNetV1 and MibileNetV2.</s>
                    <s>The two lite TiT-ViT have no special designs or tricks like efficient convolution
                        <ref type="bibr" target="#b26">[26]</ref>
                        and simply reduce the layer depth, hidden dimension, and MLP ratio.
                    </s>
                    <s>The network details are summarized in Tab. 1.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="4.">Experiments</head>
                <p>
                    <s>We conduct the following experiments with T2T-ViT for image classification on ImageNet.</s>
                    <s>a) We validate the T2T-ViT by training from scratch on ImageNet and compare it with some common convolutional neural networks such as ResNets and MobileNets of comparable size; we also transfer the pretrained T2T-ViT to downstream datasets such as CIFAR10 and CIFAR100 (Sec.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="4.1.">T2T-ViT on ImageNet</head>
                <p>
                    <s>All experiments are conducted on ImageNet dataset
                        <ref type="bibr" target="#b9">[9]</ref>, with around 1.3 million images in training set and 50k
                        images in validation set.
                    </s>
                    <s>We use batch size 512 or 1024 with 8 NVIDIA GPUs for training.</s>
                    <s>We adopt Pytorch
                        <ref type="bibr" target="#b28">[28]</ref>
                        library and Pytorch image models library (timm)
                        <ref type="bibr" target="#b41">[41]</ref>
                        to implement our models and conduct all experiments.
                    </s>
                    <s>For fair comparisons, we implement the same training scheme for the CNN models, ViT, and our T2T-ViT.</s>
                    <s>Throughout the experiments on ImageNet, we set default image size as 224 × 224 except for some specific cases on 384 × 384, and adopt some common data augmentation methods such as mixup
                        <ref type="bibr" target="#b54">[54]</ref>
                        and cutmix
                        <ref type="bibr" target="#b11">[11,</ref>
                        <ref type="bibr" target="#b51">51]</ref>
                        for both CNN and ViT&amp;T2T-ViT model training, because ViT models need more training data to
                        reach reasonable performance.
                    </s>
                    <s>We train these models for 310 epochs, using AdamW
                        <ref type="bibr" target="#b25">[25]</ref>
                        as the optimizer and cosine learning rate decay
                        <ref type="bibr" target="#b24">[24]</ref>.
                    </s>
                    <s>The details of experiment setting are given in appendix.</s>
                    <s>We also use both Transformer layer and Performer layer in T2T module for our models, resulting in T2T-ViT t -14/19/24 (Transformer) and T2T-ViT-14/19/24 (Performer).</s>
                    <s>T2T-ViT vs. ViT We first compare performance of T2T-ViT and ViT on ImageNet.</s>
                    <s>The results are given in Tab. 2. Our T2T-ViT is much smaller than ViT in number of parameters and MACs, yet giving higher performance.</s>
                    <s>For example, the small ViT model ViT-S/16 with 48.6M and 10.1G MACs has 78.1% top-1 accuracy when trained from scratch on ImageNet, while our T2T-ViT t -14 with only 44.2% parameters and 51.5% MACs achieves more than 3.0% improvement (81.5%).</s>
                    <s>If we compare T2T-ViT t -24 with ViT-L/16, the former reduces parameters and MACs around 500% but achieves more than 1.0% improvement on ImageNet.</s>
                    <s>Comparing T2T-ViT-14 with DeiT-small and DeiT-small-Distilled, our T2T-ViT can achieve higher accuracy without large CNN models as teacher to enhance ViT.</s>
                    <s>We also adopt higher image resolution as 384×384 and get 83.3% accuracy by our T2T-ViT-14↑384.</s>
                    <s>T2T-ViT vs. ResNet For fair comparisons, we set up three T2T-ViT models that have similar model size and MACs with ResNet50, ResNet101 and ResNet152.</s>
                    <s>The experimental results are given in Tab. 3. The proposed T2T-ViT achieves 1.4%-2.7%</s>
                    <s>performance gain over ResNets with similar model size and MACs.</s>
                    <s>For example, compared with ResNet50 of 25.5M parameters and 4.3G MACs, our T2T-ViT-14 have 21.5M parameters and 4.8G MACs obtain 81.5% accuracy on ImageNet.</s>
                    <s>T2T-ViT vs. MobileNets The T2T-ViT-7 and T2T-ViT-12 have similar model size with MobileNetV1
                        <ref type="bibr" target="#b17">[17]</ref>
                        and Mo- bileNetV2
                        <ref type="bibr" target="#b32">[32]</ref>, but achieve comparable or higher performance than
                        MobileNets (Tab.
                    </s>
                    <s>4).</s>
                    <s>For example, Our T2T-ViT-12 with 6.9M parameters achieves 76.5% top1 accuracy, which is higher than MobileNetsV2 1.4x by 0.9%.</s>
                    <s>But we also note the MACs of our T2T-ViT are still larger than MobileNets because of the dense operations in Transformers.</s>
                    <s>However, there are no special operations or tricks like efficient convolution
                        <ref type="bibr" target="#b26">[26,</ref>
                        <ref type="bibr" target="#b32">32]</ref>
                        in current T2T-ViT-7 and T2T-ViT-12, and we only reduce model size by reducing the hidden
                        dimension, MLP ratio and depth of layers, indicating T2T-ViT is also very promising as a lite
                        model.
                    </s>
                    <s>We also apply knowledge distillation on our T2T-ViT as the concurrent work DeiT
                        <ref type="bibr" target="#b36">[36]</ref>
                        and find that our T2T-ViT-7 and T2T-ViT-12 can be further improved by distillation.
                    </s>
                    <s>Overall, the experimental results show, our T2T-ViT can achieve superior performance when it has mid-size as ResNets and reasonable results when it has a small model size as MobileNets.</s>
                </p>
                <p>
                    <s>Transfer learning We transfer our pretrained T2T-ViT to downstream datasets such as CIFAR10 and CIFAR100.</s>
                    <s>We  finetune the pretrained T2T-ViT-14/19 with 60 epochs by using SGD optimizer and cosine learning rate decay.The results are given in Tab. 5. We find that our T2T-ViT can achieve higher performance than the original ViT with smaller model sizes on the downstream datasets.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="4.2.">From CNN to ViT</head>
                <p>
                    <s>To find an efficient backbone for vision transformers, we experimentally apply DenseNet structure, Wide-ResNet structure (wide or narrow channel dimensions), SE block (channel attention), ResNeXt structure (more heads in multihead attention), and Ghost operation from CNN to ViT.</s>
                    <s>The details of these architecture designs are given in the appendix.</s>
                    <s>From experimental results on "CNN to ViT" in Tab.</s>
                    <s>6, we can find both SE (ViT-SE) and Deep-Narrow structure (ViT-DN) benefit the ViT but the most effective structure is deep-narrow structure, which decreases model size and MACs nearly 2x and brings 0.9% improvement on the baseline model ViT-S/16.</s>
                </p>
                <p>
                    <s>We further apply these structures from CNN to our T2T-ViT, and conduct experiments on ImageNet under the same training scheme.</s>
                    <s>We take ResNet50 as the baseline for CNN, ViT-S/16 for ViT, and T2T-ViT-14 for T2T-ViT.</s>
                    <s>All experimental results are given in Tab.</s>
                    <s>6, and those on CNN and ViT&amp;T2T-ViT are marked with the same colors.</s>
                    <s>We summarize the effects of each CNN-based structure below.</s>
                    <s>Deep-narrow structure benefits ViT: The models ViT-DN (Deep-Narrow) and ViT-SW (Shallow-Wide) in Tab.</s>
                    <s>6 are two opposite designs in channel dimension and layer depth, where ViT-DN has 384 hidden dimensions and 16 layers and ViT-SW has 1,024 hidden dimensions and 4 layers.</s>
                    <s>Compared with the baseline model ViT-S/16 with 768 hidden dimensions and 8 layers, shallow-wide model ViT-SW has 8.2% decrease in performance while ViT-DN with only half of model size and MACs achieve 0.9% increase.</s>
                    <s>These results validate our hypothesis that vanilla ViT with shallow-wide structure is redundant in channel dimensions and limited feature richness with shallow layers.</s>
                </p>
                <p>
                    <s>Dense connection hurts performance of both ViT and T2T-ViT: Compared with the ResNet50, DenseNet201 has smaller parameters and comparable MACs, while it has higher performance.</s>
                    <s>However, the dense connection can hurt performance of ViT-Dense and T2T-ViT-Dense (dark blue rows in Tab. 6).</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head>SE block improves both</head>
                <p>
                    <s>ViT and T2T-ViT: From red rows in Tab.</s>
                    <s>6, we can find SENets, ViT-SE and T2T-ViT-SE are higher than the corresponding baseline.</s>
                    <s>The SE module can improve performance on both CNN and ViT, which means applying attention to channels benefits both CNN and ViT models.</s>
                </p>
                <p>
                    <s>ResNeXt structure has few effects on ViT and T2T-ViT: ResNeXts adopt multi-head on ResNets, while Transformers are also multi-head attention structure.</s>
                    <s>When we adopt more heads like 32, we can find it has few effects on performance (red rows in Tab 6).</s>
                    <s>However, adopting a large number of heads makes the GPU memory large, which is thus unnecessary in ViT and T2T-ViT.</s>
                </p>
                <p>
                    <s>Ghost can further compress model and reduce MACs of T2T-ViT: Comparing experimental results of Ghost operation (magenta row in Tab.</s>
                    <s>6), the accuracy decreases 2.9% on ResNet50, 2.0% on T2T-ViT, and 4.4% on ViT.</s>
                    <s>So the Ghost operation can further reduce the parameters and MACs of T2T-ViT with smaller performance degradation than ResNet.</s>
                    <s>But for the original ViT, it would cause more decrease than ResNet.</s>
                </p>
                <p>
                    <s>Besides, for all five structures, the T2T-ViT performs better than ViT, which further validates the superiority of our proposed T2T-ViT.</s>
                    <s>And we also wish this study of transferring CNN structure to ViT can motivate the network design of Transformers in vision tasks.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="4.3.">Ablation study</head>
                <p>
                    <s>To further identify effects of T2T module and deepnarrow structure, we do ablation study on our T2T-ViT.</s>
                </p>
                <p>
                    <s>T2T module To verify the effects of the proposed T2T module, we experimentally compare three different models: T2T-ViT-14, T2T-ViT-14 wo T 2T , and T2T-ViT t -14, where T2T-ViT-14 wo T 2T has the same T2T-ViT backbone but without T2T module.</s>
                    <s>We can find with similar model size and MACs, the T2T module can improve model performance by 2.0%-2.2% on ImageNet.</s>
                </p>
                <p>
                    <s>As the soft split in T2T module is similar to convolution operation without convolution filters, we also replace the T2T module by 3 convolution layers with kernel size
                        <ref type="bibr" target="#b7">(7,</ref>
                        <ref type="bibr" target="#b3">3,</ref>
                        <ref type="bibr" target="#b3">3)</ref>, stride size (4,2,2) respectively.
                    </s>
                    <s>Such a model with convolution layers to build T2T module is denoted as T2T-ViT c -14.</s>
                    <s>From Tab. 7, we can find the T2T-ViT c -14 is worse than T2T-ViT-14 and T2T-ViT t -14 by 0.5%-1.0%</s>
                    <s>on ImageNet.</s>
                    <s>We also note that the T2T-ViT c -14 is still higher than T2T-ViT-14 wo T 2T , as the convolution layers in the early stage can also model the structure information.</s>
                    <s>But our designed T2T module is better than the convolution layers as it can model both the global relation and the structure information of the images.</s>
                </p>
                <p>
                    <s>Deep-narrow structure We use the deep-narrow structure with fewer hidden dimensions but more layers, rather than the shallow-wide one in the original ViT.</s>
                    <s>We compare the T2T-ViT-14 and T2T-ViT-d768-4 to verify its ef-fects.</s>
                    <s>T2T-ViT-d768-4 is a shallow-wide structure with hidden dimension of 768 and 4 layers, with similar model size and MACs as T2T-ViT-14.</s>
                    <s>From Tab. 7, we can find after changing our deep-narrow to shallow-wide structure, the T2T-ViT-d768-4 has 2.7% decrease in top-1 accuracy, validating deep-narrow structure is crucial for T2T-ViT.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="5.">Conclusion</head>
                <p>
                    <s>In this work, we propose a new T2T-ViT model that can be trained from scratch on ImageNet and achieve comparable or even better performance than CNNs.</s>
                    <s>T2T-ViT effectively models the structure information of images and enhances feature richness, overcoming limitations of ViT.</s>
                    <s>It introduces the novel tokens-to-token (T2T) process to progressively tokenize images to tokens and structurally aggregate tokens.</s>
                    <s>We also explore various architecture design choices from CNNs for improving T2T-ViT performance, and empirically find the deep-narrow architecture performs better than the shallow-wide structure.</s>
                    <s>Our T2T-ViT achieves superior performance to ResNets and comparable performance to MobileNets with similar model size when trained from scratch on ImageNet.</s>
                    <s>It paves the way for further developing transformer-based models for vision tasks.</s>
                </p>
            </div>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0">
                <head>Figure 1 .</head>
                <label>1</label>
                <figDesc>
                    <div>
                        <p>
                            <s>Figure 1.</s>
                            <s>Comparison between T2T-ViT with ViT, ResNets and MobileNets when trained from scratch on ImageNet.</s>
                            <s>Left: performance curve of MACs vs. top-1 accuracy.</s>
                            <s>Right: performance curve of model size vs. top-1 accuracy.</s>
                        </p>
                    </div>
                </figDesc>
                <graphic coords="1,429.45,229.25,126.00,109.80" type="bitmap"/>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1">
                <head>Figure 3 .</head>
                <label>3</label>
                <figDesc>
                    <div>
                        <p>
                            <s>Figure 3. Illustration of T2T process.</s>
                            <s>The tokens Ti are restructurized as an image Ii after transformation and reshaping;then Ii is split with overlapping to tokens Ti+1 again.</s>
                            <s>Specifically, as shown in the pink panel, the four tokens (1,2,4,5) of the input Ii are concatenated to form one token in Ti+1.</s>
                            <s>The T2T transformer can be a normal Transformer layer
                                <ref type="bibr" target="#b37">[37]</ref>
                                or other efficient transformers like Performer layer
                                <ref type="bibr" target="#b34">[34]</ref>
                                at limited GPU memory.
                            </s>
                        </p>
                    </div>
                </figDesc>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2">
                <head>Figure 4 .</head>
                <label>4</label>
                <figDesc>
                    <div>
                        <p>
                            <s>Figure 4.</s>
                            <s>The overall network architecture of T2T-ViT.</s>
                            <s>In the T2T module, the input image is first soft split as patches, and then unfolded as a sequence of tokens T0.</s>
                            <s>The length of tokens is reduced progressively in the T2T module (we use two iterations here and output T f ).</s>
                            <s>Then the T2T-ViT backbone takes the fixed tokens as input and outputs the predictions.</s>
                            <s>The two T2T blocks are the same as Fig.3and PE is Position Embedding.</s>
                        </p>
                    </div>
                </figDesc>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3">
                <head/>
                <label/>
                <figDesc>
                    <div>
                        <p>
                            <s>4.1).</s>
                            <s>(b) We compare five T2T-ViT backbone architecture designs inspired from CNNs (Sec.</s>
                            <s>4.2).</s>
                            <s>(c) We conduct ablation study to demonstrate effects of the T2T module and the deep-narrow architecture design of T2T-ViT (Sec.</s>
                            <s>4.3).</s>
                        </p>
                    </div>
                </figDesc>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0">
                <head>Table 1 .</head>
                <label>1</label>
                <figDesc>
                    <div>
                        <p>
                            <s>[12]cture details of T2T-ViT.T2T-ViT-14/19/24 have comparable model size with ResNet50/101/152.T2T-ViT-7/12 have comparable model size with MobileNetV1/V2.For T2T transformer layer, we adopt Transformer layer for T2T-ViTt-14 and Performer layer for T2T-ViT-14 at limited GPU memory.For ViT, 'S' means Small, 'B' is Base and 'L' is Large.'ViT-S/16' is a variant from original ViT-B/16[12]with smaller MLP size and layer depth.</s>
                        </p>
                    </div>
                </figDesc>
                <table>
                    <row>
                        <cell/>
                        <cell cols="3">Tokens-to-Token module</cell>
                        <cell/>
                        <cell cols="3">T2T-ViT backbone</cell>
                        <cell cols="2">Model size</cell>
                    </row>
                    <row>
                        <cell>Models</cell>
                        <cell>T2T transformer</cell>
                        <cell>Depth</cell>
                        <cell>Hidden dim</cell>
                        <cell>MLP size</cell>
                        <cell>Depth</cell>
                        <cell>Hidden dim</cell>
                        <cell>MLP size</cell>
                        <cell>Params (M)</cell>
                        <cell>MACs (G)</cell>
                    </row>
                    <row>
                        <cell>ViT-S/16 [12]</cell>
                        <cell>-</cell>
                        <cell>-</cell>
                        <cell>-</cell>
                        <cell>-</cell>
                        <cell>8</cell>
                        <cell>786</cell>
                        <cell>2358</cell>
                        <cell>48.6</cell>
                        <cell>10.1</cell>
                    </row>
                    <row>
                        <cell>ViT-B/16 [12]</cell>
                        <cell>-</cell>
                        <cell>-</cell>
                        <cell>-</cell>
                        <cell>-</cell>
                        <cell>12</cell>
                        <cell>786</cell>
                        <cell>3072</cell>
                        <cell>86.8</cell>
                        <cell>17.6</cell>
                    </row>
                    <row>
                        <cell>ViT-L/16 [12]</cell>
                        <cell>-</cell>
                        <cell>-</cell>
                        <cell>-</cell>
                        <cell>-</cell>
                        <cell>24</cell>
                        <cell>1024</cell>
                        <cell>4096</cell>
                        <cell>304.3</cell>
                        <cell>63.6</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-14</cell>
                        <cell>Performer</cell>
                        <cell>2</cell>
                        <cell>64</cell>
                        <cell>64</cell>
                        <cell>14</cell>
                        <cell>384</cell>
                        <cell>1152</cell>
                        <cell>21.5</cell>
                        <cell>4.8</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-19</cell>
                        <cell>Performer</cell>
                        <cell>2</cell>
                        <cell>64</cell>
                        <cell>64</cell>
                        <cell>19</cell>
                        <cell>448</cell>
                        <cell>1344</cell>
                        <cell>39.2</cell>
                        <cell>8.5</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-24</cell>
                        <cell>Performer</cell>
                        <cell>2</cell>
                        <cell>64</cell>
                        <cell>64</cell>
                        <cell>24</cell>
                        <cell>512</cell>
                        <cell>1536</cell>
                        <cell>64.1</cell>
                        <cell>13.8</cell>
                    </row>
                    <row>
                        <cell cols="2">T2T-ViTt-14 Transformer</cell>
                        <cell>2</cell>
                        <cell>64</cell>
                        <cell>64</cell>
                        <cell>14</cell>
                        <cell>384</cell>
                        <cell>1152</cell>
                        <cell>21.5</cell>
                        <cell>6.1</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-7</cell>
                        <cell>Performer</cell>
                        <cell>2</cell>
                        <cell>64</cell>
                        <cell>64</cell>
                        <cell>8</cell>
                        <cell>256</cell>
                        <cell>512</cell>
                        <cell>4.2</cell>
                        <cell>1.1</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-12</cell>
                        <cell>Performer</cell>
                        <cell>2</cell>
                        <cell>64</cell>
                        <cell>64</cell>
                        <cell>12</cell>
                        <cell>256</cell>
                        <cell>512</cell>
                        <cell>6.8</cell>
                        <cell>1.8</cell>
                    </row>
                </table>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1">
                <head>Table 2 .</head>
                <label>2</label>
                <figDesc>
                    <div>
                        <p>
                            <s>Comparison between T2T-ViT and ViT by training from scratch on ImageNet.</s>
                        </p>
                    </div>
                </figDesc>
                <table>
                    <row>
                        <cell>Models</cell>
                        <cell>Top1-Acc (%)</cell>
                        <cell>Params (M)</cell>
                        <cell>MACs (G)</cell>
                    </row>
                    <row>
                        <cell>ViT-S/16 [12]</cell>
                        <cell>78.1</cell>
                        <cell>48.6</cell>
                        <cell>10.1</cell>
                    </row>
                    <row>
                        <cell>DeiT-small [36]</cell>
                        <cell>79.9</cell>
                        <cell>22.1</cell>
                        <cell>4.6</cell>
                    </row>
                    <row>
                        <cell>DeiT-small-Distilled [36]</cell>
                        <cell>81.2</cell>
                        <cell>22.1</cell>
                        <cell>4.7</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-14</cell>
                        <cell>81.5</cell>
                        <cell>21.5</cell>
                        <cell>4.8</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-14↑384</cell>
                        <cell>83.3</cell>
                        <cell>21.5</cell>
                        <cell>17.1</cell>
                    </row>
                    <row>
                        <cell>ViT-B/16 [12]</cell>
                        <cell>79.8</cell>
                        <cell>86.4</cell>
                        <cell>17.6</cell>
                    </row>
                    <row>
                        <cell>ViT-L/16 [12]</cell>
                        <cell>81.1</cell>
                        <cell>304.3</cell>
                        <cell>63.6</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-24</cell>
                        <cell>82.3</cell>
                        <cell>64.1</cell>
                        <cell>13.8</cell>
                    </row>
                </table>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2">
                <head>Table 3 .</head>
                <label>3</label>
                <figDesc>
                    <div>
                        <p>
                            <s>Comparison between our T2T-ViT with ResNets on Im-ageNet.</s>
                            <s>T2T-ViTt-14: using Transformer in T2T module.</s>
                            <s>T2T-ViT-14: using Performer in T2T module.</s>
                            <s>* means we train the model with our training scheme for fair comparisons.</s>
                        </p>
                    </div>
                </figDesc>
                <table>
                    <row>
                        <cell>Models</cell>
                        <cell>Top1-Acc (%)</cell>
                        <cell>Params (M)</cell>
                        <cell>MACs (G)</cell>
                    </row>
                    <row>
                        <cell>ResNet50 [15]</cell>
                        <cell>76.2</cell>
                        <cell>25.5</cell>
                        <cell>4.3</cell>
                    </row>
                    <row>
                        <cell>ResNet50*</cell>
                        <cell>79.1</cell>
                        <cell>25.5</cell>
                        <cell>4.3</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-14</cell>
                        <cell>81.5</cell>
                        <cell>21.5</cell>
                        <cell>4.8</cell>
                    </row>
                    <row>
                        <cell>T2T-ViTt-14</cell>
                        <cell>81.7</cell>
                        <cell>21.5</cell>
                        <cell>6.1</cell>
                    </row>
                    <row>
                        <cell>ResNet101 [15]</cell>
                        <cell>77.4</cell>
                        <cell>44.6</cell>
                        <cell>7.9</cell>
                    </row>
                    <row>
                        <cell>ResNet101*</cell>
                        <cell>79.9</cell>
                        <cell>44.6</cell>
                        <cell>7.9</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-19</cell>
                        <cell>81.9</cell>
                        <cell>39.2</cell>
                        <cell>8.5</cell>
                    </row>
                    <row>
                        <cell>T2T-ViTt-19</cell>
                        <cell>82.2</cell>
                        <cell>39.2</cell>
                        <cell>9.8</cell>
                    </row>
                    <row>
                        <cell>ResNet152 [15]</cell>
                        <cell>78.3</cell>
                        <cell>60.2</cell>
                        <cell>11.6</cell>
                    </row>
                    <row>
                        <cell>ResNet152*</cell>
                        <cell>80.8</cell>
                        <cell>60.2</cell>
                        <cell>11.6</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-24</cell>
                        <cell>82.3</cell>
                        <cell>64.1</cell>
                        <cell>13.8</cell>
                    </row>
                    <row>
                        <cell>T2T-ViTt-24</cell>
                        <cell>82.6</cell>
                        <cell>64.1</cell>
                        <cell>15.0</cell>
                    </row>
                </table>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3">
                <head>Table 4 .</head>
                <label>4</label>
                <figDesc>
                    <div>
                        <p>
                            <s>[36]arison between our lite T2T-ViT with MobileNets.Models with '-Distilled' are taught by teacher model with the method as DeiT[36].</s>
                        </p>
                    </div>
                </figDesc>
                <table>
                    <row>
                        <cell>Models</cell>
                        <cell>Top1-Acc (%)</cell>
                        <cell>Params (M)</cell>
                        <cell>MACs (G)</cell>
                    </row>
                    <row>
                        <cell>MobileNetV1 1.0x*</cell>
                        <cell>70.8</cell>
                        <cell>4.2</cell>
                        <cell>0.6</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-7</cell>
                        <cell>71.7</cell>
                        <cell>4.3</cell>
                        <cell>1.1</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-7-Distilled</cell>
                        <cell>73.1</cell>
                        <cell>4.3</cell>
                        <cell>1.1</cell>
                    </row>
                    <row>
                        <cell>MobileNetV2 1.0x*</cell>
                        <cell>72.8</cell>
                        <cell>3.5</cell>
                        <cell>0.3</cell>
                    </row>
                    <row>
                        <cell>MobileNetV2 1.4x*</cell>
                        <cell>75.6</cell>
                        <cell>6.9</cell>
                        <cell>0.6</cell>
                    </row>
                    <row>
                        <cell>MobileNetV3 (Searched)</cell>
                        <cell>75.2</cell>
                        <cell>5.4</cell>
                        <cell>0.2</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-12</cell>
                        <cell>76.5</cell>
                        <cell>6.9</cell>
                        <cell>1.8</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-12-Distilled</cell>
                        <cell>77.4</cell>
                        <cell>6.9</cell>
                        <cell>1.9</cell>
                    </row>
                </table>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4">
                <head>Table 5 .</head>
                <label>5</label>
                <figDesc>
                    <div>
                        <p>
                            <s>The results of fine-tuning the pretrained T2T-ViT to downstream datasets: CIFAR10 and CIFAR100.</s>
                        </p>
                    </div>
                </figDesc>
                <table>
                    <row>
                        <cell>Models</cell>
                        <cell cols="4">Params (M) ImageNet CIFAR10 CIFAR100</cell>
                    </row>
                    <row>
                        <cell>ViT/S-16</cell>
                        <cell>48.6</cell>
                        <cell>78.1</cell>
                        <cell>97.1</cell>
                        <cell>87.1</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-14</cell>
                        <cell>21.5</cell>
                        <cell>81.5</cell>
                        <cell>97.5</cell>
                        <cell>88.4</cell>
                    </row>
                    <row>
                        <cell>T2T-ViT-19</cell>
                        <cell>39.1</cell>
                        <cell>81.9</cell>
                        <cell>98.3</cell>
                        <cell>89.0</cell>
                    </row>
                </table>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5">
                <head>Table 6 .</head>
                <label>6</label>
                <figDesc>
                    <div>
                        <p>
                            <s>Transfer of some common designs in CNN to ViT&amp;T2T-ViT, including DenseNet, Wide-ResNet, SE module, ResNeXt, Ghost operation.</s>
                            <s>The same color means the correspond transfer.</s>
                            <s>All models are trained from scratch on ImageNet.</s>
                            <s>* means we reproduce the model with our training scheme for fair comparisons.</s>
                        </p>
                    </div>
                </figDesc>
                <table>
                    <row>
                        <cell>Model Type</cell>
                        <cell>Models</cell>
                        <cell>Top1-Acc (%)</cell>
                        <cell>Params (M)</cell>
                        <cell>MACs (G)</cell>
                        <cell>Depth</cell>
                        <cell>Hidden dim</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>AlexNet [22]</cell>
                        <cell>56.6</cell>
                        <cell>61.1</cell>
                        <cell>0.77</cell>
                        <cell>-</cell>
                        <cell>-</cell>
                    </row>
                    <row>
                        <cell>Traditional CNN</cell>
                        <cell>VGG11 [33]</cell>
                        <cell>69.1</cell>
                        <cell>132.8</cell>
                        <cell>7.7</cell>
                        <cell>11</cell>
                        <cell>-</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>Inception v3 [35]</cell>
                        <cell>77.4</cell>
                        <cell>27.2</cell>
                        <cell>5.7</cell>
                        <cell>-</cell>
                        <cell>-</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>ResNet50 [15]</cell>
                        <cell>76.2</cell>
                        <cell>25.6</cell>
                        <cell>4.3</cell>
                        <cell>50</cell>
                        <cell>-</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>ResNet50* (Baseline)</cell>
                        <cell>79.1</cell>
                        <cell>25.6</cell>
                        <cell>4.3</cell>
                        <cell>50</cell>
                        <cell>-</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>Wide-ResNet18x1.5*</cell>
                        <cell>78.0 (-1.1)</cell>
                        <cell>26.0</cell>
                        <cell>4.1</cell>
                        <cell>18</cell>
                        <cell>-</cell>
                    </row>
                    <row>
                        <cell>Skip-connection CNN</cell>
                        <cell>DenseNet201*</cell>
                        <cell>77.5 (-1.6)</cell>
                        <cell>20.1</cell>
                        <cell>4.4</cell>
                        <cell>201</cell>
                        <cell>-</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>SENet50*</cell>
                        <cell>80.3 (+1.2)</cell>
                        <cell>28.1</cell>
                        <cell>4.9</cell>
                        <cell>50</cell>
                        <cell>-</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>ResNeXt50*</cell>
                        <cell>79.9 (+0.8)</cell>
                        <cell>25.0</cell>
                        <cell>4.3</cell>
                        <cell>50</cell>
                        <cell>-</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>ResNet50-Ghost*</cell>
                        <cell>76.2 (-2.9)</cell>
                        <cell>19.9</cell>
                        <cell>3.2</cell>
                        <cell>50</cell>
                        <cell>-</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>ViT-S/16 (Baseline)</cell>
                        <cell>78.1</cell>
                        <cell>48.6</cell>
                        <cell>10.1</cell>
                        <cell>8</cell>
                        <cell>768</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>ViT-DN</cell>
                        <cell>79.0 (+0.9)</cell>
                        <cell>24.5</cell>
                        <cell>5.5</cell>
                        <cell>16</cell>
                        <cell>384</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>ViT-SW</cell>
                        <cell>69.9 (-8.2)</cell>
                        <cell>47.9</cell>
                        <cell>9.9</cell>
                        <cell>4</cell>
                        <cell>1024</cell>
                    </row>
                    <row>
                        <cell>CNN to ViT</cell>
                        <cell>ViT-Dense</cell>
                        <cell>76.8 (-1.3)</cell>
                        <cell>46.7</cell>
                        <cell>9.7</cell>
                        <cell>19</cell>
                        <cell>128-736</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>ViT-SE</cell>
                        <cell>78.4 (+0.3)</cell>
                        <cell>49.2</cell>
                        <cell>10.2</cell>
                        <cell>8</cell>
                        <cell>768</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>ViT-ResNeXt</cell>
                        <cell>78.0 (-0.1)</cell>
                        <cell>48.6</cell>
                        <cell>10.1</cell>
                        <cell>8</cell>
                        <cell>768</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>ViT-Ghost</cell>
                        <cell>73.7 (-4.4)</cell>
                        <cell>32.1</cell>
                        <cell>6.9</cell>
                        <cell>8</cell>
                        <cell>768</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>T2T-ViT-14 (Baseline)</cell>
                        <cell>81.5</cell>
                        <cell>21.5</cell>
                        <cell>4.8</cell>
                        <cell>14</cell>
                        <cell>384</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>T2T-ViT-Wide</cell>
                        <cell>77.9 (-3.4)</cell>
                        <cell>25.1</cell>
                        <cell>5.0</cell>
                        <cell>14</cell>
                        <cell>768</cell>
                    </row>
                    <row>
                        <cell>CNN to T2T-ViT</cell>
                        <cell>T2T-ViT-Dense T2T-ViT-SE</cell>
                        <cell>80.6 (-1.1) 81.6 (+0.1)</cell>
                        <cell>23.7 21.9</cell>
                        <cell>5.5 4.9</cell>
                        <cell>19 14</cell>
                        <cell>128-584 384</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>T2T-ViT-ResNeXt</cell>
                        <cell>81.5 (+0.0)</cell>
                        <cell>21.5</cell>
                        <cell>4.8</cell>
                        <cell>14</cell>
                        <cell>384</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>T2T-ViT-Ghost</cell>
                        <cell>79.5 (-2.0)</cell>
                        <cell>16.3</cell>
                        <cell>3.7</cell>
                        <cell>14</cell>
                        <cell>384</cell>
                    </row>
                </table>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6">
                <head>Table 7 .</head>
                <label>7</label>
                <figDesc>
                    <div>
                        <p>
                            <s>Ablation study results on T2T module, Deep-Narrow(DN) structure.</s>
                        </p>
                    </div>
                </figDesc>
                <table>
                    <row>
                        <cell>Ablation type</cell>
                        <cell>Models</cell>
                        <cell>Top1-Acc</cell>
                        <cell>Params</cell>
                        <cell>MACs</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell/>
                        <cell>(%)</cell>
                        <cell>(M)</cell>
                        <cell>(G)</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>T2T-ViT-14 wo T 2T</cell>
                        <cell>79.5</cell>
                        <cell>21.1</cell>
                        <cell>4.2</cell>
                    </row>
                    <row>
                        <cell>T2T module</cell>
                        <cell>T2T-ViT-14 T2T-ViTt-14</cell>
                        <cell>81.5 (+2.0) 81.7 (+2.2)</cell>
                        <cell>21.5 21.5</cell>
                        <cell>4.8 6.1</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>T2T-ViTc-14</cell>
                        <cell>80.8 (+1.3)</cell>
                        <cell>21.3</cell>
                        <cell>4.6</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>T2T-ViT-14</cell>
                        <cell>81.5</cell>
                        <cell>21.5</cell>
                        <cell>4.8</cell>
                    </row>
                    <row>
                        <cell>DN Structure</cell>
                        <cell>T2T-ViT-d768-4</cell>
                        <cell>78.8 (-2.7)</cell>
                        <cell>25.0</cell>
                        <cell>5.4</cell>
                    </row>
                </table>
            </figure>
        </body>
        <back>
            <div type="funding">
                <div>
                    <p>* Work done during an internship at
                        <rs type="person">Yitu Tech</rs>.
                    </p>
                </div>
            </div>
            <div type="references">
                <listBibl>
                    <biblStruct xml:id="b0">
                        <analytic></analytic>
                        <monogr>
                            <title level="m">conv1 ViT-L/16: block24 ResNet50: conv25 ViT-L/16: block12 T2T-ViT-24: block12 T2T-ViT-24</title>
                            <imprint>
                                <biblScope unit="volume">50</biblScope>
                            </imprint>
                        </monogr>
                        <note>conv49 ViT-L/16: block1 T2T-ViT-24: T2T block1 References</note>
                    </biblStruct>
                    <biblStruct xml:id="b1">
                        <analytic>
                            <title level="a" type="main">Attention augmented convolutional networks</title>
                            <author>
                                <persName>
                                    <forename type="first">I</forename>
                                    <surname>Bello</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">B</forename>
                                    <surname>Zoph</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Vaswani</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Shlens</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Q</forename>
                                    <forename type="middle">V</forename>
                                    <surname>Le</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
                            <meeting>the IEEE International Conference on Computer Vision</meeting>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                                <biblScope unit="page" from="3286" to="3295"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b2">
                        <monogr>
                            <title level="m" type="main">Language models are few-shot learners</title>
                            <author>
                                <persName>
                                    <forename type="first">T</forename>
                                    <forename type="middle">B</forename>
                                    <surname>Brown</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">B</forename>
                                    <surname>Mann</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">N</forename>
                                    <surname>Ryder</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Subbiah</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Kaplan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">P</forename>
                                    <surname>Dhariwal</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Neelakantan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">P</forename>
                                    <surname>Shyam</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">G</forename>
                                    <surname>Sastry</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Askell</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2005.14165</idno>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b3">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">N</forename>
                                    <surname>Carion</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">F</forename>
                                    <surname>Massa</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">G</forename>
                                    <surname>Synnaeve</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">N</forename>
                                    <surname>Usunier</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Kirillov</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Zagoruyko</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2005.12872</idno>
                            <title level="m">End-to-end object detection with transformers</title>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b4">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">T</forename>
                                    <surname>Guo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">C</forename>
                                    <surname>Xu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Deng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Ma</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">C</forename>
                                    <surname>Xu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">C</forename>
                                    <surname>Xu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">W</forename>
                                    <surname>Gao</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2012.00364</idno>
                            <title level="m">Pre-trained image processing transformer</title>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b5">
                        <analytic>
                            <title level="a" type="main">Generative pretraining from pixels</title>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Radford</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">R</forename>
                                    <surname>Child</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Wu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Jun</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">D</forename>
                                    <surname>Luan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">I</forename>
                                    <surname>Sutskever</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">International Conference on Machine Learning</title>
                            <imprint>
                                <publisher>PMLR</publisher>
                                <date type="published" when="2020">2020</date>
                                <biblScope unit="page" from="1691" to="1703"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b6">
                        <analytic>
                            <title level="a" type="main">Aˆ2-nets: Double attention networks</title>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Kalantidis</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Yan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Feng</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Advances in neural information processing systems</title>
                            <imprint>
                                <date type="published" when="2018">2018</date>
                                <biblScope unit="page" from="352" to="361"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b7">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">K</forename>
                                    <surname>Choromanski</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">V</forename>
                                    <surname>Likhosherstov</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">D</forename>
                                    <surname>Dohan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">X</forename>
                                    <surname>Song</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Gane</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">T</forename>
                                    <surname>Sarlos</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">P</forename>
                                    <surname>Hawkins</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Davis</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Mohiuddin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Kaiser</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2009.14794</idno>
                            <title level="m">Rethinking attention with performers</title>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b8">
                        <monogr>
                            <title level="m" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Dai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">B</forename>
                                    <surname>Cai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Lin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2011.09094</idno>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b9">
                        <analytic>
                            <title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Deng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">W</forename>
                                    <surname>Dong</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">R</forename>
                                    <surname>Socher</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">L.-J</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">K</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Fei-Fei</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">IEEE conference on computer vision and pattern recognition</title>
                            <imprint>
                                <publisher>Ieee</publisher>
                                <date type="published" when="2009">2009. 2009</date>
                                <biblScope unit="page" from="248" to="255"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b10">
                        <monogr>
                            <title level="m" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Devlin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M.-W</forename>
                                    <surname>Chang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">K</forename>
                                    <surname>Lee</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">K</forename>
                                    <surname>Toutanova</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <surname>Bert</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1810.04805</idno>
                            <imprint>
                                <date type="published" when="2018">2018</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b11">
                        <monogr>
                            <title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
                            <author>
                                <persName>
                                    <forename type="first">T</forename>
                                    <surname>Devries</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">G</forename>
                                    <forename type="middle">W</forename>
                                    <surname>Taylor</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1708.04552</idno>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b12">
                        <monogr>
                            <title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Dosovitskiy</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Beyer</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Kolesnikov</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">D</forename>
                                    <surname>Weissenborn</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">X</forename>
                                    <surname>Zhai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">T</forename>
                                    <surname>Unterthiner</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Dehghani</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Minderer</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">G</forename>
                                    <surname>Heigold</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Gelly</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2010.11929</idno>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b13">
                        <analytic>
                            <title level="a" type="main">Dual attention network for scene segmentation</title>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Fu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Tian</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Bao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Fang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Lu</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
                            <meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                                <biblScope unit="page" from="3146" to="3154"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b14">
                        <analytic>
                            <title level="a" type="main">Ghostnet: More features from cheap operations</title>
                            <author>
                                <persName>
                                    <forename type="first">K</forename>
                                    <surname>Han</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Q</forename>
                                    <surname>Tian</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Guo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">C</forename>
                                    <surname>Xu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">C</forename>
                                    <surname>Xu</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
                            <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                                <biblScope unit="page" from="1580" to="1589"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b15">
                        <analytic>
                            <title level="a" type="main">Deep residual learning for image recognition</title>
                            <author>
                                <persName>
                                    <forename type="first">K</forename>
                                    <surname>He</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">X</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Ren</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Sun</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
                            <meeting>the IEEE conference on computer vision and pattern recognition</meeting>
                            <imprint>
                                <date type="published" when="2016">2016</date>
                                <biblScope unit="page" from="770" to="778"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b16">
                        <monogr>
                            <title level="m" type="main">Distilling the knowledge in a neural network</title>
                            <author>
                                <persName>
                                    <forename type="first">G</forename>
                                    <surname>Hinton</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">O</forename>
                                    <surname>Vinyals</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Dean</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1503.02531</idno>
                            <imprint>
                                <date type="published" when="2015">2015</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b17">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <forename type="middle">G</forename>
                                    <surname>Howard</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Zhu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">B</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">D</forename>
                                    <surname>Kalenichenko</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">W</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">T</forename>
                                    <surname>Weyand</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Andreetto</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Adam</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1704.04861</idno>
                            <title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b18">
                        <analytic>
                            <title level="a" type="main">Local relation networks for image recognition</title>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Hu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Xie</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Lin</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
                            <meeting>the IEEE International Conference on Computer Vision</meeting>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                                <biblScope unit="page" from="3464" to="3473"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b19">
                        <analytic>
                            <title level="a" type="main">Gatherexcite: Exploiting feature context in convolutional neural networks</title>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Hu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Shen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Albanie</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">G</forename>
                                    <surname>Sun</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Vedaldi</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j">Advances in neural information processing systems</title>
                            <imprint>
                                <biblScope unit="volume">31</biblScope>
                                <biblScope unit="page" from="9401" to="9411"/>
                                <date type="published" when="2018">2018</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b20">
                        <analytic>
                            <title level="a" type="main">Squeeze-and-excitation networks</title>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Hu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Shen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">G</forename>
                                    <surname>Sun</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
                            <meeting>the IEEE conference on computer vision and pattern recognition</meeting>
                            <imprint>
                                <date type="published" when="2018">2018</date>
                                <biblScope unit="page" from="7132" to="7141"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b21">
                        <analytic>
                            <title level="a" type="main">Densely connected convolutional networks</title>
                            <author>
                                <persName>
                                    <forename type="first">G</forename>
                                    <surname>Huang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Van Der Maaten</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">K</forename>
                                    <forename type="middle">Q</forename>
                                    <surname>Weinberger</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
                            <meeting>the IEEE conference on computer vision and pattern recognition</meeting>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                                <biblScope unit="page" from="4700" to="4708"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b22">
                        <analytic>
                            <title level="a"
                                   type="main">Imagenet classification with deep convolutional neural networks</title>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Krizhevsky</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">I</forename>
                                    <surname>Sutskever</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">G</forename>
                                    <forename type="middle">E</forename>
                                    <surname>Hinton</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j">Communications of the ACM</title>
                            <imprint>
                                <biblScope unit="volume">60</biblScope>
                                <biblScope unit="issue">6</biblScope>
                                <biblScope unit="page" from="84" to="90"/>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b23">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Ott</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">N</forename>
                                    <surname>Goyal</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Du</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Joshi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">D</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">O</forename>
                                    <surname>Levy</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Lewis</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Zettlemoyer</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">V</forename>
                                    <surname>Stoyanov</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1907.11692</idno>
                            <title level="m">Roberta: A robustly optimized bert pretraining approach</title>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b24">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">I</forename>
                                    <surname>Loshchilov</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">F</forename>
                                    <surname>Hutter</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1608.03983</idno>
                            <title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
                            <imprint>
                                <date type="published" when="2016">2016</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b25">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">I</forename>
                                    <surname>Loshchilov</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">F</forename>
                                    <surname>Hutter</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1711.05101</idno>
                            <title level="m">Decoupled weight decay regularization</title>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b26">
                        <monogr>
                            <title level="m"
                                   type="main">Efficient estimation of word representations in vector space</title>
                            <author>
                                <persName>
                                    <forename type="first">T</forename>
                                    <surname>Mikolov</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">K</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">G</forename>
                                    <surname>Corrado</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Dean</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1301.3781</idno>
                            <imprint>
                                <date type="published" when="2013">2013</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b27">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">N</forename>
                                    <surname>Parmar</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Vaswani</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Uszkoreit</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Ł</forename>
                                    <surname>Kaiser</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">N</forename>
                                    <surname>Shazeer</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Ku</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">D</forename>
                                    <surname>Tran</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1802.05751</idno>
                            <title level="m">Image transformer</title>
                            <imprint>
                                <date type="published" when="2018">2018</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b28">
                        <analytic>
                            <title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Paszke</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Gross</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">F</forename>
                                    <surname>Massa</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Lerer</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Bradbury</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">G</forename>
                                    <surname>Chanan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">T</forename>
                                    <surname>Killeen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Lin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">N</forename>
                                    <surname>Gimelshein</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Antiga</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Advances in neural information processing systems</title>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                                <biblScope unit="page" from="8026" to="8037"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b29">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <forename type="middle">E</forename>
                                    <surname>Peters</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Neumann</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">R</forename>
                                    <forename type="middle">L</forename>
                                    <surname>Logan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">I</forename>
                                    <forename type="middle">V</forename>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">R</forename>
                                    <surname>Schwartz</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">V</forename>
                                    <surname>Joshi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Singh</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">N</forename>
                                    <forename type="middle">A</forename>
                                    <surname>Smith</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1909.04164</idno>
                            <title level="m">Knowledge enhanced contextual word representations</title>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b30">
                        <monogr>
                            <title level="m"
                                   type="main">Improving language understanding by generative pretraining</title>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Radford</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">K</forename>
                                    <surname>Narasimhan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">T</forename>
                                    <surname>Salimans</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">I</forename>
                                    <surname>Sutskever</surname>
                                </persName>
                            </author>
                            <imprint>
                                <date type="published" when="2018">2018</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b31">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">P</forename>
                                    <surname>Ramachandran</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">N</forename>
                                    <surname>Parmar</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Vaswani</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">I</forename>
                                    <surname>Bello</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Levskaya</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Shlens</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1906.05909</idno>
                            <title level="m">Stand-alone self-attention in vision models</title>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b32">
                        <analytic>
                            <title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Sandler</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Howard</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Zhu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Zhmoginov</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">L.-C</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
                            <meeting>the IEEE conference on computer vision and pattern recognition</meeting>
                            <imprint>
                                <date type="published" when="2018">2018</date>
                                <biblScope unit="page" from="4510" to="4520"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b33">
                        <monogr>
                            <title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
                            <author>
                                <persName>
                                    <forename type="first">K</forename>
                                    <surname>Simonyan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Zisserman</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1409.1556</idno>
                            <imprint>
                                <date type="published" when="2014">2014</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b34">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Sun</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Cao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">K</forename>
                                    <surname>Kitani</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2011.10881</idno>
                            <title level="m">Rethinking transformer-based set prediction for object detection</title>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b35">
                        <analytic>
                            <title level="a"
                                   type="main">Rethinking the inception architecture for computer vision</title>
                            <author>
                                <persName>
                                    <forename type="first">C</forename>
                                    <surname>Szegedy</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">V</forename>
                                    <surname>Vanhoucke</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Ioffe</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Shlens</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Wojna</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
                            <meeting>the IEEE conference on computer vision and pattern recognition</meeting>
                            <imprint>
                                <date type="published" when="2016">2016</date>
                                <biblScope unit="page" from="2818" to="2826"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b36">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Touvron</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Cord</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Douze</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">F</forename>
                                    <surname>Massa</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Sablayrolles</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Jégou</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2012.12877</idno>
                            <title level="m">Training data-efficient image transformers &amp; distillation through attention</title>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b37">
                        <analytic>
                            <title level="a" type="main">Attention is all you need</title>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Vaswani</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">N</forename>
                                    <surname>Shazeer</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">N</forename>
                                    <surname>Parmar</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Uszkoreit</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Jones</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <forename type="middle">N</forename>
                                    <surname>Gomez</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Ł</forename>
                                    <surname>Kaiser</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">I</forename>
                                    <surname>Polosukhin</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j">Advances in neural information processing systems</title>
                            <imprint>
                                <biblScope unit="volume">30</biblScope>
                                <biblScope unit="page" from="5998" to="6008"/>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b38">
                        <analytic>
                            <title level="a" type="main">Residual attention network for image classification</title>
                            <author>
                                <persName>
                                    <forename type="first">F</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Jiang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">C</forename>
                                    <surname>Qian</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">C</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">X</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">X</forename>
                                    <surname>Tang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
                            <meeting>the IEEE conference on computer vision and pattern recognition</meeting>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                                <biblScope unit="page" from="3156" to="3164"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b39">
                        <analytic>
                            <title level="a" type="main">Non-local neural networks</title>
                            <author>
                                <persName>
                                    <forename type="first">X</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">R</forename>
                                    <surname>Girshick</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Gupta</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">K</forename>
                                    <surname>He</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
                            <meeting>the IEEE conference on computer vision and pattern recognition</meeting>
                            <imprint>
                                <date type="published" when="2018">2018</date>
                                <biblScope unit="page" from="7794" to="7803"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b40">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Xu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">X</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">C</forename>
                                    <surname>Shen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">B</forename>
                                    <surname>Cheng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Shen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Xia</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2011.14503</idno>
                            <title level="m">End-to-end video instance segmentation with transformers</title>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b41">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">R</forename>
                                    <surname>Wightman</surname>
                                </persName>
                            </author>
                            <ptr target="https://github.com/rwightman/pytorch-image-models"/>
                            <title level="m">Pytorch image models</title>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b42">
                        <analytic>
                            <title level="a" type="main">Cbam: Convolutional block attention module</title>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Woo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Park</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J.-Y</forename>
                                    <surname>Lee</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">I</forename>
                                    <surname>So Kweon</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
                            <meeting>the European conference on computer vision (ECCV)</meeting>
                            <imprint>
                                <date type="published" when="2018">2018</date>
                                <biblScope unit="page" from="3" to="19"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b43">
                        <monogr>
                            <title level="m" type="main">Visual transformers: Token-based image representation and processing for computer vision</title>
                            <author>
                                <persName>
                                    <forename type="first">B</forename>
                                    <surname>Wu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">C</forename>
                                    <surname>Xu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">X</forename>
                                    <surname>Dai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Wan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">P</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Tomizuka</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">K</forename>
                                    <surname>Keutzer</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">P</forename>
                                    <surname>Vajda</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2006.03677</idno>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b44">
                        <analytic>
                            <title level="a"
                                   type="main">Aggregated residual transformations for deep neural networks</title>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Xie</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">R</forename>
                                    <surname>Girshick</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">P</forename>
                                    <surname>Dollár</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Tu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">K</forename>
                                    <surname>He</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
                            <meeting>the IEEE conference on computer vision and pattern recognition</meeting>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                                <biblScope unit="page" from="1492" to="1500"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b45">
                        <analytic>
                            <title level="a"
                                   type="main">Learning texture transformer network for image super-resolution</title>
                            <author>
                                <persName>
                                    <forename type="first">F</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Fu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Lu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">B</forename>
                                    <surname>Guo</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
                            <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                                <biblScope unit="page" from="5791" to="5800"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b46">
                        <analytic>
                            <title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Dai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Carbonell</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">R</forename>
                                    <forename type="middle">R</forename>
                                    <surname>Salakhutdinov</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Q</forename>
                                    <forename type="middle">V</forename>
                                    <surname>Le</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Advances in neural information processing systems</title>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                                <biblScope unit="page" from="5753" to="5763"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b47">
                        <analytic>
                            <title level="a" type="main">Stacked attention networks for image question answering</title>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">X</forename>
                                    <surname>He</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Gao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Deng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">A</forename>
                                    <surname>Smola</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
                            <meeting>the IEEE conference on computer vision and pattern recognition</meeting>
                            <imprint>
                                <date type="published" when="2016">2016</date>
                                <biblScope unit="page" from="21" to="29"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b48">
                        <analytic>
                            <title level="a"
                                   type="main">A simple baseline for pose tracking in videos of crowed scenes</title>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Yuan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Chang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Huang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Zhou</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">X</forename>
                                    <surname>Nie</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">F</forename>
                                    <forename type="middle">E</forename>
                                    <surname>Tay</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Feng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Yan</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
                            <meeting>the 28th ACM International Conference on Multimedia</meeting>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                                <biblScope unit="page" from="4684" to="4688"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b49">
                        <analytic>
                            <title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Yuan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">F</forename>
                                    <forename type="middle">E</forename>
                                    <surname>Tay</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">G</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">T</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Feng</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
                            <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                                <biblScope unit="page" from="3903" to="3911"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b50">
                        <analytic>
                            <title level="a" type="main">Toward accurate person-level action recognition in videos of crowed scenes</title>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Yuan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Zhou</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Chang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Z</forename>
                                    <surname>Huang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">X</forename>
                                    <surname>Nie</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">T</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Feng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Yan</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
                            <meeting>the 28th ACM International Conference on Multimedia</meeting>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                                <biblScope unit="page" from="4694" to="4698"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b51">
                        <analytic>
                            <title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Yun</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">D</forename>
                                    <surname>Han</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Oh</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Chun</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Choe</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Yoo</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
                            <meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                                <biblScope unit="page" from="6023" to="6032"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b52">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Zagoruyko</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">N</forename>
                                    <surname>Komodakis</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1605.07146</idno>
                            <title level="m">Wide residual networks</title>
                            <imprint>
                                <date type="published" when="2016">2016</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b53">
                        <analytic>
                            <title level="a" type="main">Learning joint spatial-temporal transformations for video inpainting</title>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Zeng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Fu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Chao</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">European Conference on Computer Vision</title>
                            <imprint>
                                <publisher>Springer</publisher>
                                <date type="published" when="2020">2020</date>
                                <biblScope unit="page" from="528" to="543"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b54">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Cisse</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <forename type="middle">N</forename>
                                    <surname>Dauphin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">D</forename>
                                    <surname>Lopez-Paz</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1710.09412</idno>
                            <title level="m">mixup: Beyond empirical risk minimization</title>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b55">
                        <analytic>
                            <title level="a" type="main">Exploring self-attention for image recognition</title>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Zhao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Jia</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">V</forename>
                                    <surname>Koltun</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
                            <meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                                <biblScope unit="page" from="10076" to="10085"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b56">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Zhao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Jiang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Jia</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">P</forename>
                                    <surname>Torr</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">V</forename>
                                    <surname>Koltun</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2012.09164</idno>
                            <title level="m">Point transformer</title>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b57">
                        <analytic>
                            <title level="a"
                                   type="main">Psanet: Point-wise spatial attention network for scene parsing</title>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Zhao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">S</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Shi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">C</forename>
                                    <forename type="middle">Change</forename>
                                    <surname>Loy</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">D</forename>
                                    <surname>Lin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Jia</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
                            <meeting>the European Conference on Computer Vision (ECCV)</meeting>
                            <imprint>
                                <date type="published" when="2018">2018</date>
                                <biblScope unit="page" from="267" to="283"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b58">
                        <monogr>
                            <title level="m" type="main">End-to-end object detection with adaptive clustering transformer</title>
                            <author>
                                <persName>
                                    <forename type="first">M</forename>
                                    <surname>Zheng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">P</forename>
                                    <surname>Gao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">X</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">H</forename>
                                    <surname>Dong</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2011.09315</idno>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct xml:id="b59">
                        <analytic>
                            <title level="a" type="main">Neural epitome search for architecture-agnostic network compression</title>
                            <author>
                                <persName>
                                    <forename type="first">D</forename>
                                    <surname>Zhou</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">X</forename>
                                    <surname>Jin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Q</forename>
                                    <surname>Hou</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">K</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Feng</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">International Conference on Learning Representations</title>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b60">
                        <analytic>
                            <title level="a"
                                   type="main">End-to-end dense video captioning with masked transformer</title>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Zhou</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Y</forename>
                                    <surname>Zhou</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Corso</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">R</forename>
                                    <surname>Socher</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">C</forename>
                                    <surname>Xiong</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
                            <meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
                            <imprint>
                                <date type="published" when="2018">2018</date>
                                <biblScope unit="page" from="8739" to="8748"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct xml:id="b61">
                        <monogr>
                            <author>
                                <persName>
                                    <forename type="first">X</forename>
                                    <surname>Zhu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">W</forename>
                                    <surname>Su</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">L</forename>
                                    <surname>Lu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">B</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">X</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">J</forename>
                                    <surname>Dai</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2010.04159</idno>
                            <title level="m">Deformable detr: Deformable transformers for end-to-end object detection</title>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>