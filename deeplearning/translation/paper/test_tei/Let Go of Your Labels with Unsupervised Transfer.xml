<?xml version="1.0" encoding="UTF-8"?>
<TEI
        xmlns="http://www.tei-c.org/ns/1.0"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve"
        xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
    <teiHeader xml:lang="en">
        <fileDesc>
            <titleStmt>
                <title level="a" type="main" coords="1,143.19,91.02,310.82,12.90">Let Go of Your Labels with Unsupervised Transfer</title>
                <funder>
                    <orgName type="full">EPFL</orgName>
                </funder>
            </titleStmt>
            <publicationStmt>
                <publisher/>
                <availability status="unknown">
                    <licence/>
                </availability>
                <date type="published" when="2024-06-11">11 Jun 2024</date>
            </publicationStmt>
            <sourceDesc>
                <biblStruct>
                    <analytic>
                        <author>
                            <persName coords="1,188.38,143.78,75.70,8.96">
                                <forename type="first">Artyom</forename>
                                <surname>Gadetsky</surname>
                            </persName>
                        </author>
                        <author>
                            <persName coords="1,281.02,143.78,51.41,8.96">
                                <forename type="first">Yulun</forename>
                                <surname>Jiang</surname>
                            </persName>
                        </author>
                        <author>
                            <persName coords="1,349.36,143.78,52.85,8.96">
                                <forename type="first">Maria</forename>
                                <surname>Brbić</surname>
                            </persName>
                        </author>
                        <title level="a" type="main" coords="1,143.19,91.02,310.82,12.90">Let Go of Your Labels with Unsupervised Transfer</title>
                    </analytic>
                    <monogr>
                        <imprint>
                            <date type="published" when="2024-06-11">11 Jun 2024</date>
                        </imprint>
                    </monogr>
                    <idno type="MD5">E558C5D0A714A74992D5861955FBA7D5</idno>
                    <idno type="arXiv">arXiv:2406.07236v1[cs.LG]</idno>
                </biblStruct>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-07-09T05:44+0000">
                    <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
                    <ref target="https://github.com/kermitt2/grobid"/>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <abstract>
                <div
                        xmlns="http://www.tei-c.org/ns/1.0">
                    <p>
                        <s coords="1,75.37,222.33,194.15,8.64;1,75.37,234.28,195.80,8.64;1,75.37,246.24,195.80,8.64;1,75.37,258.19,51.46,8.64">Foundation vision-language models have enabled remarkable zero-shot transferability of the pretrained representations to a wide range of downstream tasks.</s>
                        <s coords="1,129.91,258.19,141.26,8.64;1,75.37,270.15,194.15,8.64;1,75.37,282.10,195.89,8.64">However, to solve a new task, zeroshot transfer still necessitates human guidance to define visual categories that appear in the data.</s>
                        <s coords="1,75.37,293.88,194.15,8.82;1,75.37,306.01,194.15,8.64;1,75.37,317.97,194.15,8.64;1,75.37,329.92,194.15,8.64;1,75.37,341.88,31.90,8.64">Here, we show that fully unsupervised transfer emerges when searching for the labeling of a dataset that induces maximal margin classifiers in representation spaces of different foundation models.</s>
                        <s coords="1,112.51,341.88,158.66,8.64;1,75.12,353.83,194.40,8.64;1,75.37,365.79,194.15,8.64;1,75.37,377.74,194.14,8.64;1,75.37,389.70,150.13,8.64">We present TURTLE, a fully unsupervised method that effectively employs this guiding principle to uncover the underlying labeling of a downstream dataset without any supervision and task-specific representation learning.</s>
                        <s coords="1,230.00,389.70,41.17,8.64;1,75.37,401.65,194.15,8.64;1,75.37,413.61,195.80,8.64;1,75.37,425.56,138.97,8.64">We evaluate TURTLE on a diverse benchmark suite of 26 datasets and show that it achieves new state-ofthe-art unsupervised performance.</s>
                        <s coords="1,217.42,425.56,53.34,8.64;1,75.06,437.52,196.11,8.64;1,75.37,449.47,194.15,8.64;1,75.37,461.43,67.77,8.64">Furthermore, TURTLE, although being fully unsupervised, outperforms zero-shot transfer baselines on a wide range of datasets.</s>
                        <s coords="1,146.16,461.43,123.35,8.64;1,75.37,473.38,194.15,8.64;1,75.37,485.34,194.14,8.64;1,75.37,497.29,194.15,8.64;1,75.37,509.25,51.03,8.64">In particular, TURTLE matches the average performance of CLIP zero-shot on 26 datasets by employing the same representation space, spanning a wide range of architectures and model sizes.</s>
                        <s coords="1,132.04,509.25,139.13,8.64;1,75.37,521.20,194.15,8.64;1,75.37,533.16,194.15,8.64;1,75.37,545.12,195.80,8.64;1,75.37,557.07,194.32,8.64;1,75.37,569.03,168.95,8.64">By guiding the search for the underlying labeling using the representation spaces of two foundation models, TURTLE surpasses zero-shot transfer and unsupervised prompt tuning baselines, demonstrating the surprising power and effectiveness of unsupervised transfer.</s>
                    </p>
                </div>
            </abstract>
        </profileDesc>
    </teiHeader>
    <facsimile>
        <surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
    </facsimile>
    <text xml:lang="en">
        <body>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="1." coords="1,55.44,606.66,76.84,10.75">Introduction</head>
                <p>
                    <s coords="1,55.13,627.80,234.31,8.64;1,55.44,639.75,234.00,8.64;1,64.96,657.55,2.99,5.18;1,68.44,659.41,65.41,7.77;1,138.13,657.55,2.99,5.18;1,141.61,659.41,107.03,7.77">Transfer learning is a fundamental machine learning paradigm that leverages large-scale pre-training of deep * Equal contribution 1 EPFL, Lausanne, Switzerland.</s>
                    <s coords="1,251.40,659.41,39.53,7.77;1,55.44,669.09,155.61,8.06">Correspondence to: Maria Brbić &lt;mbrbic@epfl.ch&gt;.</s>
                </p>
                <p>
                    <s coords="1,55.16,689.01,82.49,7.86;1,138.86,687.24,6.34,5.24;1,148.86,689.14,140.58,7.73;1,55.19,699.10,163.90,7.93">Proceedings of the 41 st International Conference on Machine
                        <ref type="bibr"
                             coords="1,55.19,699.10,163.90,7.93">Learning, Vienna, Austria. PMLR 235, 2024.</ref>
                    </s>
                    <s coords="1,221.89,699.26,67.86,7.77;1,55.44,709.22,47.81,7.77">Copyright 2024 by the author(s).</s>
                </p>
                <p>
                    <s coords="1,307.44,202.20,235.65,8.64;1,307.44,214.15,235.74,8.64">neural networks to improve model performance on downstream tasks with limited resources
                        <ref type="bibr" coords="1,456.91,214.15,81.90,8.64" target="#b54">(Pan &amp; Yang, 2009)</ref>.
                    </s>
                    <s coords="1,307.44,226.11,235.66,8.64;1,307.44,238.06,234.00,8.64;1,307.44,250.02,134.77,8.64">Early transfer learning approaches relied on supervised finetuning of the entire model to solve a downstream task of interest
                        <ref type="bibr" coords="1,339.67,250.02,98.20,8.64"
                             target="#b38">(Kolesnikov et al., 2020)</ref>.
                    </s>
                    <s coords="1,445.30,250.02,97.38,8.64;1,307.44,261.97,234.83,8.64;1,307.44,273.93,234.00,8.64;1,307.44,285.88,234.00,8.64;1,307.44,297.84,234.00,8.64;1,307.44,309.79,118.43,8.64">Recent works
                        <ref type="bibr" coords="1,502.93,250.02,39.76,8.64;1,307.44,261.97,22.65,8.64" target="#b30">(He et al., 2022;</ref>
                        <ref type="bibr" coords="1,332.59,261.97,58.29,8.64" target="#b44">Li et al., 2022;</ref>
                        <ref type="bibr" coords="1,393.36,261.97,70.44,8.64" target="#b83">Zhou et al., 2022;</ref>
                        <ref type="bibr" coords="1,466.30,261.97,75.96,8.64" target="#b53">Oquab et al., 2023;</ref>
                        <ref type="bibr" coords="1,307.44,273.93,79.13,8.64" target="#b18">Darcet et al., 2024)</ref>
                        have shown that fine-tuning an entire model during transfer brings only marginal gains compared
                        to training a linear classifier on top of the frozen pre-trained backbone (i.e., linear probe).
                    </s>
                    <s coords="1,430.47,309.79,110.97,8.64;1,307.44,321.75,235.66,8.64;1,307.44,333.70,234.17,8.64;1,307.44,345.66,234.00,8.64;1,307.44,357.62,22.42,8.64">Although these approaches eliminated the need for task-specific fine-tuning of representations, they still require at least a few labeled examples per class to achieve human-level performance on downstream tasks.</s>
                </p>
                <p>
                    <s coords="1,307.44,375.55,234.00,8.64;1,307.44,387.50,235.65,8.64;1,307.44,399.46,143.41,8.64">Recently, foundation models
                        <ref type="bibr" coords="1,421.85,375.55,99.05,8.64" target="#b7">(Bommasani et al., 2022)</ref>
                        have emerged, approaching human-level intelligence on a variety of tasks in the zero-shot
                        setting.
                    </s>
                    <s coords="1,453.95,399.46,87.49,8.64;1,307.44,411.41,234.00,8.64;1,307.44,423.37,234.00,8.64;1,307.44,435.32,91.98,8.64">In particular,
                        <ref type="bibr" coords="1,508.27,399.46,33.17,8.64;1,307.44,411.41,50.04,8.64" target="#b56">Radford et al. (2021)</ref>
                        proposed CLIP, which trains representations by aligning images and their corresponding captions
                        in the joint embedding space.
                    </s>
                    <s coords="1,402.52,435.32,140.57,8.64;1,307.44,447.28,234.00,8.64;1,307.44,459.23,135.54,8.64">After pre-training, a zero-shot classifier is constructed by embedding the descriptions of visual categories that appear in the data.</s>
                    <s coords="1,446.06,459.23,95.38,8.64;1,307.44,471.19,234.00,8.64;1,307.44,483.14,235.65,8.64;1,307.44,495.10,234.00,8.64;1,307.11,507.05,235.98,8.64;1,307.44,519.01,107.25,8.64">Subsequent works have successfully adopted this representation learning principle to enable zero-shot transfer in other domains, such as audio signal processing
                        <ref type="bibr" coords="1,392.19,495.10,86.19,8.64">(Elizalde et al., 2023a;</ref>
                        <ref type="bibr" coords="1,478.37,495.10,8.26,8.64" target="#b48">b)</ref>, biomedicine
                        <ref type="bibr" coords="1,307.11,507.05,64.60,8.64" target="#b45">(Lin et al., 2023;</ref>
                        <ref type="bibr" coords="1,373.90,507.05,85.76,8.64" target="#b58">Robinson et al., 2023)</ref>
                        and symbolic regression
                        <ref type="bibr" coords="1,326.39,519.01,84.06,8.64" target="#b50">(Meidani et al., 2024)</ref>.
                    </s>
                    <s coords="1,417.82,519.01,123.62,8.64;1,307.44,530.97,234.00,8.64;1,307.44,542.92,123.18,8.64">Despite the remarkable success of foundation models, zero-shot transfer still requires human instructions to solve a new task.</s>
                    <s coords="1,433.62,542.92,107.82,8.64;1,307.44,554.88,234.00,8.64;1,307.44,566.65,110.36,8.59">But, can the representations of foundation models be utilized to solve a new task in a fully unsupervised manner?</s>
                </p>
                <p>
                    <s coords="1,307.13,584.76,234.31,8.64;1,307.44,596.72,234.67,8.64;1,307.44,608.67,157.10,8.64">The simplest approach for unsupervised transfer would be to apply off-the-shelf clustering methods
                        <ref type="bibr" coords="1,468.49,596.72,73.62,8.64" target="#b48">(MacQueen, 1967)</ref>
                        on top of the pre-trained representations.
                    </s>
                    <s coords="1,467.50,608.67,75.60,8.64;1,307.44,620.63,234.00,8.64;1,307.44,632.58,234.17,8.64;1,307.11,644.54,162.29,8.64">However, this strategy inevitably leads to a drastic decrease in performance compared to (weakly) supervised and zero-shot transfer
                        <ref type="bibr" coords="1,307.11,644.54,76.93,8.64" target="#b83">(Zhou et al., 2022;</ref>
                        <ref type="bibr" coords="1,387.12,644.54,77.92,8.64" target="#b53">Oquab et al., 2023)</ref>.
                    </s>
                    <s coords="1,474.21,644.54,68.88,8.64;1,307.44,656.49,234.00,8.64;1,307.44,668.45,235.65,8.64;1,307.44,680.40,235.74,8.64">Recently,
                        <ref type="bibr" coords="1,515.44,644.54,23.04,8.64;1,307.44,656.49,81.54,8.64" target="#b26">Gadetsky &amp; Brbić (2023)</ref>
                        introduced HUME, an unsupervised learning framework for inferring the underlying human labeling
                        of a given dataset from pre-trained representations.
                    </s>
                    <s coords="1,306.97,692.36,234.47,8.64;1,307.44,704.32,235.66,8.64;2,54.94,263.82,328.99,8.12">While HUME has achieved superior performance compared to unsupervised baselines, it still requires task-specific rep-Figure
                        <ref type="figure" coords="2,80.59,264.17,3.36,7.77">1</ref>. Types of downstream transfer
                        differ in the amount of available supervision.
                    </s>
                    <s coords="2,387.07,264.17,154.37,7.77;2,55.44,274.97,486.60,7.93;2,55.44,285.93,486.00,7.93;2,55.44,296.89,486.00,7.93;2,55.44,308.01,300.31,7.77">Given representation spaces of foundation models, (i) supervised transfer, represented as a linear probe, trains a linear classifier given labeled examples of a downstream dataset; (ii) zero-shot transfer assumes descriptions of the visual categories that appear in a downstream dataset are given, and employs them via text encoder to solve the task; and (iii) unsupervised transfer assumes the least amount of available supervision, i.e., only the number of categories is given, and aims to uncover the underlying human labeling of a dataset.</s>
                    <s coords="2,55.44,339.19,234.00,8.64;2,55.44,351.14,143.87,8.64">resentation learning and does not close the gap between unsupervised and zero-shot transfer.</s>
                </p>
                <p>
                    <s coords="2,55.44,369.08,235.65,8.64;2,55.19,381.03,152.04,8.64">Here, we present TURTLE, a method that enables unsupervised transfer from foundation models.</s>
                    <s coords="2,210.30,381.03,79.14,8.64;2,55.44,392.99,234.00,8.64;2,55.44,404.94,234.00,8.64;2,55.44,416.90,234.00,8.64;2,55.44,428.85,109.77,8.64">The key idea behind our approach is to search for the labeling of a downstream dataset that maximizes the margins of linear classifiers in the space of single or multiple foundation models to uncover the underlying human labeling.</s>
                    <s coords="2,168.32,428.85,122.78,8.64;2,55.44,440.81,234.00,8.64;2,55.44,452.76,189.72,8.64">Compared to zero-shot and supervised transfer, unsupervised transfer with TURTLE does not need the supervision in any form (Figure
                        <ref type="figure" coords="2,234.46,452.76,3.57,8.64">1</ref>).
                    </s>
                    <s coords="2,248.27,452.76,41.17,8.64;2,55.44,464.72,235.25,8.64;2,55.44,476.67,234.00,8.64;2,55.44,488.63,235.65,8.64;2,55.44,500.58,234.00,8.64;2,55.44,512.54,31.27,8.64">Compared to deep clustering methods
                        <ref type="bibr" coords="2,167.61,464.72,68.88,8.64" target="#b77">(Xie et al., 2016;</ref>
                        <ref type="bibr" coords="2,239.12,464.72,51.57,8.64;2,55.44,476.67,23.15,8.64" target="#b11">Chang et al., 2017;</ref>
                        <ref type="bibr" coords="2,81.51,476.67,76.49,8.64" target="#b9">Caron et al., 2018;</ref>
                        <ref type="bibr" coords="2,160.91,476.67,110.37,8.64"
                             target="#b69">Van Gansbeke et al., 2020;</ref>
                        <ref type="bibr" coords="2,274.19,476.67,15.24,8.64;2,55.44,488.63,45.84,8.64" target="#b52">Niu et al., 2022)</ref>
                        , TURTLE does not require task-specific representation learning that is expensive for modern
                        foundation models.
                    </s>
                </p>
                <p>
                    <s coords="2,54.97,530.47,234.46,8.64;2,55.44,542.11,235.66,8.96;2,55.44,554.38,59.06,8.64">We study the performance of TURTLE on the extensive evaluation suite spanning 26 datasets and 7 different foundation models.</s>
                    <s coords="2,117.59,554.38,171.85,8.64;2,55.44,566.34,234.00,8.64;2,55.44,578.29,85.49,8.64">We compare TURTLE to various baselines that differ in the amount of available supervision for the downstream transfer.</s>
                    <s coords="2,145.68,578.29,143.76,8.64;2,55.44,590.25,235.65,8.64;2,55.44,602.20,234.00,8.64;2,55.44,614.16,208.36,8.64">First, when compared to the recent state-of-the-art unsupervised baselines, TURTLE outperforms these baselines on all the considered datasets, setting the new state-of-the-art unsupervised performance.</s>
                    <s coords="2,267.94,614.16,23.15,8.64;2,55.44,626.11,234.00,8.64;2,55.44,638.07,234.00,8.64;2,55.44,650.02,235.66,8.64;2,55.44,661.66,190.09,8.96">Compared to zero-shot transfer, TURTLE instantiated with two foundation models surpasses CLIP zero-shot transfer across all studied model sizes, achieving exceptional absolute improvements up to 35% on the studied datasets.</s>
                    <s coords="2,249.73,661.98,39.71,8.64;2,55.44,673.93,234.00,8.64;2,55.44,685.57,234.25,8.96;2,55.44,697.84,109.57,8.64">Given the same single representation space, TURTLE closely matches the performance of the CLIP zero-shot transfer on 7 out of 8 studied model architectures.</s>
                    <s coords="2,168.05,697.84,121.39,8.64;2,307.44,339.19,235.65,8.64;2,307.44,350.82,234.25,8.96;2,307.44,363.10,34.71,8.64">In particular, the best TURTLE model, which utilizes the same model size and representation space, outperforms CLIP zero-shot on 13 out of 26 datasets.</s>
                    <s coords="2,348.42,363.10,193.19,8.64;2,307.44,375.05,235.66,8.64;2,307.44,387.01,234.00,8.64;2,307.44,398.96,234.00,8.64;2,307.08,410.92,209.37,8.64">Finally, when compared to supervised transfer represented by linear probe, TURTLE approaches its performance on 5 out of 26 studied datasets, suggesting that labels may not be needed to infer the underlying human labeling when given sufficiently high-quality representations.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="2." coords="2,307.44,437.72,74.85,10.75">Background</head>
                <p>
                    <s coords="2,307.44,458.86,235.66,8.64;2,307.19,470.81,234.25,8.64;2,307.08,482.77,59.30,8.64">In this section, we introduce the problem setting of unsupervised transfer and provide an overview of key concepts that we build upon.</s>
                </p>
                <p>
                    <s coords="2,307.44,500.31,99.03,8.96">Unsupervised transfer.</s>
                    <s coords="2,412.00,500.38,48.92,9.30;2,460.92,498.81,4.15,6.12;2,468.87,500.70,72.57,8.64;2,307.44,512.34,60.77,9.65;2,368.21,510.76,6.31,6.12;2,368.21,516.85,15.01,6.12;2,383.72,512.34,156.64,9.65;2,307.44,524.29,214.31,8.96">Let X ⊆ R d be an input space and D = {x n } N n=1 , x n ∈ X be a dataset consisting of N samples and C classes, where C is known a priori.</s>
                    <s coords="2,527.89,524.61,13.55,8.64;2,307.44,536.25,50.35,8.74;2,350.04,536.25,21.12,9.30;2,371.16,534.67,3.65,6.12;2,378.69,536.57,162.74,8.64;2,307.44,548.20,234.00,8.96;2,307.44,560.48,74.59,8.64">Let ϕ(x) : X -→ R q denotes a mapping from an input space X to a q-dimensional representation space of a pre-trained foundation model.</s>
                    <s coords="2,385.47,560.48,156.33,8.64;2,307.44,572.43,234.00,8.64;2,307.44,584.21,177.45,8.82">The question we aim to answer is how to utilize representations from foundation models to solve a new task in a fully unsupervised manner.</s>
                    <s coords="2,490.01,584.39,53.08,8.64;2,307.44,596.34,234.00,8.64;2,307.44,607.98,235.66,8.96;2,307.44,620.25,214.32,8.64">Thus, by unsupervised transfer we consider the task of inferring the underlying human labeling
                        <ref type="foot" coords="2,415.64,606.63,3.49,6.05" target="#foot_0">1</ref>
                        of a dataset D without any supervision given representations of foundation models.
                    </s>
                </p>
                <p>
                    <s coords="2,307.44,637.80,235.65,8.96;2,307.44,649.75,20.05,8.96">Generalization-based learning of human labelings.</s>
                    <s coords="2,337.05,650.14,204.39,8.64;3,55.44,70.54,235.65,8.64;3,55.44,82.49,234.00,8.64;3,55.44,94.45,149.13,8.64">
                        <ref type="bibr" coords="2,337.05,650.14,109.75,8.64"
                             target="#b26">Gadetsky &amp; Brbić (2023)</ref>
                        recently introduced a generalization-based objective that evaluates the generalization ability
                        of linear models on top of representations obtained from pre-trained models.
                    </s>
                    <s coords="3,218.08,94.45,71.36,8.64;3,55.44,106.40,234.00,8.64;3,55.44,118.36,234.35,8.64;3,55.44,130.31,82.12,8.64">The objective is motivated by a strong generalization ability of linear models in representation spaces of foundation models on many human labeled tasks.</s>
                    <s coords="3,140.65,130.31,148.80,8.64;3,55.44,142.27,234.00,8.64;3,55.44,154.22,235.75,8.64">Equipped with this insight, the goal is to find such labeling that optimizes generalization ability of a linear model over all possible labelings of a given dataset.</s>
                    <s coords="3,55.13,166.18,234.31,8.64;3,55.44,178.13,234.00,8.64;3,55.44,190.09,34.59,8.64">The quality of a labeling is measured by the ability of a linear model to generalize on a task defined by the given labeling.</s>
                </p>
                <p>
                    <s coords="3,55.44,207.70,107.94,8.96;3,155.63,207.70,40.40,8.74">In particular, let τ : X -→ {1, . . .</s>
                    <s coords="3,197.70,207.70,91.75,8.96;3,55.44,219.98,87.82,8.64">, C} denote a labeling function of a dataset.</s>
                    <s coords="3,149.09,219.66,60.15,8.96;3,209.51,218.08,4.71,6.12;3,215.78,219.66,75.31,8.96;3,55.44,231.61,235.66,8.96;3,55.44,243.89,47.57,8.64">Let f (x) = w T ϕ(x) denote a linear model in the representation space ϕ(x) of a foundation model.</s>
                    <s coords="3,110.07,243.57,179.37,9.65;3,55.44,255.52,235.17,9.65;3,55.44,265.93,185.50,11.59">Given a train-test split (D tr , D te ), one can train the model on a training split D tr with labeling τ (D tr ) and classification loss function L to obtain f .</s>
                    <s coords="3,244.87,268.88,46.23,8.64;3,55.44,280.83,234.00,8.64;3,55.44,289.84,153.35,12.28">After training, the generalization ability of the model can be assessed by computing the error of f on D te .</s>
                    <s coords="3,215.31,292.79,74.13,8.64;3,55.44,304.74,207.70,8.64">Consequently, the generalization-based objective is defined as follows:</s>
                </p>
                <formula xml:id="formula_0" coords="3,92.34,323.99,197.76,51.04">min τ x∈Dte L( f (x), τ (x)) s.t. f = arg min f x∈Dtr L(f (x), τ (x)),
                    <label>(1)</label>
                </formula>
                <p>
                    <s coords="3,55.08,385.55,234.36,8.64;3,55.44,397.19,234.00,8.96;3,55.44,409.46,89.34,8.64">where minimization is performed over the set of all possible labelings of a dataset D. This leads to a difficult discrete optimization problem.</s>
                    <s coords="3,147.89,409.46,143.21,8.64;3,55.44,421.42,234.00,8.64;3,55.44,433.05,140.09,8.96">To overcome this limitation, Gadetsky &amp; Brbić (2023) replace minimization w.r.t. a discrete labeling τ with minimization w.r.t.</s>
                    <s coords="3,198.03,433.37,91.41,8.64;3,55.44,445.01,131.20,9.65;3,178.89,445.01,21.49,8.74;3,200.38,443.43,16.40,6.12;3,217.28,445.01,41.20,8.96;3,258.48,443.43,16.40,6.12;3,278.12,445.33,12.98,8.64;3,55.44,456.96,187.86,8.96">continuous parameters θ of a task encoder τ θ (x) : X -→ ∆ C-1 , where ∆ C-1 denotes (C -1)-dimensional probability simplex.</s>
                    <s coords="3,246.39,457.28,44.30,8.64;3,55.44,468.92,234.00,9.65;3,55.44,481.19,234.00,8.64;3,55.11,493.15,14.11,8.64">As a result, careful design of τ θ becomes crucial since it defines the search space explored by the generalization-based objective (1).</s>
                    <s coords="3,55.44,510.69,85.22,8.96">HUME framework.</s>
                    <s coords="3,144.18,511.08,146.50,8.64;3,55.44,522.72,233.29,9.65;3,55.44,534.67,235.65,8.96;3,55.44,546.94,234.00,8.64;3,55.44,558.58,10.73,8.74">The instantiation of this framework, proposed in HUME
                        <ref type="bibr" coords="3,137.94,523.03,103.92,8.64"
                             target="#b26">(Gadetsky &amp; Brbić, 2023)</ref>, models τ θ using a linear model in the
                        representation space ψ(x) obtained via self-supervised pre-training on the target dataset D:
                    </s>
                </p>
                <formula xml:id="formula_1" coords="3,121.78,577.04,168.33,12.69">τ HUME θ (x) = σ(θ T ψ(x)),
                    <label>(2)</label>
                </formula>
                <p>
                    <s coords="3,55.08,600.66,58.93,9.30;3,106.26,600.66,21.04,8.74;3,127.30,599.09,16.40,6.12;3,146.69,600.98,121.92,8.64">where σ : R -→ ∆ C-1 denotes an activation function.</s>
                    <s coords="3,271.71,600.98,17.73,8.64;3,55.44,612.75,234.00,8.82;3,55.44,624.89,234.00,8.64;3,55.44,636.53,122.19,8.96">This modeling choice corresponds to restricting the search space in (1) to a set of labelings which are linearly separable in the representation space ψ(x).</s>
                    <s coords="3,180.72,636.53,109.89,8.96;3,55.44,648.62,234.00,8.82;3,55.44,660.44,235.66,8.96;3,55.44,672.71,234.00,8.64;3,55.44,684.66,235.65,8.64;3,55.44,696.62,235.65,8.64;3,55.44,708.58,32.37,8.64">In addition, obtaining ψ(x) requires task-specific representation learning, i.e., running self-supervised learning on the target dataset D. Since reliable self-supervised pre-training necessitates a large amount of data
                        <ref type="bibr" coords="3,85.93,684.66,86.57,8.64" target="#b74">(Wang &amp; Isola, 2020)</ref>
                        , this prevents successful unsupervised transfer on downstream tasks with limited resources.
                    </s>
                </p>
                <p>
                    <s coords="3,307.44,70.22,163.33,8.96;3,471.90,68.87,20.53,6.05;3,470.77,75.18,3.79,6.12;3,492.92,70.54,50.17,8.64;3,307.44,82.49,234.00,8.64;3,307.44,94.45,64.48,8.64">Given the task encoder parametrization τ HUME θ , HUME optimizes the following objective to search for the underlying human labeling:</s>
                </p>
                <formula xml:id="formula_2" coords="3,332.51,114.52,209.59,21.91">L HUME (θ) = x∈Dte L ce (f approx (x), τ HUME θ (x)), (3)</formula>
                <p>
                    <s coords="3,307.08,146.81,234.36,9.81;3,307.44,157.73,235.65,11.59;3,307.44,172.63,64.44,8.64">where L ce is the cross-entropy loss function and f approx is an approximate solution to f obtained using iterative optimization algorithms.</s>
                    <s coords="3,375.31,172.63,166.13,8.64;3,307.11,184.58,234.33,8.64;3,307.44,196.36,235.65,8.82;3,307.44,208.31,90.08,8.59">HUME resorts to iterative differentiation
                        <ref type="bibr" coords="3,307.11,184.58,61.65,8.64" target="#b20">(Domke, 2012;</ref>
                        <ref type="bibr" coords="3,371.53,184.58,81.67,8.64" target="#b63">Shaban et al., 2019)</ref>
                        to solve the resulting bilevel optimization problem, leading to an expensive overall training
                        procedure.
                    </s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="3." coords="3,307.44,235.29,233.21,10.75">Analysis of Generalization-Based Objective</head>
                <p>
                    <s coords="3,307.13,256.43,234.30,8.64;3,307.44,268.39,234.00,8.64;3,307.44,280.02,235.66,8.96;3,307.44,291.98,220.77,9.81">To understand inductive biases of the generalization-based objective proposed in (1), we consider this objective in case of binary labelings τ (x) : X → {-1, +1} with exponential loss function L exp (f (x), τ (x)) = exp(-τ (x)f (x)).</s>
                    <s coords="3,531.31,292.30,10.14,8.64;3,307.44,303.93,233.29,9.65;3,307.44,315.89,235.25,8.96;3,307.44,327.84,72.94,9.65;3,380.66,326.27,4.71,6.12;3,386.94,327.84,96.26,9.30;3,475.44,327.84,65.99,8.96;3,307.44,339.80,150.78,8.96">To simplify the analysis, we assume that the task encoder τ θ is a linear model in the same representation space ϕ(x), i.e., τ θ (x) = σ(θ T ϕ(x)), where σ : R -→ [-1; 1] is an odd activation function such as tanh.</s>
                    <s coords="3,461.35,340.12,80.10,8.64;3,307.44,351.89,234.00,8.82;3,307.44,363.71,217.14,8.96">This corresponds to restricting the search space in (1) to a set of labelings which are linearly separable in the representation space ϕ(x).</s>
                    <s coords="3,527.68,364.03,15.41,8.64;3,307.44,375.98,235.25,8.64;3,307.44,387.62,234.00,9.65;3,307.44,399.89,234.00,8.64;3,307.44,411.85,58.39,8.64">Additionally, we do not distinguish between train and test splits, i.e., D tr = D te = D. We provide a detailed discussion of the aforementioned assumptions in the remarks at the end of this section.</s>
                </p>
                <p>
                    <s coords="3,307.13,426.83,235.96,11.59;3,307.44,441.74,120.15,8.64">To obtain an approximate solution to f , we use iterative optimization algorithms.</s>
                    <s coords="3,435.09,441.42,106.35,9.65;3,307.44,453.37,235.24,9.65;3,307.44,465.33,157.36,11.15;3,467.17,465.33,17.88,8.74;3,485.32,463.75,4.71,6.12;3,485.05,469.83,7.07,6.12;3,492.62,465.33,49.99,9.65;3,307.44,477.28,175.77,8.96">Specifically, let w m+1 = Ξ(w m , D) denote a one step of an optimization algorithm, i.e., Ξ(w m , D) = w m -η∇ w x∈D L(w T m ϕ(x), τ θ (x)) for the gradient descent with a step size η.</s>
                    <s coords="3,488.25,477.60,53.19,8.64;3,307.44,489.24,39.24,9.65;3,346.68,487.66,14.57,6.12;3,361.75,489.24,179.69,9.65;3,307.44,501.19,108.20,9.65">Similarly, let w M = Ξ (M ) (w 0 , D) denote M steps of an optimization algorithm starting from w 0 .</s>
                    <s coords="3,418.73,501.51,124.36,8.64;3,307.44,513.47,230.43,8.64">Eventually, the above specifications result in the following bilevel optimization problem:</s>
                </p>
                <formula xml:id="formula_3" coords="3,346.24,532.71,195.87,39.09">L binary M (θ) = x∈D exp(-τ θ (x)w T M ϕ(x)) (4) s.t. w M = Ξ (M ) (w 0 , D),
                    <label>(5)</label>
                </formula>
                <p>
                    <s coords="3,307.08,582.87,234.36,8.82;3,307.44,595.00,49.58,8.64">where we refer to (4) and (
                        <ref type="formula" coords="3,415.14,583.05,3.90,8.64" target="#formula_3">5</ref>) as inner and
                        outer objectives respectively.
                    </s>
                </p>
                <p>
                    <s coords="3,307.13,612.93,234.31,8.64;3,307.44,624.89,235.66,8.64;3,307.44,636.67,234.35,8.82;3,307.44,648.80,164.43,8.64">The key observation underlying our main result is that the inner optimization (5) corresponds to the unregularized logistic regression on separable data, allowing us to employ the seminal result by
                        <ref type="bibr" coords="3,391.18,648.80,76.63,8.64" target="#b66">Soudry et al. (2018)</ref>.
                    </s>
                    <s coords="3,474.97,648.80,66.47,8.64;3,307.44,660.75,235.65,8.64;3,307.44,672.71,234.00,8.64;3,307.44,684.66,214.49,8.64">This work shows that gradient descent, when applied to the task of unregularized logistic regression, outputs iterates that are biased towards the direction of the max-margin hyperplane.</s>
                    <s coords="3,525.59,684.66,17.50,8.64;3,307.44,696.30,235.24,9.65;3,307.44,708.58,234.00,8.64;4,55.44,70.22,47.75,8.96">Evidently, the task encoder τ θ generates labelings of D, which, by definition, are linearly separable in the representation space ϕ(x).</s>
                    <s coords="4,108.57,70.22,180.87,9.65;4,55.44,82.17,202.41,9.65">Consequently, w M will follow the direction of max-margin hyperplane for a given labeling τ θ .</s>
                    <s coords="4,260.94,82.49,29.75,8.64;4,55.44,94.45,234.00,8.64;4,55.11,106.08,234.50,9.65;4,55.44,118.04,61.26,9.65;4,116.97,116.47,4.71,6.12;4,116.70,122.82,7.60,6.12;4,125.55,118.04,98.40,9.65">In turn, the last point to observe is that substituting the iterates in (4), the outer objective is minimized when w M has a larger margin τ θ (x)w T M ϕ(x) with respect to τ θ .</s>
                    <s coords="4,229.21,118.36,60.23,8.64;4,55.44,130.31,222.50,8.64;4,55.44,146.20,68.31,8.96">Equipped with this intuition, we are now ready to state our main result: Proposition 3.1.</s>
                    <s coords="4,129.06,146.27,160.38,8.74;4,55.44,158.23,173.98,8.74">Given M ≫ 1, θ ̸ = 0 and appropriate step size η which ensures convergence, then</s>
                </p>
                <formula xml:id="formula_4" coords="4,112.21,178.09,177.90,13.82">L binary M (θ) ≥ g(θ)∥w SVM (θ)∥ 2 2 ,
                    <label>(6)</label>
                </formula>
                <p>
                    <s coords="4,55.44,203.56,23.49,8.59">where</s>
                </p>
                <formula xml:id="formula_5" coords="4,81.15,201.84,209.46,11.23">g(θ) = (M η exp(∥r M (θ)∥ 2 )) -1 , the residual r M (θ)</formula>
                <p>
                    <s coords="4,55.44,215.37,234.00,9.65;4,55.44,227.33,202.63,8.74">is bounded with lim M →∞ ∥r M (θ)∥ 2 = 0, and w SVM (θ) is the solution of the hard-margin SVM for a given θ:</s>
                </p>
                <formula xml:id="formula_6" coords="4,63.26,247.55,226.84,31.56">w SVM (θ) = min w ∥w∥ 2 2 s.t. τ θ (x n )w T ϕ(x n ) ≥ 1 ∀x n ∈ D.
                    <label>(7)</label>
                </formula>
                <p>
                    <s coords="4,54.97,297.78,234.46,8.64;4,55.44,309.73,234.00,8.64;4,55.44,321.37,156.59,9.65">We defer the proof to Appendix A. This result shows that the generalization-based objective upper bounds the norm of hard-margin SVM fitted to a labeling τ θ .</s>
                    <s coords="4,214.99,321.69,76.10,8.64;4,55.44,333.32,39.78,8.96;4,95.22,330.65,17.82,6.05;4,95.22,338.29,7.60,6.12;4,116.03,333.64,173.40,8.64;4,55.11,345.42,220.28,8.82">Consequently, minimizing L binary M will inevitably lead to minimizing the norm (i.e., maximizing the margin) with respect to a labeling.</s>
                    <s coords="4,278.48,345.60,10.96,8.64;4,55.44,357.55,234.00,8.64;4,55.44,369.51,180.41,8.64">As a result, the optimization procedure will yield labelings with large margin of the corresponding classifier.</s>
                    <s coords="4,240.18,369.51,49.43,8.64;4,55.44,381.46,235.25,8.64;4,54.69,393.42,235.99,8.64;4,55.44,405.37,234.00,8.64;4,55.44,417.33,52.72,8.64">Overall, our result unveils that the maximum margin principle
                        <ref type="bibr" coords="4,256.27,381.46,34.42,8.64;4,54.69,393.42,21.32,8.64" target="#b70">(Vapnik, 1995)</ref>
                        , widely employed by supervised learning algorithms, emerges as the inductive bias of the
                        generalization-based objective (1).
                    </s>
                    <s coords="4,55.13,431.27,49.77,8.82">Remark 3.2.</s>
                    <s coords="4,110.06,431.27,107.63,8.59">(Search space restriction).</s>
                    <s coords="4,221.58,431.44,67.86,8.64;4,55.44,443.08,234.00,9.65;4,55.44,455.04,131.26,8.96">The result above holds when labelings generated by τ θ are linearly separable in the representation space ϕ(x).</s>
                    <s coords="4,189.80,455.36,99.64,8.64;4,55.44,467.31,234.00,8.64;4,55.44,479.27,108.71,8.64">This assumption leads to the analysis of the generalization-based objective (1) with the restricted search space.</s>
                    <s coords="4,167.23,479.27,122.21,8.64;4,55.44,491.22,234.00,8.64;4,55.44,503.18,235.65,8.64;4,55.44,515.13,235.74,8.64">
                        <ref type="bibr" coords="4,167.23,479.27,88.94,8.64"
                             target="#b34">Ji &amp; Telgarsky (2019)</ref>
                        showed that in the case of non-separable labelings, gradient descent mirrors the separable case,
                        following the max-margin direction of a maximal linearly separable subset of the data.
                    </s>
                    <s coords="4,55.13,527.09,234.31,8.64;4,55.44,539.04,235.65,8.64;4,55.44,551.00,234.00,8.64;4,55.44,562.95,59.75,8.64">Therefore, one could expect that the lower bound of the generalization-based objective (1) optimized over the complete search space inherits these properties, reflecting the separable case.</s>
                    <s coords="4,55.13,576.89,56.02,8.82">Remark 3.3.</s>
                    <s coords="4,120.48,576.89,130.52,8.59">(Train-test split assumption).</s>
                </p>
                <p>
                    <s coords="4,273.64,577.07,15.80,8.64;4,55.44,589.02,234.00,8.64;4,55.44,600.66,235.66,9.65;4,55.44,612.93,234.00,8.64;4,55.44,624.57,36.17,8.96">The generalization-based objective (1) assumes different train-test splits (D tr , D te ) on the inner-outer levels respectively to obtain an unbiased estimate of the true risk of a model f .</s>
                    <s coords="4,94.70,624.89,194.74,8.64;4,55.44,636.53,99.53,8.96">In our analysis, we simplify this assumption and employ D on both levels.</s>
                    <s coords="4,158.05,636.84,131.38,8.64;4,55.44,648.80,234.00,8.64;4,55.44,660.75,234.00,8.64;4,55.44,672.39,233.99,8.96;4,55.44,684.66,234.17,8.64;4,55.44,696.62,235.25,8.64;4,54.69,708.58,115.94,8.64">Our result shows that minimizing the generalization-based objective in this case leads to maximizing the margin of a linear model with respect to a labeling τ on D. In turn, this will inevitably lead to low error on a held out data given that margin size upper bounds generalization error
                        <ref type="bibr" coords="4,178.69,696.62,111.99,8.64;4,54.69,708.58,22.69,8.64" target="#b4">(Bartlett &amp; Shawe-Taylor, 1999;</ref>
                        <ref type="bibr" coords="4,79.88,708.58,86.47,8.64" target="#b29">Gronlund et al., 2020)</ref>.
                    </s>
                </p>
                <p>
                    <s coords="4,307.13,70.36,47.87,8.82">Remark 3.4.</s>
                    <s coords="4,359.99,70.36,181.63,8.82;4,307.44,82.49,234.00,8.64;4,307.44,94.13,235.74,9.65">(Asymptotic analysis) Proposition 3.1 is rather informal since it substitutes the asymptotic behaviour of the gradient descent iterates w M into the outer objective.</s>
                    <s coords="4,307.08,106.40,234.36,8.64;4,307.44,118.36,235.66,8.64;4,307.44,130.31,234.00,8.64;4,307.44,142.27,185.09,8.64">Although a rigorous analysis of the residual is required to establish exact bounds, these results serve to grasp the inductive bias incorporated in the generalization-based objective designed for the inference of human labelings.</s>
                </p>
                <p>
                    <s coords="4,307.44,162.10,234.00,8.64;4,307.44,174.05,235.65,8.64;4,307.44,186.01,234.00,8.64;4,307.44,197.64,21.92,8.74">In summary, this result shows that optimizing the generalization-based objective (1) yields labelings that induce maximal margin classifiers in the representation space ϕ(x).</s>
                    <s coords="4,334.75,197.96,206.69,8.64;4,307.08,209.92,234.36,8.64;4,307.44,221.87,234.00,8.64;4,307.44,233.83,34.53,8.64">Our main result is greatly inspired by the seminal works
                        <ref type="bibr" coords="4,333.08,209.92,79.38,8.64" target="#b66">(Soudry et al., 2018;</ref>
                        <ref type="bibr" coords="4,414.69,209.92,79.62,8.64"
                             target="#b34">Ji &amp; Telgarsky, 2019</ref>) that reveal the implicit bias of gradient
                        descent towards max-margin solution.
                    </s>
                    <s coords="4,345.09,233.83,198.00,8.64;4,307.44,245.46,234.00,8.96;4,307.08,257.74,234.36,8.64;4,307.44,269.37,234.00,8.96;4,307.44,281.47,220.18,8.59">Likewise, we demonstrate that the generalizationbased objective (1) encourages labelings τ such that if one were to subsequently train a max-margin classifier in the representation space ϕ(x) to fit a labeling τ , the margin obtained would be maximal over all possible labelings.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="4." coords="4,307.44,308.45,123.93,10.75">TURTLE Framework</head>
                <p>
                    <s coords="4,307.13,329.59,234.31,8.64;4,307.13,341.36,235.96,8.82;4,307.44,353.32,231.84,8.82">These insights serve us as a guiding principle to develop TURTLE, a general framework for efficient fully unsupervised transfer given representations of foundation models.</s>
                </p>
                <p>
                    <s coords="4,307.44,371.04,101.75,8.96">Optimization objective.</s>
                    <s coords="4,413.45,371.43,129.64,8.64;4,307.44,383.38,234.00,8.64;4,307.44,395.34,139.22,8.64">Proposition 3.1 provides an important insight on the inductive bias incorporated in the generalization-based objective (1).</s>
                    <s coords="4,449.75,395.34,91.69,8.64;4,307.44,407.29,235.66,8.64;4,307.44,419.25,196.49,8.64">Indeed, one can search for the underlying human labeling by maximizing the margin of a linear model with respect to a labeling.</s>
                    <s coords="4,508.69,419.25,32.75,8.64;4,307.44,431.20,235.65,8.64;4,307.44,442.98,234.00,8.82;4,307.44,454.94,194.05,8.59">Pushing the limits of this principle, we propose to search for a labeling by maximizing margins of linear models in spaces of multiple foundation models at the same time.</s>
                    <s coords="4,505.15,454.80,35.58,8.96;4,307.44,466.75,234.00,9.65;4,307.44,478.71,94.93,8.96">Given K foundation models, let ϕ k (x) be a representation space of k-th foundation model.</s>
                    <s coords="4,407.57,479.03,134.12,8.64;4,307.44,490.66,69.17,9.65;4,376.88,489.09,4.24,6.12;4,376.61,495.45,7.60,6.12;4,388.21,490.66,153.24,8.96;4,307.44,502.62,161.51,9.65">Given labeling defined by a task encoder τ θ , let w k M be k-th linear model trained to fit this labeling in a representation space ϕ k (x).</s>
                    <s coords="4,472.03,502.94,69.41,8.64;4,307.44,514.89,143.81,8.64">Then, TURTLE's optimization objective is as follows:</s>
                </p>
                <formula xml:id="formula_7" coords="4,333.95,535.00,208.16,47.79">L TURTLE M (θ) = K k=1 x∈D L ce (w k M ϕ k (x); τ θ (x)) s.t. w k M = Ξ (M ) (w k 0 , D), ∀k,
                    <label>(8)</label>
                </formula>
                <p>
                    <s coords="4,307.08,594.68,36.50,8.96;4,343.58,593.11,7.60,6.12;4,352.42,594.68,11.01,8.74;4,363.70,593.11,4.24,6.12;4,363.43,599.20,3.97,6.12;4,368.60,594.68,174.50,8.96;4,307.44,606.64,159.75,8.96;4,467.46,605.06,4.24,6.12;4,467.19,611.15,3.97,6.12;4,472.36,606.96,2.48,8.64">where, Ξ M (w k 0 , D) denotes an iterative optimization algorithm Ξ run for M steps starting from w k 0 .</s>
                    <s coords="4,477.93,606.96,63.51,8.64;4,307.44,618.59,235.65,9.65;4,307.44,630.55,234.00,8.96;4,307.44,642.50,96.15,9.65">Intuitively, each of the K terms in the loss function encourages τ θ to maximize margin of k-th linear model in the corresponding representation space ϕ k .</s>
                    <s coords="4,406.69,642.82,136.40,8.64;4,307.44,654.78,234.00,8.64;4,307.44,666.41,234.00,8.96;4,307.44,678.69,60.30,8.64">As opposed to the HUME's objective (3), which maximizes margin only in the single space ψ(x), TURTLE provides more effective guidance to the search process.</s>
                </p>
                <p>
                    <s coords="4,307.11,696.23,131.99,8.96">Task encoder parametrization.</s>
                    <s coords="4,442.20,696.62,99.25,8.64;4,307.44,708.26,234.00,9.65;5,55.44,70.54,222.51,8.64">The parametrization of a task encoder τ θ defines the search space of labelings, thus it has a crucial importance on the optimization process.</s>
                    <s coords="5,281.03,70.54,8.41,8.64;5,55.13,82.49,234.30,8.64;5,55.44,94.13,183.04,9.65">In TURTLE, we employ pre-trained representation spaces of foundation models to define a task encoder τ θ .</s>
                    <s coords="5,241.56,94.45,49.53,8.64;5,55.44,106.22,235.24,8.82;5,55.44,118.36,235.64,8.64">These representations remain fixed during the overall training procedure, alleviating the need of task-specific representation learning.</s>
                </p>
                <p>
                    <s coords="5,55.44,135.97,235.65,9.65;5,55.44,147.93,140.32,9.65">In particular, given K representation spaces ϕ k (x), we define our task encoder τ θ as follows:</s>
                </p>
                <formula xml:id="formula_8"
                         coords="5,117.41,164.72,117.61,30.55">τ TURTLE θ (x) = 1 K K k=1 τ θ k (x),</formula>
                <p>
                    <s coords="5,109.87,201.29,24.34,8.64">where</s>
                </p>
                <formula xml:id="formula_9" coords="5,136.70,183.55,153.41,28.05">τ θ k (x) = σ(θ T k ϕ k (x)),
                    <label>(9)</label>
                </formula>
                <p>
                    <s coords="5,55.44,219.52,85.30,9.65">such that θ = {θ 1 , . . .</s>
                    <s coords="5,142.41,219.52,147.03,9.65;5,55.44,231.48,147.63,8.96">, θ K } denotes all trainable parameters and σ is a softmax activation function.</s>
                    <s coords="5,206.01,231.80,83.60,8.64;5,55.44,243.75,142.23,8.64">After training, cluster assignments are computed as usual:</s>
                </p>
                <formula xml:id="formula_10" coords="5,116.68,260.04,173.43,16.43">arg max c=1,...,C τ TURTLE θ (x) c ,
                    <label>(10)</label>
                </formula>
                <p>
                    <s coords="5,55.08,286.63,35.00,8.96;5,91.21,285.28,26.31,6.05;5,90.08,291.59,3.79,6.12;5,118.02,286.63,13.44,8.74;5,135.62,292.65,3.56,6.12;5,142.17,286.95,147.27,8.64;5,55.44,298.58,110.18,8.96">where τ TURTLE θ (x) c denotes the probability of assigning a sample x to the c-th cluster.</s>
                </p>
                <p>
                    <s coords="5,55.44,316.83,234.00,8.64;5,55.44,328.79,234.35,8.64;5,55.44,340.74,234.00,8.64;5,55.44,352.38,234.00,8.96;5,55.44,364.65,68.87,8.64">Compared to the HUME framework in (2) which searches for the underlying human labeling only over all linearly separable labelings in the self-supervised representation space ψ(x), TURTLE's parametrization greatly expands the search space.</s>
                    <s coords="5,128.01,364.33,163.09,9.65;5,55.44,376.61,234.00,8.64;5,55.44,388.56,234.00,8.64;5,55.44,400.20,147.46,9.65">Indeed, modeling τ θ as a simple ensemble induces the search space which is at least union of all linearly separable labelings in each of the representation spaces of foundation models ϕ 1 , . . .</s>
                    <s coords="5,204.57,400.20,20.58,9.65">, ϕ K .</s>
                    <s coords="5,230.24,400.52,60.86,8.64;5,55.44,412.16,235.25,9.65;5,55.44,424.43,234.00,8.64;5,55.44,436.38,234.35,8.64;5,55.44,448.34,187.66,8.64">One could further suggest employing deeper architectures to model τ θ , however such modeling choice may give rise to tasks that capture spurious correlations in data and do not necessarily reflect human labelings
                        <ref type="bibr" coords="5,154.57,448.34,84.16,8.64" target="#b3">(Atanov et al., 2022)</ref>.
                    </s>
                    <s coords="5,248.09,448.34,42.60,8.64;5,55.44,460.29,234.00,8.64;5,55.44,472.25,234.00,8.64;5,55.44,484.21,177.92,8.64">Therefore, our design choice effectively increases the search space and alleviates the need of task-specific fine-tuning by employing strong representations of foundation models.</s>
                </p>
                <p>
                    <s coords="5,55.44,501.75,66.90,8.96">Regularization.</s>
                    <s coords="5,127.28,502.14,163.82,8.64;5,55.44,513.91,234.00,8.82;5,55.11,526.05,105.90,8.64">The task encoder can synthesize degenerate labelings, i.e., assign all samples to a single class
                        <ref type="bibr" coords="5,55.11,526.05,101.63,8.64"
                             target="#b26">(Gadetsky &amp; Brbić, 2023)</ref>.
                    </s>
                    <s coords="5,164.12,526.05,125.33,8.64;5,55.44,538.00,235.65,8.64;5,55.44,549.96,150.26,8.64">Although such labelings induce linear classifiers with the largest possible margin in all representation spaces, they are irrelevant.</s>
                    <s coords="5,208.81,549.96,80.63,8.64;5,55.44,561.91,234.25,8.64;5,55.44,573.87,34.30,8.64">To avoid such trivial solutions, we separately regularize each term of the task encoder:</s>
                </p>
                <formula xml:id="formula_11" coords="5,130.95,588.96,159.16,30.55">R(θ) = K k=1 H(τ k θ k ),
                    <label>(11)</label>
                </formula>
                <p>
                    <s coords="5,55.08,628.83,24.83,8.64">where</s>
                </p>
                <formula xml:id="formula_12" coords="5,55.44,626.73,235.65,34.64">τ k θ k = (|D|) -1 x∈D τ θ k (x) ∈ ∆ C-1 is an empiri- cal label distribution of k-th component τ θ k and H(•) is the entropy function of discrete distribution.</formula>
                <p>
                    <s coords="5,55.44,670.28,106.97,8.96">Final objective function.</s>
                    <s coords="5,168.49,670.67,122.20,8.64;5,55.13,682.62,235.67,8.64">Putting (
                        <ref type="formula" coords="5,205.29,670.67,3.95,8.64" target="#formula_7">8</ref>) and (
                        <ref type="formula" coords="5,239.06,670.67,8.46,8.64" target="#formula_11">11</ref>) together,
                        TURTLE finally optimizes the following objective function:
                    </s>
                </p>
                <formula xml:id="formula_13" coords="5,119.22,698.91,170.89,16.51">min θ L TURTLE M (θ) -γR(θ),
                    <label>(12)</label>
                </formula>
                <p>
                    <s coords="5,307.08,70.22,234.36,8.96;5,307.44,82.17,136.57,8.96">where we found γ = 10 is a good default choice for the entropy regularization strength γ.</s>
                    <s coords="5,447.98,82.49,93.47,8.64;5,307.44,94.45,143.55,8.64">We show robustness to this hyperparameter in Appendix G.</s>
                </p>
                <p>
                    <s coords="5,307.44,111.99,93.55,8.96">Efficient optimization.</s>
                    <s coords="5,404.09,112.38,139.00,8.64;5,307.44,124.34,234.25,8.64;5,307.44,136.29,40.51,8.64">The new optimization-based objective (
                        <ref type="formula" coords="5,328.97,124.34,3.95,8.64" target="#formula_7">8</ref>) is a bilevel
                        optimization problem with the convex inner part.
                    </s>
                    <s coords="5,351.04,135.97,121.32,9.65;5,472.62,134.40,4.24,6.12;5,472.36,140.76,7.60,6.12;5,483.68,136.29,57.76,8.64;5,307.44,147.93,235.17,9.65;5,307.44,159.88,141.11,9.65">Indeed, given τ θ , computing w k M corresponds to the logistic regression problem on D with labeling τ θ (D) in the k-th representation space ϕ k .</s>
                    <s coords="5,451.66,159.88,89.50,8.96;5,307.44,172.16,234.00,8.64;5,307.44,184.11,38.94,8.64;5,352.08,181.91,4.15,6.12;5,350.07,189.27,7.94,6.12;5,359.42,183.79,6.87,8.74;5,366.29,182.44,26.31,6.05;5,366.29,188.68,7.60,6.12;5,393.10,184.11,2.77,8.64">Learning parameters θ using gradient-based techniques involves computing a total derivative d dθ L TURTLE M :</s>
                </p>
                <formula xml:id="formula_14" coords="5,309.67,205.03,232.43,41.71">d dθ L TURTLE M = ∂ ∂θ L TURTLE M + K k=1 ( ∂w k M ∂θ ) T ∂ ∂w k M L TURTLE M ,
                    <label>(13)</label>
                </formula>
                <p>
                    <s coords="5,307.08,263.24,24.19,8.64">where</s>
                </p>
                <formula xml:id="formula_15" coords="5,334.96,258.23,16.84,9.37">∂w k M</formula>
                <p>
                    <s coords="5,339.55,268.39,8.47,6.12;5,356.53,263.24,184.92,8.64;5,307.44,275.19,196.28,8.64">∂θ is the Jacobian, which is expensive to compute in practice
                        <ref type="bibr" coords="5,353.63,275.19,61.95,8.64" target="#b20">(Domke, 2012;</ref>
                        <ref type="bibr" coords="5,418.63,275.19,80.71,8.64" target="#b63">Shaban et al., 2019)</ref>.
                    </s>
                    <s coords="5,508.50,275.19,33.29,8.64;5,307.44,286.83,234.00,8.96;5,307.44,299.10,234.00,8.64;5,307.44,311.06,111.44,8.64">The key observation is that employing the same set of samples D on both inner and outer levels allows us to discard the second term of the total derivative.</s>
                    <s coords="5,423.19,311.06,86.31,8.64;5,519.53,346.92,22.58,8.64;5,307.44,361.04,235.66,8.64;5,307.44,372.99,215.21,8.64">Indeed, after training
                        <ref type="formula" coords="5,519.53,346.92,18.06,8.64">2020</ref>) have shown a strong
                        performance of this estimator in practice for bilevel optimization problems similar to ours.
                    </s>
                    <s coords="5,525.74,372.99,15.69,8.64;5,307.44,384.95,234.00,8.64;5,307.44,396.90,162.17,8.64">The pseudocode of TURTLE is provided in Algorithm B1 with implementation details in Appendix B.3.</s>
                </p>
                <formula xml:id="formula_16" coords="5,307.44,309.16,234.00,51.54">w k M on D, one can approximate d dθ L TURTLE M ≈ ∂ ∂θ L TURTLE M since w k M is an approximate stationary point of the inner problem for a given τ θ , i.e., ∂ ∂w k M L TURTLE M ≈ 0. Ablin et al. (</formula>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="5." coords="5,307.44,423.70,77.04,10.75">Experiments</head>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="5.1." coords="5,307.44,444.45,100.73,8.96">Experimental setup</head>
                <p>
                    <s coords="5,307.44,463.11,140.58,8.96">Datasets and evaluation metric.</s>
                    <s coords="5,455.49,463.49,87.61,8.64;5,307.44,475.45,234.00,8.64;5,307.44,487.40,123.24,8.64">We study the performance of TURTLE on the extensive benchmark of 26 vision datasets
                        <ref type="bibr" coords="5,341.66,487.40,84.71,8.64" target="#b56">(Radford et al., 2021)</ref>.
                    </s>
                    <s coords="5,433.77,487.40,107.67,8.64;5,307.44,499.36,165.76,8.64">The detailed description of each dataset is provided in Appendix B.1.</s>
                    <s coords="5,476.29,499.36,65.32,8.64;5,307.44,511.31,234.00,8.64;5,307.44,523.27,234.00,8.64;5,307.44,535.23,234.00,8.64;5,307.44,547.18,109.42,8.64">We compare our framework with the baselines using accuracy metric and employ Hungarian algorithm
                        <ref type="bibr" coords="5,430.49,523.27,55.49,8.64" target="#b41">(Kuhn, 1955)</ref>
                        to match the labeling found by TURTLE (10) to the ground truth labeling of a corresponding
                        dataset.
                    </s>
                    <s coords="5,420.02,547.18,121.41,8.64;5,307.44,559.14,234.00,8.64;5,307.44,571.09,105.62,8.64">By default, we train TURTLE on the training split of a corresponding dataset and provide the results on the test split.</s>
                    <s coords="5,416.14,571.09,125.65,8.64;5,307.44,582.87,234.35,8.82;5,307.44,595.00,234.00,8.64;5,307.44,606.96,87.65,8.64">In Appendix H, we additionally show that mimicking deployment regime, i.e., having only test split available for training, does not lead to performance decrease of TURTLE.</s>
                </p>
                <p>
                    <s coords="5,307.44,624.50,138.26,8.96">Foundation models in TURTLE.</s>
                    <s coords="5,448.19,624.89,94.90,8.64;5,307.44,636.84,235.65,8.64;5,307.44,648.48,234.00,8.96;5,307.11,660.44,234.33,8.96;5,307.08,672.71,236.10,8.64">We employ CLIP
                        <ref type="bibr" coords="5,520.32,624.89,18.22,8.64;5,307.44,636.84,65.69,8.64" target="#b56">(Radford et al., 2021)</ref>
                        representations which span different architectures and model sizes, in particular, 5 different
                        ResNets (R50, R101, R50x4, R50x16 and R50x64) and 3 different Vision Transformers (ViT-B/32,
                        ViT-B/16 and ViT-L/14).
                    </s>
                    <s coords="5,306.97,684.66,234.46,8.64;5,307.44,696.30,234.66,8.96;5,307.44,708.58,35.44,8.64">We refer to the TURTLE as TURTLE 1-space if it utilizes only a single space CLIP representation (K = 1 in (
                        <ref type="formula" coords="5,534.20,696.62,3.95,8.64" target="#formula_7">8</ref>) and (
                        <ref type="formula" coords="5,328.67,708.58,3.55,8.64" target="#formula_9">9</ref>)).
                    </s>
                    <s coords="5,347.50,708.58,193.94,8.64;6,55.44,70.54,182.28,8.64">We refer to the TURTLE as TURTLE 2-spaces if it utilizes two different foundation models.</s>
                    <s coords="6,240.96,70.54,48.47,8.64;6,55.44,82.49,234.00,8.64;6,55.44,94.45,234.00,8.64;6,55.44,106.40,85.95,8.64">Namely, we use DINOv2 ViT-g/14
                        <ref type="bibr" coords="6,149.37,82.49,82.45,8.64" target="#b53">(Oquab et al., 2023)</ref>
                        as the second space while the first space is always represented with one of the CLIP variants.
                    </s>
                    <s coords="6,145.59,106.40,145.51,8.64;6,55.44,118.36,234.00,8.64;6,55.44,130.13,234.00,8.82;6,55.44,142.27,77.06,8.64">Consequently, to specify the particular CLIP architecture when utilizing two representation spaces, e.g., ViT-L/14, we refer to TURTLE as TURTLE 2-spaces ViT-L/14.</s>
                    <s coords="6,135.60,142.27,154.01,8.64;6,55.44,154.22,234.00,8.64;6,55.44,166.18,148.58,8.64">We precompute all representations for the entire benchmark and keep these representations fixed during the overall training procedure.</s>
                    <s coords="6,207.14,166.18,83.96,8.64;6,55.44,178.13,234.00,8.64;6,55.44,190.09,183.85,8.64">The detailed description of the used models and other specifications to prepare representations are provided in Appendix B.2.</s>
                </p>
                <p>
                    <s coords="6,55.44,207.63,42.42,8.96">Baselines.</s>
                    <s coords="6,100.94,208.02,190.16,8.64;6,55.13,219.98,234.31,8.64;6,55.44,231.93,77.62,8.64">We compare unsupervised transfer using TUR-TLE to baselines that differ in the amount of supervision they use (Figure
                        <ref type="figure" coords="6,122.31,231.93,3.58,8.64">1</ref>).
                    </s>
                    <s coords="6,136.17,231.93,153.27,8.64;6,55.11,243.89,234.33,8.64;6,55.44,255.84,235.66,8.64;6,55.44,267.80,235.65,8.64;6,55.44,279.75,234.83,8.64;6,55.44,291.71,77.14,8.64">First, we compare TURTLE to HUME
                        <ref type="bibr" coords="6,55.11,243.89,99.17,8.64"
                             target="#b26">(Gadetsky &amp; Brbić, 2023)</ref>, a method that has recently shown
                        state-of-the-art unsupervised learning performance and surpassed traditional deep clustering
                        approaches
                        <ref type="bibr" coords="6,244.56,267.80,41.68,8.64;6,55.44,279.75,73.10,8.64" target="#b69">(Van Gansbeke et al., 2020;</ref>
                        <ref type="bibr" coords="6,132.24,279.75,69.26,8.64" target="#b52">Niu et al., 2022;</ref>
                        <ref type="bibr" coords="6,205.21,279.75,85.06,8.64" target="#b2">Amrani et al., 2022;</ref>
                        <ref type="bibr" coords="6,55.44,291.71,72.76,8.64" target="#b24">Feng et al., 2023)</ref>.
                    </s>
                    <s coords="6,139.01,291.71,150.43,8.64;6,55.08,303.66,236.01,8.64;6,55.44,315.62,234.00,8.64;6,55.44,327.57,133.84,8.64">Next, to explore how far can we go with unsupervised transfer, we compare TURTLE in a challenging setting to zero-shot transfer, unsupervised prompt tuning and supervised baselines.</s>
                    <s coords="6,194.83,327.57,94.61,8.64;6,55.44,339.53,234.00,8.64;6,55.44,351.48,76.14,8.64">All these baselines use some form of supervision compared to TURTLE which is fully unsupervised.</s>
                    <s coords="6,134.67,351.48,154.77,8.64;6,55.44,363.44,234.00,8.64;6,55.44,375.39,235.75,8.64">We start by comparing TURTLE to the CLIP zero-shot transfer
                        <ref type="bibr" coords="6,151.09,363.44,85.26,8.64" target="#b56">(Radford et al., 2021)</ref>
                        that employs descriptions of ground truth classes as a form of supervision.
                    </s>
                    <s coords="6,55.44,387.35,235.65,8.64;6,55.44,399.30,234.17,8.64;6,55.44,411.26,66.64,8.64">Following
                        <ref type="bibr" coords="6,98.71,387.35,84.65,8.64" target="#b56">(Radford et al., 2021)</ref>,
                        we perform prompt engineering and ensembling to construct a zero-shot classifier for each
                        dataset.
                    </s>
                    <s coords="6,125.83,411.26,163.61,8.64;6,55.13,423.22,234.30,8.64;6,55.44,435.17,235.24,8.64;6,55.44,447.13,154.19,8.64">As even stronger baselines, we compare TURTLE to the state-of-the-art unsupervised prompt tuning methods UPL
                        <ref type="bibr" coords="6,112.01,435.17,76.81,8.64" target="#b32">(Huang et al., 2022)</ref>,
                        POUF
                        <ref type="bibr" coords="6,222.98,435.17,67.70,8.64;6,55.44,447.13,23.71,8.64" target="#b68">(Tanwisuth et al., 2023)</ref>
                        and GDA
                        <ref type="bibr" coords="6,126.11,447.13,79.15,8.64" target="#b75">(Wang et al., 2024)</ref>.
                    </s>
                    <s coords="6,215.91,447.13,73.53,8.64;6,55.44,459.08,234.00,8.64;6,55.44,471.04,234.00,8.64;6,55.44,482.99,18.73,8.64">These approaches enhance class prototypes defined by the CLIP zero-shot classifier via unsupervised adaptation on the downstream task.</s>
                    <s coords="6,77.25,482.99,212.18,8.64;6,55.44,494.95,234.17,8.64;6,55.44,506.90,35.84,8.64">Finally, we employ supervised linear probe on top of the CLIP representations to serve as a supervised transfer baseline.</s>
                    <s coords="6,96.36,506.90,194.73,8.64;6,55.44,518.86,72.26,8.64">Differences between types of transfer are highlighted in Table
                        <ref type="table" coords="6,120.23,518.86,3.74,8.64" target="#tab_0">1</ref>.
                    </s>
                    <s coords="7,105.53,312.00,97.79,7.77">
                        <ref type="table" coords="7,105.53,312.00,8.97,7.77" target="#tab_5">C2</ref>) and omitted for
                        clarity.
                    </s>
                </p>
                <p>
                    <s coords="7,55.44,335.99,149.58,8.96">Comparison to zero-shot transfer.</s>
                    <s coords="7,211.71,336.38,79.38,8.64;7,55.13,348.33,234.31,8.64;7,55.44,360.29,181.46,8.64">We compare TUR-TLE to the CLIP zero-shot transfer that uses descriptions of ground truth classes as a form of supervision.</s>
                    <s coords="7,239.99,360.29,50.69,8.64;7,55.08,372.24,236.01,8.64;7,55.44,384.20,234.00,8.64;7,55.44,396.15,233.99,8.64;7,55.11,408.11,42.61,8.64">Remarkably, without using any supervision, TURTLE 2-spaces outperforms the zero-shot transfer of CLIP by a large margin across 26 benchmark datasets for different ViT backbones (Figure
                        <ref type="figure" coords="7,86.93,408.11,3.60,8.64" target="#fig_1">4</ref>).
                    </s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="7,77.71,551.01,31.14,14.43">ViT-B/32</head>
                <p>
                    <s coords="7,165.68,551.01,11.68,14.43;7,55.44,660.75,235.65,8.64;7,55.44,672.39,234.42,8.96;7,55.44,684.35,234.00,8.96;7,55.08,696.62,132.23,8.64">ViT  In particular, TURTLE 2-spaces outperforms CLIP zeroshot by 9%, 7% and 4% absolute improvement (17%, 12% and 5% relative improvement) with ViT-B/32, ViT-B/16 and ViT-L/14 backbones, respectively.</s>
                    <s coords="7,190.24,696.62,99.21,8.64;7,54.69,708.58,234.74,8.64;7,307.44,439.58,89.55,8.64">Moreover, even TURTLE 1-space matches the performance of CLIP zero-shot across  all studied ViT models.</s>
                    <s coords="7,399.89,439.58,141.55,8.64;7,307.44,451.54,234.00,8.64;7,307.44,463.32,234.00,8.82;7,307.44,475.27,207.77,8.82">It is important to note that both CLIP zero-shot and TURTLE 1-space are linear models in the same representation space and differ only in the amount of supervision which is available to produce the weights.</s>
                    <s coords="7,518.12,475.45,23.31,8.64;7,307.44,487.40,234.00,8.64;7,307.44,499.04,234.00,8.96;7,307.08,511.00,234.36,8.96;7,307.44,523.27,234.35,8.64;7,307.11,535.23,43.60,8.64">When comparing performance on individual datasets, TURTLE outperforms CLIP zero-shot transfer on 15 out of 26 datasets with remarkable absolute gains of 35%, 21% and 20% on the EuroSAT, MNIST and Flowers102 datasets, respectively (Figure
                        <ref type="figure" coords="7,339.71,535.23,3.67,8.64" target="#fig_2">5</ref>).
                    </s>
                    <s coords="7,354.40,535.23,187.05,8.64;7,307.44,547.18,176.01,8.64">We provide individual scores for all TURTLE and CLIP zero-shot variants in Appendix D.</s>
                </p>
                <p>
                    <s coords="7,307.44,564.72,193.56,8.96">Comparison to unsupervised prompt tuning.</s>
                    <s coords="7,504.70,565.11,36.73,8.64;7,307.44,577.07,235.75,8.64">Next, we compare TURTLE to unsupervised prompt tuning baselines.</s>
                    <s coords="7,306.97,589.02,236.13,8.64;7,307.44,600.98,107.93,8.64">We follow previous works and use CLIP ResNet-50 representations for all methods.</s>
                    <s coords="7,419.28,600.98,123.81,8.64;7,307.19,612.93,234.25,8.64;7,307.44,624.89,154.77,8.64">Although being fully unsupervised, TURTLE consistently outperforms all the considered baselines by a large margin (Table
                        <ref type="table" coords="7,451.21,624.89,3.67,8.64" target="#tab_3">2</ref>).
                    </s>
                    <s coords="7,466.51,624.89,76.58,8.64;7,307.13,636.53,235.96,8.96;7,307.44,648.80,234.00,8.64;7,307.44,660.75,96.16,8.64">Specifically, TUR-TLE achieves 8% absolute improvement (12% relative improvement) in average accuracy over the best unsupervised prompt tuning baseline.</s>
                    <s coords="7,406.69,660.75,135.06,8.64;7,307.44,672.71,234.00,8.64;7,307.44,684.35,235.65,8.96;7,307.44,696.62,41.29,8.64">On the Flowers102 and EuroSAT datasets, our framework attains outstanding absolute gains of 27% and 41% (37% and 75% relative improvement), respectively.</s>
                    <s coords="7,351.85,696.62,189.59,8.64;7,307.44,708.58,166.73,8.64">Overall, these results demonstrate the surprising effectiveness of the unsupervised transfer.</s>
                    <s coords="8,55.44,203.35,149.75,8.96">Comparison to supervised transfer.</s>
                    <s coords="8,208.30,203.74,81.15,8.64;8,55.13,215.69,234.31,8.64;8,55.44,227.65,119.04,8.64">Finally, we compare TURTLE 1-space ViT-L/14 to supervised linear probe in the same representation space.</s>
                    <s coords="8,177.59,227.65,111.86,8.64;8,55.44,239.60,234.00,8.64;8,55.08,251.56,236.01,8.64;8,55.44,263.51,120.25,8.64">This means that in this setup both models are linear in the representation space of CLIP ViT-L/14 and differ only in the amount of supervision utilized to produce the weights.</s>
                    <s coords="8,181.26,263.51,108.17,8.64;8,55.44,275.47,136.89,8.64">Supervised linear probe is trained using all available labels.</s>
                    <s coords="8,198.45,275.47,90.99,8.64;8,55.44,287.42,235.65,8.64;8,55.44,299.38,235.75,8.64">Consequently, we can assume that it represents the maximal transfer learning performance that can be achieved by the unsupervised transfer.</s>
                </p>
                <p>
                    <s coords="8,54.97,316.99,234.46,8.96;8,55.44,328.95,21.14,8.74;8,76.58,327.37,10.20,6.12;8,87.28,329.26,202.16,8.64;8,55.44,341.22,180.73,8.64">We observe a high positive correlation of 0.87 (p-value &lt; 10 -8 ) between unsupervised transfer performance and its fully supervised counterpart (Figure
                        <ref type="figure" coords="8,225.16,341.22,3.67,8.64" target="#fig_3">6</ref>).
                    </s>
                    <s coords="8,244.54,341.22,44.90,8.64;8,55.44,353.17,235.65,8.64;8,55.44,365.13,234.00,8.64;8,55.08,377.08,199.21,8.64">This result indicates that with better supervised linear probe performance, TURTLE's performance may also increase, which we further investigate in the subsequent paragraph.</s>
                    <s coords="8,257.40,377.08,33.29,8.64;8,55.13,389.04,234.31,8.64;8,55.44,401.00,235.65,8.64;8,55.44,412.77,234.00,8.82;8,55.44,424.91,234.00,8.64;8,55.44,436.86,106.38,8.64">Notably, TURTLE approaches the "optimal" transfer performance on the STL10, CIFAR10, Flowers102, Food101 and Hateful-Memes, demonstrating that labels may not be needed when given sufficiently high-quality representations, as measured by supervised linear probe.</s>
                    <s coords="8,164.92,436.86,124.70,8.64;8,55.13,448.82,234.31,8.64;8,55.44,460.77,234.00,8.64;8,55.44,472.73,99.05,8.64">We perform similar analysis for TURTLE 2-spaces and observe stronger correlation, leading to reduced gap between TURTLE 2-spaces and supervised linear probe (Figure
                        <ref type="figure" coords="8,137.61,472.73,8.44,8.64" target="#fig_8">E2</ref>).
                    </s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="8,55.08,490.27,236.10,8.96">Ablation of different representation spaces on ImageNet.</head>
                <p>
                    <s coords="8,55.44,502.61,235.65,8.64;8,55.44,514.57,234.00,8.64;8,55.44,526.52,152.61,8.64">Results from the previous paragraph speculate that incorporating stronger representations may lead to the increased performance of unsupervised transfer.</s>
                    <s coords="8,211.14,526.52,78.30,8.64;8,55.44,538.48,234.00,8.64;8,55.44,550.43,201.32,8.64">To validate this, we run TURTLE with pairs of different representation spaces on the ImageNet-1000 dataset
                        <ref type="bibr" coords="8,178.69,550.43,73.74,8.64" target="#b19">(Deng et al., 2009)</ref>.
                    </s>
                    <s coords="8,259.83,550.43,29.60,8.64;8,55.44,562.07,234.00,8.96;8,55.44,574.03,21.14,8.74;8,76.58,572.45,10.20,6.12;8,87.28,574.35,202.16,8.64;8,55.44,586.30,234.17,8.64;8,55.44,598.26,24.67,8.64">Results in Figure
                        <ref type="figure" coords="8,97.25,562.39,5.08,8.64" target="#fig_4">7</ref>
                        show a positive correlation of 0.74 (p-value &lt; 10 -8 ) between unsupervised transfer
                        performance and the quality of representations measured by supervised linear probe.
                    </s>
                    <s coords="8,83.12,598.26,206.49,8.64;8,55.44,610.21,235.66,8.64;8,55.44,622.17,90.22,8.64">The obtained result confirms that employing stronger representations for a given dataset leads to the improved performance of TURTLE.</s>
                    <s coords="8,148.17,622.17,141.44,8.64;8,55.44,634.12,234.00,8.64;8,55.44,646.08,154.91,8.64">Consequently, TURTLE can further improve performance by exploiting continual progress in the development of foundation models.</s>
                    <s coords="8,213.44,646.08,76.00,8.64;8,55.44,658.03,234.00,8.64;8,55.44,669.99,234.00,8.64;8,55.44,681.94,234.00,8.64;8,55.44,693.90,234.00,8.64;8,55.44,705.85,18.54,8.64">Furthermore, given high positive correlation between TURTLE's accuracy and the generalization-based objective (Figure
                        <ref type="figure" coords="8,230.31,669.99,8.89,8.64" target="#fig_5">B1</ref>), TURTLE can be
                        utilized as the proxy to measure the quality of given representations in the absence of labels
                        for the downstream task.
                    </s>
                    <s coords="8,514.52,413.69,26.92,7.77;8,307.44,424.36,213.65,8.06">Dashed line y = x denotes the "optimal" unsupervised transfer.</s>
                    <s coords="8,527.22,424.65,14.22,7.77;8,307.44,435.61,234.00,7.77;8,307.44,446.28,149.20,8.06;8,456.64,444.52,9.41,5.24;8,468.28,446.57,73.16,7.77;8,307.44,457.53,85.89,7.77">The performance of TURTLE and supervised linear probe shows a strong correlation (ρ = 0.87, p = 6.3×10 -9 of two-sided Pearson correlation coefficient).</s>
                    <s coords="8,397.07,457.24,144.37,8.06;8,307.44,468.20,234.00,8.06;8,307.44,479.45,41.34,7.77">On 5 datasets TURTLE approaches the performance of the "optimal" unsupervised transfer (≤ 3 point difference).</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="6." coords="8,307.44,502.13,83.11,10.75">Related Work</head>
                <p>
                    <s coords="8,307.11,522.88,125.30,8.96">(Weakly) supervised transfer.</s>
                    <s coords="8,435.50,523.27,107.59,8.64;8,307.44,535.23,234.00,8.64;8,307.44,547.18,128.88,8.64">(Weakly) supervised transfer approaches require at least some amount of supervision to perform downstream transfer.</s>
                    <s coords="8,439.41,547.18,102.20,8.64;8,307.11,559.14,234.33,8.64;8,307.44,571.09,234.35,8.64;8,307.44,583.05,234.00,8.64;8,307.44,595.00,34.56,8.64">For instance, BigTransfer
                        <ref type="bibr" coords="8,307.11,559.14,96.31,8.64"
                             target="#b38">(Kolesnikov et al., 2020)</ref>
                        showed that supervised fine-tuning of the entire model after large-scale pre-training
                        successfully transfers knowledge in both fully supervised and few-shot regimes.
                    </s>
                    <s coords="8,345.36,595.00,196.07,8.64;8,307.44,606.96,235.25,8.64;8,307.44,618.91,234.17,8.64;8,307.44,630.87,234.00,8.64;8,307.44,642.82,139.61,8.64">Recent advances in self-supervised learning
                        <ref type="bibr" coords="8,526.20,595.00,15.23,8.64;8,307.44,606.96,47.22,8.64" target="#b30">(He et al., 2022;</ref>
                        <ref type="bibr" coords="8,357.15,606.96,58.61,8.64" target="#b44">Li et al., 2022;</ref>
                        <ref type="bibr" coords="8,418.26,606.96,70.83,8.64" target="#b83">Zhou et al., 2022;</ref>
                        <ref type="bibr" coords="8,491.59,606.96,51.10,8.64;8,307.44,618.91,23.15,8.64" target="#b53">Oquab et al., 2023;</ref>
                        <ref type="bibr" coords="8,333.25,618.91,78.54,8.64" target="#b18">Darcet et al., 2024)</ref>
                        have demonstrated that a linear probe suffices to achieve competitive performance compared to
                        the fine-tuning the entire model.
                    </s>
                    <s coords="8,450.13,642.82,91.30,8.64;8,307.44,654.78,235.66,8.64;8,307.44,666.73,105.16,8.64">Despite the strength of these approaches, they necessitate labeled examples to perform downstream transfer.</s>
                </p>
                <p>
                    <s coords="8,307.44,684.28,78.06,8.96">Zero-shot transfer.</s>
                    <s coords="8,388.62,684.66,154.48,8.64;8,307.44,696.62,235.25,8.64;8,307.08,708.58,236.01,8.64;9,55.44,349.92,234.17,8.64;9,55.44,361.88,121.72,8.64">Foundation models such as CLIP
                        <ref type="bibr" coords="8,520.87,684.66,17.78,8.64;8,307.44,696.62,68.15,8.64" target="#b56">(Radford et al., 2021)</ref>
                        have recently enabled zero-shot transfer, which relies only on a set of human instructions such
                        as de- scriptions of visual categories that appear in the data rather than a set of labeled
                        examples.
                    </s>
                    <s coords="9,180.27,361.88,110.83,8.64;9,55.44,373.83,234.82,8.64;9,55.44,385.79,235.25,8.64;9,55.44,397.74,234.00,8.64;9,55.44,409.70,235.66,8.64;9,55.44,421.65,41.52,8.64">Despite the success of zeroshot transfer in different domains
                        <ref type="bibr" coords="9,191.70,373.83,89.76,8.64">(Elizalde et al., 2023a;</ref>
                        <ref type="bibr" coords="9,281.46,373.83,8.80,8.64" target="#b48">b;</ref>
                        <ref type="bibr" coords="9,55.44,385.79,61.62,8.64" target="#b45">Lin et al., 2023;</ref>
                        <ref type="bibr" coords="9,119.34,385.79,85.49,8.64" target="#b58">Robinson et al., 2023;</ref>
                        <ref type="bibr" coords="9,207.10,385.79,79.38,8.64" target="#b50">Meidani et al., 2024)</ref>,
                        collecting zero-shot annotations still requires expert domain knowledge which can be hard to get
                        in many real-world applications.
                    </s>
                    <s coords="9,100.04,421.65,190.64,8.64;9,55.13,433.43,234.66,8.82;9,55.44,445.56,175.85,8.64">In contrast to the zero-shot transfer approaches, TURTLE enables fully unsupervised transfer, effectively alleviating the need of any human guidance.</s>
                </p>
                <p>
                    <s coords="9,55.44,463.11,67.00,8.96">Deep clustering.</s>
                    <s coords="9,125.48,463.49,164.79,8.64;9,55.44,475.45,235.25,8.64;9,55.44,487.40,235.65,8.64;9,55.44,499.36,203.69,8.64">Deep clustering methods
                        <ref type="bibr" coords="9,224.68,463.49,65.59,8.64" target="#b77">(Xie et al., 2016;</ref>
                        <ref type="bibr" coords="9,55.44,475.45,75.33,8.64" target="#b11">Chang et al., 2017;</ref>
                        <ref type="bibr" coords="9,133.25,475.45,73.67,8.64" target="#b9">Caron et al., 2018;</ref>
                        <ref type="bibr" coords="9,209.41,475.45,81.28,8.64;9,55.44,487.40,23.15,8.64" target="#b69">Van Gansbeke et al., 2020;</ref>
                        <ref type="bibr" coords="9,81.38,487.40,67.09,8.64" target="#b52">Niu et al., 2022)</ref>
                        aim to jointly perform deep representation learning and clustering on a target dataset.
                    </s>
                    <s coords="9,262.22,499.36,27.22,8.64;9,55.44,511.31,234.00,8.64;9,55.44,523.27,234.00,8.64;9,55.44,535.23,235.65,8.64;9,55.44,547.18,189.76,8.64">Recent state-of-the-art approaches
                        <ref type="bibr" coords="9,163.28,511.31,108.90,8.64"
                             target="#b69">(Van Gansbeke et al., 2020;</ref>
                        <ref type="bibr" coords="9,274.66,511.31,14.78,8.64;9,55.44,523.27,46.89,8.64" target="#b52">Niu et al., 2022)</ref>
                        rely on time-consuming three-stage procedures that involve self-supervised representation
                        learning, clustering and fine-tuning via self-labeling respectively.
                    </s>
                    <s coords="9,248.19,547.18,41.25,8.64;9,55.44,559.14,234.00,8.64;9,55.44,571.09,234.35,8.64;9,55.44,583.05,234.00,8.64;9,55.44,595.00,30.71,8.64">In contrast to the deep clustering approaches, TURTLE alleviates the need for laborious task-specific representation learning by employing representation spaces of pre-trained foundation models.</s>
                    <s coords="9,89.24,595.00,200.19,8.64;9,55.44,606.96,235.65,8.64;9,55.44,618.91,234.00,8.64;9,55.44,630.87,235.65,8.64;9,55.44,642.82,154.39,8.64">Furthermore, compared to deep clustering methods that heavily depend on image augmentations to induce semantically meaningful clusters, TURTLE builds upon the seminal maximum margin principle that is effortlessly applicable beyond image data modality.</s>
                    <s coords="9,214.95,642.82,74.66,8.64;9,55.44,654.78,234.00,8.64;9,55.44,666.73,208.08,8.64">Consequently, our approach offers an efficient and effective way to perform fully unsupervised transfer from foundation models.</s>
                </p>
                <p>
                    <s coords="9,55.44,684.28,123.97,8.96">Maximum margin clustering.</s>
                    <s coords="9,182.51,684.66,106.93,8.64;9,55.44,696.62,234.00,8.64;9,55.44,708.58,234.00,8.64;9,307.44,70.54,234.17,8.64;9,307.44,82.49,160.55,8.64">Our work has revealed that optimizing the generalization-based objective proposed in
                        <ref type="bibr" coords="9,55.44,708.58,99.49,8.64"
                             target="#b26">Gadetsky &amp; Brbić (2023)</ref>
                        results in the search for a labeling that maximizes the margin of a maximal margin classifier
                        over all possible labelings of a dataset.
                    </s>
                    <s coords="9,473.05,82.49,68.39,8.64;9,307.44,94.45,235.65,8.64;9,307.44,106.40,234.67,8.64;9,307.11,118.36,72.46,8.64">The first attempt to employ maximum margin principle to perform clustering dates back to Maximum Margin Clustering (MMC)
                        <ref type="bibr" coords="9,307.11,118.36,68.09,8.64" target="#b79">(Xu et al., 2004)</ref>.
                    </s>
                    <s coords="9,385.85,118.36,155.84,8.64;9,307.44,130.31,234.00,8.64;9,307.44,142.27,234.17,8.64;9,307.44,154.22,234.83,8.64;9,306.97,166.18,81.25,8.64">Later works extended this framework to multi-class clustering
                        <ref type="bibr" coords="9,407.87,130.31,107.48,8.64"
                             target="#b78">(Xu &amp; Schuurmans, 2005;</ref>
                        <ref type="bibr" coords="9,517.97,130.31,23.46,8.64;9,307.44,142.27,47.16,8.64" target="#b73">Wang et al., 2010)</ref>
                        , multi-view clustering
                        <ref type="bibr" coords="9,451.76,142.27,74.18,8.64" target="#b82">(Zhao et al., 2009)</ref>, or
                        focused on improving the scalability
                        <ref type="bibr" coords="9,460.85,154.22,81.42,8.64" target="#b81">(Zhang et al., 2007;</ref>
                        <ref type="bibr" coords="9,306.97,166.18,76.88,8.64" target="#b73">Wang et al., 2010)</ref>.
                    </s>
                    <s coords="9,395.61,166.18,147.48,8.64;9,307.44,178.13,235.25,8.64;9,307.44,190.09,235.65,8.64;9,307.44,202.04,124.17,8.64">Compared to TURTLE, which employs efficient first-order gradient optimization techniques, the aforementioned approaches rely on the expensive discrete optimization techniques.</s>
                    <s coords="9,437.79,202.04,103.65,8.64;9,307.44,214.00,234.00,8.64;9,307.08,225.96,234.36,8.64;9,307.13,237.91,234.31,8.64;9,307.44,249.87,134.45,8.64">Furthermore, each of the approaches adopts maximum margin principle in its own way to enable multi-class or multi-space scenario, while TURTLE provides a unified framework for any number of classes and representation spaces.</s>
                </p>
                <p>
                    <s coords="9,307.44,267.41,171.99,8.96">Implicit bias of optimization algorithms.</s>
                    <s coords="9,482.55,267.80,58.89,8.64;9,307.44,279.75,234.00,8.64;9,307.44,291.71,139.16,8.64">Understanding the implicit bias of optimization algorithms plays a crucial role in modern machine learning.</s>
                    <s coords="9,452.83,291.71,88.97,8.64;9,307.44,303.66,234.00,8.64;9,307.44,315.62,234.00,8.64;9,307.44,327.57,234.00,8.64;9,307.44,339.53,234.00,8.64;9,307.44,351.48,58.43,8.64">The seminal work by
                        <ref type="bibr" coords="9,307.44,303.66,78.18,8.64" target="#b66">Soudry et al. (2018)</ref>
                        showed that the gradient descent, when applied to the task of unregularized logistic regression
                        on separable data, converges to the direction of the maximal margin hyperplane without
                        explicitly enforcing such margin maximization.
                    </s>
                    <s coords="9,370.09,351.48,171.35,8.64;9,307.44,363.44,234.00,8.64;9,307.44,375.39,166.78,8.64">Later, Ji &amp; Telgarsky (
                        <ref type="formula" coords="9,464.18,351.48,18.06,8.64">2019</ref>) extended the analysis and
                        demonstrated a similar behavior of gradient descent in the case of non-separable data.
                    </s>
                    <s coords="9,477.31,375.39,64.13,8.64;9,307.44,387.35,233.99,8.64;9,307.44,399.30,166.22,8.64">In our work, we employ the aforementioned findings to study the inductive bias of the generalization-based objective.</s>
                    <s coords="9,476.74,399.30,64.69,8.64;9,307.44,411.26,234.00,8.64;9,307.44,423.22,212.49,8.64">Surprisingly, we reveal that it yields labelings that maximize the margin of a maximal margin classifier with respect to labeling.</s>
                    <s coords="9,523.12,423.22,18.32,8.64;9,307.44,435.17,234.00,8.64;9,307.44,447.13,235.65,8.64;9,307.44,459.08,109.59,8.64">As a result, this insight allows us to develop TURTLE, a method that enables fully unsupervised transfer given representations of foundation models.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="7." coords="9,307.44,485.88,69.09,10.75">Conclusion</head>
                <p>
                    <s coords="9,307.44,507.02,235.66,8.64;9,307.44,518.97,234.35,8.64;9,307.44,530.93,86.04,8.64">In this work, we have shown that the representations of foundation models can be utilized to solve a new task in a fully unsupervised manner.</s>
                    <s coords="9,396.60,530.93,144.84,8.64;9,307.44,542.88,235.66,8.64;9,307.44,554.84,218.34,8.64">The key insight behind our approach is to search for a labeling that induces maximal margin classifiers in the representation spaces of foundation models.</s>
                    <s coords="9,528.69,554.84,12.75,8.64;9,307.44,566.79,234.25,8.64;9,307.44,578.75,234.00,8.64;9,307.44,590.70,112.90,8.64">We utilize this insight to develop TURTLE, a general framework for effective unsupervised transfer given representations of different foundation models.</s>
                    <s coords="9,423.43,590.70,119.25,8.64;9,307.08,602.66,234.36,8.64;9,307.44,614.62,234.34,8.64;9,307.44,626.57,180.34,8.64">Through extensive evaluation, we found that TURTLE, being fully unsupervised, achieves competitive performance compared to zero-shot transfer by employing only a single representation space.</s>
                    <s coords="9,490.86,626.57,51.82,8.64;9,307.44,638.53,235.65,8.64;9,307.44,650.48,152.67,8.64">Furthermore, utilizing an additional representation space results in remarkable gains over zero-shot transfer.</s>
                    <s coords="9,463.20,650.48,78.58,8.64;9,307.44,662.44,234.00,8.64;9,307.44,674.39,234.35,8.64;9,307.44,686.35,234.00,8.64;9,307.44,698.30,118.25,8.64">Given the flexibility of our framework, the results also suggest that TURTLE can deliver even better unsupervised transfer performance by taking advantage of new more powerful foundation models that will emerge in the future.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="10,55.44,176.49,91.30,10.75">Impact Statement</head>
                <p>
                    <s coords="10,55.08,197.62,234.35,8.64;10,55.44,209.58,234.00,8.64;10,55.44,221.54,175.67,8.64">Although the main goal of our work is to advance the field of Machine Learning, the proposed framework relies on representation spaces of foundation models.</s>
                    <s coords="10,234.22,221.54,55.23,8.64;10,55.44,233.49,234.00,8.64;10,55.44,245.45,153.88,8.64">These models inherit biases embedded in the data on which they were trained on
                        <ref type="bibr" coords="10,100.84,245.45,104.10,8.64"
                             target="#b7">(Bommasani et al., 2022)</ref>.
                    </s>
                    <s coords="10,215.47,245.45,73.97,8.64;10,55.44,257.40,234.00,8.64;10,55.44,269.36,230.91,8.64">Consequently, the extensive evaluation and alignment is recommended when deploying TURTLE to critical use-cases such as medicine.</s>
                </p>
                <p>
                    <s coords="14,55.44,68.77,135.72,10.75">A. Proof of Proposition 3.1</s>
                </p>
                <p>
                    <s coords="14,55.44,89.91,486.00,8.64;14,55.44,101.87,146.77,8.64">Here, we first provide the simplified version of the main results from
                        <ref type="bibr" coords="14,346.09,89.91,83.48,8.64" target="#b66">Soudry et al. (2018)</ref>
                        for completeness and then present the proof of Proposition 3.1.
                    </s>
                    <s coords="14,205.30,101.55,336.14,9.65;14,55.44,113.50,174.27,9.65">For clarity, we overload notation for x n and consider x n is already represented in a representation space ϕ(x), i.e., x n = ϕ(x n ).</s>
                    <s coords="14,232.77,113.50,291.97,9.65;14,524.74,111.93,6.31,6.12;14,524.74,118.02,15.01,6.12;14,540.25,113.82,2.44,8.64;14,55.44,125.46,166.32,8.96">Given binary labeling function τ (x) ∈ {-1, +1} of the dataset D = {x n } N n=1 , let L(w) be the exponential loss function:</s>
                </p>
                <formula xml:id="formula_17" coords="14,55.08,147.48,488.10,53.85">L(w) = N n=1 exp(-τ (x n )w T x n ) (14) Assumption A.1. (Linear separability) The dataset D is linearly separable: ∃w * ∈ R d s.t. τ (x n )w T * x n &gt; 0 for all x n ∈ D.</formula>
                <p>
                    <s coords="14,54.97,213.00,282.59,8.96">We consider minimizing (
                        <ref type="formula" coords="14,159.32,213.32,8.30,8.64">14</ref>) using gradient descent with a
                        step size η:
                    </s>
                </p>
                <formula xml:id="formula_18" coords="14,236.12,235.69,305.98,9.65">w m = w m-1 -η∇ w L(w m-1 )
                    <label>(15)</label>
                </formula>
                <p>
                    <s coords="14,55.44,258.37,284.08,9.81">Let w SVM denote the primal solution to the hard margin SVM problem:</s>
                </p>
                <formula xml:id="formula_19" coords="14,208.68,278.79,329.28,31.56">w SVM = min w ∥w∥ 2 2 s.t. τ (x n )w T x n ≥ 1 ∀x n ∈ D. (
                    <label>16</label>
                </formula>
                <formula xml:id="formula_20" coords="14,537.96,291.08,4.15,8.64">)</formula>
                <p>
                    <s coords="14,55.44,322.83,274.46,9.81">Let α SVM denote the dual solution to the hard margin SVM problem:</s>
                </p>
                <formula xml:id="formula_21" coords="14,181.09,344.51,361.02,46.52">α SVM = max α N n=1 α n - 1 2 N i=1 N j=1 α i α j τ (x i )τ (x j )x T i x j s.t. α n ≥ 0 ∀n,
                    <label>(17)</label>
                </formula>
                <p>
                    <s coords="14,55.08,406.79,182.61,8.64;14,375.35,406.79,62.26,8.64">where primal and dual variables are related as
                        <ref type="bibr" coords="14,375.35,406.79,57.97,8.64" target="#b70">(Vapnik, 1995)</ref>.
                    </s>
                    <s coords="14,55.08,422.72,353.83,9.72;14,409.18,421.22,4.71,6.12;14,408.92,427.79,15.11,6.05;14,424.53,422.79,118.16,9.65;14,55.44,434.75,486.00,9.65;14,55.44,446.70,296.92,9.81">Assumption A.2. (Non-degenerate dataset) Support vectors S = {x n ∈ D|τ (x n )w T SVM x n = 1} span the data, i.e., rank(D S ) = rank(D), where D S is a matrix whose columns are x n ∈ S. Furthermore, for each x n ∈ S, the corresponding dual variables are strictly positive, i.e., (α SVM ) n &gt; 0, and the rest are zero.</s>
                </p>
                <formula xml:id="formula_22"
                         coords="14,240.19,403.50,132.17,14.11">w SVM = N n=1 (α SVM ) n τ (x n )x n</formula>
                <p>
                    <s coords="14,55.08,469.32,385.01,8.64;14,275.21,485.47,266.23,8.59;14,54.83,497.28,486.61,9.65;14,55.44,509.38,144.01,8.59">After above specifications, the simplified version of the seminal result by
                        <ref type="bibr" coords="14,348.50,469.32,79.69,8.64" target="#b66">Soudry et al. (2018)</ref>
                        is:
                        <ref type="bibr" coords="14,275.21,485.47,79.98,8.59" target="#b66">Soudry et al. (2018)</ref>)
                        For almost any non-degenerate (Assumption A.2) dataset which is linearly separable (Assumption
                        A.1), any starting point w 0 and step size η &lt; 1/L(w 0 ), the gradient descent iterates (15)
                        will behave as:
                    </s>
                </p>
                <formula xml:id="formula_23" coords="14,55.44,485.26,217.26,8.96">Proposition A.3. (Implicit Bias of Gradient Descent,</formula>
                <formula xml:id="formula_24" coords="14,238.10,522.34,304.01,9.65">w m = w SVM log m + w + r m ,
                    <label>(18)</label>
                </formula>
                <p>
                    <s coords="14,55.44,541.42,245.90,9.65">where w SVM is the max-margin vector (
                        <ref type="formula" coords="14,211.41,541.56,7.64,8.59" target="#formula_19">16</ref>), w is a
                        solution to:
                    </s>
                </p>
                <formula xml:id="formula_25" coords="14,208.52,564.10,329.44,9.65">∀x n ∈ S : η exp(-τ (x) wT x n ) = (α SVM ) n , (
                    <label>19</label>
                </formula>
                <formula xml:id="formula_26" coords="14,537.96,564.42,4.15,8.64">)</formula>
                <p>
                    <s coords="14,55.44,586.79,230.74,9.65">and the residual r m is bounded with lim m→∞ ∥r m ∥ 2 = 0</s>
                </p>
                <p>
                    <s coords="14,55.44,609.41,290.40,8.64">Equipped with this result, we analyze the generalization-based objective:</s>
                </p>
                <formula xml:id="formula_27" coords="14,220.24,630.75,321.87,39.09">L binary M (θ) = x∈D exp(-τ θ (x)w T M ϕ(x)) (20) s.t. w M = Ξ (M ) (w 0 , D),
                    <label>(21)</label>
                </formula>
                <p>
                    <s coords="14,55.08,684.35,77.14,9.65;14,132.50,682.77,4.71,6.12;14,138.78,684.35,322.78,9.65;14,461.55,682.77,14.57,6.12;14,476.62,684.35,64.82,9.65;14,55.44,696.30,303.13,9.65">where τ θ (x) = σ(θ T x) is the task encoder with an odd activation function such as tanh and w M = Ξ (M ) (w 0 , D) denotes M steps of gradient descent with step size η and labeling defined by τ θ .</s>
                    <s coords="14,364.66,696.62,176.79,8.64;14,55.44,708.26,383.78,9.65">Without loss of generality, we can assume ∥x n ∥ 2 ≤ 1, ∀x n ∈ D. Given the above specifications, we are now ready to state our main result:</s>
                </p>
                <p>
                    <s coords="15,55.44,70.15,486.00,8.96;15,55.44,82.17,115.04,8.74">Proposition A.4. (Lower bound for the generalization-based objective) Following the assumptions of Proposition A.3, given M ≫ 1 and θ ̸ = 0, we have:</s>
                </p>
                <formula xml:id="formula_28" coords="15,238.21,101.36,299.75,13.82">L binary M (θ) ≥ g(θ)∥w SVM (θ)∥ 2 2 , (
                    <label>22</label>
                </formula>
                <formula xml:id="formula_29" coords="15,537.96,104.41,4.15,8.64">)</formula>
                <p>
                    <s coords="15,55.44,126.01,146.65,9.65;15,202.10,124.44,10.20,6.12;15,212.79,126.01,328.65,9.65;15,55.44,137.96,187.97,8.74">where g(θ) = (M η exp(∥r M (θ)∥ 2 )) -1 , the residual r M (θ) is bounded with lim M →∞ ∥r M (θ)∥ 2 = 0, and w SVM (θ) is the solution of the hard-margin SVM for a given θ:</s>
                </p>
                <formula xml:id="formula_30" coords="15,201.42,157.52,336.54,31.56">w SVM (θ) = min w ∥w∥ 2 2 s.t. τ θ (x n )w T x n ≥ 1 ∀x n ∈ D. (
                    <label>23</label>
                </formula>
                <formula xml:id="formula_31" coords="15,537.96,169.81,4.15,8.64">)</formula>
                <p>
                    <s coords="15,55.44,204.57,24.74,8.59">Proof.</s>
                    <s coords="15,84.85,204.43,456.94,9.65;15,55.44,216.39,367.68,9.65">The key observation is that the task encoder τ θ (x) generates linearly separable labelings, allowing us to apply Proposition A.3 and substitute the explicit form of iterates w M into the outer objective (20).</s>
                    <s coords="15,426.22,216.39,114.95,9.65;15,55.44,228.34,152.52,9.65;15,208.23,226.77,4.71,6.12;15,214.51,228.34,327.10,9.65;15,55.44,240.62,72.10,8.64">Indeed, for example, w * = θ satisfies Assumption A.1, i.e., τ θ (x n )θ T x n &gt; 0 for all x n ∈ D and θ ̸ = 0. Thus, substituting the iterates w M into the outer objective leads to:</s>
                </p>
                <formula xml:id="formula_32" coords="15,153.24,251.25,388.87,30.20">L binary M (θ) = N n=1 exp(-τ θ (x n ) (w SVM (θ) log M + w(θ) + r M (θ)) T x n ),
                    <label>(24)</label>
                </formula>
                <p>
                    <s coords="15,55.08,288.74,443.61,9.81">where we explicitly indicate that w SVM (θ), w(θ) and r M (θ) depend on the parameters θ of the task encoder τ θ .</s>
                    <s coords="15,501.77,288.74,40.83,9.65;15,55.44,300.70,323.14,9.65">Let L n (θ) be n-th term of the sum and S θ be the indices of support vectors, i.e., n ∈ {1, . . .</s>
                    <s coords="15,380.24,300.70,32.61,8.96">, N } s.t.</s>
                    <s coords="15,415.94,300.70,34.86,9.65;15,451.07,299.12,4.71,6.12;15,450.80,305.70,15.11,6.05;15,466.41,300.70,31.90,9.65">τ θ (x n )w T SVM x n = 1.</s>
                    <s coords="15,501.41,301.02,40.03,8.64;15,55.44,312.65,161.91,8.96">Then, due to the non-negativity of exp(•), we have:</s>
                </p>
                <formula xml:id="formula_33" coords="15,248.53,323.92,289.43,23.41">L binary M (θ) ≥ n∈S θ L n (θ). (
                    <label>25</label>
                </formula>
                <formula xml:id="formula_34" coords="15,537.96,326.92,4.15,8.64">)</formula>
                <p>
                    <s coords="15,55.44,355.01,307.41,9.65">Considering single term L n (θ), n ∈ S θ and opening the brackets, we obtain:</s>
                </p>
                <formula xml:id="formula_35" coords="15,106.94,374.85,435.16,25.56">L n (θ) = exp(-log M • τ θ (x n )w T SVM (θ)x n ) L1 exp(-τ θ (x n ) w(θ) T x n ) L2 exp(-τ θ (x n )r M (θ) T x n ) L3 .
                    <label>(26)</label>
                </formula>
                <p>
                    <s coords="15,55.44,415.27,223.45,9.65">Inspecting (
                        <ref type="formula" coords="15,104.64,415.59,8.46,8.64" target="#formula_35">26</ref>)
                        separately for each term L i , we obtain:
                    </s>
                </p>
                <formula xml:id="formula_36" coords="15,282.26,412.48,187.72,14.38">(i) L 1 = 1 M since n ∈ S θ ; (ii) L 2 = (αSVM(θ))n</formula>
                <p>
                    <s coords="15,449.39,420.74,4.10,6.12;15,474.31,415.41,67.80,8.82;15,55.44,428.53,485.99,9.65;15,55.44,440.80,27.68,8.64">η by (
                        <ref type="formula" coords="15,491.05,415.59,7.90,8.64" target="#formula_25">19</ref>); and
                        (iii) L 3 ≥ exp(-∥r M (θ)∥ 2 ) by Cauchy-Schwarz inequality given that ∥τ θ (x n )x n ∥ 2 ≤ 1.
                        Combining this with (25), finally we obtain:
                    </s>
                </p>
                <formula xml:id="formula_37" coords="15,116.76,449.76,421.20,23.41">L binary M (θ) ≥ (M η exp(∥r M (θ)∥ 2 )) -1 n∈S θ α n (θ) = (M η exp(∥r M (θ)∥ 2 )) -1 ∥w SVM (θ)∥ 2 2 , (
                    <label>27</label>
                </formula>
                <formula xml:id="formula_38" coords="15,537.96,452.76,4.15,8.64">)</formula>
                <p>
                    <s coords="15,55.08,480.48,191.92,8.64">where the last equality comes from the fact that:</s>
                </p>
                <formula xml:id="formula_39" coords="15,105.19,501.67,386.50,51.21">∥w SVM (θ)∥ 2 2 = w SVM (θ) T w SVM (θ) = w SVM (θ) T n∈S θ (α SVM (θ)) n τ θ (x n )x n = = n∈S θ (α SVM (θ)) n τ θ (x n )w SVM (θ) T x n = n∈S θ (α SVM (θ)) n • 1 = n∈S θ (α SVM (θ)) n ,</formula>
                <p>
                    <s coords="15,55.44,564.18,85.50,8.64">concluding the proof.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="16,55.44,68.77,121.87,10.75">B. Experimental Details</head>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="16,55.44,89.52,55.06,8.96">B.1. Datasets</head>
                <p>
                    <s coords="16,54.97,108.56,333.26,8.64">We evaluate our framework on 26 vision datasets studied in
                        <ref type="bibr" coords="16,300.23,108.56,83.77,8.64" target="#b56">Radford et al. (2021)</ref>.
                    </s>
                    <s coords="16,391.71,108.56,149.73,8.64;16,55.19,120.52,486.59,8.64;16,55.05,132.47,486.38,8.64;16,55.44,144.43,486.00,8.64;16,55.44,156.38,486.82,8.64;16,55.44,168.34,487.24,8.64;16,55.44,180.29,486.36,8.64;16,55.44,192.25,485.99,8.64;16,55.44,204.20,487.24,8.64;16,55.44,216.16,485.99,8.64;16,55.44,228.11,486.82,8.64;16,55.44,240.07,486.00,8.64;16,55.44,252.03,203.33,8.64">These datasets cover a wide range of vision tasks, including general object classification datasets CIFAR10
                        <ref type="bibr" coords="16,329.16,120.52,113.14,8.64" target="#b40">(Krizhevsky &amp; Hinton, 2009)</ref>
                        , CIFAR100
                        <ref type="bibr" coords="16,493.93,120.52,47.86,8.64;16,55.05,132.47,63.60,8.64" target="#b40">(Krizhevsky &amp; Hinton, 2009)</ref>
                        , STL10
                        <ref type="bibr" coords="16,155.20,132.47,77.82,8.64" target="#b16">(Coates et al., 2011)</ref>,
                        ImageNet
                        <ref type="bibr" coords="16,281.03,132.47,68.11,8.64" target="#b19">(Deng et al., 2009</ref>
                        <ref type="bibr"
                             coords="16,349.14,132.47,137.97,8.64">), Caltech101 (Fei-Fei et al., 2004)</ref>;
                        fine-grained object classification datasets Food101
                        <ref type="bibr" coords="16,214.84,144.43,87.76,8.64" target="#b8">(Bossard et al., 2014)</ref>,
                        Flowers
                        <ref type="bibr" coords="16,346.35,144.43,124.17,8.64" target="#b51">(Nilsback &amp; Zisserman, 2008)</ref>
                        , Birdsnap
                        <ref type="bibr" coords="16,518.49,144.43,22.96,8.64;16,55.44,156.38,47.48,8.64" target="#b5">(Berg et al., 2014)</ref>
                        , Stanford Cars
                        <ref type="bibr" coords="16,169.80,156.38,82.57,8.64" target="#b39">(Krause et al., 2013)</ref>,
                        FGVC Aircraft
                        <ref type="bibr" coords="16,324.90,156.38,72.99,8.64" target="#b49">(Maji et al., 2013)</ref>,
                        Oxford Pets
                        <ref type="bibr" coords="16,457.43,156.38,80.41,8.64" target="#b55">(Parkhi et al., 2012)</ref>;
                        handwritten digits classification dataset
                        <ref type="bibr" coords="16,219.02,168.34,115.69,8.64">MNIST (LeCun et al., 1998)</ref>; texture
                        classification dataset DTD
                        <ref type="bibr" coords="16,483.47,168.34,59.21,8.64;16,55.44,180.29,21.24,8.64" target="#b15">(Cimpoi et al., 2014)</ref>
                        ; scene classification dataset SUN397
                        <ref type="bibr" coords="16,226.90,180.29,69.65,8.64" target="#b76">(Xiao et al., 2016)</ref>;
                        the facial emotion recognition dataset FER2013
                        <ref type="bibr" coords="16,492.15,180.29,49.65,8.64;16,55.44,192.25,46.16,8.64" target="#b28">(Goodfellow et al., 2015)</ref>
                        ; the satellite image classification datasets EuroSAT
                        <ref type="bibr" coords="16,312.18,192.25,78.94,8.64" target="#b31">(Helber et al., 2019)</ref>,
                        RESISC45
                        <ref type="bibr" coords="16,444.30,192.25,78.08,8.64" target="#b13">(Cheng et al., 2017)</ref>;
                        the German Traffic Sign Recognition Benchmark (GTSRB)
                        <ref type="bibr" coords="16,279.50,204.20,91.78,8.64"
                             target="#b67">(Stallkamp et al., 2012)</ref>; the KITTI Distance dataset
                        <ref type="bibr" coords="16,488.18,204.20,54.50,8.64;16,55.44,216.16,21.48,8.64" target="#b27">(Geiger et al., 2012)</ref>
                        ; the metastatic tissue classification dataset PatchCamelyon (PCam)
                        <ref type="bibr" coords="16,347.76,216.16,81.57,8.64" target="#b71">(Veeling et al., 2018)</ref>
                        ; action recognition datasets UCF101
                        <ref type="bibr" coords="16,93.13,228.11,85.22,8.64" target="#b65">(Soomro et al., 2012)</ref>,
                        Kinetics700
                        <ref type="bibr" coords="16,237.25,228.11,86.58,8.64"
                             target="#b10">(Carreira et al., 2019)</ref>; the CLEVR counting dataset
                        <ref type="bibr" coords="16,451.24,228.11,86.60,8.64" target="#b35">(Johnson et al., 2017)</ref>
                        ; the Hateful Memes dataset
                        <ref type="bibr" coords="16,167.08,240.07,75.22,8.64" target="#b36">(Kiela et al., 2020)</ref>;
                        the country classification dataset Country211
                        <ref type="bibr" coords="16,436.01,240.07,88.10,8.64" target="#b56">(Radford et al., 2021)</ref>
                        and the Rendered SST2 dataset
                        <ref type="bibr" coords="16,168.17,252.03,86.23,8.64" target="#b56">(Radford et al., 2021)</ref>
                        .
                    </s>
                    <s coords="16,262.30,252.03,279.14,8.64;16,55.44,263.98,117.08,8.64">For CLEVR, we take 2000 random samples as training split and 500 random samples as test split.</s>
                    <s coords="16,176.15,263.98,365.29,8.64;16,55.44,275.94,176.34,8.64">For two video datasets UCF101 and Kinetics700, we take the middle frame of each video clip as the input of the pre-trained models.</s>
                    <s coords="16,236.81,275.94,202.22,8.64">Details of each dataset are provided in Table
                        <ref type="table" coords="16,424.63,275.94,9.60,8.64" target="#tab_0">B1</ref>.
                    </s>
                    <s coords="16,444.06,275.76,97.38,8.82;16,55.44,287.89,151.20,8.64">We use accuracy as the evaluation metric for all the datasets.</s>
                    <s coords="16,210.81,287.89,332.28,8.64;16,55.44,299.85,486.00,8.64">Finally, it's worth noting that TURTLE could also be applied to the tasks in various modalities besides vision or even in cross-modalities scenarios, provided that the pre-trained representations are available.</s>
                </p>
                <p>
                    <s coords="16,115.11,338.95,34.65,7.77">Table
                        <ref type="table" coords="16,137.05,338.95,8.47,7.77" target="#tab_0">B1</ref>.
                    </s>
                    <s coords="16,152.84,338.60,121.29,8.06">Benchmark suite of 26 datasets.</s>
                    <s coords="16,276.91,338.79,204.87,7.93">We use accuracy as the evaluation metric for all datasets.</s>
                </p>
                <p>
                    <s coords="16,119.54,357.13,29.88,8.64;16,300.83,357.13,174.02,8.64;16,119.54,370.68,123.27,8.64;16,331.27,370.68,14.94,8.64;16,394.83,370.68,27.40,8.64;16,443.92,370.68,27.40,8.64;16,119.54,383.83,158.53,8.64;16,333.76,383.83,9.96,8.64;16,394.83,383.83,27.40,8.64;16,443.92,383.83,27.40,8.64;16,119.54,396.98,163.52,8.64;16,331.27,396.98,14.94,8.64;16,394.83,396.98,27.40,8.64;16,443.92,396.98,27.40,8.64;16,119.54,410.13,111.05,8.64;16,331.27,410.13,14.94,8.64;16,394.83,410.13,27.40,8.64;16,446.41,410.13,22.42,8.64;16,119.54,423.28,110.13,8.64;16,331.27,423.28,14.94,8.64;16,394.83,423.28,27.40,8.64;16,443.92,423.28,27.40,8.64;16,119.54,436.43,136.68,8.64;16,331.27,436.43,14.94,8.64;16,397.32,436.43,22.42,8.64;16,446.41,436.43,22.42,8.64;16,119.54,449.58,135.30,8.64;16,331.27,449.58,14.94,8.64;16,397.32,449.58,22.42,8.64;16,446.41,449.58,22.42,8.64;16,119.54,462.74,106.26,8.64;16,333.76,462.74,9.96,8.64;16,397.32,462.74,22.42,8.64;16,446.41,462.74,22.42,8.64;16,119.54,475.89,127.13,8.64;16,333.76,475.89,9.96,8.64;16,397.32,475.89,22.42,8.64;16,446.41,475.89,22.42,8.64;16,119.54,489.04,130.05,8.64;16,331.27,489.04,14.94,8.64;16,397.32,489.04,22.42,8.64;16,446.41,489.04,22.42,8.64;16,119.54,502.19,155.54,8.64;16,331.27,502.19,14.94,8.64;16,397.32,502.19,22.42,8.64;16,446.41,502.19,22.42,8.64;16,119.54,515.34,114.00,8.64;16,333.76,515.34,9.96,8.64;16,394.83,515.34,27.40,8.64;16,443.92,515.34,27.40,8.64;16,119.54,528.49,141.43,8.64;16,336.25,528.49,4.98,8.64;16,394.83,528.49,27.40,8.64;16,446.41,528.49,22.42,8.64;16,119.54,541.64,110.68,8.64;16,333.76,541.64,9.96,8.64;16,397.32,541.64,22.42,8.64;16,446.41,541.64,22.42,8.64;16,119.54,554.79,120.08,8.64;16,333.76,554.79,9.96,8.64;16,394.83,554.79,27.40,8.64;16,446.41,554.79,22.42,8.64;16,119.54,567.94,125.64,8.64;16,333.76,567.94,9.96,8.64;16,394.83,567.94,27.40,8.64;16,446.41,567.94,22.42,8.64;16,119.54,581.09,128.41,8.64;16,333.76,581.09,9.96,8.64;16,394.83,581.09,27.40,8.64;16,443.92,581.09,27.40,8.64;16,119.54,594.24,146.35,8.64;16,336.25,594.24,4.98,8.64;16,397.32,594.24,22.42,8.64;16,446.41,594.24,22.42,8.64;16,119.54,607.39,136.14,8.64;16,331.27,607.39,14.94,8.64;16,394.83,607.39,27.40,8.64;16,443.92,607.39,27.40,8.64;16,119.54,620.54,149.27,8.64;16,336.25,620.54,4.98,8.64;16,392.34,620.54,32.38,8.64;16,443.92,620.54,27.40,8.64;16,119.54,633.70,121.76,8.64;16,331.27,633.70,14.94,8.64;16,397.32,633.70,22.42,8.64;16,446.41,633.70,22.42,8.64;16,119.54,646.85,136.68,8.64;16,331.27,646.85,14.94,8.64;16,392.34,646.85,32.38,8.64;16,443.92,646.85,27.40,8.64;16,119.54,660.00,151.93,8.64;16,336.25,660.00,4.98,8.64;16,397.32,660.00,22.42,8.64;16,450.15,660.00,14.94,8.64;16,119.54,673.15,136.68,8.64;16,336.25,673.15,4.98,8.64;16,397.32,673.15,22.42,8.64;16,450.15,673.15,14.94,8.64;16,119.54,686.30,169.33,8.64;16,336.25,686.30,4.98,8.64;16,397.32,686.30,22.42,8.64;16,446.41,686.30,22.42,8.64;16,119.54,699.45,116.75,8.64;16,328.78,699.45,19.93,8.64;16,388.60,699.45,39.85,8.64;16,443.92,699.45,27.40,8.64">Dataset Number of Classes Train size Test size Food101
                        <ref type="bibr" coords="16,157.31,370.68,85.50,8.64" target="#b8">(Bossard et al., 2014)</ref>
                        101 75,750 25,250 CIFAR10
                        <ref type="bibr" coords="16,160.60,383.83,117.48,8.64" target="#b40">(Krizhevsky &amp; Hinton, 2009)</ref>
                        10 50,000 10,000 CIFAR100
                        <ref type="bibr" coords="16,165.58,396.98,117.48,8.64" target="#b40">(Krizhevsky &amp; Hinton, 2009)</ref>
                        100 50,000 10,000 Birdsnap
                        <ref type="bibr" coords="16,158.01,410.13,72.59,8.64" target="#b5">(Berg et al., 2014)</ref>
                        500 37,221 2,500 SUN397
                        <ref type="bibr" coords="16,156.90,423.28,72.77,8.64" target="#b76">(Xiao et al., 2016)</ref>
                        397 19,850 19,850 StanfordCars
                        <ref type="bibr" coords="16,174.61,436.43,81.61,8.64" target="#b39">(Krause et al., 2013)</ref>
                        196 8,144 8,041 FGVC Aircraft
                        <ref type="bibr" coords="16,182.63,449.58,72.22,8.64" target="#b49">(Maji et al., 2013)</ref>
                        100 6,667 3,333 DTD
                        <ref type="bibr" coords="16,142.51,462.74,83.30,8.64" target="#b15">(Cimpoi et al., 2014)</ref>
                        47 3,760 1,880 OxfordPets
                        <ref type="bibr" coords="16,167.41,475.89,79.26,8.64" target="#b55">(Parkhi et al., 2012)</ref>
                        37 3,680 3,669 Caltech101
                        <ref type="bibr" coords="16,167.41,489.04,82.18,8.64" target="#b23">(Fei-Fei et al., 2004)</ref>
                        102 3,060 6,084 Flowers
                        <ref type="bibr" coords="16,153.88,502.19,121.19,8.64" target="#b51">(Nilsback &amp; Zisserman, 2008)</ref>
                        102 2,040 6,149
                        <ref type="bibr" coords="16,119.54,515.34,114.00,8.64">MNIST (LeCun et al., 1998)</ref>
                        10 60,000 10,000 FER2013
                        <ref type="bibr" coords="16,160.23,528.49,100.74,8.64"
                             target="#b28">(Goodfellow et al., 2015)</ref>
                        7 28,709 3,589 STL10
                        <ref type="bibr" coords="16,149.71,541.64,80.52,8.64" target="#b16">(Coates et al., 2011)</ref>
                        10 5,000 8,000 EuroSAT
                        <ref type="bibr" coords="16,159.11,554.79,80.51,8.64" target="#b31">(Helber et al., 2019)</ref>
                        10 10,000 5,000 RESISC45
                        <ref type="bibr" coords="16,165.77,567.94,79.41,8.64" target="#b13">(Cheng et al., 2017)</ref>
                        45 25,200 6,300 GTSRB
                        <ref type="bibr" coords="16,154.14,581.09,93.81,8.64"
                             target="#b67">(Stallkamp et al., 2012)</ref>
                        43 26,640 12,630 KITTI Distance
                        <ref type="bibr" coords="16,185.38,594.24,80.51,8.64" target="#b27">(Geiger et al., 2012)</ref>
                        4 5,985 1,496 Country211
                        <ref type="bibr" coords="16,169.63,607.39,86.05,8.64" target="#b56">(Radford et al., 2021)</ref>
                        211 42,200 21,100 PatchCamelyon
                        <ref type="bibr" coords="16,184.98,620.54,83.83,8.64" target="#b71">(Veeling et al., 2018)</ref>
                        2 294,912 32,768 UCF101
                        <ref type="bibr" coords="16,156.35,633.70,84.95,8.64" target="#b65">(Soomro et al., 2012)</ref>
                        101 9,537 3,783 Kinetics700
                        <ref type="bibr" coords="16,170.18,646.85,86.04,8.64"
                             target="#b10">(Carreira et al., 2019)</ref>
                        700 536,485 33,966 CLEVR Counts
                        <ref type="bibr" coords="16,185.41,660.00,86.06,8.64" target="#b35">(Johnson et al., 2017)</ref>
                        8 2,000 500 HatefulMemes
                        <ref type="bibr" coords="16,181.24,673.15,74.98,8.64" target="#b36">(Kiela et al., 2020)</ref>
                        2 8,500 500 The Rendered SST2
                        <ref type="bibr" coords="16,202.83,686.30,86.05,8.64" target="#b56">(Radford et al., 2021)</ref>
                        2 7,792 1,821 ImageNet
                        <ref type="bibr" coords="16,161.32,699.45,74.98,8.64" target="#b19">(Deng et al., 2009)</ref>
                        1000 1,281,167 50,000
                    </s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="18,55.44,70.15,118.99,8.96">B.3. Implementation Details</head>
                <p>
                    <s coords="18,55.44,88.80,143.75,8.96">Efficient alternating optimization.</s>
                    <s coords="18,202.28,89.19,339.15,8.64;18,55.44,101.15,356.42,8.64">TURTLE contains a bilevel objective that measures the loss of the task encoder using the training loss of a linear classifier trained on the task produced by the task encoder.</s>
                    <s coords="18,416.72,101.15,124.96,8.64;18,55.44,112.78,73.14,9.65;18,132.54,110.90,10.21,6.12;18,133.30,118.26,8.48,6.12;18,146.16,112.78,13.83,8.74;18,161.20,110.90,10.44,6.12;18,162.17,118.26,8.47,6.12;18,173.03,112.78,3.87,8.74;18,176.90,110.90,17.89,6.42;18,184.37,118.26,10.44,6.12;18,196.20,112.78,106.35,9.65;18,306.24,110.90,10.45,6.12;18,307.21,118.26,8.48,6.12;18,320.56,113.10,127.42,8.64">The hyper-gradient of the task encoder is ∇ θ L = ∂L ∂θ + ( ∂w ∂θ ) T ∂L ∂w | w=w * , where the Jacobian ∂w ∂θ is generally expensive to obtain.</s>
                    <s coords="18,451.07,113.10,90.72,8.64;18,55.44,125.06,487.74,8.64;18,55.11,137.01,488.07,8.64">Existing works usually estimate the hyper-gradient via unrolling or approximation based on the implicit function theorem, e.g., see
                        <ref type="bibr" coords="18,501.32,125.06,41.86,8.64;18,55.11,137.01,24.64,8.64" target="#b25">Finn et al. (2017)</ref>
                        ;
                        <ref type="bibr" coords="18,86.18,137.01,81.75,8.64" target="#b47">Lorraine et al. (2020)</ref>;
                        <ref type="bibr" coords="18,174.35,137.01,54.65,8.64" target="#b33">Ji et al. (2021)</ref>;
                        <ref type="bibr" coords="18,235.43,137.01,71.89,8.64" target="#b42">Kwon et al. (2023)</ref>;
                        <ref type="bibr" coords="18,313.75,137.01,81.75,8.64" target="#b17">Dagréou et al. (2022)</ref>;
                        <ref type="bibr" coords="18,401.94,137.01,69.29,8.64" target="#b6">Bolte et al. (2023)</ref>;
                        <ref type="bibr" coords="18,477.65,137.01,61.46,8.64" target="#b46">Liu et al. (2022)</ref>.
                    </s>
                    <s coords="18,55.44,148.97,370.01,8.64">However, these methods might be inefficient and suboptimal in practice
                        <ref type="bibr" coords="18,343.94,148.97,77.22,8.64" target="#b62">(Scieur et al., 2022)</ref>.
                    </s>
                    <s coords="18,428.58,148.97,112.86,8.64;18,55.44,160.60,191.11,8.96;18,247.74,158.72,10.45,6.12;18,248.71,166.08,8.48,6.12;18,259.57,160.60,3.87,8.74;18,263.45,158.72,17.89,6.42;18,270.92,166.08,10.45,6.12;18,285.29,160.92,72.60,8.64;18,361.84,158.72,10.20,6.12;18,361.63,166.08,10.44,6.12;18,373.46,160.60,168.23,9.65;18,55.44,173.80,128.33,9.65;18,187.95,171.92,10.21,6.12;18,188.71,179.28,8.48,6.12;18,199.35,173.80,27.99,9.65">Fortunately, in the TURTLE framework, one could avoid the estimation of ( ∂w ∂θ ) T ∂L ∂w given the fact that ∂L ∂w | w=w * ≈ 0. Thus, the gradient of the task encoder is simplified to ∇ θ L = ∂L ∂θ | w=w * .</s>
                    <s coords="18,230.77,173.94,310.66,8.82;18,55.44,186.07,336.21,8.64">This inspires us to train the task encoder via alternating optimization, which has been shown efficient for the min-min optimization problems
                        <ref type="bibr" coords="18,313.20,186.07,74.17,8.64" target="#b0">(Ablin et al., 2020)</ref>.
                    </s>
                    <s coords="18,394.73,186.07,146.95,8.64;18,55.44,197.71,317.90,8.96">At each iteration, we first fix the task encoder and train the linear classifier for M steps to find its approximate optima.</s>
                    <s coords="18,376.43,198.03,165.01,8.64;18,55.44,209.98,486.00,8.64;18,55.44,221.94,211.14,8.64">Note that one could choose to re-initialize the linear classifier every time (cold-start), or just start from the values of last iteration (warm-start), which might introduce different implicit bias as noted by
                        <ref type="bibr" coords="18,191.73,221.94,70.71,8.64" target="#b72">Vicol et al. (2022)</ref>.
                    </s>
                    <s coords="18,269.67,221.94,271.94,8.64;18,55.44,233.89,36.86,8.64">After that, we update the task encoder based on the loss of the linear classifier.</s>
                    <s coords="18,95.40,233.89,316.14,8.64">The training is efficient since no second-order gradient is needed in this process.</s>
                    <s coords="18,414.64,233.89,126.80,8.64;18,55.44,245.85,106.12,8.64">The pseudo-code of TURTLE is provided in Algorithm B1.</s>
                </p>
                <p>
                    <s coords="18,55.08,268.97,205.74,9.03;18,60.42,282.98,481.02,9.72;18,72.38,295.00,145.93,9.65;18,218.58,293.43,3.97,6.12;18,223.05,295.00,24.29,8.74;18,247.61,293.43,6.69,6.12;18,255.29,295.00,286.15,8.96;18,72.38,306.96,141.00,8.96;18,311.92,306.96,229.52,8.96;18,60.42,318.91,158.48,9.65;18,219.17,317.34,3.97,6.12;18,218.90,323.43,3.97,6.12;18,223.64,318.91,24.29,8.74;18,248.20,317.34,6.69,6.12;18,247.93,323.43,3.97,6.12;18,60.42,330.80,81.73,9.03;18,60.42,343.79,6.98,7.77">Algorithm B1 TURTLE for Unsupervised Transfer 1: Input: Dataset D, number of classes C, number of iterations T , representation spaces ϕ 1 (•), ..., ϕ K (•), task parameters θ = {θ 1 , ..., θ K }, linear classifiers w 1 , ..., w K , learning rate η, optimization operator Ξ(•), number of adaptation steps M , entropy regularization weight γ // Ξ(•) can be any iterative operator, e.g., gradient descent 2: Randomly initialize θ 1 , ..., θ K and w 1 0 , ..., w K 0 3: for t = 1 to T do 4:</s>
                </p>
                <p>
                    <s coords="18,82.34,342.83,160.06,8.96">Sample mini-batch from dataset X ∼ D</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="18,60.42,355.75,6.98,7.77">5:</head>
                <p>
                    <s coords="18,82.34,355.10,128.92,8.64">Generate task from task encoder</s>
                </p>
                <formula xml:id="formula_40" coords="18,60.42,351.81,479.25,55.87">τ θ ← 1 K K k=1 σ(θ k ϕ k (X)) 6: Update linear classifiers for M steps ∀k ∈ [K] : w k M ← Ξ (M ) (w k 0 , X) 7: Update task parameters ∀k ∈ [K] : θ k ← θ k -η ∂ ∂θ k L ce (w k M ϕ k (X), τ θ ) + γR(τ θ ) // partial derivative ∂ ∂θ k 8:</formula>
                <p>
                    <s coords="18,82.34,398.87,200.47,9.03;18,283.07,397.37,4.24,6.12;18,282.80,403.46,3.97,6.12;18,290.74,398.94,19.86,8.74;18,310.87,397.37,4.24,6.12;18,310.60,403.73,7.60,6.12;18,415.51,398.94,120.76,8.96;18,536.54,397.37,4.24,6.12;18,536.27,403.46,3.97,6.12;18,60.42,410.83,42.42,8.96;18,55.94,422.78,187.42,9.72;18,55.11,451.15,71.59,8.96">if warm-start then update start points ∀k ∈ [K], w k 0 ← w k M // cold-start keeps the initial w k 0 9: end for 10: Output: Task parameters θ = {θ 1 , ..., θ K } Training details.</s>
                    <s coords="18,131.81,451.54,321.74,8.64">We precompute the feature representations for all datasets before the training.</s>
                    <s coords="18,458.65,451.54,84.45,8.64;18,55.44,463.49,487.74,8.64">We use Weight Normalization
                        <ref type="bibr" coords="18,101.15,463.49,115.95,8.64" target="#b59">(Salimans &amp; Kingma, 2016)</ref>
                        to parameterize the task encoder since we found it helpful for the convergence.
                    </s>
                    <s coords="18,55.08,475.45,426.55,8.64">ADAM
                        <ref type="bibr" coords="18,88.26,475.45,89.18,8.64" target="#b37">(Kingma &amp; Ba, 2015)</ref>
                        optimizer is used for the training of both linear classifier and task encoder.
                    </s>
                    <s coords="18,484.86,475.13,56.83,8.96;18,55.44,487.40,101.07,8.64">We use 10000 as the default batch-size.</s>
                    <s coords="18,160.46,487.09,345.41,8.96">For datasets smaller than 10000, we train the model with full-batch at each iteration.</s>
                    <s coords="18,509.82,487.40,32.86,8.64;18,55.08,499.36,247.86,8.64">Overall, we found TURTLE is robust to the choice of the batch-size.</s>
                    <s coords="18,307.71,499.04,233.73,8.96;18,55.44,511.00,280.90,8.96">We update the linear classifier for M = 10 steps at each iteration and train the task encoder for T = 6000 iterations in total.</s>
                    <s coords="18,341.46,511.31,200.33,8.64;18,55.44,522.95,219.01,8.96">If not specifically mentioned, we set the entropy regularization parameter γ = 10 for all experiments.</s>
                    <s coords="18,279.81,523.27,263.28,8.64;18,55.44,535.23,40.11,8.64">We show robustness of TURTLE to this hyperparameter in Appendix G.</s>
                    <s coords="18,98.13,534.91,443.47,8.96;18,55.08,546.86,242.74,8.96">For each dataset, we do a grid search over 5 different learning rates for both task encoder and linear classifier with η ∈ {0.01, 0.005, 0.001, 0.0005, 0.0001}, respectively.</s>
                    <s coords="18,302.41,547.18,239.03,8.64;18,55.08,558.82,351.16,8.96">We combine each pair of learning rates with the choice of warm-start or cold-start, and finally get set of 50 triplets to search over for each dataset.</s>
                    <s coords="18,409.33,558.96,132.11,8.82;18,55.44,571.09,150.68,8.64">We use cross-validation to select hyper-parameters, as described below.</s>
                    <s coords="18,209.22,571.09,332.22,8.64;18,55.44,583.05,486.00,8.64;18,55.44,595.00,37.85,8.64">Following
                        <ref type="bibr" coords="18,251.95,571.09,99.24,8.64"
                             target="#b26">Gadetsky &amp; Brbić (2023)</ref>;
                        <ref type="bibr" coords="18,357.84,571.09,105.64,8.64"
                             target="#b69">Van Gansbeke et al. (2020)</ref>, we use Hungarian algorithm
                        <ref type="bibr" coords="18,97.11,583.05,54.11,8.64" target="#b41">(Kuhn, 1955)</ref>
                        to match the labeling found by TURTLE and the ground truth labeling to compute the clustering
                        accuracy.
                    </s>
                    <s coords="18,96.68,595.00,432.91,8.64">If not specified, we train our model on the training split and report the clustering accuracy on the test split.</s>
                    <s coords="18,532.98,595.00,8.46,8.64;18,55.44,606.96,487.75,8.64">In Section H, we also consider the setting of both training and evaluating TURTLE on the test split, mimicking low data regime.</s>
                </p>
                <p>
                    <s coords="18,55.44,624.50,149.67,8.96">Cross-validation for task selection.</s>
                    <s coords="18,208.40,624.57,333.04,8.96;18,55.44,636.84,90.18,8.64">For each dataset, we obtain 50 tasks after grid search, i.e., each corresponds to the hyperparameter triplet.</s>
                    <s coords="18,148.70,636.53,213.90,8.96">We use 10-fold cross-validation to select the best task.</s>
                    <s coords="18,365.69,636.84,176.00,8.64;18,55.44,648.62,436.16,8.82">The cross-validation regards the learned task as "pseudo-labels" and measures the generalization error of a linear classifier trained on these "pseudo-labels".</s>
                    <s coords="18,494.69,648.80,48.00,8.64;18,55.08,660.44,177.15,8.96">Specifically, we randomly split the dataset into 10 folds.</s>
                    <s coords="18,236.23,660.44,305.20,8.96;18,55.44,672.71,18.91,8.64">In each round, a linear classifier is trained on 9 folds and tested on the rest fold.</s>
                    <s coords="18,78.00,672.71,308.06,8.64">The final cross-validation score is the average test accuracy over all rounds.</s>
                    <s coords="18,389.70,672.71,152.08,8.64;18,55.44,684.49,347.53,8.82">Importantly, this process relies solely on the learned tasks and does not need any information about the ground-truth labels.</s>
                    <s coords="18,406.41,684.66,135.03,8.64;18,55.44,696.62,428.78,8.64">For TURTLE trained on multiple representations, we do cross-validation on each representation space separately and average the final scores.</s>
                    <s coords="18,487.31,696.62,54.13,8.64;18,55.44,708.58,296.24,8.64">The task with the highest cross-validation score is selected as the final output of TURTLE.</s>
                    <s coords="18,354.01,708.58,187.43,8.64;19,55.44,492.31,486.00,8.64;19,55.44,504.26,33.76,8.64">Figure
                        <ref type="figure" coords="18,381.84,708.58,11.39,8.64" target="#fig_5">B1</ref>
                        shows the performance of the learned tasks obtained by TURTLE 2-spaces CLIP ViT-L/14 and DINOv2
                        and their corresponding cross-validation scores over 26 datasets.
                    </s>
                    <s coords="19,92.28,504.26,449.16,8.64;19,55.44,515.90,290.81,8.96">As indicated by the plot, the cross-validation score is well correlated with the clustering accuracy with an average of ρ = 0.61 two-sided Pearson correlation coefficient over 26 datasets.</s>
                    <s coords="19,350.63,515.90,190.80,8.96;19,55.44,527.85,442.86,8.96">Moreover, among 20 datasets, cross-validation successfully identifies the best or near-best task (i.e., with less than 1.5 point difference of clustering accuracy).</s>
                    <s coords="19,501.39,528.17,40.05,8.64;19,55.44,540.13,486.00,8.64;19,55.44,552.08,486.00,8.64;19,55.44,564.04,219.00,8.64">The result of cross-validation also empirically verifies the effectiveness of the generalization-based objective and suggests that the labelings with low generalization error tend to be more aligned with human labeled tasks, confirming the original findings of
                        <ref type="bibr" coords="19,55.44,564.04,101.40,8.64"
                             target="#b26">Gadetsky &amp; Brbić (2023)</ref>
                        on the wide suite of datasets.
                    </s>
                </p>
                <p>
                    <s coords="19,55.44,581.58,58.16,8.96">Linear probe.</s>
                    <s coords="19,116.71,581.97,424.73,8.64;19,55.44,593.93,123.00,8.64">Supervised linear probe is a widely used method to evaluate the quality of representation learning
                        <ref type="bibr" coords="19,505.69,581.97,35.75,8.64;19,55.44,593.93,45.07,8.64" target="#b56">(Radford et al., 2021;</ref>
                        <ref type="bibr" coords="19,102.44,593.93,71.79,8.64" target="#b53">Oquab et al., 2023)</ref>.
                    </s>
                    <s coords="19,181.34,593.93,361.75,8.64;19,55.44,605.88,261.83,8.64">It trains a linear classifier on the train split on top of the representations extracted from the pretrained models and then evaluates the performance on the test split.</s>
                    <s coords="19,320.33,605.88,221.11,8.64;19,55.44,617.84,241.22,8.64">We use the cuML.LogisticRegression
                        <ref type="bibr" coords="19,505.11,605.88,36.33,8.64;19,55.44,617.84,52.91,8.64" target="#b57">(Raschka et al., 2020)</ref>
                        for linear probe evaluation in our paper
                        <ref type="foot" coords="19,290.13,616.17,3.49,6.05" target="#foot_2">3</ref>
                        .
                    </s>
                    <s coords="19,306.46,617.84,235.16,8.64;19,55.44,629.47,138.17,8.96">The linear classifier is trained with L-BFGS optimizer for maximum of 1000 iterations.</s>
                    <s coords="19,201.48,629.79,339.96,8.64;19,55.44,642.68,83.69,7.01">The cuML library allows for GPU acceleration and, thus, it is much faster than sklearn.linear</s>
                    <s coords="19,142.71,641.75,385.11,8.64">model.LogisticRegression counterpart, especially on large datasets such as ImageNet.</s>
                    <s coords="19,530.95,641.75,10.50,8.64;19,55.44,653.70,486.17,8.64;19,55.44,665.34,175.62,8.96;19,231.06,663.76,10.20,6.12;19,244.60,665.34,20.71,8.96;19,265.31,663.76,3.97,6.12;19,269.78,665.66,2.54,8.64">To determine the strength of L2 norm regularization, we randomly take 20% of the training split for validation and search over 96 values in the log-space ranging from 10 -6 to 10 6 .</s>
                    <s coords="19,276.48,665.66,264.96,8.64;19,55.44,677.29,255.88,8.96">The selection process takes a few minutes on small datasets, and around 8 hours on ImageNet, with a single NVIDIA A100 GPU.</s>
                    <s coords="19,313.80,677.61,227.64,8.64;19,55.44,689.57,353.20,8.64">After that, we train the model with the best regularization strength on the entire training split and report the classification accuracy on the test split.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="20,55.44,68.77,307.40,10.75">C. Details on Unsupervised Baselines and Numerical Results</head>
                <p>
                    <s coords="20,55.44,89.52,86.09,8.96">K-Means clustering.</s>
                    <s coords="20,144.62,89.91,396.82,8.64;20,55.44,101.87,235.03,8.64">We apply K-Means
                        <ref type="bibr" coords="20,224.23,89.91,75.16,8.64" target="#b48">(MacQueen, 1967)</ref>
                        clustering on top of pre-trained features as a simple baseline that does not require
                        task-specific representation learning.
                    </s>
                    <s coords="20,293.68,101.87,247.76,8.64;20,55.44,113.82,269.36,8.64">Similarly to the linear probe, we also use the implementation from CuML library for the GPU acceleration (i.e., CuML.KMeans).</s>
                    <s coords="20,327.88,113.82,214.81,8.64;20,55.08,125.46,486.35,8.96;20,55.44,137.73,238.81,8.64">For each dataset and the corresponding representation, we train K-Means with maximum 1000 iterations (max iter=1000) and 10 random initializations (n init=10) on the train split, and report the clustering accuracy on the test split.</s>
                    <s coords="20,297.34,137.73,244.10,8.64;20,55.44,149.69,486.00,8.64;20,55.44,161.64,94.89,8.64">In the case when multiple representations are used, we first L2 normalize representation from each pre-trained model, and then apply K-Means clustering on top of the concatenation of all L2 normalized features.</s>
                </p>
                <p>
                    <s coords="20,55.44,179.19,34.15,8.96">HUME.</s>
                    <s coords="20,92.47,179.57,448.97,8.64;20,55.44,191.53,230.95,8.64">HUME
                        <ref type="bibr" coords="20,125.26,179.57,106.97,8.64" target="#b26">(Gadetsky &amp; Brbić, 2023)</ref>
                        is the recent state-of-the-art unsupervised learning baseline that introduced the instantiation
                        of the generalization-based objective (3).
                    </s>
                    <s coords="20,289.48,191.53,251.95,8.64;20,55.44,203.48,486.00,8.64;20,55.44,215.44,90.27,8.64">Specifically, it learns task-specific representations on the target dataset to model the task encoder, and then measures the generalization error of a linear classifier in the representation space of a foundation model.</s>
                    <s coords="20,148.80,215.44,392.63,8.64;20,55.44,227.40,112.84,8.64">We use the original source code
                        <ref type="foot" coords="20,277.37,213.77,3.49,6.05" target="#foot_3">4</ref>
                        for the implementation of HUME, with modifications to improve the speed and performance.
                    </s>
                    <s coords="20,172.62,227.40,368.82,8.64;20,55.44,239.35,227.72,8.64">In particular, we replace task-specific representations with a pre-trained foundation model since we empirically found it yields better performance.</s>
                    <s coords="20,286.51,239.35,254.93,8.64;20,55.44,251.31,486.34,8.64;20,55.44,263.26,302.72,8.64">Besides, we remove the variance reduction used in the original HUME and sample only the single mini-batch at every iteration (i.e., the same as TURTLE), since we found it significantly reduces the computational cost and does not influence the final performance.</s>
                    <s coords="20,361.27,262.94,180.41,8.96;20,55.44,274.90,332.07,8.96">We update the linear classifier with M = 300 steps at each iteration, and train the task encoder with T = 6000 iterations in total.</s>
                    <s coords="20,390.60,274.90,152.58,8.96">The default batch-size is set to 10000.</s>
                    <s coords="20,55.44,287.17,486.00,8.64;20,55.08,299.13,131.87,8.64">Moreover, we follow the same hyperparameter selection procedure of TURTLE to select the inner/outer learning rates and warm-start/cold-start for HUME.</s>
                </p>
                <p>
                    <s coords="20,55.44,316.67,209.73,8.96">Comparison of TURTLE to HUME and K-Means.</s>
                    <s coords="20,268.25,317.06,273.19,8.64;20,55.44,329.01,291.27,8.64">For a fair comparison, we train TURTLE, HUME and K-Means using the same representation spaces, i.e., CLIP ViT-L/14 and DINOv2 ViT-g/14.</s>
                    <s coords="20,349.67,329.01,191.77,8.64;20,55.44,340.97,486.00,8.64;20,55.11,352.75,449.03,8.82">Given that HUME's task encoder parametrization uses only the single space, we run HUME with the task encoder modeled using CLIP or DINOv2 (denoted as HUME CLIP and HUME DINOv2 respectively), and measure the generalization error using the rest representation space.</s>
                    <s coords="20,507.25,352.92,34.19,8.64;20,55.44,364.56,416.38,8.96">For each method, we report the training time in minutes and the clustering accuracy averaged over 3 random seeds.</s>
                    <s coords="20,474.91,364.88,66.53,8.64;20,55.44,376.83,487.75,8.64">For each random seed, we perform the hyperparameter selection for HUME and TURTLE as described in the corresponding subsection above.</s>
                    <s coords="20,55.13,388.47,258.49,8.96">Table
                        <ref type="table" coords="20,80.24,388.79,11.86,8.64" target="#tab_4">C1</ref>
                        and Table
                        <ref type="table" coords="20,137.40,388.79,11.86,8.64" target="#tab_5">C2</ref>
                        show the obtained results on 5 datasets.
                    </s>
                    <s coords="20,317.53,388.79,223.91,8.64;20,55.44,400.75,487.74,8.64">Overall, the results indicate that TURTLE outperforms HUME and K-Means on all the considered datasets, highlighting the effectiveness of design choices made in TURTLE.</s>
                </p>
                <p>
                    <s coords="20,55.44,412.70,486.00,8.64;20,55.44,424.66,83.74,8.64">For instance, combining multiple representation spaces for modeling the task encoder in TURTLE brings substantial gains compared to HUME.</s>
                    <s coords="20,141.67,424.66,399.77,8.64;20,55.44,436.43,355.41,8.82">Namely, TURTLE achieves remarkable 28% and 23% absolute improvement (40% and 30% relative improvement) over HUME DINOv2 and HUME CLIP respectively on the MNIST dataset.</s>
                    <s coords="20,413.96,436.61,127.65,8.64;20,55.44,448.25,486.00,8.96;20,55.44,460.52,52.56,8.64">Furthermore, efficient first-order optimization techniques used in TURTLE allow for fast optimization, taking just 5 minutes even on large-scale datasets such as ImageNet.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="22,55.44,68.77,220.42,10.75">E. Additional Results on 26 Vision Datasets</head>
                <p>
                    <s coords="22,54.97,89.91,486.47,8.64;22,55.44,101.87,165.03,8.64">We show all experimental results of supervised transfer with linear probe, CLIP zero-shot transfer, and TURTLE unsupervised transfer on 26 vision datasets in Table
                        <ref type="table" coords="22,206.10,101.87,9.58,8.64" target="#tab_0">D1</ref>.
                    </s>
                    <s coords="22,223.58,101.87,319.60,8.64">The linear probe performance of DINOv2 ViT-g/14 is also included for reference.</s>
                </p>
                <p>
                    <s coords="22,55.08,113.82,488.10,8.64">As indicated in the table, TURTLE achieves strong unsupervised transfer performance across various datasets and models.</s>
                    <s coords="22,55.44,125.78,486.00,8.64;22,55.44,137.73,138.12,8.64">For example, as illustrated in Figure
                        <ref type="figure" coords="22,205.88,125.78,9.22,8.64" target="#fig_6">E1</ref>, TURTLE 1-space
                        CLIP ViT-L/14 surpasses the corresponding CLIP zero-shot transfer on 13 out of 26 datasets.
                    </s>
                    <s coords="22,199.38,137.73,342.06,8.64;22,55.08,149.69,434.45,8.64">When trained with multiple representations (i.e., using CLIP models and DINOv2 ViT-g/14), TURTLE 2-spaces achieves superior performance on most datasets compared to TURTLE 1-space.</s>
                    <s coords="22,492.65,149.69,50.03,8.64;22,55.44,161.64,486.00,8.64;22,55.13,173.60,275.73,8.64">Remarkably, on datasets such as MNIST and CIFAR100, the absolute improvement is 31% and 21%, indicating the effectiveness of TURTLE in combining the knowledge of multiple foundation models.</s>
                    <s coords="22,333.95,173.60,207.49,8.64;22,55.08,185.55,387.96,8.64">Furthermore, as shown in Figure
                        <ref type="figure" coords="22,464.78,173.60,3.68,8.64" target="#fig_2">5</ref>, TURTLE trained
                        with CLIP ViT-L/14 and DINOv2 ViT-g/14 outperforms CLIP zero-shot on 15 out of 26 datasets.
                    </s>
                </p>
                <p>
                    <s coords="22,55.44,203.48,486.00,8.64;22,55.44,215.44,36.83,8.64">In addition, we compare the performance of TURTLE to supervised linear probe using single representation space in Figure
                        <ref type="figure" coords="22,84.65,215.44,3.81,8.64" target="#fig_3">6</ref>.
                    </s>
                    <s coords="22,95.89,215.44,445.72,8.64;22,55.44,227.40,116.86,8.64">It can be seen that there exists a strong positive correlation between the performance of unsupervised transfer and supervised linear probe.</s>
                    <s coords="22,177.82,227.40,363.62,8.64;22,55.44,239.35,486.17,8.64;22,55.44,251.31,79.83,8.64">Figure
                        <ref type="figure" coords="22,207.65,227.40,11.29,8.64" target="#fig_8">E2</ref>
                        provides the analysis of TURTLE 2-spaces trained using CLIP ViT-L/14 and DINOv2 ViT-g/14,
                        indicating that the performance of TURTLE 2-spaces is also strongly correlated with the average
                        linear probe performance.
                    </s>
                    <s coords="22,139.31,251.31,402.13,8.64;22,55.44,263.26,224.66,8.64">Overall, these results suggest that TURTLE could potentially benefit from the improved quality of representations, as measured by supervised linear probe.</s>
                </p>
                <p>
                    <s coords="22,55.44,281.19,486.00,8.64;22,55.13,293.15,40.34,8.64">Finally, it's worth noting that TURTLE 2-spaces might underperform TURTLE 1-space on some datasets, as shown in Table
                        <ref type="table" coords="22,80.52,293.15,9.97,8.64" target="#tab_0">D1</ref>.
                    </s>
                    <s coords="22,100.22,293.15,441.22,8.64;22,55.44,305.10,486.00,8.64;22,55.44,317.06,326.68,8.64">We hypothesize that the discrepancy might stem from the suboptimality of DINOv2 representations for the tasks heavily related to semantic comprehension, such as semantic analysis (Rendered SST, HatefulMemes), traffic sign recognition (GTSRB), geolocation (Country211) and object counting (CLEVR).</s>
                    <s coords="22,384.68,317.06,158.41,8.64;22,55.44,329.01,486.00,8.64;22,55.44,340.97,22.86,8.64">Since DINOv2 is pre-trained with selfsupervised objective
                        <ref type="bibr" coords="22,138.12,329.01,77.22,8.64" target="#b53">(Oquab et al., 2023)</ref>,
                        the learned features might not be directly transferable to these semantic-intensive tasks.
                    </s>
                    <s coords="22,82.07,340.97,459.37,8.64;22,55.08,352.92,385.79,8.64">Such trend could also be observed by the linear probe performance, where CLIP ViT-L/14 outperforms DINOv2 ViT-g/14 by a large margin on the Rendered SST2, CLEVR, Country211, GTSRB and FER2013.</s>
                    <s coords="22,443.98,352.92,97.46,8.64;22,55.44,364.88,329.00,8.64">Therefore, incorporating DINOv2 representations might not yield optimal results for these specific datasets.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="25,55.44,68.77,263.47,10.75">G. Imbalanced Dataset and Entropy Regularization</head>
                <p>
                    <s coords="25,55.44,89.91,486.00,8.64;25,55.44,101.69,425.29,8.82">Following
                        <ref type="bibr" coords="25,99.23,89.91,66.88,8.64" target="#b79">Xu et al. (2004);</ref>
                        <ref type="bibr" coords="25,168.59,89.91,108.39,8.64"
                             target="#b69">Van Gansbeke et al. (2020)</ref>;
                        <ref type="bibr" coords="25,283.73,89.91,101.38,8.64"
                             target="#b26">Gadetsky &amp; Brbić (2023)</ref>, we use entropy regularization (11) to
                        prevent the task encoder from producing trivial solutions, i.e., assigning all the samples to a
                        single class.
                    </s>
                    <s coords="25,484.31,101.87,57.13,8.64;25,55.44,113.50,263.92,8.96">By default we set the regularization strength to γ = 10 for all the experiments.</s>
                    <s coords="25,323.73,113.82,217.71,8.64;25,55.44,125.78,234.35,8.64">Note that the optimal solution of (
                        <ref type="formula" coords="25,465.15,113.82,8.46,8.64" target="#formula_11">11</ref>) is to
                        produce a labeling with the equal number of samples for each class.
                    </s>
                    <s coords="25,293.44,125.78,218.52,8.64">However, some of the datasets are not class balanced.</s>
                    <s coords="25,515.61,125.78,25.83,8.64;25,55.44,137.73,271.30,8.64">In this case, a strong entropy regularization might hurt the learning process.</s>
                    <s coords="25,329.83,137.73,212.86,8.64;25,55.08,149.37,487.60,8.96;25,55.44,161.32,370.93,8.96">To understand the effect of the entropy regularization, we show the average performance of TURTLE with γ ∈ {0, 1, 3, 5, 10} separately on the imbalanced datasets (Birdsnap, FER2013, GTSRB, KITTI, HatefulMemes), and the rest 21 balanced datasets in Figure
                        <ref type="figure" coords="25,411.44,161.64,9.95,8.64" target="#fig_9">G1</ref>.
                    </s>
                    <s coords="25,429.48,161.64,111.96,8.64;25,55.44,173.28,331.91,8.96">The results indicate that the entropy regularization is generally helpful since η = 0 might lead to trivial solutions.</s>
                    <s coords="25,390.40,173.60,152.28,8.64;25,55.13,185.55,274.22,8.64">Furthermore, for the balanced datasets, TURTLE is robust to the choice of the regularization hyperparameter.</s>
                    <s coords="25,332.43,185.55,209.01,8.64;25,55.44,197.51,257.11,8.64">While for the imbalanced datasets, a properly chosen regularization parameter could further improve the performance.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="25,55.44,376.08,250.52,10.75">H. TURTLE Trained and Evaluated on Test Split</head>
                <p>
                    <s coords="25,55.44,396.90,486.00,9.65;25,55.44,408.86,17.51,9.65">In previous experiments, we train TURTLE on the training split D tr and evaluate the clustering accuracy on the test split D te .</s>
                    <s coords="25,76.03,409.18,465.41,8.64;25,55.44,421.13,174.65,8.64">In this section, to study the performance of TURTLE in low data regime, we consider the setting when training and evaluating TURTLE directly on the test split.</s>
                    <s coords="25,233.08,420.81,308.36,9.65;25,55.44,432.77,135.19,9.65">Figure
                        <ref type="figure" coords="25,260.79,421.13,11.93,8.64" target="#fig_10">H1</ref>
                        compares the performance of TURTLE trained on D tr and TURTLE trained on D te on the 26
                        datasets.
                    </s>
                    <s coords="25,193.72,433.09,175.02,8.64">Both settings are evaluated on the test split.</s>
                    <s coords="25,371.82,433.09,169.62,8.64;25,55.44,444.72,486.00,9.65;25,55.44,457.00,49.83,8.64">As shown in the plot, TURTLE trained on D te achieves nearly identical performance as TURTLE trained on D tr for 24 out of 26 datasets, except Caltech101 and Flowers102.</s>
                    <s coords="25,108.38,457.00,433.06,8.64;25,55.44,468.95,151.54,8.64">We found the discrepancy might be attributed to the fact that the Caltech101 and Flowers102 have balanced training split but imbalanced test split.</s>
                    <s coords="25,210.07,468.95,331.37,8.64;25,55.44,480.91,165.45,8.64">Overall, the results suggest that TURTLE does not require a large amount of data to perform successful unsupervised transfer.</s>
                </p>
            </div>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"
                    coords="6,306.94,617.38,235.99,8.12;6,307.44,628.69,234.00,7.77;6,307.44,639.65,235.49,7.77;6,307.44,650.61,234.00,7.77;6,307.44,661.57,132.24,7.77">
                <head>Figure 2 .Figure 3 .</head>
                <label>23</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="6,306.94,617.38,213.00,8.12">Figure 2. TURTLE outperforms unsupervised baselines.</s>
                            <s coords="6,522.75,617.73,20.18,7.77;6,307.44,628.69,234.00,7.77;6,307.44,639.65,33.14,7.77">Comparison of TURTLE to unsupervised baselines with respect to accuracy.</s>
                            <s coords="6,343.36,639.65,199.57,7.77;6,307.44,650.61,38.15,7.77">All methods use the CLIP ViT L/14 and DINOv2 representations.</s>
                            <s coords="6,348.37,650.61,193.07,7.77;6,307.44,661.57,132.24,7.77">Bars represent the average performance with standard deviations computed over three runs.</s>
                        </p>
                    </div>
                </figDesc>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"
                    coords="7,54.94,570.28,235.99,8.12;7,55.44,581.23,234.00,8.12;7,55.44,592.54,235.49,7.77;7,55.44,603.50,234.00,7.77;7,55.44,614.46,234.00,7.77;7,55.44,625.42,234.00,7.77;7,55.44,636.38,194.60,7.77">
                <head>Figure 4 .</head>
                <label>4</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="7,54.94,570.28,235.99,8.12;7,55.44,581.23,152.33,8.06">Figure 4. TURTLE enables unsupervised transfer given representation spaces of foundation models.</s>
                            <s coords="7,212.88,581.58,76.57,7.77;7,55.44,592.54,235.49,7.77;7,55.44,603.50,234.00,7.77;7,55.44,614.46,62.85,7.77">Employing the same CLIP representation space, TURTLE closely matches the performance of the corresponding CLIP zero-shot classifier on average over 26 datasets.</s>
                            <s coords="7,124.06,614.46,165.38,7.77;7,55.44,625.42,234.00,7.77;7,55.44,636.38,194.60,7.77">With the use of an additional representation space, TURTLE outperforms zero-shot transfer, demonstrating exceptional abilities of unsupervised transfer learning.</s>
                        </p>
                    </div>
                </figDesc>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"
                    coords="7,306.94,357.21,234.70,8.12;7,307.44,368.17,234.00,8.12;7,307.44,379.48,234.00,7.77;7,307.44,390.43,234.00,7.77;7,307.12,401.39,234.32,7.77;7,307.44,412.07,132.62,8.06">
                <head>Figure 5 .</head>
                <label>5</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="7,306.94,357.56,32.38,7.77">Figure5.</s>
                            <s coords="7,341.56,357.21,200.08,8.06;7,307.44,368.17,89.84,8.06">TURTLE outperforms the CLIP zero-shot classifier on 15 out of 26 datasets.</s>
                            <s coords="7,400.05,368.52,141.39,7.77;7,307.44,379.48,108.39,7.77">TURTLE is trained with CLIP ViT-L/14 and DINOv2 representations.</s>
                            <s coords="7,420.16,379.48,121.28,7.77;7,307.44,390.43,103.70,7.77">CLIP zero-shot utilizes the same CLIP ViT-L/14 architecture.</s>
                            <s coords="7,414.32,390.43,127.13,7.77;7,307.12,401.39,234.32,7.77;7,307.44,412.07,132.62,8.06">Furthermore, we observe that even with only a single CLIP representation space TURTLE outperforms CLIP on 13/26 datasets (FigureE1).</s>
                        </p>
                    </div>
                </figDesc>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"
                    coords="8,306.94,402.39,234.50,8.12;8,307.44,413.34,234.00,8.12;8,307.44,424.36,234.00,8.06;8,307.44,435.61,234.00,7.77;8,307.44,446.28,149.20,8.06;8,456.64,444.52,9.41,5.24;8,468.28,446.57,73.16,7.77;8,307.44,457.24,234.00,8.06;8,307.44,468.20,234.00,8.06;8,307.44,479.45,41.34,7.77">
                <head>Figure 6 .</head>
                <label>6</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="8,306.94,402.73,32.38,7.77">Figure6.</s>
                            <s coords="8,341.56,402.39,199.88,8.06;8,307.44,413.34,204.29,8.06">Unsupervised transfer performance of TURTLE is correlated with supervised linear probe performance.</s>
                            <s coords="8,514.52,413.69,26.92,7.77;8,307.44,424.36,213.65,8.06">Dashed line y = x denotes the "optimal" unsupervised transfer.</s>
                            <s coords="8,527.22,424.65,14.22,7.77;8,307.44,435.61,234.00,7.77;8,307.44,446.28,149.20,8.06;8,456.64,444.52,9.41,5.24;8,468.28,446.57,73.16,7.77;8,307.44,457.53,85.89,7.77">The performance of TURTLE and supervised linear probe shows a strong correlation (ρ = 0.87, p = 6.3×10 -9 of two-sided Pearson correlation coefficient).</s>
                            <s coords="8,397.07,457.24,144.37,8.06;8,307.44,468.20,234.00,8.06;8,307.44,479.45,41.34,7.77">On 5 datasets TURTLE approaches the performance of the "optimal" unsupervised transfer (≤ 3 point difference).</s>
                        </p>
                    </div>
                </figDesc>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"
                    coords="9,54.94,246.01,234.51,8.12;9,55.44,256.96,234.00,8.12;9,55.44,268.27,235.49,7.77;9,54.77,279.23,235.79,7.77;9,55.12,290.19,234.48,7.77;9,55.44,301.15,235.49,7.77;9,55.16,312.11,234.28,7.77;9,55.44,322.78,176.43,8.06;9,231.87,321.01,9.41,5.24;9,244.03,323.07,45.42,7.77;9,55.44,334.03,114.56,7.77;9,132.09,109.43,108.81,108.81">
                <head>Figure 7 .</head>
                <label>7</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="9,54.94,246.01,234.51,8.12;9,55.44,257.03,161.37,8.06">Figure 7. Top: Supervised linear probe on the ImageNet-1000 dataset for 7 different representation spaces.</s>
                            <s coords="9,219.75,256.96,69.69,8.12;9,55.44,268.27,235.49,7.77;9,54.77,279.23,20.54,7.77">Bottom: Heat map represents unsupervised performance of TURTLE on ImageNet-1000.</s>
                            <s coords="9,78.11,279.23,212.45,7.77;9,55.12,290.19,234.48,7.77;9,55.44,301.15,139.75,7.77">Secondary diagonal cells correspond to TURTLE 1-space, while off-diagonal cells refer to TURTLE 2-spaces with the pair of corresponding representation spaces.</s>
                            <s coords="9,197.97,301.15,92.96,7.77;9,55.16,312.11,234.28,7.77;9,55.44,322.78,176.43,8.06;9,231.87,321.01,9.41,5.24;9,244.03,323.07,45.42,7.77;9,55.44,334.03,114.56,7.77">The performance of TUR-TLE indicates a strong positive correlation with the performance of supervised linear probe (ρ = 0.74, p = 1.4 × 10 -9 of two-sided Pearson correlation coefficient).</s>
                        </p>
                    </div>
                </figDesc>
                <graphic coords="9,132.09,109.43,108.81,108.81" type="bitmap"/>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"
                    coords="19,54.94,427.67,486.82,8.12;19,55.44,438.69,486.00,8.06;19,55.44,449.65,486.00,8.06;19,55.44,460.90,136.88,7.77">
                <head>Figure B1 .</head>
                <label>B1</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="19,54.94,428.02,38.36,7.77">Figure B1.</s>
                            <s coords="19,95.54,427.67,131.69,8.06">Task selection via cross-validation.</s>
                            <s coords="19,230.02,428.02,276.33,7.77">We use TURTLE 2-spaces CLIP ViT-L/14 and DINOv2 to produce the tasks.</s>
                            <s coords="19,509.15,428.02,32.61,7.77;19,55.44,438.69,486.00,8.06;19,55.44,449.94,59.27,7.77">We show the cross-validation score and corresponding clustering accuracy of the tasks learned by TURTLE with 50 different hyperparameters for each dataset.</s>
                            <s coords="19,117.50,449.65,423.94,8.06;19,55.44,460.90,136.88,7.77">The cross-validation score is well correlated with the clustering accuracy (ρ = 0.61 of two-sided Pearson correlation coefficient averaged over 26 datasets).</s>
                        </p>
                    </div>
                </figDesc>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"
                    coords="22,54.94,633.92,219.20,8.12;22,55.44,644.88,218.70,8.06;22,55.44,655.84,218.70,8.12;22,55.44,667.15,218.70,7.77;22,55.44,678.10,220.18,7.77;22,55.44,689.06,105.45,7.77">
                <head>Figure E1 .</head>
                <label>E1</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="22,54.94,634.27,37.86,7.77">Figure E1.</s>
                            <s coords="22,95.04,633.92,179.10,8.06;22,55.44,644.88,218.70,8.06;22,55.44,655.84,33.45,8.06">Using the same representation space, TURTLE outperforms CLIP zero-shot classifier on 13 out of 26 datasets.</s>
                            <s coords="22,94.44,656.19,179.70,7.77;22,55.44,667.15,99.47,7.77">TURTLE is trained with CLIP ViT-L/14 and does not require any supervision.</s>
                            <s coords="22,157.68,667.15,116.46,7.77;22,55.44,678.10,220.18,7.77;22,55.44,689.06,105.45,7.77">CLIP zero-shot utilizes the same architecture, but requires the additional text encoder and description of visual categories.</s>
                        </p>
                    </div>
                </figDesc>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"
                    coords="22,302.92,633.92,235.26,8.12;22,303.42,644.88,234.76,8.12;22,303.42,656.19,233.28,7.77;22,303.42,667.15,234.77,7.77;22,303.42,677.82,144.70,8.06;22,448.12,676.05,9.41,5.24;22,460.04,678.10,76.66,7.77;22,303.42,689.06,83.93,7.77">
                <head>Figure E2 .</head>
                <label>E2</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="22,302.92,634.27,37.86,7.77">Figure E2.</s>
                            <s coords="22,343.02,633.92,195.17,8.06;22,303.42,644.88,198.42,8.06">Unsupervised transfer learning performance is correlated with supervised linear probe performance.</s>
                            <s coords="22,506.54,645.23,31.64,7.77;22,303.42,656.19,233.28,7.77;22,303.42,667.15,234.77,7.77;22,303.42,677.82,144.70,8.06;22,448.12,676.05,9.41,5.24;22,460.04,678.10,76.66,7.77;22,303.42,689.06,83.93,7.77">The performance of TURTLE 2-spaces is strongly correlated with the average performance of linear probe using CLIP ViT-L/14 and DI-NOv2 ViT-g/14 (ρ = 0.88, p = 2.3 × 10 -9 for two-sided Pearson correlation coefficient).</s>
                        </p>
                    </div>
                </figDesc>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"
                    coords="25,54.94,340.39,487.62,8.12;25,55.44,351.70,466.93,7.77">
                <head>Figure G1 .</head>
                <label>G1</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="25,54.94,340.74,38.85,7.77">Figure G1.</s>
                            <s coords="25,96.03,340.39,143.92,8.06">Ablation of the entropy regularization.</s>
                            <s coords="25,242.64,340.74,299.92,7.77;25,55.44,351.70,466.93,7.77">We show the average performance for class imbalanced datasets (Birdsnap, FER2013, GTSRB, KITTI, HatefulMemes) and class balanced datasets (the rest 21 datasets) for the different entropy regularization strength.</s>
                        </p>
                    </div>
                </figDesc>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"
                    coords="25,54.94,678.60,486.50,8.12;25,55.44,689.56,486.00,8.12;25,55.44,700.86,148.52,7.77">
                <head>Figure H1 .</head>
                <label>H1</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="25,54.94,678.95,38.85,7.77">Figure H1.</s>
                            <s coords="25,96.03,678.60,445.41,8.06;25,55.44,689.56,32.56,8.06">TURTLE trained on test split achieves similar performance as TURTLE trained on training split for 24 out 26 of datasets.</s>
                            <s coords="25,90.79,689.91,150.67,7.77">Results are both evaluated on the test split.</s>
                            <s coords="25,244.25,689.91,297.19,7.77;25,55.44,700.86,148.52,7.77">The discrepancy of Caltech101 and Flowers102 is because that they are balanced on training split but imbalanced on test split.</s>
                        </p>
                    </div>
                </figDesc>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" coords="2,91.89,67.06,413.10,182.03">
                <head/>
                <label/>
                <figDesc>
                    <div>
                        <p/>
                    </div>
                </figDesc>
                <graphic coords="2,91.89,67.06,413.10,182.03" type="bitmap"/>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"
                    coords="6,54.89,70.54,488.29,646.68">
                <head>Table 1 .</head>
                <label>1</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="6,85.80,548.48,205.13,8.06;6,55.44,559.44,60.68,8.06">Differences between the considered types of downstream transfer.</s>
                            <s coords="6,238.72,708.58,50.72,8.64;6,307.44,70.54,234.17,8.64;6,307.44,82.49,114.12,8.64">We refer the reader to Appendix B.3 for the detailed description of our model selection procedures.</s>
                            <s coords="6,425.39,82.49,116.05,8.64;6,307.44,95.38,218.18,7.01">Code is publicly available at https://github.com/mlbio-epfl/turtle.</s>
                        </p>
                    </div>
                </figDesc>
                <table coords="6,55.44,570.80,235.65,86.63">
                    <row>
                        <cell/>
                        <cell cols="2">Available Supervision Training on D</cell>
                    </row>
                    <row>
                        <cell>Unsupervised transfer (ours)</cell>
                        <cell>Number of classes</cell>
                        <cell>✓</cell>
                    </row>
                    <row>
                        <cell>Zero-shot transfer</cell>
                        <cell>Class descriptions</cell>
                        <cell>✗</cell>
                    </row>
                    <row>
                        <cell>Unsupervised prompt tuning</cell>
                        <cell>Class descriptions</cell>
                        <cell>✓</cell>
                    </row>
                    <row>
                        <cell>Supervised transfer</cell>
                        <cell>Labeled samples</cell>
                        <cell>✓</cell>
                    </row>
                    <row>
                        <cell cols="3">Model Selection. Gadetsky &amp; Brbić (2023) showed</cell>
                    </row>
                    <row>
                        <cell cols="3">that generalization-based objective (1) is strikingly well-</cell>
                    </row>
                </table>
                <note coords="6,55.44,660.75,234.00,8.64;6,55.44,672.53,235.65,8.82;6,55.44,684.49,235.65,8.82;6,55.19,696.62,234.25,8.64;6,55.44,708.58,178.93,8.64;6,307.13,293.65,235.96,8.64;6,307.44,305.61,234.00,8.64;6,307.44,317.56,234.00,8.64;6,307.44,329.52,235.66,8.64;6,307.44,341.47,234.00,8.64;6,307.44,353.43,93.52,8.64;6,307.08,371.36,234.36,8.64;6,307.44,383.31,235.65,8.64;6,307.44,395.27,234.00,8.64;6,307.44,407.23,234.00,8.64;6,307.44,419.18,235.74,8.64;6,307.44,430.82,235.66,8.96;6,307.44,442.77,233.99,8.96;6,307.44,455.05,235.25,8.64;6,307.44,467.00,234.00,8.64;6,307.44,478.96,235.65,8.64;6,307.44,490.59,235.65,8.96;6,307.44,502.55,154.67,8.96">
                    <p>
                        <s coords="6,55.44,660.75,39.71,8.64">(Alkin et al., 2024)n labelings, which we further confirm in FigureB1on 26 datasets.Notably, this enables unsupervised hyperparameter search in TURTLE.For supervised linear probes, we perform standard cross-validation to search for the L2-regularization strength.TURTLE.The K-Means clustering serves as the simple unsupervised transfer baseline since, like TURTLE, it does not require task-specific representation learning.We refer the readers to Appendix C for the detailed description of the improvements made to HUME as well as the implementation details of the K-Means.As shown in Figure2, TURTLE substiantially outperforms HUME on all considered datasets, confirming that maximizing margin in both spaces simultaneously to search for the underlying human labeling (8) and expanding the search space of labelings (9) is the effective design choice.Remarkably, TURTLE leads to 23% and 11% absolute improvements (30% and 18% relative improvement) on the MNIST and Birdsnap datasets, respectively.Furthermore, among other datasets, TURTLE sets the new state-of-the-art unsupervised performance on the ImageNet dataset, achieving 72.9% accuracy and outperforming the previous stateof-the-art(Alkin et al., 2024)by 5.5%.</s>
                    </p>
                </note>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"
                    coords="8,54.89,77.01,488.04,105.23">
                <head>Table 2 .</head>
                <label>2</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="8,85.80,77.01,457.13,8.12;8,55.44,88.32,486.00,7.77;8,55.44,99.28,181.77,7.77;8,72.42,115.05,449.25,8.96">TURTLE 2-spaces outperforms unsupervised prompt tuning methods.ZS column indicates whether method utilizes zeroshot supervision to make predictions.All methods employ CLIP ResNet-50 representations.TURTLE additionally uses DINOv2 representations as the second representation space.Method ZS Pets Flowers FGVC DTD EuroSAT Cars Food SUN Caltech UCF ImageNet Avg.</s>
                        </p>
                    </div>
                </figDesc>
                <table coords="8,70.23,131.83,450.90,50.41">
                    <row>
                        <cell>POUF</cell>
                        <cell>✓</cell>
                        <cell>88.0 66.7</cell>
                        <cell>16.7 41.5</cell>
                        <cell>42.1</cell>
                        <cell>57.4 74.7 58.6 86.9 61.1</cell>
                        <cell>55.2</cell>
                        <cell>59.0</cell>
                    </row>
                    <row>
                        <cell>UPL</cell>
                        <cell>✓</cell>
                        <cell>88.3 68.9</cell>
                        <cell>17.3 46.6</cell>
                        <cell>54.8</cell>
                        <cell>62.1 77.6 64.0 89.9 67.2</cell>
                        <cell>60.5</cell>
                        <cell>63.4</cell>
                    </row>
                    <row>
                        <cell>GDA</cell>
                        <cell>✓</cell>
                        <cell>89.9 72.7</cell>
                        <cell>18.7 46.8</cell>
                        <cell>49.9</cell>
                        <cell>60.8 78.3 63.6 87.5 68.7</cell>
                        <cell>61.2</cell>
                        <cell>63.5</cell>
                    </row>
                    <row>
                        <cell cols="2">TURTLE ✗</cell>
                        <cell>90.9 99.7</cell>
                        <cell>25.3 57.0</cell>
                        <cell>95.5</cell>
                        <cell>32.6 84.1 65.7 88.6 77.7</cell>
                        <cell>66.3</cell>
                        <cell>71.2</cell>
                    </row>
                </table>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"
                    coords="20,54.89,485.40,488.12,83.94">
                <head>Table C1 .</head>
                <label>C1</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="20,91.78,485.40,191.64,8.06">Accuracy of TURTLE and unsupervised baselines.</s>
                            <s coords="20,286.19,485.47,256.82,8.06">The results are averaged with standard deviations computed over 3 runs.</s>
                        </p>
                    </div>
                </figDesc>
                <table coords="20,129.97,507.50,334.46,61.85">
                    <row>
                        <cell>Method</cell>
                        <cell>MNIST</cell>
                        <cell>CIFAR100 Food101 Birdsnap ImageNet</cell>
                    </row>
                    <row>
                        <cell>K-Means</cell>
                        <cell cols="2">68.9 ± 0.6 75.1 ± 0.5 78.0 ± 0.7 54.0 ± 0.8 64.8 ± 0.3</cell>
                    </row>
                    <row>
                        <cell>HUME CLIP</cell>
                        <cell cols="2">75.3 ± 5.0 71.2 ± 2.2 86.5 ± 1.3 45.8 ± 1.8 65.2 ± 0.9</cell>
                    </row>
                    <row>
                        <cell cols="3">HUME DINOv2 69.7 ± 5.9 83.9 ± 1.2 85.3 ± 1.7 57.3 ± 0.7 68.1 ± 0.2</cell>
                    </row>
                    <row>
                        <cell>TURTLE</cell>
                        <cell cols="2">98.0 ± 0.4 89.1 ± 1.0 92.8 ± 0.5 67.8 ± 0.4 72.4 ± 0.3</cell>
                    </row>
                </table>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"
                    coords="20,54.89,590.00,486.55,94.90">
                <head>Table C2 .</head>
                <label>C2</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="20,91.78,590.00,264.56,8.06">Training time (in minutes) of TURTLE and unsupervised baselines.</s>
                            <s coords="20,359.88,590.35,181.56,7.77;20,55.44,601.02,80.04,8.06">The results are averaged with standard deviations computed over 3 runs.</s>
                            <s coords="20,138.26,601.30,229.95,7.77">The standard deviation for K-Means and TURTLE is negligible.</s>
                        </p>
                    </div>
                </figDesc>
                <table coords="20,125.46,623.05,343.47,61.85">
                    <row>
                        <cell>Method</cell>
                        <cell>MNIST</cell>
                        <cell cols="2">CIFAR100 Food101</cell>
                        <cell>Birdsnap</cell>
                        <cell>ImageNet</cell>
                    </row>
                    <row>
                        <cell>K-Means</cell>
                        <cell cols="2">0.02 ± 0.0 0.04 ± 0.0</cell>
                        <cell>0.1 ± 0.0</cell>
                        <cell>0.1 ± 0.0</cell>
                        <cell>6.4 ± 0.0</cell>
                    </row>
                    <row>
                        <cell>HUME CLIP</cell>
                        <cell cols="5">15.2 ± 2.4 44.1 ± 0.5 45.5 ± 1.3 156.8 ± 1.2 285.4 ± 4.1</cell>
                    </row>
                    <row>
                        <cell cols="6">HUME DINOv2 11.1 ± 0.3 31.7 ± 0.4 31.0 ± 0.3 96.7 ± 0.6 185.6 ± 7.9</cell>
                    </row>
                    <row>
                        <cell>TURTLE</cell>
                        <cell>1.6 ± 0.0</cell>
                        <cell>1.6 ± 0.0</cell>
                        <cell>1.7 ± 0.0</cell>
                        <cell>2.1 ± 0.0</cell>
                        <cell>4.8 ± 0.0</cell>
                    </row>
                </table>
            </figure>
            <note
                    xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"
                    coords="2,323.16,669.80,218.28,7.77;2,307.44,679.76,235.49,7.77;2,307.44,689.72,234.00,7.77;2,307.44,699.40,186.56,8.06">
                <p>
                    <s coords="2,323.16,669.80,218.28,7.77;2,307.44,679.76,204.74,7.77">We interchangeably use terms "task" and "labeling" in the context of this paper, since any labeling defines a task.</s>
                    <s coords="2,517.02,679.76,25.91,7.77;2,307.44,689.72,234.00,7.77;2,307.44,699.40,186.56,8.06">Consequently, we refer to a task as human labeled if it corresponds to the underlying human labeling of a given dataset D.</s>
                </p>
            </note>
            <note
                    xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"
                    coords="17,71.58,699.26,469.86,7.77;17,55.44,709.22,190.18,7.77">
                <p>
                    <s coords="17,71.58,699.26,469.86,7.77;17,55.44,709.22,171.26,7.77">Since the original paper does not release the models, we use the reproduced version from the OpenCLIP project, which could be found at https://github.com/mlfoundations/open</s>
                    <s coords="17,229.92,709.22,15.69,7.77">clip.</s>
                </p>
            </note>
            <note
                    xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"
                    coords="19,71.30,709.22,216.42,7.77">
                <p>
                    <s coords="19,71.30,709.22,216.42,7.77">This library is available at https://github.com/rapidsai/cuml.</s>
                </p>
            </note>
            <note
                    xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"
                    coords="20,71.58,709.22,217.83,7.77">
                <p>
                    <s coords="20,71.58,709.22,217.83,7.77">Code could be found at https://github.com/mlbio-epfl/hume.</s>
                </p>
            </note>
        </body>
        <back>
            <div type="acknowledgement">
                <div>
                    <head coords="10,55.44,68.77,98.83,10.75">Acknowledgements</head>
                    <p>We thank
                        <rs type="person">Chanakya Ekbote</rs>,
                        <rs type="person">Shuo Wen</rs>
                        and
                        <rs type="person">Tingyang Yu</rs>
                        for valuable suggestions that helped to improve the clarity of the manuscript.We also thank
                        <rs type="person">Nikita Doikov</rs>
                        for fruitful discussions regarding efficient bilevel optimization techniques.We gratefully
                        acknowledge the support of
                        <rs type="funder">EPFL</rs>
                        and ZEISS.
                    </p>
                </div>
            </div>
            <listOrg type="funding"></listOrg>
            <div type="annex">
                <div
                        xmlns="http://www.tei-c.org/ns/1.0">
                    <head coords="17,55.44,70.15,87.54,8.96">B.2. Representations</head>
                    <p>
                        <s coords="17,55.13,89.19,238.13,8.64">TURTLE is compatible with any pre-trained representations.</s>
                        <s coords="17,296.40,89.19,245.05,8.64;17,55.44,101.15,487.74,8.64">This paper presents the comprehensive evaluation of TURTLE on a wide range of representation spaces that vary on the pre-training datasets, model architectures and training objectives.</s>
                        <s coords="17,55.44,113.10,486.00,8.64;17,55.11,125.06,488.07,8.64">Specifically, we consider CLIP ResNets (RN50, RN101, RN50x4, RN50x16, RN50x64) and CLIP Vision Transformers (ViT-B/32, ViT-B/16, ViT-L/14) pre-trained on WebImageText-400M
                            <ref type="bibr" coords="17,333.99,125.06,86.08,8.64"
                                 target="#b56">(Radford et al., 2021)</ref>
                            for training TURTLE 1-space.
                        </s>
                        <s coords="17,55.13,137.01,370.55,8.64">These models are pre-trained on the same dataset, scaling with number of the parameters.</s>
                        <s coords="17,430.45,137.01,110.99,8.64;17,55.44,148.97,443.92,8.64">For TURTLE 2-spaces, we incorporate DINOv2 ViT-g/14 pre-trained on LVD-142M
                            <ref type="bibr" coords="17,281.95,148.97,78.45,8.64"
                                 target="#b53">(Oquab et al., 2023)</ref>
                            as the second representation space.
                        </s>
                        <s coords="17,502.45,148.97,40.24,8.64;17,55.08,160.92,486.36,8.64;17,55.44,172.88,160.72,8.64;17,218.64,171.21,3.49,6.05;17,222.63,172.88,318.81,8.64;17,55.08,184.83,487.60,8.64;17,55.44,196.79,486.00,8.64;17,55.44,208.74,166.51,8.64">Moreover, we also include SWAG ViT-H/14 pre-trained on IG-3.6B
                            <ref type="bibr" coords="17,284.44,160.92,74.95,8.64"
                                 target="#b64">(Singh et al., 2022)</ref>, CoCa ViT-L/14
                            <ref type="bibr" coords="17,430.61,160.92,64.56,8.64" target="#b80">(Yu et al., 2022)</ref>
                            pre-trained on LAION-2B
                            <ref type="bibr" coords="17,116.09,172.88,100.07,8.64" target="#b61">(Schuhmann et al., 2022)</ref>
                            2 , OpenCLIP ViT-L/14 pre-trained on LAION-2B
                            <ref type="bibr" coords="17,418.76,172.88,76.22,8.64"
                                 target="#b14">(Cherti et al., 2023)</ref>, MOCOv3 ViT-B/16 pre-trained on ImageNet-1000
                            <ref type="bibr" coords="17,220.79,184.83,75.43,8.64"
                                 target="#b12">(Chen et al., 2021)</ref>
                            and EsViT Swim-B pre-trained on ImageNet-1000
                            <ref type="bibr" coords="17,505.69,184.83,36.99,8.64;17,55.44,196.79,23.34,8.64"
                                 target="#b44">(Li et al., 2022)</ref>
                            to study whether incorporating stronger representations on the given dataset may lead to the
                            increased performance of unsupervised transfer with TURTLE.
                        </s>
                        <s coords="17,225.56,208.74,315.88,8.64;17,55.44,220.70,378.65,8.64">For all the models, we precompute the representations with standard image preprocessing pipelines and do not use any data augmentations during training of TURTLE.</s>
                        <s coords="17,437.08,220.70,104.36,8.64;17,55.44,232.65,166.17,8.64">The details of pre-trained representations are provided on Table
                            <ref type="table" coords="17,207.49,232.65,9.41,8.64">B2</ref>.
                        </s>
                        <s coords="17,120.92,459.63,76.65,8.64;17,224.08,459.63,37.78,8.64;17,289.98,459.63,23.80,8.64;17,363.14,459.63,32.93,8.64;17,470.00,459.06,7.52,9.54;17,99.31,473.38,89.65,8.64;17,224.63,473.38,36.67,8.64;17,289.97,473.38,23.80,8.64;17,356.64,473.38,45.93,8.64;17,470.00,472.81,7.52,9.54;17,85.75,487.13,116.77,8.64;17,224.35,487.13,37.23,8.64;17,292.46,487.13,18.82,8.64;17,348.34,487.13,62.53,8.64;17,470.92,486.56,5.69,9.54;17,99.06,500.88,90.16,8.64;17,227.74,500.88,30.45,8.64;17,292.47,500.88,18.82,8.64;17,348.35,500.88,62.53,8.64;17,470.92,500.31,5.69,9.54;17,85.48,514.62,117.31,8.64;17,225.19,514.62,35.57,8.64;17,292.32,514.62,19.10,8.64;17,356.31,514.62,46.60,8.64;17,470.92,514.06,5.69,9.54;23,54.97,89.91,486.46,8.64;23,55.44,101.87,95.23,8.64">
                            <ref type="bibr" coords="17,120.92,459.63,76.65,8.64"
                                 target="#b64">(Singh et al., 2022)</ref>
                            ViT-H/14 630M IG-3.6B ✓ CoCa
                            <ref type="bibr" coords="17,124.49,473.38,64.47,8.64" target="#b80">(Yu et al., 2022)</ref>
                            ViT-L/14 640M LAION-2B ✓ MOCOv3
                            <ref type="bibr" coords="17,128.09,487.13,74.43,8.64"
                                 target="#b12">(Chen et al., 2021)</ref>
                            ViT-B/16 86M ImageNet-1000 ✗ EsViT
                            <ref type="bibr" coords="17,126.96,500.88,62.26,8.64" target="#b44">(Li et al., 2022)</ref>
                            Swin-B 88M ImageNet-1000 ✗ DINOv2
                            <ref type="bibr" coords="17,122.83,514.62,79.96,8.64"
                                 target="#b53">(Oquab et al., 2023)</ref>
                            ViT-g/14 1.1B LVD-142M ✗ We provide the complete results for TURTLE on each dataset for
                            multiple architectures and sizes scaling in the number of parameters in Table
                            <ref type="table" coords="23,136.04,101.87,9.75,8.64">D1</ref>.
                        </s>
                        <s coords="23,153.76,101.55,387.68,8.96;23,55.44,113.82,469.12,8.64">Specifically, we consider 8 CLIP models trained in
                            <ref type="bibr" coords="23,358.19,101.87,83.33,8.64"
                                 target="#b56">Radford et al. (2021)</ref>
                            with scaling model sizes of ResNets (RN50, RN101, RN50x4, RN50x16, RNx64) and Vision
                            Transformers (ViT-B/32, ViT-B/16, ViT-L/14).
                        </s>
                        <s coords="23,527.66,113.82,13.96,8.64;23,55.44,125.78,486.00,8.64;23,55.08,137.73,255.52,8.64">For each CLIP representation, we train TURTLE 1-space with the aforemention representation spaces, and TURTLE 2-spaces with DINOv2 ViT-g/14 used as the second representation space.</s>
                    </p>
                </div>
                <div
                        xmlns="http://www.tei-c.org/ns/1.0">
                    <head coords="21,55.44,68.77,252.36,10.75">D. Complete List of Individual Numerical Results</head>
                    <p>
                        <s coords="23,55.44,155.66,486.00,8.64;23,55.13,167.62,319.37,8.64">Figure
                            <ref type="figure" coords="23,83.81,155.66,10.47,8.64">F1</ref>
                            and Figure
                            <ref type="figure" coords="23,141.94,155.66,10.47,8.64">F2</ref>
                            show the average performance of TURTLE trained with different CLIP ResNets and CLIP Vision
                            Transformers, and compare it with the performance of the CLIP zero-shot transfer.
                        </s>
                        <s coords="23,377.48,167.62,163.97,8.64;23,55.44,179.57,380.97,8.64">As indicated by the plots, the performance of TURTLE 1-space and TURTLE 2-spaces smoothly improves as the model compute increases.</s>
                        <s coords="23,439.49,179.57,101.95,8.64;23,55.44,191.21,486.00,8.96;23,55.44,203.48,342.00,8.64">Moreover, although being fully unsupervised, TURTLE 1-space achieves competitive performance (i.e., with less than 1 point difference) compared to the zero-shot transfer for RN50, RN101, RN50x4, ViT-B/32, ViT-B/16 and ViT-L/14.</s>
                        <s coords="23,400.53,203.48,140.92,8.64;23,55.44,215.44,327.54,8.64">Using RN50x16 and RN50x64, the performance TURTLE 1-space becomes a little bit worse than zero-shot transfer.</s>
                        <s coords="23,386.07,215.44,155.37,8.64;23,55.44,227.40,469.11,8.64">However, with the additional DINOv2 representations, TURTLE 2-spaces consistently outperforms zero-shot transfer for all the models by a large margin.</s>
                        <s coords="23,527.64,227.40,13.97,8.64;23,55.44,239.03,368.28,8.96">For example, TURTLE 2-spaces CLIP ViT-L/14 is on average 4% better than zero-shot transfer.</s>
                        <s coords="23,426.82,239.35,114.62,8.64;23,55.44,251.31,447.51,8.64">For completeness, Figure
                            <ref type="figure" coords="23,530.84,239.35,10.59,8.64">F3</ref>
                            and Figure
                            <ref type="figure" coords="23,100.82,251.31,10.52,8.64">F4</ref>
                            also provide the performance of TURTLE and CLIP zero-shot transfer on each individual
                            dataset.
                        </s>
                    </p>
                    <p>
                        <s coords="23,55.44,269.24,487.74,8.64">Overall, the performance of TURTLE follows a similar scaling trend as the zero-shot transfer
                            <ref type="bibr" coords="23,450.09,269.24,88.72,8.64"
                                 target="#b56">(Radford et al., 2021)</ref>.
                        </s>
                        <s coords="23,55.44,281.19,486.00,8.64;23,55.44,293.15,151.05,8.64">Furthermore, TURTLE can effectively combine the knowledge of multiple foundation models to further improve the performance of unsupervised transfer.</s>
                    </p>
                </div>
                <div
                        xmlns="http://www.tei-c.org/ns/1.0">
                    <head coords="26,55.44,68.77,267.04,10.75">I. Additional Analysis on Fine-grained Classification</head>
                    <p>
                        <s coords="26,55.44,89.59,486.00,8.96;26,55.44,101.87,352.17,8.64">In previous sections, we have evaluated the performance of TURTLE on 26 datasets, including 6 fine-grained classification datasets: Food101, Flowers102, Birdsnap, StanfordCars, FGVCAircraft and OxfordPets.</s>
                        <s coords="26,410.70,101.87,132.39,8.64;26,55.44,113.50,486.67,8.96;26,55.44,125.46,265.10,8.96">According to the results from Table D1 and Figure
                            <ref type="figure" coords="26,128.20,113.82,3.66,8.64">5</ref>, TURTLE outperforms CLIP
                            zero-shot transfer on 3 datasets (Flowers102, Birdsnap and FGVCAircraft) and performs
                            comparably on 2 datasets (Food101 and OxfordPets).
                        </s>
                        <s coords="26,323.64,125.78,217.97,8.64;26,55.44,137.73,149.14,8.64">The results indicate that TURTLE remains effective for the task of fine-grained classification.</s>
                    </p>
                    <p>
                        <s coords="26,55.13,155.66,486.31,8.64;26,55.44,167.62,316.01,8.64">To further study the dependence between number of classes and the performance of TURTLE, we perform the additional experiments on 4 datasets from the BREEDS benchmark
                            <ref type="bibr" coords="26,279.01,167.62,88.24,8.64"
                                 target="#b60">(Santurkar et al., 2021)</ref>.
                        </s>
                        <s coords="26,374.49,167.62,166.95,8.64;26,55.44,179.57,187.43,8.64">These datasets are the subsets of ImageNet that contain both coarse and fine-grained labels.</s>
                        <s coords="26,245.95,179.57,295.49,8.64;26,55.44,191.53,299.06,8.64">We run TURTLE for each dataset to infer the coarse and fine grained labels separately by specifying the ground truth number of classes for each case.</s>
                        <s coords="26,357.59,191.53,183.85,8.64;26,55.44,203.48,150.65,8.64">The statistics for each dataset and TURTLE's performance are provided in Table
                            <ref type="table" coords="26,195.30,203.48,7.19,8.64">I1</ref>.
                        </s>
                    </p>
                    <p>
                        <s coords="26,54.97,221.42,486.47,8.64;26,55.44,233.37,60.70,8.64">We observe that TURTLE performs worse on the fine-grained classification compared to the coarse-grained classification on LIVING-17.</s>
                        <s coords="26,119.23,233.37,381.83,8.64">The result is expected since fine-grained classification is considered to be a more difficult task.</s>
                        <s coords="26,504.14,233.37,38.55,8.64;26,55.44,245.33,486.00,8.64;26,55.44,257.28,192.13,8.64">However, on ENTITY-13, ENTITY-30 and NONLOVING-26, the performance of the fine-grained classification is better than the performance of the coarse-grained classification.</s>
                        <s coords="26,250.68,257.28,290.76,8.64;26,55.44,269.24,474.73,8.64">We hypothesize that this might be due to the high intra-variance of coarse classes, which was also reported in the previous works on unsupervised image classification
                            <ref type="bibr" coords="26,418.77,269.24,107.18,8.64" target="#b69">(Van Gansbeke et al., 2020)</ref>
                            .
                        </s>
                        <s coords="26,533.28,269.24,8.17,8.64;26,55.44,281.19,486.00,8.64;26,55.44,293.15,322.35,8.64">In conclusion, the results suggest that the performance of TURTLE is not largely affected by the granularity of the dataset, but rather by the quality of the representations as indicated by Figure
                            <ref type="figure" coords="26,317.47,293.15,4.98,8.64">6</ref>
                            and Figure
                            <ref type="figure" coords="26,370.32,293.15,3.74,8.64">7</ref>.
                        </s>
                    </p>
                </div>
            </div>
            <div type="references">
                <listBibl>
                    <biblStruct
                            coords="10,55.44,315.85,234.00,8.64;10,65.04,327.81,226.05,8.64;10,65.40,339.58,225.28,8.82;10,65.40,351.72,22.42,8.64"
                            xml:id="b0">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,211.24,315.85,78.20,8.64;10,65.04,327.81,226.05,8.64;10,65.40,339.76,20.18,8.64">Super-efficiency of Automatic Differentiation for Functions Defined as a Minimum</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Ablin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Peyré</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Moreau</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,103.82,339.58,182.61,8.59">International Conference on Machine Learning</title>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,55.44,371.96,235.25,8.64;10,65.21,383.91,224.22,8.64;10,65.40,395.69,224.04,8.82;10,65.40,407.64,100.17,8.82"
                            xml:id="b1">
                        <monogr>
                            <title level="m" type="main"
                                   coords="10,78.62,383.91,210.82,8.64;10,65.40,395.87,161.06,8.64">MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Alkin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Miklautz</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Hochreiter</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Brandstetter</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2402.10093</idno>
                            <imprint>
                                <date type="published" when="2024">2024</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct
                            coords="10,55.44,428.06,234.00,8.64;10,65.40,439.84,225.69,8.82;10,65.40,451.79,75.64,8.82"
                            xml:id="b2">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,228.70,428.06,60.74,8.64;10,65.40,440.01,85.68,8.64">Self-Supervised Classification Network</title>
                            <author>
                                <persName>
                                    <forename type="first">Elad</forename>
                                    <surname>Amrani</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Leonid</forename>
                                    <surname>Karlinsky</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Alex</forename>
                                    <surname>Bronstein</surname>
                                </persName>
                            </author>
                            <idno type="DOI">10.1007/978-3-031-19821-2_7</idno>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,169.52,439.84,121.57,8.59;10,65.40,451.79,46.85,8.59">Lecture Notes in Computer Science</title>
                            <imprint>
                                <publisher>Springer Nature Switzerland</publisher>
                                <date type="published" when="2022">2022</date>
                                <biblScope unit="page" from="116" to="132"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,55.44,472.21,235.25,8.64;10,65.04,484.16,226.05,8.64;10,65.04,495.94,224.39,8.82;10,65.09,507.89,105.25,8.82"
                            xml:id="b3">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,79.48,484.16,211.61,8.64;10,65.04,496.12,79.47,8.64">Task Discovery: Finding the Tasks that Neural Networks Generalize on</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Atanov</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Filatov</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Yeo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Sohmshetty</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Zamir</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,162.85,495.94,126.59,8.59;10,65.09,507.89,76.09,8.59">Advances in Neural Information Processing Systems</title>
                            <imprint>
                                <date type="published" when="2022">2022</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,55.44,528.31,235.65,8.64;10,65.40,540.27,224.04,8.64;10,65.40,552.04,224.04,8.82;10,64.80,564.00,91.87,8.82"
                            xml:id="b4">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,198.92,528.31,92.18,8.64;10,65.40,540.27,224.04,8.64;10,65.40,552.22,41.14,8.64">Generalization Performance of Support Vector Machines and Other Pattern Classifiers</title>
                            <author>
                                <persName>
                                    <forename type="first">Peter</forename>
                                    <surname>Bartlett</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">John</forename>
                                    <surname>Shawe-Taylor</surname>
                                </persName>
                            </author>
                            <idno type="DOI">10.7551/mitpress/1130.003.0007</idno>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,129.09,552.04,160.35,8.59;10,64.80,564.00,62.63,8.59">Advances in Kernel Methods</title>
                            <imprint>
                                <publisher>The MIT Press</publisher>
                                <date type="published" when="1999">1999</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,55.44,584.41,235.25,8.64;10,65.40,596.37,225.69,8.64;10,65.40,608.15,224.04,8.82;10,64.80,620.10,151.68,8.82"
                            xml:id="b5">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,179.94,596.37,111.16,8.64;10,65.40,608.32,159.76,8.64">Birdsnap: Large-Scale Finegrained Visual Categorization of Birds</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Berg</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Woo Lee</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <forename type="middle">L</forename>
                                    <surname>Alexander</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <forename type="middle">W</forename>
                                    <surname>Jacobs</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <forename type="middle">N</forename>
                                    <surname>Belhumeur</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,248.79,608.15,40.65,8.59;10,64.80,620.10,122.56,8.59">Computer Vision and Pattern Recognition</title>
                            <imprint>
                                <date type="published" when="2014">2014</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,55.44,640.52,235.65,8.64;10,65.40,652.29,224.04,8.82;10,65.23,664.25,155.34,8.82"
                            xml:id="b6">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,211.95,640.52,79.14,8.64;10,65.40,652.47,121.53,8.64">One-Step Differentiation of Iterative Algorithms</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Bolte</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Pauwels</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Vaiter</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,208.36,652.29,81.08,8.59;10,65.23,664.25,126.18,8.59">Advances in Neural Information Processing Systems</title>
                            <imprint>
                                <date type="published" when="2023">2023</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,55.44,684.66,235.25,8.64;10,65.04,696.62,226.05,8.64;10,65.40,708.40,222.94,8.82"
                            xml:id="b7">
                        <monogr>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Bommasani</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <forename type="middle">A</forename>
                                    <surname>Hudson</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Adeli</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Altman</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Arora</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2108.07258</idno>
                            <title level="m" coords="10,129.02,696.62,162.07,8.64;10,65.40,708.58,55.15,8.64">On the Opportunities and Risks of Foundation Models</title>
                            <imprint>
                                <date type="published" when="2022">2022</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct
                            coords="10,307.44,70.54,235.65,8.64;10,316.66,82.49,224.78,8.64;10,317.40,94.27,225.28,8.82;10,317.40,106.40,22.42,8.64"
                            xml:id="b8">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,518.97,70.54,24.12,8.64;10,316.66,82.49,224.78,8.64;10,317.40,94.45,27.78,8.64">Food-101-Mining Discriminative Components with Random Forests</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Bossard</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Guillaumin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Van Gool</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m"
                                   coords="10,365.92,94.27,172.80,8.59">European Conference on Computer Vision</title>
                            <imprint>
                                <date type="published" when="2014">2014</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,307.44,128.69,234.00,8.64;10,317.40,140.64,224.04,8.64;10,317.09,152.42,196.30,8.82"
                            xml:id="b9">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,520.78,128.69,20.66,8.64;10,317.40,140.64,208.83,8.64">Deep clustering for unsupervised learning of visual features</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Caron</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Bojanowski</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Joulin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Douze</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,317.09,152.42,167.51,8.59">European Conference on Computer Vision</title>
                            <imprint>
                                <date type="published" when="2018">2018</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,307.44,174.88,234.36,8.64;10,317.40,186.84,225.78,8.64;10,317.40,198.62,159.58,8.82"
                            xml:id="b10">
                        <monogr>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Carreira</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Noland</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Hillier</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Zisserman</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1907.06987</idno>
                            <title level="m" coords="10,534.47,174.88,7.34,8.64;10,317.40,186.84,221.65,8.64">A Short Note on the Kinetics-700 Human Action Dataset</title>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct
                            coords="10,307.44,221.08,234.00,8.64;10,317.04,232.86,224.40,8.82;10,317.40,244.81,106.92,8.82"
                            xml:id="b11">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,520.84,221.08,20.60,8.64;10,317.04,233.04,105.71,8.64">Deep Adaptive Image Clustering</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Chang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Meng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Xiang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Pan</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,441.10,232.86,100.34,8.59;10,317.40,244.81,78.12,8.59">International Conference on Computer Vision</title>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,307.44,267.28,234.00,8.64;10,317.09,279.05,226.00,8.82;10,317.40,291.01,190.67,8.82"
                            xml:id="b12">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,444.83,267.28,96.61,8.64;10,317.09,279.23,184.48,8.64">An Empirical Study of Training Self-Supervised Vision Transformers</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Xie</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>He</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,520.36,279.05,22.73,8.59;10,317.40,291.01,161.88,8.59">International Conference on Computer Vision</title>
                            <imprint>
                                <date type="published" when="2021">2021</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,307.44,313.47,234.00,8.64;10,317.40,325.43,224.03,8.64;10,317.09,337.20,125.73,8.82"
                            xml:id="b13">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,445.19,313.47,96.24,8.64;10,317.40,325.43,208.41,8.64">Remote Sensing Image Scene Classification: Benchmark and State of the Art</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Cheng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Han</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Lu</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,317.09,337.20,96.01,8.59">Proceedings of the IEEE</title>
                            <meeting>the IEEE</meeting>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,307.44,359.67,235.25,8.64;10,317.40,371.62,225.28,8.64;10,317.40,383.58,224.03,8.64;10,317.40,395.36,225.69,8.82;10,317.40,407.31,93.99,8.82"
                            xml:id="b14">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,370.63,383.58,170.81,8.64;10,317.40,395.53,103.96,8.64">Reproducible Scaling Laws for Contrastive Language-Image Learning</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Cherti</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Beaumont</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Wightman</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Wortsman</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Ilharco</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Gordon</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Schuhmann</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Schmidt</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Jitsev</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,439.96,395.36,103.13,8.59;10,317.40,407.31,64.87,8.59">Computer Vision and Pattern Recognition</title>
                            <imprint>
                                <date type="published" when="2023">2023</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,307.44,429.77,234.00,8.64;10,317.04,441.55,224.40,8.82;10,316.80,453.51,151.68,8.82"
                            xml:id="b15">
                        <analytic>
                            <title level="a" type="main" coords="10,363.71,441.73,121.37,8.64">Describing Textures in the Wild</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Cimpoi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Maji</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">I</forename>
                                    <surname>Kokkinos</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Mohamed</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Vedaldi</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,502.39,441.55,39.05,8.59;10,316.80,453.51,122.56,8.59">Computer Vision and Pattern Recognition</title>
                            <imprint>
                                <date type="published" when="2014">2014</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,307.44,475.97,235.65,8.64;10,317.40,487.93,224.04,8.64;10,317.23,499.70,224.20,8.59;10,317.15,511.66,63.38,8.82"
                            xml:id="b16">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,447.25,475.97,95.85,8.64;10,317.40,487.93,205.16,8.64">An Analysis of Singlelayer Networks in Unsupervised Feature Learning</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Coates</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Ng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Lee</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,317.23,499.70,224.20,8.59;10,317.15,511.66,34.98,8.59">International Conference on Artificial Intelligence and Statistics</title>
                            <imprint>
                                <date type="published" when="2011">2011</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,307.44,534.12,235.65,8.64;10,317.04,546.08,224.39,8.64;10,317.40,557.85,224.04,8.82;10,317.40,569.81,195.60,8.82"
                            xml:id="b17">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,505.24,534.12,37.85,8.64;10,317.04,546.08,224.39,8.64;10,317.40,558.03,168.01,8.64">A Framework for Bilevel Optimization that Enables Stochastic and Global Variance Reduction Algorithms</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Dagréou</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Ablin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Vaiter</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Moreau</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,503.91,557.85,37.53,8.59;10,317.40,569.81,166.44,8.59">Advances in Neural Information Processing Systems</title>
                            <imprint>
                                <date type="published" when="2022">2022</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,307.44,592.27,234.00,8.64;10,317.09,604.05,226.00,8.82;10,317.40,616.00,163.44,8.82"
                            xml:id="b18">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,515.99,592.27,25.44,8.64;10,317.09,604.23,117.81,8.64">Vision Transformers Need Registers</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Darcet</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Oquab</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Mairal</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Bojanowski</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,455.15,604.05,87.94,8.59;10,317.40,616.00,134.39,8.59">International Conference on Learning Representations</title>
                            <imprint>
                                <date type="published" when="2024">2024</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,307.44,638.47,235.25,8.64;10,317.40,650.42,225.78,8.64;10,317.40,662.20,204.81,8.82"
                            xml:id="b19">
                        <analytic>
                            <title level="a" type="main" coords="10,328.88,650.42,210.05,8.64">Imagenet: A Large-scale Hierarchical Image Database</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Deng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Dong</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Socher</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L.-J</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Fei-Fei</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,328.19,662.20,164.90,8.59">Computer Vision and Pattern Recognition</title>
                            <imprint>
                                <date type="published" when="2009">2009</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="10,307.44,684.66,235.66,8.64;10,317.40,696.44,224.04,8.82;10,317.40,708.40,80.82,8.82"
                            xml:id="b20">
                        <analytic>
                            <title level="a" type="main"
                                   coords="10,350.18,684.66,192.91,8.64;10,317.40,696.62,11.19,8.64">Generic Methods for Optimization-Based Modeling</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Domke</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="10,345.78,696.44,195.66,8.59;10,317.40,708.40,52.41,8.59">International Conference on Artificial Intelligence and Statistics</title>
                            <imprint>
                                <date type="published" when="2012">2012</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,55.44,70.54,235.74,8.64;11,65.40,82.49,224.04,8.64;11,65.40,94.27,225.78,8.82;11,65.15,106.22,151.59,8.82"
                            xml:id="b21">
                        <analytic>
                            <title level="a" type="main" coords="11,65.40,82.49,224.04,8.64;11,65.40,94.45,46.84,8.64">CLAP: Learning Audio Concepts from Natural Language Supervision</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Elizalde</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Deshmukh</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Al Ismail</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,131.57,94.27,159.61,8.59;11,65.15,106.22,118.05,8.59">International Conference on Acoustics, Speech and Signal Processing</title>
                            <imprint>
                                <date type="published" when="2023">2023</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,55.44,128.54,234.00,8.64;11,65.40,140.50,225.78,8.64;11,65.40,152.27,164.56,8.82"
                            xml:id="b22">
                        <monogr>
                            <title level="m" type="main"
                                   coords="11,219.69,128.54,69.75,8.64;11,65.40,140.50,221.68,8.64">Natural Language Supervision for General-Purpose Audio Representations</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Elizalde</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Deshmukh</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2309.05767</idno>
                            <imprint>
                                <date type="published" when="2023">2023</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct
                            coords="11,55.44,174.59,235.65,8.64;11,65.40,186.54,224.04,8.64;11,65.40,198.50,224.04,8.64;11,65.40,210.28,225.69,8.82;11,65.40,222.23,135.96,8.82"
                            xml:id="b23">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,218.53,174.59,72.57,8.64;11,65.40,186.54,224.04,8.64;11,65.40,198.50,224.04,8.64;11,65.40,210.45,41.36,8.64">Learning Generative Visual Models From Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Fei-Fei</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Fergus</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Perona</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,125.38,210.28,165.71,8.59;11,65.40,222.23,106.39,8.59">Conference on Computer Vision and Pattern Recognition Workshop</title>
                            <imprint>
                                <date type="published" when="2004">2004</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,55.44,244.55,235.65,8.64;11,65.40,256.50,225.69,8.64;11,65.40,268.28,221.13,8.82"
                            xml:id="b24">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,271.34,244.55,19.75,8.64;11,65.40,256.50,225.69,8.64;11,65.40,268.46,14.39,8.64">Selfsupervised Learning by Estimating Twin Class Distribution</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Feng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Tao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Rufeng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Huaping</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j"
                                   coords="11,97.77,268.28,159.64,8.59">IEEE Transactions on Image Processing</title>
                            <imprint>
                                <date type="published" when="2023">2023</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,55.44,290.60,235.65,8.64;11,65.40,302.37,225.69,8.82;11,65.40,314.33,197.34,8.82"
                            xml:id="b25">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,199.42,290.60,91.67,8.64;11,65.40,302.55,185.20,8.64">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Finn</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Abbeel</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Levine</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,269.06,302.37,22.04,8.59;11,65.40,314.33,168.10,8.59">International Conference on Machine Learning</title>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,55.44,336.64,235.65,8.64;11,65.40,348.60,225.78,8.64;11,65.40,360.38,225.28,8.82;11,65.40,372.51,22.42,8.64"
                            xml:id="b26">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,179.36,336.64,111.73,8.64;11,65.40,348.60,221.42,8.64">The Pursuit of Human Labeling: A New Perspective on Unsupervised Learning</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Gadetsky</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Brbić</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,76.32,360.38,210.05,8.59">Advances in Neural Information Processing Systems</title>
                            <imprint>
                                <date type="published" when="2023">2023</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,55.44,394.65,234.17,8.64;11,65.04,406.60,224.65,8.64;11,65.40,418.38,225.78,8.82"
                            xml:id="b27">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,213.90,394.65,75.71,8.64;11,65.04,406.60,224.65,8.64;11,65.40,418.56,18.76,8.64">Are We Ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Geiger</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Lenz</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Urtasun</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,101.61,418.38,161.12,8.59">Computer Vision and Pattern Recognition</title>
                            <imprint>
                                <date type="published" when="2012">2012</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,55.44,440.70,235.25,8.64;11,65.40,452.65,225.28,8.64;11,65.40,464.61,225.69,8.64;11,65.40,476.56,224.04,8.64;11,65.07,488.34,91.18,8.82"
                            xml:id="b28">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,148.44,464.61,142.65,8.64;11,65.40,476.56,207.82,8.64">Challenges in Representation Learning: A Report on Three Machine Learning Contests</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">I</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Goodfellow</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Erhan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <forename type="middle">L</forename>
                                    <surname>Carrier</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Courville</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Mirza</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Hamner</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Cukierski</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Tang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Thaler</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D.-H</forename>
                                    <surname>Lee</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,65.07,488.34,61.74,8.59">Neural Network</title>
                            <imprint>
                                <date type="published" when="2015">2015</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,55.44,510.65,234.00,8.64;11,65.40,522.61,225.69,8.64;11,65.40,534.39,224.04,8.82;11,65.12,546.34,63.93,8.82"
                            xml:id="b29">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,244.84,510.65,44.60,8.64;11,65.40,522.61,225.69,8.64;11,65.40,534.57,51.22,8.64">Near-Tight Margin-Based Generalization Bounds for Support Vector Machines</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Gronlund</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Kamma</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <forename type="middle">G</forename>
                                    <surname>Larsen</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,136.10,534.39,153.34,8.59;11,65.12,546.34,34.69,8.59">International Conference on Machine Learning</title>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,55.44,568.66,235.74,8.64;11,65.40,580.61,224.03,8.64;11,65.07,592.39,194.02,8.82"
                            xml:id="b30">
                        <analytic>
                            <title level="a" type="main" coords="11,65.40,580.61,207.77,8.64">Masked Autoencoders are Scalable Vision Learners</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>He</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Xie</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Dollár</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Girshick</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,65.07,592.39,164.90,8.59">Computer Vision and Pattern Recognition</title>
                            <imprint>
                                <date type="published" when="2022">2022</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,55.44,614.71,235.38,8.64;11,65.04,626.66,224.40,8.64;11,65.40,638.44,224.03,8.82;11,65.40,650.39,224.04,8.59;11,65.09,662.35,90.76,8.82"
                            xml:id="b31">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,257.42,614.71,33.40,8.64;11,65.04,626.66,224.40,8.64;11,65.40,638.62,144.18,8.64">Eurosat: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Helber</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Bischke</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Dengel</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Borth</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j"
                                   coords="11,232.50,638.44,56.94,8.59;11,65.40,650.39,224.04,8.59;11,65.09,662.35,61.67,8.59">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,55.44,684.66,234.00,8.64;11,65.40,696.44,224.04,8.82;11,65.40,708.40,100.17,8.82"
                            xml:id="b32">
                        <monogr>
                            <title level="m" type="main"
                                   coords="11,199.60,684.66,89.84,8.64;11,65.40,696.62,155.71,8.64">Unsupervised Prompt Learning for Vision-Language Models</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Huang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Chu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Wei</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2204.03649</idno>
                            <imprint>
                                <date type="published" when="2022">2022</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct
                            coords="11,307.44,70.54,235.66,8.64;11,317.40,82.31,224.04,8.82;11,317.07,94.27,161.63,8.82"
                            xml:id="b33">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,424.46,70.54,118.63,8.64;11,317.40,82.49,151.09,8.64">Bilevel Optimization: Convergence Analysis and Enhanced Design</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Ji</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Liang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,487.81,82.31,53.63,8.59;11,317.07,94.27,132.39,8.59">International Conference on Machine Learning</title>
                            <imprint>
                                <date type="published" when="2021">2021</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,307.44,113.75,235.65,8.64;11,317.40,125.52,224.03,8.82;11,316.85,137.48,55.62,8.82"
                            xml:id="b34">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,406.30,113.75,136.80,8.64;11,317.40,125.70,109.03,8.64">The Implicit Bias of Gradient Descent on Nonseparable Data</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Ji</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Telgarsky</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,445.03,125.52,96.41,8.59;11,316.85,137.48,26.33,8.59">Conference on Learning Theory</title>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,307.44,156.95,235.25,8.64;11,317.40,168.91,224.39,8.64;11,317.40,180.86,225.69,8.64;11,317.40,192.64,224.04,8.82;11,317.09,204.59,107.03,8.82"
                            xml:id="b35">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,534.46,168.91,7.34,8.64;11,317.40,180.86,225.69,8.64;11,317.40,192.82,111.87,8.64">A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Johnson</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Hariharan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Van Der Maaten</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Fei-Fei</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Lawrence Zitnick</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Girshick</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Clevr</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,453.09,192.64,88.36,8.59;11,317.09,204.59,77.91,8.59">Computer Vision and Pattern Recognition</title>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,307.44,224.07,235.24,8.64;11,317.40,236.02,224.04,8.64;11,317.40,247.98,225.78,8.64;11,317.40,259.76,225.28,8.82;11,317.40,271.89,22.42,8.64"
                            xml:id="b36">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,458.37,236.02,83.07,8.64;11,317.40,247.98,220.58,8.64">The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Kiela</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Firooz</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Mohan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">V</forename>
                                    <surname>Goswami</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Singh</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Ringshia</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Testuggine</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,328.94,259.76,209.55,8.59">Advances in neural information processing systems</title>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,307.44,291.19,234.00,8.64;11,317.40,302.96,224.03,8.82;11,317.09,314.92,91.23,8.82"
                            xml:id="b37">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,441.41,291.19,100.02,8.64;11,317.40,303.14,51.45,8.64">A Method for Stochastic Optimization</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <forename type="middle">P</forename>
                                    <surname>Kingma</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Ba</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Adam</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,387.60,302.96,153.83,8.59;11,317.09,314.92,62.18,8.59">International Conference on Learning Representations</title>
                            <imprint>
                                <date type="published" when="2015">2015</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,307.44,334.39,235.25,8.64;11,317.40,346.35,224.04,8.64;11,317.04,358.13,224.40,8.82;11,317.40,370.08,106.92,8.82"
                            xml:id="b38">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,427.49,346.35,113.96,8.64;11,317.04,358.30,121.57,8.64">Big Transfer (BiT): General Visual Representation Learning</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Kolesnikov</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Beyer</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Zhai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Puigcerver</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Yung</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Gelly</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>Houlsby</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,456.44,358.13,85.01,8.59;11,317.40,370.08,78.12,8.59">European Conference on Computer Vision</title>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,307.44,389.56,234.00,8.64;11,317.40,401.33,225.69,8.82;11,317.40,413.29,225.28,8.59;11,317.40,425.42,22.42,8.64"
                            xml:id="b39">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,499.36,389.56,42.08,8.64;11,317.40,401.51,194.62,8.64">3D Object Representations for Fine-grained Categorization</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Krause</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Stark</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Deng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Fei-Fei</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,539.14,401.33,3.95,8.59;11,317.40,413.29,220.64,8.59">ternational Conference on Computer Vision Workshops</title>
                            <imprint>
                                <date type="published" when="2013">2013</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,307.44,444.72,234.00,8.64;11,317.40,456.50,224.03,8.82;11,317.40,468.45,68.38,8.82"
                            xml:id="b40">
                        <monogr>
                            <title level="m" type="main"
                                   coords="11,430.21,444.72,111.23,8.64;11,317.40,456.67,103.88,8.64">Learning Multiple Layers of Features from Tiny Images</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Krizhevsky</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Hinton</surname>
                                </persName>
                            </author>
                            <imprint>
                                <date type="published" when="2009">2009</date>
                            </imprint>
                            <respStmt>
                                <orgName>University of Toronto</orgName>
                            </respStmt>
                        </monogr>
                        <note type="report_type">Technical Report</note>
                    </biblStruct>
                    <biblStruct coords="11,307.44,487.93,234.00,8.64;11,317.40,499.70,221.03,8.82" xml:id="b41">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,364.07,487.93,177.36,8.64;11,317.40,499.88,31.72,8.64">The Hungarian Method for the Assignment Problem</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <forename type="middle">W</forename>
                                    <surname>Kuhn</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j"
                                   coords="11,368.03,499.70,141.31,8.59">Naval Research Logistics Quarterly</title>
                            <imprint>
                                <date type="published" when="1955">1955</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,307.44,519.18,234.34,8.64;11,317.40,531.13,225.78,8.64;11,317.40,542.91,225.78,8.82"
                            xml:id="b42">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,511.37,519.18,30.42,8.64;11,317.40,531.13,221.51,8.64">A Fully First-Order Method for Stochastic Bilevel Optimization</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Kwon</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Kwon</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Wright</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <forename type="middle">D</forename>
                                    <surname>Nowak</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,328.12,542.91,186.03,8.59">International Conference on Machine Learning</title>
                            <imprint>
                                <date type="published" when="2023">2023</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,307.44,562.39,235.65,8.64;11,317.40,574.34,224.04,8.64;11,317.09,586.12,125.73,8.82"
                            xml:id="b43">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,505.52,562.39,37.57,8.64;11,317.40,574.34,205.88,8.64">Gradientbased Learning Applied to Document Recognition</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Lecun</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Bottou</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Bengio</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Haffner</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,317.09,586.12,96.01,8.59">Proceedings of the IEEE</title>
                            <meeting>the IEEE</meeting>
                            <imprint>
                                <date type="published" when="1998">1998</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,307.44,605.59,235.25,8.64;11,317.40,617.55,225.69,8.64;11,317.40,629.32,224.04,8.82;11,317.07,641.28,190.75,8.82"
                            xml:id="b44">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,383.57,617.55,159.53,8.64;11,317.40,629.50,148.73,8.64">Efficient Self-supervised Vision Transformers for Representation Learning</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Gao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Xiao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Dai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Yuan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Gao</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="11,487.81,629.32,53.63,8.59;11,317.07,641.28,161.70,8.59">International Conference on Learning Representations</title>
                            <imprint>
                                <date type="published" when="2022">2022</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="11,307.44,660.75,235.66,8.64;11,317.40,672.71,224.04,8.64;11,317.40,684.49,224.04,8.82;11,317.40,696.44,224.04,8.59;11,317.23,708.40,76.09,8.82"
                            xml:id="b45">
                        <analytic>
                            <title level="a" type="main"
                                   coords="11,519.23,660.75,23.87,8.64;11,317.40,672.71,224.04,8.64;11,317.40,684.66,94.69,8.64">PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Lin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Zhao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Wu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m"
                                   coords="11,437.43,684.49,104.01,8.59;11,317.40,696.44,224.04,8.59;11,317.23,708.40,47.25,8.59">International Conference on Medical Image Computing and Computer Assisted Intervention</title>
                            <imprint>
                                <date type="published" when="2023">2023</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,55.44,70.54,234.33,8.64;12,65.40,82.49,224.21,8.64;12,65.04,94.27,224.39,8.82;12,65.15,106.22,58.94,8.82"
                            xml:id="b46">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,262.11,70.54,27.66,8.64;12,65.40,82.49,224.21,8.64;12,65.04,94.45,36.39,8.64">Bome! Bilevel Optimization Made Easy: A Simple First-Order Approach</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Ye</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Wright</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Stone</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Q</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,119.18,94.27,170.25,8.59;12,65.15,106.22,29.78,8.59">Advances in Neural Information Processing Systems</title>
                            <imprint>
                                <date type="published" when="2022">2022</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,55.44,128.54,235.65,8.64;12,65.40,140.50,224.04,8.64;12,65.23,152.27,224.20,8.59;12,65.15,164.23,63.38,8.82"
                            xml:id="b47">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,224.17,128.54,66.93,8.64;12,65.40,140.50,208.27,8.64">Optimizing Millions of Hyperparameters by Implicit Differentiation</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Lorraine</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Vicol</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Duvenaud</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,65.23,152.27,224.20,8.59;12,65.15,164.23,34.98,8.59">International Conference on Artificial Intelligence and Statistics</title>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,55.44,186.54,234.00,8.64;12,65.04,198.32,226.05,8.82;12,65.40,210.28,225.72,8.82"
                            xml:id="b48">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,133.84,186.54,155.60,8.64;12,65.04,198.50,150.89,8.64">Some Methods for Classification and Analysis of MultiVariate Observations</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <forename type="middle">B</forename>
                                    <surname>Macqueen</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,234.50,198.32,56.59,8.59;12,65.40,210.28,197.36,8.59">Berkeley Symposium on Mathematical Statistics and Probability</title>
                            <imprint>
                                <date type="published" when="1967">1967</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,55.44,232.59,235.24,8.64;12,65.04,244.37,224.40,8.82;12,65.40,256.32,129.97,8.82"
                            xml:id="b49">
                        <monogr>
                            <title level="m" type="main" coords="12,78.45,244.55,181.14,8.64">Fine-grained Visual Classification of Aircraft</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Maji</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Rahtu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Kannala</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Blaschko</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Vedaldi</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1306.5151</idno>
                            <imprint>
                                <date type="published" when="2013">2013</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct
                            coords="12,55.44,278.64,235.75,8.64;12,65.40,290.60,224.04,8.64;12,65.40,302.37,225.69,8.82;12,65.40,314.33,174.14,8.82"
                            xml:id="b50">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,93.85,290.60,195.60,8.64;12,65.40,302.55,131.94,8.64">Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Meidani</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Shojaee</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <forename type="middle">K</forename>
                                    <surname>Reddy</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <forename type="middle">B</forename>
                                    <surname>Farimani</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Snip</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,215.66,302.37,75.44,8.59;12,65.40,314.33,145.09,8.59">International Conference on Learning Representations</title>
                            <imprint>
                                <date type="published" when="2024">2024</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,55.44,336.64,235.66,8.64;12,65.40,348.42,224.04,8.82;12,65.07,360.38,226.02,8.59;12,65.40,372.33,56.73,8.82"
                            xml:id="b51">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,194.94,336.64,96.16,8.64;12,65.40,348.60,160.10,8.64">Automated Flower Classification over a Large Number of Classes</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">M.-E</forename>
                                    <surname>Nilsback</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Zisserman</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,242.35,348.42,47.09,8.59;12,65.07,360.38,125.49,8.59">Sixth Indian Conference on Computer Vision</title>
                            <imprint>
                                <publisher>Graphics &amp; Image Processing</publisher>
                                <date type="published" when="2008">2008</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,55.44,394.65,235.66,8.64;12,65.40,406.42,224.03,8.82;12,65.23,418.38,160.95,8.82"
                            xml:id="b52">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,187.40,394.65,103.70,8.64;12,65.40,406.60,119.82,8.64">SPICE: Semantic pseudolabeling for image clustering</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Niu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Shan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="12,197.13,406.42,92.31,8.59;12,65.23,418.38,69.29,8.59">IEEE Transactions on Image Processing</title>
                            <imprint>
                                <biblScope unit="volume">31</biblScope>
                                <biblScope unit="page" from="7264" to="7278"/>
                                <date type="published" when="2022">2022</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,55.44,440.70,235.24,8.64;12,65.40,452.65,225.28,8.64;12,65.40,464.61,224.04,8.64;12,65.40,476.38,225.69,8.82;12,65.40,488.34,126.89,8.82"
                            xml:id="b53">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,150.39,464.61,139.05,8.64;12,65.40,476.56,119.96,8.64">DINOv2: Learning Robust Visual Features Without Supervision</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Oquab</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Darcet</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Moutakanni</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Vo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Szafraniec</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">V</forename>
                                    <surname>Khalidov</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Fernandez</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Haziza</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Massa</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>El-Nouby</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,205.84,476.38,85.25,8.59;12,65.40,488.34,97.65,8.59">Transactions on Machine Learning Research</title>
                            <imprint>
                                <date type="published" when="2023">2023</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,55.44,510.65,234.00,8.64;12,65.23,522.43,225.45,8.59;12,65.40,534.57,22.42,8.64"
                            xml:id="b54">
                        <analytic>
                            <title level="a" type="main" coords="12,150.43,510.65,122.84,8.64">A Survey on Transfer Learning</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Pan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Q</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,65.23,522.43,221.21,8.59">IEEE Transactions on Knowledge and Data Engineering</title>
                            <imprint>
                                <date type="published" when="2009">2009</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,55.44,556.70,235.25,8.64;12,65.40,568.48,224.03,8.82;12,65.09,580.43,75.45,8.82"
                            xml:id="b55">
                        <analytic>
                            <title level="a" type="main" coords="12,82.59,568.66,59.60,8.64">Cats and Dogs</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">O</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Parkhi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Vedaldi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Zisserman</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Jawahar</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,167.14,568.48,122.30,8.59;12,65.09,580.43,46.33,8.59">Computer Vision and Pattern Recognition</title>
                            <imprint>
                                <date type="published" when="2012">2012</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,55.44,602.75,235.25,8.64;12,65.04,614.71,225.64,8.64;12,65.40,626.66,224.04,8.64;12,65.40,638.44,224.04,8.82;12,64.99,650.39,101.13,8.82"
                            xml:id="b56">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,88.21,626.66,201.23,8.64;12,65.40,638.62,89.47,8.64">Learning Transferable Visual Models from Natural Language Supervision</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Radford</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <forename type="middle">W</forename>
                                    <surname>Kim</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Hallacy</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Ramesh</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Goh</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Agarwal</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Sastry</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Askell</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Mishkin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Clark</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,174.02,638.44,115.42,8.59;12,64.99,650.39,71.89,8.59">International Conference on Machine Learning</title>
                            <imprint>
                                <date type="published" when="2021">2021</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,55.44,672.71,234.00,8.64;12,65.40,684.66,224.03,8.64;12,65.40,696.62,225.69,8.64;12,65.40,708.40,144.98,8.82"
                            xml:id="b57">
                        <monogr>
                            <title level="m" type="main"
                                   coords="12,216.47,672.71,72.97,8.64;12,65.40,684.66,224.03,8.64;12,65.40,696.62,225.69,8.64;12,65.40,708.58,21.44,8.64">Machine Learning in Python: Main Developments and Technology Trends in Data Science, Machine Learning, and Artificial Intelligence</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Raschka</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Patterson</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Nolet</surname>
                                </persName>
                            </author>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                                <publisher>MDPI</publisher>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,307.44,70.54,235.25,8.64;12,317.09,82.49,225.73,8.64;12,317.40,94.27,225.69,8.82;12,317.40,106.22,224.03,8.59;12,317.15,118.18,99.79,8.82"
                            xml:id="b58">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,391.72,82.49,151.11,8.64;12,317.40,94.45,189.86,8.64">Contrasting Sequence with Structure: Pre-training Graph Representations with PLMs</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Robinson</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Atkinson</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Copoiu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Bordes</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Pierrot</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Barrett</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m"
                                   coords="12,528.42,94.27,14.67,8.59;12,317.40,106.22,224.03,8.59;12,317.15,118.18,70.22,8.59">Advances in Neural Information Processing Systems AI for Science Workshop</title>
                            <imprint>
                                <date type="published" when="2023">2023</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,307.44,137.93,235.39,8.64;12,317.04,149.88,224.39,8.64;12,317.40,161.66,225.69,8.82;12,317.40,173.61,123.25,8.82"
                            xml:id="b59">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,448.51,137.93,94.32,8.64;12,317.04,149.88,224.39,8.64;12,317.40,161.84,89.99,8.64">Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Salimans</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <forename type="middle">P</forename>
                                    <surname>Kingma</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,426.35,161.66,116.75,8.59;12,317.40,173.61,94.09,8.59">Advances in Neural Information Processing Systems</title>
                            <imprint>
                                <date type="published" when="2016">2016</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,307.44,193.36,235.65,8.64;12,317.40,205.14,225.69,8.82;12,317.40,217.09,163.44,8.82"
                            xml:id="b60">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,479.16,193.36,63.94,8.64;12,317.40,205.32,120.88,8.64">Breeds: Benchmarks for Subpopulation Shift</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Santurkar</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Tsipras</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Madry</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,456.40,205.14,86.70,8.59;12,317.40,217.09,134.39,8.59">International Conference on Learning Representations</title>
                            <imprint>
                                <date type="published" when="2021">2021</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,307.44,236.84,235.25,8.64;12,316.94,248.80,225.75,8.64;12,317.40,260.75,225.28,8.64;12,317.40,272.71,225.29,8.64;12,317.21,284.66,224.22,8.64;12,317.40,296.44,225.69,8.82;12,317.40,308.40,225.28,8.59;12,317.40,320.53,22.42,8.64"
                            xml:id="b61">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,327.05,284.66,214.39,8.64;12,317.40,296.62,141.43,8.64">LAION-5b: An Open Large-scale Dataset for Training Next Generation Image-text Models</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Schuhmann</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Beaumont</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Vencu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <forename type="middle">W</forename>
                                    <surname>Gordon</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Wightman</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Cherti</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Coombes</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Katta</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Mullis</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Wortsman</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Schramowski</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <forename type="middle">R</forename>
                                    <surname>Kundurthy</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Crowson</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Schmidt</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Kaczmarczyk</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Jitsev</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,477.79,296.44,65.30,8.59;12,317.40,308.40,221.23,8.59">Neural Information Processing Systems Datasets and Benchmarks Track</title>
                            <imprint>
                                <date type="published" when="2022">2022</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,307.44,340.10,234.00,8.64;12,317.40,352.05,225.69,8.64;12,317.40,363.83,224.04,8.82;12,317.15,375.79,58.94,8.82"
                            xml:id="b62">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,525.64,340.10,15.80,8.64;12,317.40,352.05,225.69,8.64;12,317.40,364.01,32.81,8.64">The Curse of Unrolling: Rate of Differentiating Through Optimization</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Scieur</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Gidel</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Q</forename>
                                    <surname>Bertrand</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Pedregosa</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,368.59,363.83,172.85,8.59;12,317.15,375.79,29.78,8.59">Advances in Neural Information Processing Systems</title>
                            <imprint>
                                <date type="published" when="2022">2022</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,307.44,395.53,235.65,8.64;12,317.40,407.49,224.04,8.64;12,317.23,419.27,224.20,8.59;12,317.15,431.22,63.38,8.82"
                            xml:id="b63">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,520.30,395.53,22.79,8.64;12,317.40,407.49,202.85,8.64">Truncated Back-propagation for Bilevel Optimization</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Shaban</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C.-A</forename>
                                    <surname>Cheng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>Hatch</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Boots</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,317.23,419.27,224.20,8.59;12,317.15,431.22,34.98,8.59">International Conference on Artificial Intelligence and Statistics</title>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,307.44,450.97,235.25,8.64;12,317.40,462.92,225.28,8.64;12,317.40,474.88,224.38,8.64;12,317.40,486.83,224.04,8.64;12,317.07,498.61,194.02,8.82"
                            xml:id="b64">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,466.43,474.88,75.36,8.64;12,317.40,486.83,207.59,8.64">Revisiting Weakly Supervised Pre-training of Visual Perception Models</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Singh</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Gustafson</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Adcock</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">V</forename>
                                    <surname>De Freitas Reis</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Gedik</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <forename type="middle">P</forename>
                                    <surname>Kosaraju</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Mahajan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Girshick</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Dollár</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Van Der Maaten</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,317.07,498.61,164.90,8.59">Computer Vision and Pattern Recognition</title>
                            <imprint>
                                <date type="published" when="2022">2022</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,307.44,518.36,234.00,8.64;12,317.40,530.31,225.78,8.64;12,317.40,542.09,154.60,8.82"
                            xml:id="b65">
                        <monogr>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Soomro</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <forename type="middle">R</forename>
                                    <surname>Zamir</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Shah</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Ucf</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1212.0402</idno>
                            <title level="m" coords="12,503.09,518.36,38.35,8.64;12,317.40,530.31,221.35,8.64">A Dataset of 101 Human Actions Classes from Videos in the Wild</title>
                            <imprint>
                                <date type="published" when="2012">2012</date>
                                <biblScope unit="volume">101</biblScope>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct
                            coords="12,307.44,561.84,234.00,8.64;12,317.40,573.79,225.69,8.64;12,317.40,585.57,225.28,8.82;12,317.40,597.70,22.42,8.64"
                            xml:id="b66">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,361.25,573.79,181.84,8.64;12,317.40,585.75,44.70,8.64">The Implicit Bias of Gradient Descent on Separable Data</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Soudry</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Hoffer</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <forename type="middle">S</forename>
                                    <surname>Nacson</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Gunasekar</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>Srebro</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j"
                                   coords="12,380.97,585.57,157.29,8.59">Journal of Machine Learning Research</title>
                            <imprint>
                                <date type="published" when="2018">2018</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,307.44,617.27,234.00,8.64;12,317.15,629.23,225.94,8.64;12,317.40,641.01,225.28,8.82;12,317.40,653.14,22.42,8.64"
                            xml:id="b67">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,331.56,629.23,211.53,8.64;12,317.40,641.18,137.36,8.64">Computer: Benchmarking Machine Learning Algorithms for Traffic Sign Recognition</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Stallkamp</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Schlipsing</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Salmen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Igel</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Man</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Vs</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,473.28,641.01,64.99,8.59">Neural Networks</title>
                            <imprint>
                                <date type="published" when="2012">2012</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="12,307.44,672.71,235.74,8.64;12,317.40,684.66,224.21,8.64;12,317.40,696.44,224.04,8.82;12,317.40,708.40,142.70,8.82"
                            xml:id="b68">
                        <analytic>
                            <title level="a" type="main"
                                   coords="12,349.62,684.66,192.00,8.64;12,317.40,696.62,102.01,8.64">Prompt-oriented Unsupervised Fine-tuning for Large Pre-trained Models</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Tanwisuth</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Zheng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>He</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Zhou</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Pouf</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="12,438.81,696.44,102.63,8.59;12,317.40,708.40,113.65,8.59">International Conference on Learning Representations</title>
                            <imprint>
                                <date type="published" when="2023">2023</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="13,55.44,70.54,235.66,8.64;13,65.40,82.49,224.39,8.64;13,65.40,94.27,225.69,8.82;13,65.40,106.22,75.64,8.82"
                            xml:id="b69">
                        <analytic>
                            <title level="a" type="main" coords="13,178.19,82.49,111.61,8.64;13,65.40,94.45,87.01,8.64">SCAN: Learning to Classify Images without Labels</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Van Gansbeke</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Vandenhende</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Georgoulis</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Proesmans</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Van Gool</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="13,170.07,94.27,121.03,8.59;13,65.40,106.22,46.85,8.59">European Conference on Computer Vision</title>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="13,55.44,127.69,234.00,8.64;13,65.15,139.47,62.17,8.82" xml:id="b70">
                        <monogr>
                            <title level="m" type="main" coords="13,102.48,127.69,169.62,8.64">The Nature of Statistical Learning Theory</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">V</forename>
                                    <surname>Vapnik</surname>
                                </persName>
                            </author>
                            <imprint>
                                <date type="published" when="1995">1995</date>
                                <publisher>Springer</publisher>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="13,55.44,160.94,234.00,8.64;13,64.94,172.89,224.51,8.64;13,65.40,184.67,224.04,8.82;13,64.80,196.62,111.24,8.82"
                            xml:id="b71">
                        <analytic>
                            <title level="a" type="main"
                                   coords="13,124.97,172.89,164.47,8.64;13,65.40,184.85,38.12,8.64">Rotation Equivariant CNNs for Digital Pathology</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <forename type="middle">S</forename>
                                    <surname>Veeling</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Linmans</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Winkens</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Cohen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Welling</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="13,122.16,184.67,167.28,8.59;13,64.80,196.62,82.40,8.59">Medical Image Computing and Computer Assisted Intervention</title>
                            <imprint>
                                <date type="published" when="2018">2018</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="13,55.44,218.09,234.00,8.64;13,65.40,230.05,224.04,8.64;13,65.40,241.82,224.04,8.82;13,64.99,253.78,101.13,8.82"
                            xml:id="b72">
                        <analytic>
                            <title level="a" type="main"
                                   coords="13,127.88,230.05,161.56,8.64;13,65.40,242.00,84.22,8.64">On Implicit Bias in Overparameterized Bilevel Optimization</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Vicol</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <forename type="middle">P</forename>
                                    <surname>Lorraine</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Pedregosa</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Duvenaud</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <forename type="middle">B</forename>
                                    <surname>Grosse</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="13,172.35,241.82,117.09,8.59;13,64.99,253.78,71.89,8.59">International Conference on Machine Learning</title>
                            <imprint>
                                <date type="published" when="2022">2022</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="13,55.44,275.25,234.00,8.64;13,65.40,287.02,225.69,8.82;13,65.40,298.98,51.20,8.82"
                            xml:id="b73">
                        <analytic>
                            <title level="a" type="main"
                                   coords="13,195.82,275.25,93.62,8.64;13,65.40,287.20,71.02,8.64">Linear Time Maximum Margin Clustering</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Zhao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="13,154.71,287.02,136.38,8.59;13,65.40,298.98,21.91,8.59">IEEE Transactions on Neural Networks</title>
                            <imprint>
                                <date type="published" when="2010">2010</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="13,55.44,320.45,235.65,8.64;13,65.40,332.40,224.04,8.64;13,65.40,344.18,224.04,8.82;13,65.12,356.13,63.93,8.82"
                            xml:id="b74">
                        <analytic>
                            <title level="a" type="main"
                                   coords="13,144.12,320.45,146.97,8.64;13,65.40,332.40,224.04,8.64;13,65.40,344.36,49.92,8.64">Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Isola</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="13,135.50,344.18,153.94,8.59;13,65.12,356.13,34.69,8.59">International Conference on Machine Learning</title>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="13,55.44,377.60,235.75,8.64;13,65.04,389.56,224.39,8.64;13,65.04,401.33,224.40,8.82;13,65.09,413.29,91.23,8.82"
                            xml:id="b75">
                        <analytic>
                            <title level="a" type="main"
                                   coords="13,65.04,389.56,224.39,8.64;13,65.04,401.51,43.36,8.64">A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Liang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Sheng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>He</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Tan</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="13,131.45,401.33,157.99,8.59;13,65.09,413.29,62.18,8.59">International Conference on Learning Representations</title>
                            <imprint>
                                <date type="published" when="2024">2024</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="13,55.44,434.76,235.25,8.64;13,65.04,446.71,224.40,8.64;13,65.40,458.49,225.28,8.82;13,65.40,470.62,22.42,8.64"
                            xml:id="b76">
                        <analytic>
                            <title level="a" type="main"
                                   coords="13,78.20,446.71,211.24,8.64;13,65.40,458.67,40.46,8.64">Sun Database: Exploring a Large Collection of Scene Categories</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Xiao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <forename type="middle">A</forename>
                                    <surname>Ehinger</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Hays</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Torralba</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Oliva</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="13,124.18,458.49,162.65,8.59">International Journal of Computer Vision</title>
                            <imprint>
                                <date type="published" when="2016">2016</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="13,55.44,491.91,234.00,8.64;13,65.40,503.69,224.04,8.82;13,65.07,515.64,161.63,8.82"
                            xml:id="b77">
                        <analytic>
                            <title level="a" type="main"
                                   coords="13,209.45,491.91,79.98,8.64;13,65.40,503.87,146.24,8.64">Unsupervised Deep Embedding for Clustering Analysis</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Xie</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Girshick</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Farhadi</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="13,235.81,503.69,53.63,8.59;13,65.07,515.64,132.39,8.59">International Conference on Machine Learning</title>
                            <imprint>
                                <date type="published" when="2016">2016</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="13,55.44,537.11,235.65,8.64;13,65.40,548.89,224.04,8.82;13,65.07,560.84,172.75,8.82"
                            xml:id="b78">
                        <analytic>
                            <title level="a" type="main"
                                   coords="13,186.90,537.11,104.19,8.64;13,65.40,549.07,185.85,8.64">Unsupervised and Semisupervised Multi-class Support Vector Machines</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Xu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Schuurmans</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="13,268.29,548.89,21.15,8.59;13,65.07,560.84,144.04,8.59">AAAI Conference on Artificial Intelligence</title>
                            <imprint>
                                <date type="published" when="2005">2005</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="13,55.44,582.31,235.65,8.64;13,65.40,594.09,225.69,8.82;13,65.40,606.04,123.25,8.82"
                            xml:id="b79">
                        <analytic>
                            <title level="a" type="main"
                                   coords="13,266.62,582.31,24.47,8.64;13,65.40,594.27,93.08,8.64">Maximum Margin Clustering</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Xu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Neufeld</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Larson</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Schuurmans</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="13,176.63,594.09,114.46,8.59;13,65.40,606.04,94.09,8.59">Advances in Neural Information Processing Systems</title>
                            <imprint>
                                <date type="published" when="2004">2004</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="13,55.44,627.51,235.24,8.64;13,65.40,639.47,225.69,8.64;13,65.40,651.24,224.04,8.82;13,65.12,663.20,102.97,8.82"
                            xml:id="b80">
                        <analytic>
                            <title level="a" type="main"
                                   coords="13,156.72,639.47,134.38,8.64;13,65.40,651.42,96.31,8.64">Contrastive Captioners are Imagetext Foundation Models</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Yu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">V</forename>
                                    <surname>Vasudevan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Yeung</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Seyedhosseini</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Wu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Coca</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="13,184.73,651.24,104.72,8.59;13,65.12,663.20,73.73,8.59">Transactions on Machine Learning Research</title>
                            <imprint>
                                <date type="published" when="2022">2022</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="13,55.44,684.66,234.00,8.64;13,65.40,696.44,224.03,8.82;13,65.40,708.40,113.58,8.82"
                            xml:id="b81">
                        <analytic>
                            <title level="a" type="main"
                                   coords="13,217.70,684.66,71.74,8.64;13,65.40,696.62,103.97,8.64">Maximum Margin Clustering Made Practical</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">I</forename>
                                    <forename type="middle">W</forename>
                                    <surname>Tsang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <forename type="middle">T</forename>
                                    <surname>Kwok</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="13,187.63,696.44,101.81,8.59;13,65.40,708.40,84.34,8.59">International Conference on Machine Learning</title>
                            <imprint>
                                <date type="published" when="2007">2007</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="13,307.44,70.54,235.65,8.64;13,317.40,82.31,223.77,8.82" xml:id="b82">
                        <analytic>
                            <title level="a" type="main" coords="13,446.16,70.54,96.93,8.64;13,317.40,82.49,11.42,8.64">Multiple Kernel Clustering</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Zhao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Kwok</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="13,347.00,82.31,164.80,8.59">International Conference on Data Mining</title>
                            <imprint>
                                <date type="published" when="2009">2009</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="13,307.44,102.42,235.25,8.64;13,317.40,114.37,225.69,8.64;13,317.40,126.15,224.04,8.82;13,317.09,138.10,91.23,8.82;21,81.96,199.70,16.66,6.05;21,118.75,199.70,404.63,6.05;21,81.96,209.02,20.15,6.05;21,118.75,209.02,404.63,6.05;21,81.96,218.35,441.42,6.05;21,81.96,227.67,441.42,6.05;21,81.96,237.00,441.42,6.05;21,81.96,250.61,441.42,6.05;21,81.96,259.93,441.42,6.05;21,81.96,269.26,441.42,6.05;21,70.23,330.49,6.05,15.11;21,70.23,300.47,6.05,28.28;21,81.96,283.07,16.66,6.05;21,118.75,283.07,404.63,6.05;21,81.96,292.39,20.15,6.05;21,118.75,292.39,238.61,6.05"
                            xml:id="b83">
                        <analytic>
                            <title level="a" type="main"
                                   coords="13,400.92,114.37,142.18,8.64;13,317.40,126.33,54.40,8.64">Image BERT Pre-training with Online Tokenizer</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Zhou</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Wei</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Shen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Xie</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Yuille</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Kong</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Ibot</surname>
                                </persName>
                            </author>
                            <idno>32 88.6 95.1 80.2 59.9 76.1 80</idno>
                        </analytic>
                        <monogr>
                            <title level="m" coords="13,390.22,126.15,151.22,8.59;13,317.09,138.10,62.18,8.59">International Conference on Learning Representations</title>
                            <imprint>
                                <date type="published" when="2022">2022</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>