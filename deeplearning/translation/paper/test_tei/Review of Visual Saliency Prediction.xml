<?xml version="1.0" encoding="UTF-8"?>
<TEI
        xmlns="http://www.tei-c.org/ns/1.0"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve"
        xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
    <teiHeader xml:lang="en">
        <fileDesc>
            <titleStmt>
                <title level="a" type="main" coords="1,35.72,120.00,490.68,16.82;1,35.72,138.83,360.13,16.82">Review of Visual Saliency Prediction: Development Process from Neurobiological Basis to Deep Models</title>
                <funder ref="#_3KxAEYx">
                    <orgName type="full">Non-profit Central Research Institute Fund of Chinese Academy of Medical Sciences</orgName>
                </funder>
                <funder ref="#_3va5pvx">
                    <orgName type="full">National Natural Science Foundation of China</orgName>
                    <orgName type="abbreviated">NSFC</orgName>
                    <idno type="DOI" subtype="crossref">10.13039/501100001809</idno>
                </funder>
                <funder ref="#_Ec4ErJA">
                    <orgName
                            type="full">Major Science and Technology Project of Zhejiang Province Health Commission</orgName>
                </funder>
                <funder ref="#_WGAwppF">
                    <orgName type="full">National Key Research and Development Program of China</orgName>
                    <orgName type="abbreviated">NKRDPC</orgName>
                    <idno type="DOI" subtype="crossref">10.13039/501100012166</idno>
                </funder>
                <funder ref="#_vp8nUyd">
                    <orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
                    <idno type="DOI" subtype="crossref">10.13039/501100012226</idno>
                </funder>
            </titleStmt>
            <publicationStmt>
                <publisher>MDPI AG</publisher>
                <availability status="unknown">
                    <licence/>
                </availability>
                <date type="published" when="2021-12-29">29 December 2021</date>
            </publicationStmt>
            <sourceDesc>
                <biblStruct>
                    <analytic>
                        <author>
                            <persName coords="1,35.72,170.80,33.31,9.34">
                                <forename type="first">Fei</forename>
                                <surname>Yan</surname>
                            </persName>
                        </author>
                        <author>
                            <persName coords="1,80.78,170.80,56.73,9.34">
                                <forename type="first">Cheng</forename>
                                <surname>Chen</surname>
                            </persName>
                            <idno type="ORCID">0000-0002-4203-2145</idno>
                        </author>
                        <author>
                            <persName coords="1,158.33,170.80,45.67,9.34">
                                <forename type="first">Peng</forename>
                                <surname>Xiao</surname>
                            </persName>
                        </author>
                        <author>
                            <persName coords="1,215.76,170.80,35.14,9.34">
                                <forename type="first">Siyu</forename>
                                <surname>Qi</surname>
                            </persName>
                        </author>
                        <author>
                            <persName coords="1,262.65,170.80,67.62,9.34">
                                <forename type="first">Zhiliang</forename>
                                <surname>Wang</surname>
                            </persName>
                        </author>
                        <author role="corresp">
                            <persName coords="1,359.18,170.80,56.18,9.34">
                                <forename type="first">Ruoxiu</forename>
                                <surname>Xiao</surname>
                            </persName>
                            <email>xiaoruoxiu@ustb.edu.cn</email>
                            <idno type="ORCID">0000-0002-2721-4813</idno>
                        </author>
                        <title level="a" type="main" coords="1,35.72,120.00,490.68,16.82;1,35.72,138.83,360.13,16.82">Review of Visual Saliency Prediction: Development Process from Neurobiological Basis to Deep Models</title>
                    </analytic>
                    <monogr>
                        <title level="j" type="main">Applied Sciences</title>
                        <title level="j" type="abbrev">Applied Sciences</title>
                        <idno type="eISSN">2076-3417</idno>
                        <imprint>
                            <publisher>MDPI AG</publisher>
                            <biblScope unit="volume">12</biblScope>
                            <biblScope unit="issue">1</biblScope>
                            <biblScope unit="page">309</biblScope>
                            <date type="published" when="2021-12-29">29 December 2021</date>
                        </imprint>
                    </monogr>
                    <idno type="MD5">343B157A42E6169ABBD0B2ED6E44749F</idno>
                    <idno type="DOI">10.3390/app12010309</idno>
                    <note type="submission">Received: 20 November 2021 Accepted: 21 December 2021</note>
                </biblStruct>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-07-09T04:04+0000">
                    <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
                    <ref target="https://github.com/kermitt2/grobid"/>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords>
                    <term>visual attention</term>
                    <term>visual saliency</term>
                    <term>saliency prediction</term>
                    <term>deep learning</term>
                </keywords>
            </textClass>
            <abstract>
                <div
                        xmlns="http://www.tei-c.org/ns/1.0">
                    <p>
                        <s coords="1,205.42,294.12,353.86,8.63;1,166.39,307.09,246.09,8.63">The human attention mechanism can be understood and simulated by closely associating the saliency prediction task to neuroscience and psychology.</s>
                        <s coords="1,416.62,307.09,142.66,8.63;1,166.02,320.06,259.68,8.63">Furthermore, saliency prediction is widely used in computer vision and interdisciplinary subjects.</s>
                        <s coords="1,431.97,320.06,127.31,8.63;1,166.39,333.03,394.45,8.63">In recent years, with the rapid development of deep learning, deep models have made amazing achievements in saliency prediction.</s>
                        <s coords="1,166.39,346.00,392.88,8.63;1,166.39,358.97,292.41,8.63">Deep learning models can automatically learn features, thus solving many drawbacks of the classic models, such as handcrafted features and task settings, among others.</s>
                        <s coords="1,465.03,358.97,94.51,8.63;1,166.39,371.94,392.88,8.63;1,166.39,384.92,59.93,8.63">Nevertheless, the deep models still have some limitations, for example in tasks involving multi-modality and semantic understanding.</s>
                        <s coords="1,229.09,384.92,330.53,8.63;1,166.13,397.89,393.15,8.63;1,166.39,410.86,393.15,8.63;1,166.39,423.83,106.36,8.63">This study focuses on summarizing the relevant achievements in the field of saliency prediction, including the early neurological and psychological mechanisms and the guiding role of classic models, followed by the development process and data comparison of classic and deep saliency prediction models.</s>
                        <s coords="1,275.53,423.83,283.74,8.63;1,166.14,436.80,393.13,8.63;1,166.39,449.77,393.23,8.63;1,166.13,462.74,263.20,8.63">This study also discusses the relationship between the model and human vision, as well as the factors that cause the semantic gaps, the influences of attention in cognitive research, the limitations of the saliency model, and the emerging applications, to provide new saliency predictions for follow-up work and the necessary help and advice.</s>
                    </p>
                </div>
            </abstract>
        </profileDesc>
    </teiHeader>
    <facsimile>
        <surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
        <surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
    </facsimile>
    <text xml:lang="en">
        <body>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="1." coords="1,166.39,537.17,68.11,9.34">Introduction</head>
                <p>
                    <s coords="1,187.65,552.63,371.62,9.58;1,166.12,565.19,29.17,9.58">Approximately 80% of the information that humans receive every day comes from vision.</s>
                    <s coords="1,198.38,565.19,242.18,9.58">However, human visual nerve resources are limited
                        <ref type="bibr" coords="1,426.51,565.19,10.53,9.58" target="#b0">[1]</ref>.
                    </s>
                    <s coords="1,443.64,565.19,115.91,9.58;1,166.39,577.74,163.30,9.58">An information bottleneck exists in the human visual pathway.</s>
                    <s coords="1,334.37,577.74,224.90,9.58;1,166.39,590.29,392.88,9.58;1,166.39,602.85,132.98,9.58">For instance, the visual system receives hundreds of megabytes of visual media data every second, but the information processing speed is only 40 bits per second
                        <ref type="bibr" coords="1,285.08,602.85,10.72,9.58" target="#b1">[2]</ref>.
                    </s>
                    <s coords="1,304.31,602.85,254.96,9.58;1,166.39,615.40,78.95,9.58">In this process, the visual attention mechanism plays an important role
                        <ref type="bibr" coords="1,231.41,615.40,10.45,9.58" target="#b2">[3]</ref>.
                    </s>
                    <s coords="1,248.43,615.40,310.84,9.58;1,166.39,627.95,392.88,9.58;1,166.39,640.51,276.53,9.58">Among the information received in our daily lives, only a small amount of stimuli can enter the visual system for further processing at any time, thereby avoiding computational waste and reducing the difficulty of analysis.</s>
                    <s coords="1,449.21,640.51,110.06,9.58;1,166.39,653.06,392.88,9.58;1,166.39,665.61,316.37,9.58">The development of the Internet and the popularization of smart devices have enhanced the speed of information collection and dissemination, even reaching an unprecedented level.</s>
                    <s coords="1,490.04,665.61,69.23,9.58;1,166.39,678.16,392.88,9.58;1,166.39,690.72,316.23,9.58">However, if all information is indiscriminately allocated with the same computing resources, then it will lead to a waste of computing resources and excessive time consumption.</s>
                    <s coords="1,485.73,690.72,73.55,9.58;1,166.39,703.27,393.27,9.58;1,166.39,715.82,248.37,9.58">Knowing how to select interesting content from massive scenes for analysis and processing in the same way as human beings is therefore a very important endeavor.</s>
                </p>
                <p>
                    <s coords="1,187.65,728.38,373.28,9.58;1,166.39,740.93,394.63,9.58">Visual saliency prediction is a mechanism that imitates human visual attention, including relevant knowledge such as neurobiological, psychological, and computer vision.</s>
                    <s coords="1,166.39,753.48,392.88,9.58;1,166.39,766.03,141.16,9.58">Early attention models often used cognitive psychological knowledge to find information about behaviors, tasks, or goals.</s>
                    <s coords="1,310.66,766.03,248.62,9.58;2,166.39,98.05,393.27,9.58;2,166.39,110.60,47.47,9.58">For example, Itti et al.
                        <ref type="bibr" coords="1,409.48,766.03,11.65,9.58" target="#b3">[4]</ref>
                        proposed a saliency prediction model based on the bottom-up model, from which the deep learning
                        models have gradually flourished.
                    </s>
                    <s coords="2,216.95,110.60,342.33,9.58;2,166.39,123.15,392.88,9.58;2,166.39,135.71,100.03,9.58">Compared with the classic models, the performance of these newly developed models has been greatly improved, and the performance is gradually approaching the human inter-observer.</s>
                    <s coords="2,270.97,135.71,288.30,9.58;2,166.39,148.26,394.53,9.58;2,166.39,160.81,394.62,9.58">The significance of the research on visual saliency detection lies in two aspects: first, as a verifiable prediction, it can be used as a model-based hypothesis test to understand human attention mechanisms at the behavioral and neural levels.</s>
                    <s coords="2,166.39,173.37,393.27,9.58;2,166.39,185.92,394.53,9.58;2,166.39,198.47,393.08,9.58;2,166.12,211.02,393.16,9.58;2,166.39,223.58,394.54,9.58;2,166.10,236.13,394.42,9.58;2,166.39,248.68,146.60,9.58">Second, the saliency prediction model based on the attention mechanism has been widely used in numerous ways, such as target prediction
                        <ref type="bibr" coords="2,391.14,185.92,10.72,9.58" target="#b3">[4]</ref>, target tracking
                        <ref type="bibr" coords="2,476.12,185.92,10.72,9.58" target="#b4">[5]</ref>, image segmentation
                        <ref type="bibr" coords="2,195.91,198.47,10.71,9.58" target="#b5">[6]</ref>, image
                        classification
                        <ref type="bibr" coords="2,305.72,198.47,10.72,9.58" target="#b6">[7]</ref>, image stitching
                        <ref type="bibr" coords="2,396.77,198.47,10.72,9.58" target="#b7">[8]</ref>, video surveillance
                        <ref type="bibr" coords="2,501.09,198.47,10.72,9.58" target="#b8">[9]</ref>, image or video
                        compression
                        <ref type="bibr" coords="2,253.42,211.02,15.42,9.58" target="#b9">[10]</ref>, image or video
                        retrieval
                        <ref type="bibr" coords="2,385.49,211.02,15.42,9.58" target="#b10">[11]</ref>, salient object
                        detection
                        <ref type="bibr" coords="2,512.12,211.02,15.42,9.58" target="#b11">[12]</ref>, video
                        segmentation
                        <ref type="bibr" coords="2,227.09,223.58,15.12,9.58" target="#b12">[13]</ref>, image cropping
                        <ref type="bibr" coords="2,318.62,223.58,15.13,9.58" target="#b13">[14]</ref>, visual SLAM
                        (Simultaneous Localization and Mapping)
                        <ref type="bibr" coords="2,192.11,236.13,15.26,9.58" target="#b14">[15]</ref>, end-to-end
                        driving
                        <ref type="bibr" coords="2,300.01,236.13,15.26,9.58" target="#b15">[16]</ref>, video question
                        answering
                        <ref type="bibr" coords="2,438.00,236.13,15.27,9.58" target="#b16">[17]</ref>, medical diagnosis
                        <ref type="bibr" coords="2,541.44,236.13,15.26,9.58" target="#b17">[18]</ref>, health monitoring
                        <ref type="bibr" coords="2,248.67,248.68,16.60,9.58" target="#b18">[19]</ref>
                        and so on.
                    </s>
                </p>
                <p>
                    <s coords="2,187.65,261.24,372.87,9.58;2,166.39,273.79,369.41,9.58">The current research on saliency detection mainly involves two types of tasks, namely, saliency prediction (or eye fixation prediction) and Salient Object Detection (SOD).</s>
                    <s coords="2,538.29,273.79,20.98,9.58;2,166.39,286.34,345.43,9.58">Both types of tasks aim to detect the most significant area of a picture or a video.</s>
                    <s coords="2,516.74,286.34,43.78,9.58;2,166.39,298.90,348.75,9.58">However, differences exist between these two models and their application scenarios.</s>
                    <s coords="2,522.39,298.90,37.27,9.58;2,166.10,311.45,393.57,9.58;2,166.39,324.00,271.76,9.58">Saliency prediction is informed by the human visual attention mechanism and predicts the possibility of the human eyes to stay in a certain position in the scene.</s>
                    <s coords="2,443.42,324.00,115.85,9.58;2,166.39,336.55,394.13,9.58;2,165.98,349.11,164.78,9.58">By contrast, salient object detection, as the other branch, focuses on the perception and description of the object level, which is a pure computer vision task.</s>
                    <s coords="2,333.84,349.11,202.82,9.58">The two types of tasks asre shown in Figure
                        <ref type="figure" coords="2,529.19,349.11,3.74,9.58" target="#fig_0">1</ref>.
                    </s>
                    <s coords="2,187.65,560.60,271.18,9.58">Numerous researchers have recently investigated SOD tasks.</s>
                    <s coords="2,461.93,560.60,97.34,9.58;2,166.12,573.16,393.16,9.58;2,166.39,585.71,229.22,9.58">Presumably, as a pure visual task, SOD can be more easily and directly applied to certain visual scenes, which is more driven by applications in different fields.</s>
                    <s coords="2,403.44,585.71,157.49,9.58;2,166.39,598.26,392.88,9.58;2,166.39,610.82,80.38,9.58">Benefiting from large-scale benchmarking and deep learning, SOD has been developing rapidly and has shown amazing achievements
                        <ref type="bibr" coords="2,227.86,610.82,15.13,9.58" target="#b19">[20]</ref>.
                    </s>
                    <s coords="2,249.87,610.82,311.14,9.58">In recent years, many researchers have made outstanding contributions.</s>
                    <s coords="2,165.90,623.37,393.68,9.58;2,166.39,635.92,79.63,9.58">Wang et al.
                        <ref type="bibr" coords="2,217.99,623.37,16.72,9.58" target="#b20">[21]</ref>
                        proposed a general framework using iterative top-down and bottom-up saliency inference.
                    </s>
                    <s coords="2,249.12,635.92,310.15,9.58;2,166.39,648.48,165.11,9.58">In addition, the framework used parameter sharing and weight sharing to reduce the amount of parameters.</s>
                    <s coords="2,335.93,648.48,224.58,9.58;2,165.98,661.03,368.78,9.58">Besides, Wang et al.
                        <ref type="bibr" coords="2,428.71,648.48,16.73,9.58" target="#b21">[22]</ref>
                        proposed the PAGE-Net, which mainly included two modules: pyramid attention and salient edge
                        detection.
                    </s>
                    <s coords="2,537.87,661.03,21.40,9.58;2,166.39,673.58,392.88,9.58;2,166.39,686.13,215.32,9.58">With an enlarged receptive field, PAGE-Net obtained the edge information by predicting the edge of significant objects through supervised learning.</s>
                    <s coords="2,384.75,686.13,174.52,9.58;2,166.39,698.69,392.88,9.58;2,166.39,711.24,77.88,9.58">The model proposed by Zhang et al.
                        <ref type="bibr" coords="2,542.81,686.13,16.47,9.58" target="#b22">[23]</ref>
                        applied joint training to the two almost opposite tasks of SOD and COD(Camouflaged Object
                        Detection).
                    </s>
                    <s coords="2,247.36,711.24,311.92,9.58;2,166.39,723.79,222.30,9.58">The Dual ReFinement Network (DRFNet) proposed by Zhang et al.
                        <ref type="bibr" coords="2,542.75,711.24,16.53,9.58" target="#b23">[24]</ref>
                        can be directly applied to high-resolution images.
                    </s>
                    <s coords="2,391.97,723.79,167.31,9.58;2,166.39,736.35,392.88,9.58;2,166.39,748.90,165.40,9.58">DRFNet consisted of a shared feature extractor and two effective refinement heads, which could obtain more discriminative features from high-resolution images.</s>
                </p>
                <p>
                    <s coords="2,187.65,761.45,371.62,9.58;2,166.39,774.01,29.31,9.58">However, the salient object is not necessarily the only possible salient target in the graph.</s>
                    <s coords="2,199.04,774.01,217.49,9.58">Other complicated factors should be considered.</s>
                    <s coords="2,419.88,774.01,139.39,9.58;3,166.39,98.05,393.27,9.58;3,166.39,110.60,172.07,9.58">In addition to its wide range of applications, the saliency prediction task is related to human vision itself, and it is closely related to neuroscience and psychology.</s>
                    <s coords="3,341.61,110.60,218.05,9.58;3,166.39,123.15,212.39,9.58">Consequently, saliency prediction has been widely used in interdisciplinary and emerging subjects.</s>
                    <s coords="3,381.87,123.15,177.41,9.58;3,166.39,135.71,46.71,9.58">The main contributions of this study are as follows:</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="3,166.52,152.92,5.19,9.10">•</head>
                <p>
                    <s coords="3,187.42,153.24,371.85,9.58;3,187.42,165.79,371.85,9.58;3,187.42,178.35,371.85,9.58;3,187.42,190.90,174.54,9.58">This research focused on the task of saliency prediction, analyzed the psychological and physiological mechanisms related to saliency prediction, introduced the classic models that have been affected by saliency prediction, and determined the impact of these theories on deep learning models.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="3,166.52,203.14,5.19,9.10">•</head>
                <p>
                    <s coords="3,187.42,203.45,371.85,9.58;3,187.13,216.01,372.15,9.58;3,187.42,228.56,373.09,9.58;3,187.42,241.11,54.31,9.58">The visual saliency model based on deep learning was analyzed in detail, and the performance evaluation measures of the representative experimental datasets and the model under static and dynamic conditions were discussed and summarized, respectively.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="3,166.52,253.35,5.19,9.10">•</head>
                <p>
                    <s coords="3,187.42,253.66,373.51,9.58;3,187.42,266.22,371.85,9.58;3,187.13,278.77,372.15,9.58;3,187.42,291.32,344.10,9.58">The limitations of the current deep learning model were analyzed, the possible directions for improvement were proposed, new application areas based on the latest progress of deep learning were discussed, and the contribution and significance of saliency prediction with respect to future development trends were presented.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="2." coords="3,166.39,313.82,280.63,9.34">Psychological and Neurobiological Basis of Visual Saliency</head>
                <p>
                    <s coords="3,187.65,329.28,371.62,9.58;3,166.10,341.83,53.28,9.58">Attention mechanism has always been an important subject of neuroscience and psychology.</s>
                    <s coords="3,223.46,341.83,268.00,9.58">In the mid-1950s, cognitive psychology gradually emerged.</s>
                    <s coords="3,495.54,341.83,63.73,9.58;3,166.39,354.39,392.88,9.58;3,166.39,366.94,392.88,9.58;3,166.39,379.49,274.55,9.58">Attention was regarded as an important mechanism of human brain information processing, and several influential attention models, such as the filter model (1958), attenuation model
                        <ref type="bibr" coords="3,511.14,366.94,24.75,9.58">(1960)</ref>, and response selection model
                        <ref type="bibr" coords="3,277.49,379.49,24.48,9.58">(1963)</ref>, among others, were produced.
                    </s>
                    <s coords="3,444.03,379.49,115.24,9.58;3,166.39,392.05,392.88,9.58;3,166.39,404.60,102.34,9.58">Treisman
                        <ref type="bibr" coords="3,486.24,379.49,16.49,9.58" target="#b24">[25]</ref>
                        proposed an important model called Feature Integration Theory (FIT) to vividly illustrate the
                        selective role of visual attention.
                    </s>
                    <s coords="3,271.85,404.60,287.42,9.58;3,166.39,417.15,141.39,9.58">The visual process in this model was divided into a pre-attention stage and a focal attention stage.</s>
                    <s coords="3,310.88,417.15,248.39,9.58;3,166.39,429.70,111.29,9.58">Feature integration was implemented after extracting the location-related features.</s>
                    <s coords="3,281.18,429.70,279.74,9.58;3,166.39,442.26,204.28,9.58">Koch and Ullman
                        <ref type="bibr" coords="3,363.41,429.70,16.73,9.58" target="#b25">[26]</ref>
                        enhanced FIT by integrating the returninhibition mechanism to achieve a focus shift.
                    </s>
                    <s coords="3,373.76,442.26,185.52,9.58;3,166.39,454.81,392.88,9.58;3,166.39,467.36,32.25,9.58">Moreover, on the basis of criticisms of the early FIT model, Wolfe
                        <ref type="bibr" coords="3,266.34,454.81,16.46,9.58" target="#b26">[27]</ref>
                        proposed the guided search model to explain and predict search results.
                    </s>
                    <s coords="3,201.84,467.36,357.43,9.58;3,166.39,479.92,392.88,9.58;3,166.39,492.47,169.92,9.58">These neurological and psychological studies have provided an important basis and criteria for calculating visual saliency, such as center surround antagonism, global rarity, or maximization of information.</s>
                </p>
                <p>
                    <s coords="3,187.65,505.02,371.62,9.58;3,166.12,517.58,394.91,9.58">Visual saliency prediction mainly used mathematical models to simulate the human visual attention function and subsequently calculated the importance of visual information.</s>
                    <s coords="3,166.09,530.13,393.19,9.58;3,166.39,542.68,318.31,9.58">The simulation of the human visual attention system mainly used some of the important achievements in visual physiology and psychology mentioned above.</s>
                    <s coords="3,490.46,542.68,68.82,9.58;3,166.39,555.23,393.08,9.58;3,166.39,567.79,392.89,9.58;3,166.39,580.34,204.86,9.58">Notably, visual saliency prediction did not study eye movement strategies in visual attention but rather calculates the information pertaining to the different degrees of importance with respect to scenes for eye movement decision-making.</s>
                    <s coords="3,374.84,580.34,184.43,9.58;3,166.39,592.89,351.11,9.58">These studies have played a guiding and standardizing role in the subsequent development of saliency detection models.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="3." coords="3,166.39,615.39,152.49,9.34">Classic Visual Saliency Models</head>
                <p>
                    <s coords="3,187.65,630.85,371.62,9.58;3,166.39,643.40,247.12,9.58">The classic visual saliency model considered the psychological and neurobiological basis, and most of them were handcrafted feature models.</s>
                    <s coords="3,416.48,643.40,144.04,9.58;3,166.39,655.96,392.88,9.58;3,166.39,668.51,392.88,9.58;3,166.39,681.06,323.60,9.58">As a research basis of psychology, classic visual saliency models could be usually divided into two models according to the level of information processing: bottom-up saliency models (data-driven, task-agnostic model), and top-down saliency models (task-driven, task-specific model).</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="3.1." coords="3,166.39,703.35,159.99,9.50">Bottom-Up Visual Saliency Models</head>
                <p>
                    <s coords="3,187.65,719.02,372.87,9.58;3,166.39,731.57,82.05,9.58">Bottom-up visual saliency models usually extract low-level features, such as contrast, color, and texture.</s>
                    <s coords="3,253.36,731.57,305.91,9.58;3,166.39,744.13,158.70,9.58">The difference between low-level features and background features strongly attract attention resources.</s>
                    <s coords="3,328.63,744.13,231.04,9.58;3,166.39,756.68,118.63,9.58">This attention prediction mechanism is involuntary and entails fast processing.</s>
                    <s coords="3,288.15,756.68,271.12,9.58;3,166.39,769.23,287.67,9.58">For example, the presence of pedestrians, vehicles, individual flowers, and beasts in an image will show strong visual saliency.</s>
                    <s coords="3,457.15,769.23,102.12,9.58;4,166.39,98.05,392.88,9.58;4,166.39,110.60,392.88,9.58;4,166.39,123.15,334.62,9.58">Among them, the local contrast model is based on the physiological and psychological principles of FIT and the center surround antagonism, and it defines a certain mechanism when selecting salient areas in an image to realize the simulation of the visual attention mechanism.</s>
                    <s coords="4,504.14,123.15,56.38,9.58;4,166.39,135.71,392.88,9.58;4,165.98,148.26,134.06,9.58">For example, the earliest model of Itti
                        <ref type="bibr" coords="4,274.47,135.71,11.65,9.58" target="#b3">[4]</ref>
                        could simulate the process of shifting human visual attention without any prior information.
                    </s>
                    <s coords="4,303.13,148.26,256.14,9.58;4,166.39,160.81,392.88,9.58;4,166.39,173.37,392.88,9.58;4,166.39,185.92,136.86,9.58">According to the features captured from images, the model analyzed visual stimuli, allocated computing resources, selected the salient areas in the scene according to the saliency intensity of different positions, and simulated the process of human visual attention transfer.</s>
                    <s coords="4,306.20,185.92,253.08,9.58;4,166.39,198.47,394.63,9.58">Although the performance of the model was general, it was the first successful attempt from the neurobiological model, which is of great significance.</s>
                    <s coords="4,166.39,211.02,282.73,9.58">Since then, other researchers have contributed improvements.</s>
                    <s coords="4,454.93,211.02,104.35,9.58;4,166.39,223.58,393.08,9.58;4,166.39,236.13,154.45,9.58">Harel
                        <ref type="bibr" coords="4,483.51,211.02,16.73,9.58" target="#b27">[28]</ref>
                        changed the graph-based visual saliency (GBVS) model to the Markov random field with non-linear
                        combination in the synthesis stage.
                    </s>
                    <s coords="4,323.97,236.13,235.30,9.58;4,166.39,248.68,392.88,9.58;4,166.39,261.24,129.66,9.58">The model formed activation maps on certain feature channels, and then normalized them in a way which highlighted conspicuity and admitted combination with other maps.</s>
                    <s coords="4,299.09,261.24,260.19,9.58;4,166.39,273.47,392.88,9.90;4,166.39,286.34,282.01,9.58">Ma and Zhang
                        <ref type="bibr" coords="4,364.93,261.24,16.46,9.58" target="#b28">[29]</ref>
                        used local contrast analysis to extract the saliency maps of an image, and on this basis, Tie
                        Liu et al.
                        <ref type="bibr" coords="4,423.77,273.79,16.64,9.58" target="#b29">[30]</ref>
                        used 9 × 9 neighborhoods and adopted a conditional random field (CRF) learning model.
                    </s>
                    <s coords="4,451.78,286.34,107.49,9.58;4,166.39,298.90,147.55,9.58">Borji
                        <ref type="bibr" coords="4,475.48,286.34,16.73,9.58" target="#b30">[31]</ref>
                        analyzed local rarity based on the sparse coding.
                    </s>
                    <s coords="4,317.04,298.90,242.24,9.58;4,166.39,311.45,100.93,9.58">Sclaroff et al.
                        <ref type="bibr" coords="4,375.74,298.90,16.56,9.58" target="#b31">[32]</ref>
                        proposed a saliency prediction model based on Boolean Map.
                    </s>
                    <s coords="4,270.41,311.45,289.25,9.58;4,166.39,324.00,148.56,9.58">In addition, researchers have used other models to predict saliency by using local or global contrast.</s>
                    <s coords="4,319.78,324.00,239.49,9.58;4,166.39,336.55,392.89,9.58;4,166.39,349.11,392.88,9.58;4,166.10,361.66,393.18,9.58;4,166.39,374.21,248.68,9.58">Some of the notable examples include the pixel-level contrast saliency model proposed by Zhai and Shah
                        <ref type="bibr" coords="4,391.62,336.55,15.13,9.58" target="#b32">[33]</ref>, the sliding
                        windows-based model for global contrast calculation proposed by Wei
                        <ref type="bibr" coords="4,374.62,349.11,15.20,9.58" target="#b33">[34]</ref>, the color
                        contrast linear fusion model proposed by Margolin
                        <ref type="bibr" coords="4,265.34,361.66,15.14,9.58" target="#b34">[35]</ref>, the frequency
                        tuning model proposed by Achanta
                        <ref type="bibr" coords="4,505.25,361.66,15.14,9.58" target="#b35">[36]</ref>, and the color
                        space quantization model proposed by Cheng
                        <ref type="bibr" coords="4,395.98,374.21,15.27,9.58" target="#b36">[37]</ref>.
                    </s>
                    <s coords="4,418.19,374.21,141.09,9.58;4,166.39,386.77,392.88,9.58;4,166.39,399.32,236.84,9.58">Other researchers have used the superpixel
                        <ref type="bibr" coords="4,214.21,386.77,13.42,9.58" target="#b37">[38]</ref>
                        <ref type="bibr" coords="4,227.63,386.77,4.47,9.58" target="#b38">[39]</ref>
                        <ref type="bibr" coords="4,232.11,386.77,13.42,9.58" target="#b39">[40]</ref>
                        as the processing unit to calculate the variance of color space distribution as a means of
                        improving the computational efficiency.
                    </s>
                </p>
                <p>
                    <s coords="4,187.65,411.87,351.95,9.58">Some models have been based on information theory and image transformation.</s>
                    <s coords="4,542.70,411.87,16.58,9.58;4,166.39,424.42,394.53,9.58;4,166.39,436.98,392.88,9.58;4,166.39,449.53,120.20,9.58">The essence of these models based on information theory is to calculate the maximum information sampling from the visual environment, select the richest part from the scene, and discard the remaining part.</s>
                    <s coords="4,289.69,449.53,271.25,9.58;4,166.39,462.08,271.16,9.58">Among them, the Attention-based on Information Maximization (AIM) model of Bruce and Tsotsos
                        <ref type="bibr" coords="4,347.63,462.08,16.73,9.58" target="#b40">[41]</ref>
                        was influential.
                    </s>
                    <s coords="4,442.93,462.08,116.35,9.58;4,166.39,474.64,321.05,9.58">The AIM model has used Shannon's self-information measure to calculate the saliency of the image.</s>
                    <s coords="4,490.53,474.64,68.75,9.58;4,166.39,487.19,392.88,9.58;4,166.39,499.74,38.75,9.58">Firstly, a certain number of natural image blocks were randomly selected for training to obtain the basic function.</s>
                    <s coords="4,208.26,499.74,351.02,9.58;4,166.39,512.30,392.88,9.58;4,166.39,524.85,394.53,9.58;4,166.39,537.40,394.62,9.58">Then, the image was divided into blocks of the same size, the basis coefficients of the corresponding blocks were extracted as the features of the block through Independent Component Analysis (ICA), the distribution of each feature was obtained through probability density estimation, and finally the probability density of the feature was obtained.</s>
                    <s coords="4,166.39,549.95,394.13,9.58;4,166.39,562.51,392.88,9.58;4,166.39,575.06,392.88,9.58;4,166.39,587.61,86.89,9.58">Other notable models included the incremental coding length model proposed by Hou
                        <ref type="bibr" coords="4,541.62,549.95,15.12,9.58" target="#b41">[42]</ref>, the rare linear
                        combination model proposed by Mancas
                        <ref type="bibr" coords="4,412.72,562.51,15.27,9.58" target="#b42">[43]</ref>, the
                        self-similarity prediction model proposed by Seo
                        <ref type="bibr" coords="4,275.94,575.06,16.73,9.58" target="#b43">[44]</ref>
                        and the Mahalanobis distance calculation model proposed by Rosenholtz
                        <ref type="bibr" coords="4,234.01,587.61,15.42,9.58" target="#b44">[45]</ref>.
                    </s>
                    <s coords="4,258.07,587.61,302.85,9.58;4,166.39,600.17,392.88,9.58;4,166.39,612.72,261.50,9.58">As for the use of image transformation models for saliency prediction, the spectral residual model proposed by Hou
                        <ref type="bibr" coords="4,396.13,600.17,16.74,9.58" target="#b45">[46]</ref>
                        did not examine the foreground characteristics but rather utilizes the research background.
                    </s>
                    <s coords="4,431.02,612.72,128.25,9.58;4,166.39,625.27,164.43,9.58">The areas that did not match these features are the areas of interest.</s>
                    <s coords="4,333.92,625.27,225.36,9.58;4,166.39,637.83,392.88,9.58;4,166.39,650.38,74.56,9.58">After calculating the residual spectrum, the residual spectrum was mapped back to the spatial domain by inverse Fourier transform to obtain the saliency map.</s>
                    <s coords="4,243.95,650.38,315.32,9.58;4,166.39,662.93,392.89,9.58;4,166.10,675.48,33.58,9.58">On this basis, Guo
                        <ref type="bibr" coords="4,324.50,650.38,16.46,9.58" target="#b46">[47]</ref>
                        proposed a model that used the phase spectrum to obtain the saliency map and Holtzman-Gazit
                        <ref type="bibr" coords="4,365.61,662.93,16.56,9.58" target="#b47">[48]</ref>
                        extracted a variety of resolutions for the picture.
                    </s>
                    <s coords="4,202.81,675.48,356.47,9.58;4,166.39,688.04,174.85,9.58">Sclaroff
                        <ref type="bibr" coords="4,238.49,675.48,16.60,9.58" target="#b48">[49]</ref>
                        proposed a Boolean Map based saliency model(BMS) by discovering surrounding regions via boolean
                        maps.
                    </s>
                    <s coords="4,344.34,688.04,214.94,9.58;4,166.39,700.59,187.96,9.58">The model obtained saliency maps by analyzing the topological structure of boolean maps.</s>
                    <s coords="4,357.44,700.59,201.83,9.58;4,166.39,713.14,249.58,9.58">Although BMS was simple to implement and efficient to run, it performed well in the classical models.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="3.2." coords="4,166.39,735.43,157.23,9.50">Top-Down Visual Saliency Models</head>
                <p>
                    <s coords="4,187.65,751.10,338.24,9.58">The top-down visual saliency model is often based on certain specific tasks.</s>
                    <s coords="4,529.04,751.10,30.23,9.58;4,166.39,763.65,326.36,9.58">Due to the diversity and complexity of tasks, modeling is also more difficult
                        <ref type="bibr" coords="4,473.57,763.65,15.35,9.58" target="#b49">[50]</ref>.
                    </s>
                    <s coords="4,495.87,763.65,63.41,9.58;5,166.12,98.05,277.40,9.58">The top-down visual saliency model is mainly based on the Bayesian model.</s>
                    <s coords="5,447.07,98.05,112.21,9.58;5,166.39,110.60,392.88,9.58;5,166.39,123.15,258.71,9.58">In addition, the Bayesian model can be regarded as a special case of the decision theoretical model, as both simulate the biological calculation process of human visual saliency.</s>
                </p>
                <p>
                    <s coords="5,187.65,135.71,371.62,9.58;5,166.39,148.26,368.69,9.58">The Bayesian model in saliency prediction is a probabilistic combination model that combined scene information and prior information according to Bayesian rules.</s>
                    <s coords="5,542.26,148.26,17.01,9.58;5,166.39,160.81,393.27,9.58;5,166.39,173.37,174.55,9.58">The model proposed by Torrallba et al.
                        <ref type="bibr" coords="5,322.05,160.81,16.69,9.58" target="#b50">[51]</ref>
                        multiplied the bottom-up and top-down saliency maps to obtain the final saliency map.
                    </s>
                    <s coords="5,346.38,173.37,212.89,9.58;5,166.39,185.92,287.98,9.58">On this basis, Ehinger et al.
                        <ref type="bibr" coords="5,475.72,173.37,16.73,9.58" target="#b51">[52]</ref>
                        integrated the feature prior information of the target into the above framework.
                    </s>
                    <s coords="5,457.46,185.92,101.81,9.58;5,166.39,198.47,260.38,9.58">Xie et al.
                        <ref type="bibr" coords="5,497.97,185.92,16.67,9.58" target="#b52">[53]</ref>
                        proposed a saliency prediction model based on posterior probability.
                    </s>
                    <s coords="5,429.86,198.47,129.80,9.58;5,166.39,211.02,357.02,9.58">The SUN model proposed by Zhang et al.
                        <ref type="bibr" coords="5,221.02,211.02,16.60,9.58" target="#b53">[54]</ref>
                        used visual features and spatial location as the prior knowledge.
                    </s>
                </p>
                <p>
                    <s coords="5,187.65,223.58,371.62,9.58;5,166.39,236.13,394.13,9.58;5,166.39,248.68,392.88,9.58;5,166.39,261.24,58.01,9.58">The model based on decision theory in saliency prediction is a strategy model that decides the optimal plan based on the information and evaluation criteria requirements, i.e., how to make optimal decisions about perceptual information of the surrounding environment.</s>
                    <s coords="5,227.34,261.24,331.94,9.58;5,166.10,273.79,393.18,9.58;5,166.39,286.34,252.96,9.58">Gao and Vasconcelos
                        <ref type="bibr" coords="5,319.42,261.24,16.50,9.58" target="#b54">[55,</ref>
                        <ref type="bibr" coords="5,335.92,261.24,12.37,9.58" target="#b55">56]</ref>
                        believe that the salient features in the recognition process are derived from other classes of
                        interest, and they defined top-down attention as a classification problem with the smallest
                        expected error.
                    </s>
                    <s coords="5,422.47,286.34,136.80,9.58;5,166.39,298.90,333.44,9.58">Kim et al.
                        <ref type="bibr" coords="5,467.92,286.34,16.71,9.58" target="#b56">[57]</ref>
                        recommended a temporal and spatial saliency model based on motion perception grouping.
                    </s>
                    <s coords="5,502.93,298.90,56.34,9.58;5,166.10,311.45,389.70,9.58">Gu et al.
                        <ref type="bibr" coords="5,542.63,298.90,16.65,9.58" target="#b57">[58]</ref>
                        proposed a model based on the decision theory mechanism to predict regions of interest.
                    </s>
                </p>
                <p>
                    <s coords="5,187.65,324.00,372.87,9.58;5,166.39,336.55,393.08,9.58;5,166.10,349.11,394.83,9.58;5,166.39,361.66,70.44,9.58">Early machine learning models often use a variety of machine learning methods, such as Convolutional Neural Networks (CNNs), Support Vector Machines (SVMs), or probability kernel density estimation, and they mostly combined the bottom-up and topdown methods.</s>
                    <s coords="5,240.31,361.66,319.36,9.58;5,166.39,374.21,392.88,9.58;5,166.39,386.77,206.22,9.58">Notable examples included the nonlinear mapping model proposed by Kienzle et al.
                        <ref type="bibr" coords="5,227.60,374.21,15.42,9.58" target="#b58">[59]</ref>, the regression
                        classifier model proposed by Peters et al.
                        <ref type="bibr" coords="5,503.00,374.21,15.42,9.58" target="#b59">[60]</ref>, and the linear
                        SVM model proposed by Judd et al.
                        <ref type="bibr" coords="5,353.56,386.77,15.25,9.58" target="#b60">[61]</ref>.
                    </s>
                    <s coords="5,375.71,386.77,183.57,9.58;5,166.39,399.32,392.88,9.58;5,166.39,411.87,342.87,9.58">Those early machine learning models had a certain exploratory nature for subsequent deep learning models, and they played an important guiding role for the subsequently developed deep learning models.</s>
                </p>
                <p>
                    <s coords="5,187.65,424.42,371.62,9.58;5,166.39,436.98,264.62,9.58">Although these classical models were designed in a variety of ways, their performance gradually reached a bottleneck due to handcrafted features.</s>
                    <s coords="5,434.14,436.98,125.13,9.58;5,166.39,449.53,281.87,9.58">The development process of neurobiological models and classic models is shown in Figure
                        <ref type="figure" coords="5,440.79,449.53,3.74,9.58" target="#fig_1">2</ref>.
                    </s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="4." coords="5,166.39,695.42,144.76,9.34">Deep Visual Saliency Models</head>
                <p>
                    <s coords="5,187.65,710.88,371.62,9.58;5,166.39,723.43,319.71,9.58">In 2014, Vig et al.
                        <ref type="bibr" coords="5,264.25,710.88,16.51,9.58" target="#b61">[62]</ref>
                        proposed a deep convolutional network named eDN that could be implemented in fully automatic
                        data-driven mode to extract features.
                    </s>
                    <s coords="5,489.18,723.43,70.09,9.58;5,166.39,735.99,392.89,9.58;5,166.39,748.54,268.20,9.58">Compared with the classic model, eDN could automatically learn the image expression and obtain the final saliency map by fusing the feature maps from different layers.</s>
                    <s coords="5,437.70,748.54,121.57,9.58;5,166.39,761.09,392.89,9.58;5,166.39,773.64,316.82,9.58">However, due to the limited number of datasets and the limited number of trainable graphs in the data set, the depth of the network was not enough, as the structural scalability was limited.</s>
                    <s coords="5,486.30,773.64,72.98,9.58;6,166.39,98.05,392.88,9.58;6,166.39,110.60,360.67,9.58">Since then, more researchers have used deep models to study saliency prediction, and the application of deep models in static and dynamic saliency prediction has achieved better results.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="4.1." coords="6,166.39,132.89,73.27,9.50">Static Models</head>
                <p>
                    <s coords="6,187.65,148.56,371.62,9.58;6,166.39,161.11,257.64,9.58">After eDN, Kümmerer et al.
                        <ref type="bibr" coords="6,314.31,148.56,16.68,9.58" target="#b62">[63]</ref>
                        proposed a CNN model named Deep Gaze I based on image classification by using the AlexNet
                        <ref type="bibr" coords="6,365.28,161.11,16.62,9.58" target="#b63">[64]</ref>
                        network.
                    </s>
                    <s coords="6,427.15,161.11,132.43,9.58;6,166.39,173.66,392.89,9.58;6,165.98,186.22,333.56,9.58">The major innovation of Deep Gaze I was the application of transfer learning for saliency prediction by using pre-trained weights on ImageNet
                        <ref type="bibr" coords="6,264.40,186.22,15.37,9.58" target="#b64">[65]</ref>, connecting them
                        to the output layer of AlexNet.
                    </s>
                    <s coords="6,502.64,186.22,56.91,9.58;6,166.39,198.77,392.88,9.58;6,166.39,211.32,86.51,9.58">The network contains a central deviation that was converted into a probability distribution by using a softmax function.</s>
                    <s coords="6,256.57,211.32,302.70,9.58;6,166.39,223.88,79.67,9.58">The typical saliency datasets were relatively small, and the training effect was limited.</s>
                    <s coords="6,249.15,223.88,310.13,9.58;6,166.39,236.43,269.15,9.58">ImageNet has a good training effect as a million-level database, but the training resources are huge and the training time is excessive.</s>
                    <s coords="6,438.64,236.43,120.64,9.58;6,166.39,248.98,392.88,9.58;6,166.39,261.54,152.60,9.58">The use of transfer learning based on ImageNet makes it easier to learn the features of deep CNNs (DCNN) and attain much better generalization effects.</s>
                    <s coords="6,322.09,261.54,237.18,9.58;6,166.39,274.09,394.12,9.58;6,166.39,286.64,258.28,9.58">Kruthiventi et al.
                        <ref type="bibr" coords="6,399.97,261.54,16.66,9.58" target="#b65">[66]</ref>
                        proposed the DeepFix model in the same year, by using the VGG-16
                        <ref type="bibr" coords="6,327.77,274.09,16.66,9.58" target="#b66">[67]</ref>
                        network as the main feature extraction network, allowing the network to use location-related
                        information.
                    </s>
                    <s coords="6,428.40,286.64,130.87,9.58;6,166.04,299.19,210.63,9.58">Compared with AlexNet, the VGG-16 network is simpler and more effective.</s>
                    <s coords="6,379.75,299.19,179.80,9.58;6,166.39,311.75,108.77,9.58">Using a better target prediction network becomes a better choice.</s>
                    <s coords="6,278.94,311.75,280.34,9.58;6,166.39,324.30,392.88,9.58;6,166.39,336.85,33.35,9.58">Then, DeepGaze II
                        <ref type="bibr" coords="6,365.95,311.75,16.73,9.58" target="#b67">[68]</ref>
                        switched to VGG-19
                        <ref type="bibr" coords="6,479.36,311.75,15.41,9.58" target="#b66">[67]</ref>, retrained the
                        image features on the SALICON
                        <ref type="bibr" coords="6,317.19,324.30,16.73,9.58" target="#b68">[69]</ref>
                        dataset, and then fine-tuned on the MIT1003
                        <ref type="bibr" coords="6,542.55,324.30,16.73,9.58" target="#b69">[70]</ref>
                        dataset.
                    </s>
                    <s coords="6,202.77,336.85,356.50,9.58;6,166.39,349.41,165.31,9.58">As a result, the performance of the updated model has been significantly improved compared with that of Deep Gaze I.</s>
                    <s coords="6,335.13,349.41,224.14,9.58;6,166.39,361.96,364.10,9.58">This development trend indicated that retraining deep features and the task of fine-tuning contribute to performance enhancement.</s>
                    <s coords="6,533.59,361.96,26.08,9.58;6,166.39,374.51,392.88,9.58;6,166.39,387.06,75.82,9.58">Many researchers have adopted small-scale retraining and fine-tuning with the successful use of transfer learning.</s>
                </p>
                <p>
                    <s coords="6,187.65,399.62,371.82,9.58;6,166.39,412.17,393.08,9.58;6,166.39,424.72,32.25,9.58">Similarly, many researchers have adopted models that can capture relatively fine or coarse features by adjusting the input of different resolutions as a means of achieving better results.</s>
                    <s coords="6,203.62,424.72,355.66,9.58;6,166.07,437.28,340.82,9.58">Among them, Pan et al.
                        <ref type="bibr" coords="6,314.84,424.72,16.73,9.58" target="#b70">[71]</ref>
                        proposed two saliency models: shallow ConvNet (JuntingNet) and deep ConvNet (SalNet) to train
                        end-to-end architectures.
                    </s>
                    <s coords="6,512.77,437.28,46.51,9.58;6,165.98,449.83,393.30,9.58;6,166.39,462.38,88.28,9.58">SALICON was used to train a convolutional network by using VGG-16 network with dual-branch multi-scale features.</s>
                    <s coords="6,257.79,462.38,301.48,9.58;6,166.39,474.94,277.03,9.58">Dual-branch can effectively improve the model performance, but the calculation cost and memory are higher in training and testing.</s>
                </p>
                <p>
                    <s coords="6,187.65,487.49,371.62,9.58;6,166.39,500.04,294.22,9.58">Then, by combining migration-integrating information on different image scales, the model could greatly surpass the level of advancements at the time.</s>
                    <s coords="6,463.71,500.04,97.22,9.58;6,166.39,512.59,392.88,9.58;6,166.39,525.15,392.88,9.58;6,166.39,537.70,366.78,9.58">The probability distribution prediction model proposed by Jetley et al.
                        <ref type="bibr" coords="6,390.95,512.59,16.73,9.58" target="#b71">[72]</ref>
                        defined saliency as a generalized Bernoulli distribution, and it included a fully end-to-end
                        training deep CNN that combined the classic softmax loss with the calculation of the different
                        probability distributions.
                    </s>
                    <s coords="6,536.27,537.70,23.21,9.58;6,166.39,550.25,392.88,9.58;6,166.07,562.81,169.91,9.58">Their results showed that the new loss function was more effective than the classic loss function (e.g., Euclidean) in saliency prediction.</s>
                    <s coords="6,339.08,562.81,221.85,9.58;6,166.39,575.36,319.37,9.58">Liu and Han
                        <ref type="bibr" coords="6,397.35,562.81,16.58,9.58" target="#b72">[73]</ref>
                        proposed a Deep Spatial Contextual Long-Term Recurrent Convolutional Network (DSCLRCN) model.
                    </s>
                    <s coords="6,489.26,575.36,70.02,9.58;6,166.39,587.91,392.88,9.58;6,166.39,600.47,393.87,9.58;6,166.39,613.02,124.32,9.58">First, CNN was used to learn the local saliency of small image regions, and then images in the horizontal and vertical directions were scanned using the Long Short-Term Memory networks (LSTMs) to capture the global context.</s>
                    <s coords="6,293.74,613.02,265.53,9.58;6,166.39,625.57,379.95,9.58">These two operations allowed DSCLRCN to effectively merge local and global contexts at the same time for inferring the saliency maps of the image.</s>
                </p>
                <p>
                    <s coords="6,187.65,638.12,371.62,9.58;6,166.39,650.68,65.72,9.58">The ML-Net model proposed by Cornia et al.
                        <ref type="bibr" coords="6,395.25,638.12,16.73,9.58" target="#b73">[74]</ref>
                        combined the advantages of the above models.
                    </s>
                    <s coords="6,238.11,650.68,321.17,9.58;6,166.39,663.23,197.16,9.58">Their model consisted of a feature extraction DCNN, a feature coding network, and an a priori learning network.</s>
                    <s coords="6,369.20,663.23,190.07,9.58;6,166.39,675.78,251.86,9.58">At the same time, the loss function of the network was weighted by three parts: NSS, CC, and SIM.</s>
                    <s coords="6,420.76,675.78,138.51,9.58;6,166.39,688.34,373.32,9.58">The SALICON model also used differentiable metrics, such as NSS, CC, SIM, and KL divergence, to train the network.</s>
                    <s coords="6,542.84,688.34,16.44,9.58;6,166.39,700.89,392.88,9.58;6,166.39,713.44,392.88,9.58;6,166.39,725.99,128.33,9.58">The SAM-ResNet model and SAM-VGG model subsequently proposed by Cornia et al.
                        <ref type="bibr" coords="6,542.55,700.89,16.73,9.58" target="#b74">[75]</ref>
                        combined the full convolutional network and the cyclic convolutional network to obtain a spatial
                        attention mechanism.
                    </s>
                    <s coords="6,297.81,725.99,261.47,9.58;6,166.39,738.55,234.33,9.58">SalGAN
                        <ref type="bibr" coords="6,337.56,725.99,16.69,9.58" target="#b75">[76]</ref>
                        used adversarial networks for training, and it consisted of two parts, a generator and a
                        discriminator.
                    </s>
                    <s coords="6,403.67,738.55,155.60,9.58;6,166.39,751.10,394.63,9.58">The network learned the parameters through the backpropagation of the downsampled binary cross entropy loss calculation.</s>
                </p>
                <p>
                    <s coords="7,166.09,98.05,393.19,9.58;7,166.39,110.60,243.84,9.58">The success of the model indicated that the choice of an appropriate loss function can be treated as a method for improving the prediction effect.</s>
                </p>
                <p>
                    <s coords="7,187.65,123.15,373.37,9.58">In recent years, some excellent models have been proposed for saliency prediction.</s>
                    <s coords="7,166.22,135.71,393.05,9.58;7,166.39,148.26,299.83,9.58">Jia et al.
                        <ref type="bibr" coords="7,203.13,135.71,16.56,9.58" target="#b76">[77]</ref>
                        proposed a saliency model called EML-Net based on the similarities between images and the
                        integration of Extreme Learning Machines (ELMs).
                    </s>
                    <s coords="7,469.92,148.26,91.01,9.58;7,166.10,160.81,394.83,9.58;7,166.39,173.37,287.29,9.58">Wang et al.
                        <ref type="bibr" coords="7,522.64,148.26,16.73,9.58" target="#b77">[78]</ref>
                        proposed the Deep Visual Attention (DVA) model in which the architecture was trained in multiple
                        scales to predict pixel saliency based on a skip-layer network.
                    </s>
                    <s coords="7,456.77,173.37,102.90,9.58;7,166.39,185.92,274.59,9.58">The model proposed by Gorji
                        <ref type="bibr" coords="7,190.86,185.92,16.52,9.58" target="#b78">[79]</ref>
                        used shared attention to enhance saliency prediction.
                    </s>
                    <s coords="7,444.10,185.92,115.18,9.58;7,166.39,198.47,318.32,9.58">Dodge et al.
                        <ref type="bibr" coords="7,499.02,185.92,16.52,9.58" target="#b79">[80]</ref>
                        proposed a model called MxSalNet, which was formulated as a mixture of experts.
                    </s>
                    <s coords="7,487.81,198.47,71.47,9.58;7,166.10,211.02,393.18,9.58;7,166.39,223.58,175.88,9.58">Mahdi et al.
                        <ref type="bibr" coords="7,542.69,198.47,16.58,9.58" target="#b80">[81]</ref>
                        proposed a deep feature-based saliency (DeepFeat) model to utilize features by combining
                        bottom-up and top-down saliency maps.
                    </s>
                    <s coords="7,345.34,223.58,213.94,9.58;7,166.39,236.13,392.88,9.58;7,166.39,248.68,236.04,9.58">AKa et al.
                        <ref type="bibr" coords="7,390.25,223.58,16.47,9.58" target="#b81">[82]</ref>
                        proposed the MSI-Net based on an encoder-decoder structure and it includes a module with
                        multiple convolutional layers at different dilation rates to capture multi-scale features.
                    </s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="4.2." coords="7,166.39,270.97,87.12,9.50">Dynamic Models</head>
                <p>
                    <s coords="7,187.65,286.64,371.62,9.58;7,166.39,299.19,204.53,9.58">Unlike the settings of the static models, the observation time in dynamic models is reduced from approximately 4 s to 0.05 s.</s>
                    <s coords="7,377.90,299.19,181.37,9.58;7,166.39,311.75,370.36,9.58">In addition, due to the obvious motion information in videos, predicting the saliency of the dynamic video is more difficult.</s>
                    <s coords="7,539.84,311.75,19.44,9.58;7,166.39,324.30,187.26,9.58">As a result, much fewer dynamic models exist.</s>
                    <s coords="7,356.94,324.30,202.33,9.58;7,166.39,336.85,394.43,9.58">Nevertheless, as the demand for applications continues to grow, the research on dynamic models has also been continuously developing.</s>
                </p>
                <p>
                    <s coords="7,187.65,349.41,373.27,9.58;7,166.39,361.96,24.89,9.58">Dynamic models usually add temporal information to CNNs or use LSTMs for modeling.</s>
                    <s coords="7,197.68,361.96,361.59,9.58;7,166.39,374.51,196.52,9.58">Early dynamic models mainly combined static saliency features with temporal information based on the bottom-up model.</s>
                    <s coords="7,366.00,374.51,193.27,9.58;7,166.39,387.06,393.27,9.58;7,166.39,399.62,256.89,9.58">Gao et al.
                        <ref type="bibr" coords="7,410.56,374.51,16.72,9.58" target="#b82">[83]</ref>
                        integrated additional motion information, and Seo et al.
                        <ref type="bibr" coords="7,289.02,387.06,16.73,9.58" target="#b83">[84]</ref>
                        used a local regression kernel to calculate the similarity between the pixels in the video and
                        its surrounding area.
                    </s>
                    <s coords="7,427.03,399.62,132.25,9.58;7,166.39,412.17,252.12,9.58">However, the performance of these models were restricted by their handcrafted features.</s>
                    <s coords="7,421.54,412.17,137.73,9.58;7,166.39,424.72,179.17,9.58">The emergence of deep learning frameworks has improved this situation.</s>
                    <s coords="7,348.65,424.72,210.62,9.58;7,166.39,437.28,251.48,9.58">Bak et al.
                        <ref type="bibr" coords="7,391.17,424.72,16.61,9.58" target="#b84">[85]</ref>
                        proposed the dynamic model and added motion features based on the two-stream network.
                    </s>
                    <s coords="7,420.98,437.28,139.95,9.58;7,166.39,449.83,394.62,9.58">Due to the final fusion of the information of the two streams, the network was limited in learning spatiotemporal features.</s>
                    <s coords="7,166.39,462.38,392.88,9.58;7,166.39,474.94,181.67,9.58">Chaabouni et al.
                        <ref type="bibr" coords="7,244.30,462.38,16.73,9.58" target="#b85">[86]</ref>
                        added residual motion and RGB color planes of two consecutive frames to CNN based on transfer
                        learning.
                    </s>
                    <s coords="7,351.02,474.94,208.26,9.58;7,166.39,487.49,394.62,9.58">The model of Leifman et al.
                        <ref type="bibr" coords="7,469.97,474.94,16.46,9.58" target="#b86">[87]</ref>
                        merged the RGB color planes, dense optical flow map, and saliency map into a seven-layer CNN
                        network.
                    </s>
                    <s coords="7,165.90,500.04,393.38,9.58;7,166.39,512.59,392.88,9.58;7,166.39,525.15,55.04,9.58">Wang et al.
                        <ref type="bibr" coords="7,216.99,500.04,16.59,9.58" target="#b87">[88]</ref>
                        proposed a spatiotemporal residual attentive network (STRA-Net), which learned a stack of local
                        attentions as well as global attention priors to filter out unrelated information.
                    </s>
                    <s coords="7,224.52,525.15,334.76,9.58;7,165.98,537.70,228.34,9.58">The model has advantages in precisely locating dynamic human fixations as well as capturing the temporal attention transitions.</s>
                </p>
                <p>
                    <s coords="7,187.65,550.25,218.25,9.58">LSTMs are also widely used in dynamic models.</s>
                    <s coords="7,409.52,550.25,149.75,9.58;7,166.39,562.81,392.88,9.58;7,166.39,575.36,29.88,9.58">Bazzani et al.
                        <ref type="bibr" coords="7,471.68,550.25,16.74,9.58" target="#b88">[89]</ref>
                        used 3D CNNs to connect with the LSTMs and projected the output of the LSTMs to a Gaussian
                        mixture model.
                    </s>
                    <s coords="7,199.22,575.36,361.71,9.58;7,166.39,587.91,394.63,9.58">The Object-to-Motion (OM)-CNN model proposed by Jiang et al.
                        <ref type="bibr" coords="7,477.40,575.36,16.46,9.58" target="#b89">[90]</ref>
                        analyzed intraframe saliency based on the salient object networks and the motion information
                        networks.
                    </s>
                    <s coords="7,166.39,600.47,393.16,9.58;7,165.98,613.02,393.30,9.58;7,166.39,625.57,70.66,9.58">On this basis, Gorji
                        <ref type="bibr" coords="7,249.19,600.47,16.47,9.58" target="#b90">[91]</ref>
                        proposed a multi-stream convolutional LSTM (ConvLSTM) network with three networks (gaze
                        following, rapid scene change, and attention feedback) based on the static model.
                    </s>
                    <s coords="7,240.09,625.57,319.19,9.58;7,166.39,638.12,159.14,9.58">The ACLNet proposed by Wang et al.
                        <ref type="bibr" coords="7,403.79,625.57,16.47,9.58" target="#b91">[92]</ref>
                        used an enhanced CNN-LSTMs to encode static saliency information.
                    </s>
                    <s coords="7,328.51,638.12,230.76,9.58;7,166.39,650.68,110.39,9.58">However, the ability of the network to capture motion information was limited.</s>
                    <s coords="7,279.88,650.68,279.79,9.58;7,166.39,663.23,280.62,9.58">In this manner, LSTMs can focus on learning temporal saliency representations across consecutive frames and avoid overfitting.</s>
                    <s coords="7,450.10,663.23,109.18,9.58;7,166.39,675.78,392.88,9.58;7,166.39,688.34,290.80,9.58">Hang et al.
                        <ref type="bibr" coords="7,500.30,663.23,16.56,9.58" target="#b92">[93]</ref>
                        designed an attention-aware ConvLSTM to obtain spatial features from static networks and
                        temporal features from dynamic networks, subsequently integrating them.
                    </s>
                    <s coords="7,460.28,688.34,99.00,9.58;7,166.39,700.89,392.88,9.58;7,166.39,713.44,139.65,9.58">The features extracted from consecutive frames were used to predict the salient regions, and a final salient map is generated for each video frame.</s>
                </p>
                <p>
                    <s coords="7,187.65,725.99,371.62,9.58;7,166.39,738.55,256.78,9.58">In the past two years, the dynamic saliency field has gradually developed in the direction of omnidirectional images (ODIs) and 3D ODIs.</s>
                    <s coords="7,426.40,738.55,132.88,9.58;7,166.39,751.10,392.88,9.58;8,166.39,98.05,312.82,9.58">Xu et al.
                        <ref type="bibr" coords="7,465.66,738.55,16.73,9.58" target="#b93">[94]</ref>
                        used adversarial networks to predict the saliency of ODIs by imitating the head trajectory of
                        the object and applied generative adversarial simulation models to train deep models.
                    </s>
                    <s coords="8,482.34,98.05,76.94,9.58;8,166.10,110.60,258.44,9.58">The development process of saliency prediction models is shown in Figure
                        <ref type="figure" coords="8,417.06,110.60,3.74,9.58" target="#fig_2">3</ref>.
                    </s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="5." coords="8,166.39,356.55,171.85,9.34">Visual Saliency Prediction Datasets</head>
                <p>
                    <s coords="8,187.65,372.01,373.27,9.58;8,166.39,384.56,392.88,9.58;8,166.39,397.12,55.43,9.58">Many databases for target detection and image segmentation can be used as experimental data; many of them have been obtained by eye-tracking devices and manual annotations.</s>
                    <s coords="8,225.98,397.12,333.29,9.58;8,166.39,409.67,162.29,9.58">The performance of saliency maps generated by different saliency models needs to be quantitatively evaluated.</s>
                    <s coords="8,331.77,409.67,229.16,9.58;8,166.39,422.22,218.61,9.58">At present, the application of visual saliency prediction is mainly conducted for images and videos.</s>
                    <s coords="8,389.50,422.22,169.77,9.58;8,166.39,434.78,191.55,9.58">The corresponding databases are also divided into two types: static and dynamic.</s>
                    <s coords="9,187.65,404.30,345.18,9.58">In these datasets, SALICON has the largest amount of data for static models.</s>
                    <s coords="9,536.49,404.30,22.78,9.58;9,166.39,416.85,284.63,9.58">Most models could use transfer learning to fine-tune on SALICON.</s>
                    <s coords="9,454.65,416.85,104.63,9.58;9,166.39,429.41,394.53,9.58;9,166.39,441.96,63.48,9.58">Mit300 and cat2000, as databases containing the most model comparisons, are usually used for model performance testing.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="5.1." coords="8,166.39,457.06,78.80,9.50">Static Datasets</head>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="5.2." coords="9,166.39,464.24,92.65,9.50">Dynamic Datasets</head>
                <p>
                    <s coords="9,187.65,479.92,371.62,9.58;9,166.39,492.47,392.88,9.58;9,166.39,505.02,173.56,9.58">The discussion in Section 4.2 has established the particularities of dynamic information and human attention and the limitation of eye movement equipment, which have led to difficulties in observing dynamic data.</s>
                    <s coords="9,343.82,505.02,215.45,9.58;9,166.39,517.58,283.63,9.58">Incidentally, owing to the growth of application requirements, some large datasets have emerged in recent years.</s>
                    <s coords="9,453.12,517.58,106.16,9.58;9,166.39,530.13,177.00,9.58;10,187.65,190.90,371.62,9.58;10,166.39,203.45,281.43,9.58">At present, the dynamic dataset mainly consists of the following: For early dynamic models, the DIEM, Hollywood-2, and UCF-sports datasets were the three most widely used datasets in video saliency research.</s>
                    <s coords="10,451.14,203.45,108.13,9.58;10,166.39,216.01,393.08,9.58;10,166.39,228.56,88.42,9.58">In recent years, with the continuous updating of datasets, there are more models also using the DHF1K database for training and testing.</s>
                    <s coords="10,257.92,228.56,301.36,9.58;10,166.39,241.11,52.03,9.58">The DHF1K database has a huge amount of data and a wide range of application.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="6."
                      coords="10,166.39,263.61,244.37,9.34">Evaluation Measures for Visual Saliency Prediction</head>
                <p>
                    <s coords="10,187.65,279.07,373.27,9.58;10,166.39,291.62,394.53,9.58;10,166.39,304.18,323.97,9.58">The metrics of visual saliency prediction mostly use similarities and differences between estimated predicted values and the Ground Truth (GT) and then outputs an evaluation score to judge the similarity or difference degree between them.</s>
                    <s coords="10,495.76,304.18,63.51,9.58;10,166.39,316.73,392.88,9.58;10,166.39,329.28,372.37,9.58">Given a set of true values used to define the scoring function, the saliency prediction map can be used as the input, and the result of evaluating the accuracy of the prediction is returned.</s>
                    <s coords="10,542.26,329.28,17.01,9.58;10,166.39,341.83,155.88,9.58">The evaluation measures are as follows:</s>
                </p>
                <p>
                    <s coords="10,187.65,354.39,371.82,9.58;10,166.39,366.94,185.37,9.58">AUC variant: The Area Under Curve (AUC) is used as a measurement standard for the two-class pattern recognition problem.</s>
                    <s coords="10,354.84,366.94,204.43,9.58;10,166.39,379.49,394.13,9.58;10,166.39,392.05,327.26,9.58">Different from the AUC in tasks, such as target detection and image segmentation, given the particularity of the saliency prediction task, the following AUC variants are often used in the saliency prediction tasks:</s>
                </p>
                <p>
                    <s coords="10,166.52,409.26,5.19,9.10;10,187.42,409.58,101.32,9.58">• AUC-Judd: Judd et al.</s>
                </p>
                <p>
                    <s coords="10,291.66,409.58,248.24,9.58">[102] proposed a variant of the AUC called AUC-Judd.</s>
                    <s coords="10,544.26,409.58,15.21,9.58;10,187.42,422.13,371.85,9.58;10,187.42,434.69,371.85,9.58;10,187.42,447.24,278.99,9.58">For a given threshold, the true-positive probability is the ratio of the pixels predicted as significant on all true-valued salient points, whereas the false-positive probability is the ratio of pixels predicted as significant on non-salient points.</s>
                    <s coords="10,187.12,472.35,372.16,9.58;10,187.42,484.90,371.85,9.58;10,187.42,497.45,126.25,9.58">This variant uses the uniform random sampling of non-focus points to calculate the false positive rate and defines the saliency mapping value above the threshold of these pixels as false positive.</s>
                    <s coords="10,316.75,497.45,242.52,9.58;10,187.42,510.00,209.66,9.58">The false positive calculation in AUC-Borji is a discrete approximation of the calculation in AUC-Judd.</s>
                    <s coords="10,400.17,510.00,160.35,9.58;10,187.42,522.56,247.15,9.58">Due to the use of random sampling, the same model may be evaluated with different results.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="10,166.52,534.79,5.19,9.10">•</head>
                <p>
                    <s coords="10,187.42,535.11,362.03,9.58">Shuffled AUC: Shuffled AUC (sAUC)
                        <ref type="bibr" coords="10,358.02,535.11,16.72,9.58" target="#b96">[97]</ref>
                        is also a commonly used AUC variant.
                    </s>
                    <s coords="10,552.54,535.11,6.73,9.58;10,187.42,547.66,371.85,9.58;10,187.42,560.22,125.32,9.58">It reduces the sensitivity of the AUC to the center shift by sampling the salient point distribution of other images.</s>
                </p>
                <p>
                    <s coords="10,187.65,577.75,373.28,9.58;10,166.39,590.30,106.43,9.58">AUC-Judd, AUC-Borji, and sAUC, as variants of AUC, are widely used in the evaluation of saliency models.</s>
                    <s coords="10,276.57,590.30,284.45,9.58">Their values are positively correlated with model performance.</s>
                    <s coords="10,166.01,602.86,394.92,9.58;10,166.10,615.41,132.03,9.58">Although AUC is an important evaluation measure, it cannot distinguish the relative importance of different regions.</s>
                    <s coords="10,303.32,615.41,255.96,9.58;10,166.39,627.96,94.80,9.58">Therefore, other distribution-based similarity evaluation measures are needed:</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="10,166.52,645.18,5.19,9.10">•</head>
                <p>
                    <s coords="10,187.42,645.50,372.24,9.58;10,187.13,658.05,48.76,9.58">Normalized Scanpath Saliency (NSS): NSS is a unique evaluation measure of saliency prediction.</s>
                    <s coords="10,240.70,658.05,318.57,9.58;10,187.13,670.60,96.39,9.58">It is used to calculate the average normalized significance value at the point of interest
                        <ref type="bibr" coords="10,259.44,670.60,20.06,9.58">[104]</ref>.
                    </s>
                    <s coords="10,286.60,670.60,147.13,9.58">The calculation formula of NSS is</s>
                </p>
                <formula xml:id="formula_0" coords="10,307.67,687.26,247.73,28.78">NSS = 1 N N ∑ i=1 P(i) × Q(i) (
                    <label>1</label>
                </formula>
                <formula xml:id="formula_1" coords="10,555.40,696.39,3.87,9.58">)</formula>
                <p>
                    <s coords="10,165.98,724.34,393.30,9.58;10,166.39,736.89,392.88,9.58;10,166.10,749.44,25.58,9.58">where P is the average value at the gaze point Q of the human eye, N is the total number of human eye gazes, i represents the i-th pixel, and N is the total number of pixels at the gaze point.</s>
                    <s coords="10,194.80,749.44,364.48,9.58;10,166.39,762.00,56.80,9.58">A positive NSS indicates consistency between mappings, whereas a negative NSS is the opposite.</s>
                    <s coords="10,226.28,762.00,284.04,9.58">The NSS value is negatively correlated with model performance.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="11,166.52,97.73,5.19,9.10;11,187.42,98.05,159.30,9.58">• Linear Correlation Coefficient (CC):</head>
                <p>
                    <s coords="11,346.72,98.05,212.75,9.58;11,187.42,110.60,187.06,9.58">The CC is the statistic used to measure the linear correlation between two random variables.</s>
                    <s coords="11,377.57,110.60,182.95,9.58;11,187.42,123.15,371.85,9.58;11,187.42,135.71,99.34,9.58">For the significance prediction evaluation, the prediction significance map (P) and the true value view (G) can be regarded as the two random variables.</s>
                    <s coords="11,289.85,135.71,142.52,9.58">The calculation formula of CC is</s>
                </p>
                <formula xml:id="formula_2" coords="11,320.17,161.92,235.24,22.37">CC = cov(P, G) σ(P) × σ(G) (
                    <label>2</label>
                </formula>
                <formula xml:id="formula_3" coords="11,555.40,168.67,3.87,9.58">)</formula>
                <p>
                    <s coords="11,187.65,192.58,250.85,9.69">where cov is the covariance, σ is the standard deviation.</s>
                    <s coords="11,441.63,192.69,119.30,9.58;11,166.39,205.24,302.79,9.58">The CC can equally distinguish false positives and false negatives at the value range of (-1,1).</s>
                    <s coords="11,472.30,205.24,86.98,9.58;11,166.39,217.79,209.26,9.58">A value close to the two ends indicates a better model performance.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="11,166.52,235.01,5.19,9.10">•</head>
                <p>
                    <s coords="11,187.42,235.33,371.85,9.58;11,187.42,247.88,371.85,9.58;11,187.42,260.43,373.51,9.58;11,187.42,272.99,372.24,9.58;11,187.42,285.54,59.24,9.58">Earth Movers Distance (EMD): EMD [105] represents the distance between the two 2D maps denoted by G and S, and it calculates the minimum cost of converting the estimated probability distribution of the saliency map S into the probability distribution of the GT map denoted by G. Therefore, a low EMD corresponds to a high-quality saliency map.</s>
                    <s coords="11,249.75,285.54,309.52,9.58;11,187.42,298.09,371.85,9.58;11,187.42,310.65,74.59,9.58">In saliency prediction, EMD represents the minimum cost of converting the probability distribution of the saliency map into human-eye attention maps called the fixation map.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="11,166.52,322.88,5.19,9.10">•</head>
                <p>
                    <s coords="11,187.42,323.20,372.24,9.58;11,187.42,335.75,373.59,9.58">Kullback-Leibler (KL) Divergence: KL divergence is a general information theory measurement corresponding to the difference between two probability distributions.</s>
                    <s coords="11,187.12,348.30,141.71,9.58">The calculation formula of KL is</s>
                </p>
                <formula xml:id="formula_4" coords="11,290.00,361.40,269.28,26.35">KL(P, G) = ∑ i G i log(ε + G ε + P i )
                    <label>(3)</label>
                </formula>
                <p>
                    <s coords="11,187.65,394.34,371.62,9.58;11,166.39,406.89,394.54,9.58;11,166.39,419.34,314.87,9.69">Similar to other distribution-based measures, KL divergence takes the predicted saliency map (P) and the true value view (G) as the input and evaluates the loss of information where P is used to approximate G, ε is the regularization constant.</s>
                    <s coords="11,484.35,419.45,75.24,9.58;11,166.39,432.00,223.68,9.58">Furthermore, KL divergence is an asymmetric dissimilarity measure.</s>
                    <s coords="11,393.20,432.00,166.46,9.58;11,166.39,444.55,130.76,9.58">A low score indicates that the saliency map is close to the true value.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="11,166.52,461.77,5.19,9.10">•</head>
                <p>
                    <s coords="11,187.42,462.09,373.59,9.58">(6) Similarity Metric (SIM): SIM measures the similarity between two distributions.</s>
                    <s coords="11,187.04,474.64,372.24,9.58;11,187.42,487.19,57.37,9.58">After normalizing the input map, SIM is calculated as the sum of the minimum values at each pixel.</s>
                    <s coords="11,247.89,487.19,146.40,9.58">The calculation formula of SIM is</s>
                </p>
                <formula xml:id="formula_5" coords="11,302.83,503.11,256.44,21.22">SIM(P, G) = ∑ i min(P i , G i )
                    <label>(4)</label>
                </formula>
                <p>
                    <s coords="11,187.65,530.92,371.62,9.58;11,166.39,543.48,313.82,9.58">Given the predicted significance map (P) and the true value view (G), a SIM of 1 means that the distribution is the same, whereas a SIM of 0 means no overlap.</s>
                    <s coords="11,483.30,543.48,75.97,9.58;11,166.10,556.03,214.68,9.58">SIM can penalize predictions that fail to consider all true densities.</s>
                </p>
                <p>
                    <s coords="11,187.65,568.58,258.14,9.58">In general, these evaluation measures are complementary.</s>
                    <s coords="11,448.88,568.58,110.39,9.58;11,166.39,581.14,392.88,9.58;11,166.39,593.69,123.98,9.58">A good model should be good under a variety of evaluation measures, because these measures reflect different aspects of the saliency map.</s>
                    <s coords="11,293.47,593.69,265.81,9.58;11,166.39,606.24,97.85,9.58">Usually, a variety of evaluation measures are selected when evaluating the model.</s>
                    <s coords="11,267.33,606.24,277.64,9.58">As a widely used measure of location based, AUC is essential.</s>
                    <s coords="11,548.05,606.24,11.22,9.58;11,166.39,618.79,392.88,9.58;11,166.39,631.35,393.27,9.58;11,166.39,643.90,86.27,9.58">At the same time, a variety of other measures such as CC, SIM and other distribution-based measures should be selected to reflect other salient map factors such as relatively saliency region or similarity.</s>
                </p>
                <p>
                    <s coords="11,187.65,656.45,373.28,9.58;11,166.39,669.01,392.88,9.58;11,166.39,681.56,225.32,9.58">Thus far, we have summarized the abovementioned six common evaluation measures based on whether they are appropriate as probability distribution, similarity, and continuous GT tools for statistics and classification.</s>
                    <s coords="11,394.81,681.56,143.33,9.58">The details are shown in Table
                        <ref type="table" coords="11,530.67,681.56,3.74,9.58" target="#tab_3">1</ref>.
                    </s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="7."
                      coords="12,166.39,249.87,239.40,9.34">Performance of Visual Saliency Prediction Models</head>
                <p>
                    <s coords="12,187.65,265.33,371.62,9.58;12,166.39,277.88,51.99,9.58">The MIT benchmark has the most comprehensive saliency model and evaluation benchmark.</s>
                    <s coords="12,221.47,277.88,337.80,9.58;12,166.39,290.44,344.41,9.58">In this chapter, the static image performance evaluation results of the models in the MIT300 and CAT2000 datasets are selected over the MIT benchmark.</s>
                    <s coords="12,516.25,290.44,43.03,9.58;12,166.10,302.99,324.42,9.58">Then, the performance of the dynamic model is selected over the DHF1K dataset.</s>
                    <s coords="12,495.03,302.99,64.24,9.58;12,166.39,315.54,392.88,9.58;12,166.39,328.09,130.33,9.58">The data have been obtained from the running results of the MIT benchmark, the author's study, and the author's program on GitHub.</s>
                </p>
                <p>
                    <s coords="12,187.65,340.65,371.62,9.58;12,166.12,353.20,42.33,9.58">The MIT benchmark has a total of eight evaluation measures (including three AUC variants).</s>
                    <s coords="12,211.71,353.20,181.96,9.58">A total of 93 static models are evaluated.</s>
                    <s coords="12,396.92,353.20,162.36,9.58;12,166.39,365.75,394.12,9.58;12,166.39,378.31,392.88,9.58;12,166.39,390.86,131.49,9.58">The following 16 models with much better performance are selected for comparison: eDN, Deep Gaze I, Deep Gaze II, DeepFix, SALICON, SalNet, ML-Net, SalGAN, EML-Net, SAM-VGG, SAM-ResNet, AIM, Judd Model, GBVS, ITTI, and SUN.</s>
                    <s coords="12,300.38,390.86,200.92,9.58">In addition, MIT also considers five baselines.</s>
                    <s coords="12,504.42,390.86,54.86,9.58;12,166.39,403.41,332.57,9.58">One of these baselines, namely, the infinite humans, is used as the reference measure.</s>
                    <s coords="12,505.00,403.41,55.93,9.58;12,166.39,415.96,394.13,9.58;12,165.98,428.52,162.24,9.58">The infinitehumans baseline can simulate the gaze point under the observation of infinite people, which is similar to the highest score.</s>
                    <s coords="12,331.30,428.52,187.39,9.58">The obtained results are shown in Table
                        <ref type="table" coords="12,511.09,428.52,3.80,9.58" target="#tab_4">2</ref>.
                    </s>
                    <s coords="12,521.78,428.52,37.50,9.58;12,166.39,441.07,132.64,9.58">The best indicators are marked in bold.</s>
                    <s coords="12,187.65,725.45,371.62,9.58;12,166.39,738.00,150.02,9.58">Thus far, the CAT2000 dataset comprises a total of 31 evaluated models, 10 of which are neural network-based models.</s>
                    <s coords="12,319.50,738.00,184.58,9.58;13,187.65,457.93,210.67,9.58">The obtained results are shown in Table
                        <ref type="table" coords="12,496.60,738.00,3.74,9.58" target="#tab_5">3</ref>. Table
                        <ref type="table" coords="13,213.63,457.93,5.01,9.58" target="#tab_4">2</ref>
                        shows the results of the MIT300 dataset.
                    </s>
                    <s coords="13,401.45,457.93,157.82,9.58;13,166.39,470.48,76.56,9.58">The AUC-Judd index is arranged in descending order.</s>
                    <s coords="13,245.87,470.48,197.40,9.58">The top models are all based on deep learning.</s>
                    <s coords="13,446.17,470.48,114.35,9.58;13,166.39,483.03,245.60,9.58">EML-NET performed best, and it got the highest scores under a variety of measures.</s>
                    <s coords="13,415.07,483.03,145.45,9.58;13,166.39,495.59,309.99,9.58">Based on the AUC-Judd measure, DeepGaze II and EML-NET are in the top two ranks with a score of 0.88.</s>
                    <s coords="13,479.41,495.59,79.86,9.58;13,166.39,508.14,165.72,9.58">DeepGaze II ranks first in AUC-Borji with a score of 0.86.</s>
                    <s coords="13,335.20,508.14,224.08,9.58;13,166.39,520.69,111.02,9.58">Based on the sAUC measure, SALICON performed best with a score of 0.74.</s>
                    <s coords="13,281.94,520.69,277.72,9.58;13,166.39,533.24,32.78,9.58">The rankings produced by different evaluation methods vary greatly.</s>
                    <s coords="13,204.76,533.24,356.26,9.58">DeepGaze II and DeepFix perform well in AUC, but other scores are average.</s>
                    <s coords="13,166.01,545.80,393.27,9.58;13,166.39,558.35,170.51,9.58">Although SAM-ResNet, SAM-VGG, EML-NET and SalGAN did not get the highest score in AUC, these models are outstanding.</s>
                </p>
                <p>
                    <s coords="13,187.65,570.90,209.40,9.58">Table
                        <ref type="table" coords="13,212.69,570.90,4.88,9.58" target="#tab_5">3</ref>
                        shows the results of the CAT2000 dataset.
                    </s>
                    <s coords="13,400.04,570.90,159.24,9.58;13,166.39,583.46,25.48,9.58">AUC-Judd is arranged in descending order.</s>
                    <s coords="13,194.96,583.46,364.32,9.58;13,166.39,596.01,229.45,9.58">Based on the AUC-Judd measure, SAM-ResNet, MSI-Net and SAM-VGG are tied in the top rank with 0.88 (infinite-humans score of 0.90).</s>
                    <s coords="13,398.89,596.01,160.38,9.58;13,166.39,608.56,118.76,9.58">In the classic model, the performance of BMS is the superior one.</s>
                    <s coords="13,288.24,608.56,271.04,9.58;13,166.39,621.12,79.45,9.58">Its AUC-Borji score is the highest, and other scores are almost higher than eDN.</s>
                    <s coords="13,248.94,621.12,310.33,9.58;13,166.10,633.67,166.71,9.58">In general, the models that perform well on the MIT300 dataset also perform well on the CAT2000 dataset.</s>
                </p>
                <p>
                    <s coords="13,187.65,646.22,361.89,9.58;14,187.65,682.61,371.62,9.58;14,166.39,695.16,274.55,9.58">The saliency maps of the model over the CAT2000 database are shown in Figure
                        <ref type="figure" coords="13,542.07,646.22,3.74,9.58" target="#fig_3">4</ref>. AUC-Judd, sAUC,
                        NSS, CC, and SIM are used as the five evaluation measures to judge the performance of the model
                        over the DHF1K dataset.
                    </s>
                    <s coords="14,444.21,695.16,115.27,9.58;14,166.39,707.72,156.61,9.58">The average is taken after calculating the score for each frame.</s>
                    <s coords="14,326.09,707.72,233.19,9.58;14,166.39,720.27,129.45,9.58">The evaluation results are mainly based on the public results of the DHF1K dataset.</s>
                    <s coords="14,298.94,720.27,194.48,9.58;15,187.65,342.73,245.23,9.58">The model performance is shown in Table
                        <ref type="table" coords="14,485.94,720.27,3.74,9.58" target="#tab_6">4</ref>. STRA -Net ranks
                        first in all ratings, followed by ACLNet.
                    </s>
                    <s coords="15,435.89,342.73,124.64,9.58;15,166.39,355.29,179.34,9.58">Among the dynamic models, OM-CNN outperforms the other types.</s>
                    <s coords="15,351.49,355.29,207.78,9.58;15,166.39,367.84,94.89,9.58">Among the static models, the performance of SALICON is superior.</s>
                    <s coords="15,264.37,367.84,295.11,9.58;15,166.39,380.39,221.99,9.58">The results indicate that the performance of the deep model is better than adding time information to the classic model.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="8." coords="15,166.39,402.89,288.64,9.34">Commonalities and Limitations of the Deep Saliency Models</head>
                <p>
                    <s coords="15,187.65,418.35,372.87,9.58;15,166.39,430.90,137.19,9.58">Although the structures of the various deep saliency models differ from one another, they have many commonalities.</s>
                    <s coords="15,306.63,430.90,252.64,9.58;15,166.39,443.46,137.21,9.58">Compared with the classic model, the deep saliency model automatically captures features.</s>
                    <s coords="15,306.64,443.46,253.88,9.58;15,166.39,456.01,372.86,9.58">Although the classic models can manually encode features, deep networks with multi-layer structures can automatically capture more features.</s>
                    <s coords="15,542.35,456.01,16.93,9.58;15,166.39,468.56,392.88,9.58;15,166.39,481.12,392.88,9.58;15,166.39,493.67,112.43,9.58">The CNN-based saliency model is trained in an end-to-end manner, and combined with feature extraction and saliency prediction, it can greatly improve the performance compared with that of the classic model.</s>
                    <s coords="15,283.48,493.67,275.79,9.58;15,166.39,506.22,382.57,9.58">The success of these saliency prediction models indicates the importance of automatically capturing features based on the deep learning framework.</s>
                </p>
                <p>
                    <s coords="15,187.65,518.77,371.82,9.58;15,166.39,531.33,58.04,9.58">Aiming at improving model performance, saliency models often perform similar optimization.</s>
                    <s coords="15,227.52,531.33,331.75,9.58;15,166.39,543.88,392.88,9.58;15,166.39,556.43,89.95,9.58">First, in view of reducing the loss of image features in a series of convolution and pooling layers, some models use the multi-scale network or skip layers to preserve the loss information.</s>
                    <s coords="15,259.46,556.43,299.82,9.58;15,166.39,568.99,394.12,9.58;15,166.39,581.54,288.78,9.58">Second, using transfer-learning methods or adding some pre-trained classification networks or LSTMs to the model can play a role in adding prior knowledge, and this scheme has a significant impact on the model results.</s>
                    <s coords="15,461.95,581.54,97.32,9.58;15,166.39,594.09,392.89,9.58;15,166.39,606.64,233.50,9.58">Finally, as evaluation measures have a great influence on model performance, some models often select multiple evaluation measures to train the model (i.e., ML-Net).</s>
                    <s coords="15,402.97,606.64,157.96,9.58;15,166.39,619.20,240.83,9.58">Dynamic models also include multistream, multi-modal, and 3D CNNs and other forms.</s>
                    <s coords="15,411.62,619.20,147.94,9.58;15,166.39,631.75,393.08,9.58;15,166.39,644.30,193.67,9.58">However, the overall framework type is less than the static models in terms of multi-tasking, action recognition, and other frameworks and thus need to be developed.</s>
                </p>
                <p>
                    <s coords="15,187.65,656.86,371.63,9.58;15,166.39,669.41,137.00,9.58">Although the deep saliency model can sufficiently capture features, a wide gap exists between the result and the GT.</s>
                    <s coords="15,305.90,669.41,253.38,9.58;15,166.39,681.96,344.27,9.58">The problem can be resolved by studying how to imitate human analysis scenes and understand the mechanism of the human gaze.</s>
                    <s coords="15,516.55,681.96,42.72,9.58;15,166.39,694.52,394.62,9.58">Aimed at achieving these aspects on the model, a higher level of visual understanding is required.</s>
                    <s coords="15,166.39,707.07,392.89,9.58;15,166.39,719.62,391.72,9.58">In particular, besides using the conventional optimization model and finding a better loss function, saliency prediction can be explored and improved on the basis of the following:</s>
                </p>
                <p>
                    <s coords="15,165.90,736.35,7.47,9.58">1.</s>
                </p>
                <p>
                    <s coords="15,189.46,736.35,334.69,9.58">New Datasets: Datasets are extremely important to model performance
                        <ref type="bibr" coords="15,500.24,736.35,19.92,9.58">[120]</ref>.
                    </s>
                    <s coords="15,527.25,736.35,32.33,9.58;15,189.46,748.90,369.82,9.58;15,189.46,761.45,120.17,9.58">The GT and measurement prediction errors obtained from the data have a significant impact on the model performance.</s>
                    <s coords="15,312.73,761.45,246.55,9.58;15,189.46,774.01,255.12,9.58">In earlier years, the collection of saliency datasets relied on eye tracking data, and the datasets had fewer images.</s>
                    <s coords="15,448.18,774.01,111.10,9.58;16,189.46,98.05,369.82,9.58;16,189.46,110.60,231.38,9.58">Although the emergence of SALICON improved the result, the gap remains to be an order of magnitude with respect to datasets in related fields (e.g., ImageNet).</s>
                    <s coords="16,424.03,110.60,135.63,9.58;16,189.46,123.15,97.62,9.58">The JFT-300M dataset recently collected by Sun et al.</s>
                </p>
                <p>
                    <s coords="16,289.69,123.15,269.58,9.58;16,189.46,135.71,234.13,9.58">[121] contains 300 million images, and it performs the target recognition model that is trained on this dataset well.</s>
                    <s coords="16,426.67,135.71,132.61,9.58;16,189.46,148.26,369.82,9.58;16,189.46,160.81,128.39,9.58">The difference in performance between the use of eye tracking data and similar SALICON data collected with mouse clicks is clearly controversial.</s>
                    <s coords="16,166.39,173.37,7.47,9.58">2.</s>
                </p>
                <p>
                    <s coords="16,189.46,173.37,369.82,9.58;16,189.46,185.92,371.06,9.58;16,189.46,198.47,184.82,9.58">Multi-modal approaches: With the development of saliency prediction in the dynamic field, an increasing number of features in different modes, such as vision, hearing, and subtitles, can be used to train models.</s>
                    <s coords="16,377.36,198.47,181.92,9.58;16,189.16,211.02,264.57,9.58">This multi-modal feature input mode has proven to be an effective way to improve model performance.</s>
                    <s coords="16,189.46,336.55,369.82,9.58;16,189.46,349.11,288.11,9.58">Understand high-level semantics: The deep saliency models are good at extracting common features, such as humans and textures, among others.</s>
                    <s coords="16,482.96,349.11,77.97,9.58;16,189.46,361.66,219.85,9.58">The saliency predictor can also be used to handle these features.</s>
                    <s coords="16,414.86,361.66,145.66,9.58;16,189.46,374.21,369.82,9.58;16,189.46,386.77,38.32,9.58">However, as shown in Figure
                        <ref type="figure" coords="16,552.89,361.66,3.81,9.58" target="#fig_4">5</ref>, the most
                        interesting or significant parts of an image are not necessarily all of these features.
                    </s>
                    <s coords="16,232.34,386.77,327.32,9.58;16,189.46,399.32,34.13,9.58">Human visual models often entail a reasoning process based on sensory stimuli.</s>
                    <s coords="16,226.68,399.32,332.60,9.58;16,189.46,411.87,369.82,9.58;16,189.46,424.42,123.50,9.58">To establish the reason behind the relative importance of image regions on the saliency model, researchers can use higher-level features, such as emotions, gaze direction, and body posture.</s>
                    <s coords="16,316.08,424.42,243.59,9.58;16,189.16,436.98,371.77,9.58;16,189.46,449.53,167.66,9.58">Moreover, aiming to approach the human-level saliency prediction, researchers need to carry out cognitive attention research to help overcome the aforementioned limitations.</s>
                    <s coords="16,360.30,449.53,200.72,9.58">A few useful explorations have been offered.</s>
                    <s coords="16,189.46,462.08,369.82,9.58;16,189.46,474.64,71.69,9.58">For example, Zhao
                        <ref type="bibr" coords="16,276.91,462.08,16.73,9.58" target="#b97">[98]</ref>
                        showed through his experimental results that emotion has a priority effect.
                    </s>
                    <s coords="16,264.82,474.64,294.46,9.58;16,189.46,487.19,161.93,9.58">Nonetheless, the existing saliency model still cannot fully explain the high-level semantics in the scene.</s>
                    <s coords="16,354.48,487.19,204.79,9.58;16,189.46,499.74,371.06,9.58;16,189.04,512.30,370.23,9.58;16,189.46,524.85,98.83,9.58">The concept of "semantic gap" and the process of determining the relative importance of objects still cannot be resolved; moreover, whether the saliency in natural scenes is guided by objects or low-level features is a matter of debate
                        <ref type="bibr" coords="16,264.20,524.85,20.07,9.58">[124]</ref>.
                    </s>
                    <s coords="16,291.40,524.85,267.87,9.58;16,189.46,537.40,369.82,9.58;16,189.18,549.95,68.10,9.58">The research on the saliency prediction task is closely related to cognitive disciplines, and its findings can help to improve the subsequent various visual research.</s>
                    <s coords="16,187.65,736.35,371.62,9.58;16,166.39,748.90,392.88,9.58;16,166.39,761.45,74.02,9.58">With the great success of the deep model in saliency prediction, new developments in deep learning have also provided the possibility for new applications and tasks of saliency models.</s>
                    <s coords="16,243.52,761.45,315.75,9.58;16,166.39,774.01,394.62,9.58">For example, Aksoy et al.
                        <ref type="bibr" coords="16,359.51,761.45,16.71,9.58" target="#b15">[16]</ref>
                        proposed a novel attention-based model for making braking decisions and other driving decisions
                        like steering and acceleration.
                    </s>
                </p>
                <p>
                    <s coords="17,166.22,98.05,393.05,9.58;17,166.39,110.60,393.27,9.58;17,166.39,123.15,281.86,9.58">Jia et al.
                        <ref type="bibr" coords="17,201.96,98.05,16.47,9.58" target="#b18">[19]</ref>
                        proposed a multimodal salient wave detection network for sleep staging called SalientSleepNet,
                        which translated the time series classification problem into a saliency detection problem and
                        applies it to sleep stage classification.
                    </s>
                    <s coords="17,455.60,123.15,103.67,9.58;17,166.39,135.71,338.47,9.58">
                        <ref type="bibr" coords="17,455.60,123.15,69.26,9.58">Wei et al. [125]</ref>
                        used a saliency model to pursue their research on autism spectrum disorder (ASD).
                    </s>
                    <s coords="17,507.37,135.71,51.91,9.58;17,166.39,148.26,392.88,9.58;17,166.39,160.81,392.88,9.58;17,166.39,173.37,252.34,9.58">They found that children with ASD, particularly autism, were informed by special objects and less on social objects (e.g., face), and the application of the verification model of obviousness is helpful in monitoring and evaluating their condition.</s>
                    <s coords="17,423.66,173.37,135.61,9.58;17,166.39,185.92,392.88,9.58;17,166.39,198.47,251.42,9.58">O'Shea et al.
                        <ref type="bibr" coords="17,483.74,173.37,21.72,9.58">[126]</ref>
                        proposed a model for detecting seizure events from raw electroencephalogram (EEG) signals with
                        less dependency on the availability of precise clinical labels.
                    </s>
                    <s coords="17,421.93,198.47,137.34,9.58;17,166.39,211.02,245.88,9.58">This work opens new avenues for the application of deep learning to neonatal EEG.</s>
                    <s coords="17,415.99,211.02,59.44,9.58">Theism et al.</s>
                </p>
                <p>
                    <s coords="17,479.14,211.02,80.53,9.58;17,166.39,223.58,392.88,9.58;17,166.39,236.13,394.63,9.58">[127] used a fully connected network and Fisher pruning to increase the saliency calculation speed by 10 times as a means of providing ideas for applications with high real-time requirements.</s>
                    <s coords="17,166.39,248.68,392.88,9.58;17,166.39,261.24,392.88,9.58;17,166.39,273.79,83.43,9.58">
                        <ref type="bibr" coords="17,166.39,248.68,68.02,9.58">Fan et al. [128]</ref>
                        proposed a model to detect shared attention in videos to infer shared attention in third-person
                        social scene videos, which were significant for studying human social interactions.
                    </s>
                    <s coords="17,253.90,273.79,307.03,9.58;17,166.39,286.34,392.88,9.58;17,166.39,298.90,352.07,9.58">They proposed a new video dataset VACATION [129] and a spatialtemporal graph reasoning model to explicitly represent the diverse gaze interactions in the social scenes and to infer atomic-level gaze communication by message passing.</s>
                </p>
            </div>
            <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                <head n="9." coords="17,166.39,321.39,67.00,9.34">Conclusions</head>
                <p>
                    <s coords="17,187.65,336.85,372.87,9.58;17,166.39,349.41,324.73,9.58">The development of visual saliency prediction tasks has produced numerous methods, and all of them have played an important role in various research directions.</s>
                    <s coords="17,494.05,349.41,65.22,9.58;17,166.39,361.96,393.27,9.58;17,166.10,374.51,48.76,9.58">Deep networks can automatically capture features and effectively combine feature extraction and saliency prediction.</s>
                    <s coords="17,218.17,374.51,341.10,9.58;17,166.39,387.06,206.19,9.58">Furthermore, performance can be significantly improved with respect to the classic model that uses handcrafted features.</s>
                    <s coords="17,379.57,387.06,179.70,9.58;17,166.39,399.62,394.13,9.58;17,166.39,412.17,394.13,9.58;17,166.39,424.72,112.97,9.58">However, the features extracted by the deep saliency model may not fully represent the salient objects and regions in an image, especially in complex scenes that contain advanced information, such as emotion, text, or symbolic information.</s>
                    <s coords="17,285.08,424.72,275.44,9.58;17,166.39,437.28,393.27,9.58;17,166.39,449.83,126.01,9.58">In view of further improving the performance of the model, the reasoning process of HVS must be imitated to realize the discrimination of relatively important areas in the scene.</s>
                </p>
                <p>
                    <s coords="17,187.65,462.38,371.62,9.58;17,166.39,474.94,393.27,9.58;17,166.39,487.49,392.88,9.58;17,166.39,500.04,301.11,9.58">In this review, we have summarized the literature about saliency prediction, including the early psychological and physiological mechanisms, the classic models affected by this task, the introduction of visual saliency models based on deep learning, and the data comparisons and summaries in the static and dynamic fields.</s>
                    <s coords="17,472.25,500.04,87.03,9.58;17,166.39,512.59,392.88,9.58;17,166.39,525.15,297.15,9.58">The reasons for the superiority and the limitations of the saliency model are also analyzed, and the ways of improvement and possible development directions are identified.</s>
                    <s coords="17,468.48,525.15,90.80,9.58;17,166.39,537.70,393.08,9.58;17,166.39,550.25,392.88,9.58;17,166.39,562.81,392.88,9.58;17,166.39,575.36,121.13,9.58">Although the visual saliency model based on deep learning has made great progress, there is still room for exploration in the aspects of visualization and multi-modality and the understanding of high-level semantics, especially the research on attention mechanisms and the application related to cognitive science.</s>
                </p>
            </div>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"
                    coords="2,166.39,526.36,392.88,8.63;2,166.39,539.18,82.69,8.63;2,44.46,371.37,506.06,144.14">
                <head>Figure 1 .</head>
                <label>1</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="2,166.39,526.43,35.53,8.41">Figure 1.</s>
                            <s coords="2,205.20,526.36,354.08,8.63;2,166.39,539.18,82.69,8.63">Two saliency detection tasks: (a) Original image, (b) Saliency prediction task, (c) Salient object detection task.</s>
                        </p>
                    </div>
                </figDesc>
                <graphic coords="2,44.46,371.37,506.06,144.14" type="bitmap"/>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"
                    coords="5,166.39,675.73,307.63,8.63;5,52.50,472.07,490.00,192.82">
                <head>Figure 2 .</head>
                <label>2</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="5,166.39,675.73,307.63,8.63">Figure 2. Development process of neurobiological models and classic models.</s>
                        </p>
                    </div>
                </figDesc>
                <graphic coords="5,52.50,472.07,490.00,192.82" type="bitmap"/>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"
                    coords="8,166.39,338.47,266.33,8.63;8,63.03,131.53,468.94,196.09">
                <head>Figure 3 .</head>
                <label>3</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="8,166.39,338.47,266.33,8.63">Figure 3. Development process of deep saliency prediction models.</s>
                        </p>
                    </div>
                </figDesc>
                <graphic coords="8,63.03,131.53,468.94,196.09" type="bitmap"/>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"
                    coords="14,166.39,660.91,200.80,8.63;14,76.52,94.86,441.95,555.20">
                <head>Figure 4 .</head>
                <label>4</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="14,166.39,660.91,200.80,8.63">Figure 4. Saliency maps over the CAT2000 dataset.</s>
                        </p>
                    </div>
                </figDesc>
                <graphic coords="14,76.52,94.86,441.95,555.20" type="bitmap"/>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"
                    coords="16,166.39,715.84,292.27,8.63;16,166.39,568.70,374.67,138.28">
                <head>Figure 5 .</head>
                <label>5</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="16,166.39,715.91,35.37,8.41">Figure 5.</s>
                            <s coords="16,204.55,715.84,254.12,8.63">An animal in the picture attracting more attention than humans.</s>
                        </p>
                    </div>
                </figDesc>
                <graphic coords="16,166.39,568.70,374.67,138.28" type="bitmap"/>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"
                    coords="8,166.52,472.42,394.50,311.17">
                <head>•</head>
                <label/>
                <figDesc>
                    <div>
                        <p>
                            <s coords="8,187.42,472.73,82.79,9.58">[41]NTO dataset: In 2006, Bruce et al.[41]established the TORONTO dataset.</s>
                            <s coords="8,270.21,472.73,4.27,9.58">It is one of the earliest and most widely used datasets of computer vision.</s>
                            <s coords="8,277.57,472.73,9.18,9.58">It includes 120 color images with a resolution of 511 × 681.</s>
                            <s coords="8,289.25,472.73,17.99,9.58">The images contain indoor and outdoor scenes and a total of 20 recorded observers' eye movement data.</s>
                            <s coords="8,307.23,472.73,4.50,9.58">• MIT300 dataset: In 2012, Judd et al. [70] of MIT established the MIT300 dataset.</s>
                            <s coords="8,314.22,472.73,25.22,9.58">It contains 300 natural images from Flickr's creation and sharing and the eye movement data of 39 observers.</s>
                            <s coords="8,341.93,472.73,20.93,9.58">At that time, the MIT300 dataset was the most influential and most widely used in the dataset saliency field.</s>
                            <s coords="8,365.36,472.73,4.15,9.58">The dataset is generally not used as a training set.</s>
                            <s coords="8,369.51,472.73,8.31,9.58">However, the model comprising the MIT300 dataset can be evaluated.</s>
                            <s coords="8,377.82,472.73,56.20,9.58">• MIT1003 dataset: The MIT1003 dataset was also established by Judd et al. [61].</s>
                            <s coords="8,436.51,472.73,13.86,9.58">It contains a total of 1003 images from Flickr's collection of images and the LabelMe website and the eye movement data of 15 observers.</s>
                            <s coords="8,452.87,472.73,87.05,9.58">The MIT1003 dataset can be regarded as a supplement to the MIT300 dataset.</s>
                            <s coords="8,543.01,472.73,6.63,9.58">The MIT1003 and MIT300 datasets can be used as a training set and a test set for performance evaluation, respectively.</s>
                            <s coords="8,552.13,472.73,7.14,9.58">• DUT-OMRON dataset: In 2013, Yang et al. [95] established the DUT-OMRON dataset.</s>
                            <s coords="8,187.42,485.29,27.31,9.58">It contains 5168 images, and each image provides eye movement data of 5 observers.</s>
                            <s coords="8,217.23,485.29,13.85,9.58">This dataset is annotated with eye movement data, but it mainly focuses on salient object detection, with one or more salient objects and a relatively complex background.</s>
                            <s coords="8,233.56,485.29,31.79,9.58">images in MS-COCO.</s>
                            <s coords="8,267.85,485.29,16.90,9.58">It is currently the largest attention dataset in terms of scale and context variability.</s>
                            <s coords="8,287.25,485.29,21.75,9.58">The difference from the abovementioned databases is that the SALICON dataset does not use an eye tracker to record eye movement data but rather uses the Amazon Mechanical Turk platform; however, the eye movement data recorded by the mouse was used to evaluate the performance of the model.</s>
                            <s coords="8,311.50,485.29,30.57,9.58">Tavakoli et al. [97] emphasized that problems may arise in evaluating model performance when eye movement data are recorded by the mouse.</s>
                            <s coords="8,344.56,485.29,21.13,9.58">Nonetheless, the SALICON dataset is the largest dataset in the current field, and it continues to be widely used by current mainstream saliency prediction models based on deep learning technology.</s>
                            <s coords="8,368.19,485.29,47.10,9.58">The SALICON dataset offers eye movement data for the training set (10,000 pictures) and validation set (5000 pictures), and it can retain the eye movement data of the test set (5000 pictures).</s>
                            <s coords="8,417.78,485.29,42.70,9.58">• EMOd dataset: The EMOd dataset is a new dataset proposed by Fan et al. [98].</s>
                            <s coords="8,462.97,485.29,29.44,9.58">It contains 1019 emotional images with target-level and image-level annotations.</s>
                            <s coords="8,495.51,485.29,6.62,9.58">It was designed for studying visual saliency and image emotion.</s>
                            <s coords="8,504.62,485.29,37.18,9.58">In the image labeling process of the EMOd dataset, the main target objects in each image are labeled with attributes, such as target contour, target name, emotional category (negative, neutral, or positive), and semantic category.</s>
                            <s coords="8,544.30,485.29,14.97,9.58">The four semantic categories are as follows: the target directly related to humans, the target related to human non-visual perception, the target designed to attract attention or interact with humans, and the target with implicit signs.</s>
                            <s coords="8,187.42,497.84,22.25,9.58">Each target is coded to have one or more categories.</s>
                            <s coords="8,212.15,497.84,61.70,9.58">Furthermore, the EMOd dataset has a total of 4302 targets with fine contours, emotional labels, and semantic labels.</s>
                            <s coords="8,276.33,497.84,55.99,9.58">The number of positive, neutral, and negative targets are 839, 2429, and 1034, respectively.</s>
                        </p>
                    </div>
                </figDesc>
                <table/>
                <note coords="8,166.52,698.37,5.19,9.10;8,187.42,698.69,371.85,9.58;8,187.42,711.24,371.85,9.58;8,187.42,723.79,371.85,9.58;8,187.42,736.35,371.85,9.58;8,187.42,748.90,117.33,9.58;8,166.52,761.14,5.19,9.10;8,187.42,761.45,371.85,9.58;8,187.42,774.01,371.85,9.58;9,187.42,98.05,27.95,9.58">
                    <p>
                        <s coords="8,166.52,698.37,5.19,9.10;8,187.42,698.69,323.34,9.58">• CAT2000 dataset: The CAT2000 dataset was established by Borji et al. [96].</s>
                        <s coords="8,513.85,698.69,45.42,9.58;8,187.42,711.24,226.14,9.58">It contains 2000 images under free observation by 24 observers.</s>
                        <s coords="8,416.64,711.24,142.64,9.58;8,187.42,723.79,173.87,9.58">Twenty scenes are categorized as cartoon, art, indoor, and outdoor scenes.</s>
                        <s coords="8,364.36,723.79,194.92,9.58;8,187.42,736.35,119.68,9.58">These categories contain bottom-up attention cues and top-down factors.</s>
                        <s coords="8,310.20,736.35,249.07,9.58;8,187.42,748.90,117.33,9.58">The different types of images are suitable for a variety of attention behavior studies.</s>
                        <s coords="8,166.52,761.14,5.19,9.10;8,187.42,761.45,348.84,9.58">• SALICON dataset: In 2015, Ming et al. [69] established the SALICON dataset.</s>
                        <s coords="8,539.86,761.45,19.41,9.58;8,187.42,774.01,371.85,9.58;9,187.42,98.05,27.95,9.58">This large mouse tracking dataset for contextual saliency was established by selecting 20,000</s>
                    </p>
                </note>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"
                    coords="9,166.52,547.35,394.00,235.85">
                <head/>
                <label/>
                <figDesc>
                    <div>
                        <p>
                            <s coords="10,242.16,98.05,92.10,9.58">duration of 19,420 s.</s>
                            <s coords="10,339.09,98.05,220.39,9.58;10,187.42,110.60,371.85,9.58;10,187.42,123.15,322.35,9.58">The DHF1K dataset also provides calibration for movement mode and number of objects, among others, thus providing convenience for studying high-level information of the dynamic attention mechanism.</s>
                            <s coords="10,166.52,135.39,5.19,9.10;10,187.42,135.71,362.12,9.58">• LEDOV dataset: The LEDOV dataset [101] was established by Wang et al. in 2018.</s>
                            <s coords="10,552.62,135.71,6.65,9.58;10,187.42,148.26,373.60,9.58">It includes daily activities, sports, social activities, art performances, and other content.</s>
                            <s coords="10,187.04,160.81,372.24,9.58;10,187.15,173.37,153.62,9.58">A total of 538 videos, with a resolution of 720px, contain a total of 179,336 frames of video and 5,058,178 gaze locations.</s>
                        </p>
                    </div>
                </figDesc>
                <table coords="9,166.52,547.35,394.00,160.54">
                    <row>
                        <cell>•</cell>
                        <cell>DIEM dataset: The DIEM dataset was established in 2011 by Mital et al. [99]. It contains</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>a total of 84 videos, including advertisements, movie trailers, and documentaries,</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>among others. A total of 50 observers have provided eye movement data through free</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>viewing. The scene content and data scale are both limited.</cell>
                    </row>
                    <row>
                        <cell>•</cell>
                        <cell>UCF-sports dataset: The UCF-sports dataset was established by Mathe et al. [100]. The</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>dataset contains 150 videos, including 9 common sports categories. Different from</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>the DIEM dataset, the observation object in the UCF-sports dataset is prompted by</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>time-based actions in the video during the viewing process. The result is found to be</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>purposeful.</cell>
                    </row>
                    <row>
                        <cell>•</cell>
                        <cell>Hollywood-2 dataset: The Hollywood-2 dataset was also established in 2012 by</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>Mathe et al. [100]. The dataset contains 1770 videos that are labeled according to</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>12 action categories, such as eating and running, among others. Unlike the UCF-sports</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>dataset, the observation objects of the Hollywood-2 dataset are divided into three</cell>
                    </row>
                </table>
                <note coords="9,187.42,710.85,371.85,9.58;9,187.42,723.40,371.85,9.58;9,187.13,735.96,124.28,9.58;9,166.52,748.19,5.19,9.10;9,187.42,748.51,371.85,9.58;9,187.42,761.06,371.85,9.58;9,187.42,773.62,371.85,9.58;10,187.01,98.05,52.09,9.58">
                    <p>
                        <s coords="9,187.42,710.85,351.51,9.58">groups: free viewing, human action annotation, and video content annotation.</s>
                        <s coords="9,542.27,710.85,17.01,9.58;9,187.42,723.40,371.85,9.58;9,187.13,735.96,124.28,9.58">The human-eye focus data are in the free viewing mode only and accounts for a small proportion of all of the data.</s>
                        <s coords="9,166.52,748.19,5.19,9.10;9,187.42,748.51,352.09,9.58">• DHF1K dataset: The DHF1K dataset was established by Wang et al. [92] in 2018.</s>
                        <s coords="9,542.60,748.51,16.68,9.58;9,187.42,761.06,371.85,9.58;9,187.42,773.62,222.26,9.58">The dataset consists of a total of 1000 video sequences watched by 17 observers and covers seven main categories and 150 scene sub-categories.</s>
                        <s coords="9,412.67,773.62,146.60,9.58;10,187.01,98.05,52.09,9.58">The video contains 582,605 frames with a total</s>
                    </p>
                </note>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"
                    coords="12,166.10,97.74,357.78,133.14">
                <head>Table 1 .</head>
                <label>1</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="12,200.26,97.74,252.30,8.63">Summary of evaluation measures for visual saliency prediction.</s>
                        </p>
                    </div>
                </figDesc>
                <table coords="12,183.52,118.89,340.37,111.98">
                    <row>
                        <cell>Evaluation Measures</cell>
                        <cell>Location Based</cell>
                        <cell cols="2">Distribution √</cell>
                    </row>
                    <row>
                        <cell>KL</cell>
                        <cell/>
                        <cell>√</cell>
                        <cell>√</cell>
                    </row>
                </table>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"
                    coords="12,58.32,463.89,486.65,238.56">
                <head>Table 2 .</head>
                <label>2</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="12,200.26,463.89,229.85,8.63">Performance of the static models over the MIT300 dataset.</s>
                        </p>
                    </div>
                </figDesc>
                <table coords="12,58.32,487.03,486.65,215.42">
                    <row>
                        <cell>Model Name</cell>
                        <cell>AUC-Judd</cell>
                        <cell>AUC-Borji</cell>
                        <cell>sAUC</cell>
                        <cell>SIM</cell>
                        <cell>EMD</cell>
                        <cell>CC</cell>
                        <cell>NSS</cell>
                        <cell>KL</cell>
                    </row>
                    <row>
                        <cell>infinite humans</cell>
                        <cell>0.92</cell>
                        <cell>0.88</cell>
                        <cell>0.81</cell>
                        <cell>1</cell>
                        <cell>0</cell>
                        <cell>1</cell>
                        <cell>3.29</cell>
                        <cell>0</cell>
                    </row>
                    <row>
                        <cell>Deep Gaze II [68]</cell>
                        <cell>0.88</cell>
                        <cell>0.86</cell>
                        <cell>0.72</cell>
                        <cell>0.46</cell>
                        <cell>3.98</cell>
                        <cell>0.52</cell>
                        <cell>1.29</cell>
                        <cell>0.96</cell>
                    </row>
                    <row>
                        <cell>EML-NET [77]</cell>
                        <cell>0.88</cell>
                        <cell>0.77</cell>
                        <cell>0.7</cell>
                        <cell>0.68</cell>
                        <cell>1.84</cell>
                        <cell>0.79</cell>
                        <cell>2.47</cell>
                        <cell>0.84</cell>
                    </row>
                    <row>
                        <cell>DeepFix [66]</cell>
                        <cell>0.87</cell>
                        <cell>0.8</cell>
                        <cell>0.71</cell>
                        <cell>0.67</cell>
                        <cell>2.04</cell>
                        <cell>0.78</cell>
                        <cell>2.26</cell>
                        <cell>0.63</cell>
                    </row>
                    <row>
                        <cell>SALICON [69]</cell>
                        <cell>0.87</cell>
                        <cell>0.85</cell>
                        <cell>0.74</cell>
                        <cell>0.6</cell>
                        <cell>2.62</cell>
                        <cell>0.74</cell>
                        <cell>2.12</cell>
                        <cell>0.54</cell>
                    </row>
                    <row>
                        <cell>SAM-ResNet [75]</cell>
                        <cell>0.87</cell>
                        <cell>0.78</cell>
                        <cell>0.7</cell>
                        <cell>0.68</cell>
                        <cell>2.15</cell>
                        <cell>0.78</cell>
                        <cell>2.34</cell>
                        <cell>1.27</cell>
                    </row>
                    <row>
                        <cell>SAM-VGG [75]</cell>
                        <cell>0.87</cell>
                        <cell>0.78</cell>
                        <cell>0.71</cell>
                        <cell>0.67</cell>
                        <cell>2.14</cell>
                        <cell>0.77</cell>
                        <cell>2.3</cell>
                        <cell>1.13</cell>
                    </row>
                    <row>
                        <cell>SalGAN [76]</cell>
                        <cell>0.86</cell>
                        <cell>0.81</cell>
                        <cell>0.72</cell>
                        <cell>0.63</cell>
                        <cell>2.29</cell>
                        <cell>0.73</cell>
                        <cell>2.04</cell>
                        <cell>1.07</cell>
                    </row>
                    <row>
                        <cell>ML-Net [74]</cell>
                        <cell>0.85</cell>
                        <cell>0.75</cell>
                        <cell>0.7</cell>
                        <cell>0.59</cell>
                        <cell>2.63</cell>
                        <cell>0.67</cell>
                        <cell>2.05</cell>
                        <cell>1.1</cell>
                    </row>
                    <row>
                        <cell>Deep Gaze I [63]</cell>
                        <cell>0.84</cell>
                        <cell>0.83</cell>
                        <cell>0.66</cell>
                        <cell>0.39</cell>
                        <cell>4.97</cell>
                        <cell>0.48</cell>
                        <cell>1.22</cell>
                        <cell>1.23</cell>
                    </row>
                    <row>
                        <cell>SalNet [71]</cell>
                        <cell>0.83</cell>
                        <cell>0.82</cell>
                        <cell>0.69</cell>
                        <cell>0.52</cell>
                        <cell>3.31</cell>
                        <cell>0.58</cell>
                        <cell>1.51</cell>
                        <cell>0.81</cell>
                    </row>
                    <row>
                        <cell>eDN [62]</cell>
                        <cell>0.82</cell>
                        <cell>0.81</cell>
                        <cell>0.62</cell>
                        <cell>0.41</cell>
                        <cell>4.56</cell>
                        <cell>0.45</cell>
                        <cell>1.14</cell>
                        <cell>1.1</cell>
                    </row>
                    <row>
                        <cell>Judd Model [61]</cell>
                        <cell>0.81</cell>
                        <cell>0.8</cell>
                        <cell>0.6</cell>
                        <cell>0.42</cell>
                        <cell>4.45</cell>
                        <cell>0.47</cell>
                        <cell>1.18</cell>
                        <cell>1.12</cell>
                    </row>
                    <row>
                        <cell>GBVS [28]</cell>
                        <cell>0.81</cell>
                        <cell>0.8</cell>
                        <cell>0.63</cell>
                        <cell>0.48</cell>
                        <cell>3.51</cell>
                        <cell>0.48</cell>
                        <cell>1.24</cell>
                        <cell>0.87</cell>
                    </row>
                    <row>
                        <cell>AIM [41]</cell>
                        <cell>0.77</cell>
                        <cell>0.75</cell>
                        <cell>0.66</cell>
                        <cell>0.4</cell>
                        <cell>4.73</cell>
                        <cell>0.31</cell>
                        <cell>0.79</cell>
                        <cell>1.18</cell>
                    </row>
                    <row>
                        <cell>IttiKoch2 [4]</cell>
                        <cell>0.75</cell>
                        <cell>0.74</cell>
                        <cell>0.63</cell>
                        <cell>0.44</cell>
                        <cell>4.26</cell>
                        <cell>0.37</cell>
                        <cell>0.97</cell>
                        <cell>1.03</cell>
                    </row>
                    <row>
                        <cell>SUN saliency [54]</cell>
                        <cell>0.67</cell>
                        <cell>0.66</cell>
                        <cell>0.61</cell>
                        <cell>0.38</cell>
                        <cell>5.1</cell>
                        <cell>0.25</cell>
                        <cell>0.68</cell>
                        <cell>1.27</cell>
                    </row>
                </table>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"
                    coords="13,46.56,97.74,498.41,337.19">
                <head>Table 3 .</head>
                <label>3</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="13,200.26,97.74,235.50,8.63">Performance of the static models over the CAT2000 dataset.</s>
                        </p>
                    </div>
                </figDesc>
                <table coords="13,46.56,120.88,498.41,314.05">
                    <row>
                        <cell>Model Name</cell>
                        <cell>AUC-Judd</cell>
                        <cell>AUC-Borji</cell>
                        <cell>sAUC</cell>
                        <cell>SIM</cell>
                        <cell>EMD</cell>
                        <cell>CC</cell>
                        <cell>NSS</cell>
                        <cell>KL</cell>
                    </row>
                    <row>
                        <cell>infinite humans</cell>
                        <cell>0.9</cell>
                        <cell>0.84</cell>
                        <cell>0.62</cell>
                        <cell>1</cell>
                        <cell>0</cell>
                        <cell>1</cell>
                        <cell>2.85</cell>
                        <cell>0</cell>
                    </row>
                    <row>
                        <cell>SAM-ResNet [75]</cell>
                        <cell>0.88</cell>
                        <cell>0.8</cell>
                        <cell>0.58</cell>
                        <cell>0.77</cell>
                        <cell>1.04</cell>
                        <cell>0.89</cell>
                        <cell>2.38</cell>
                        <cell>0.56</cell>
                    </row>
                    <row>
                        <cell>SAM-VGG [75]</cell>
                        <cell>0.88</cell>
                        <cell>0.79</cell>
                        <cell>0.58</cell>
                        <cell>0.76</cell>
                        <cell>1.07</cell>
                        <cell>0.89</cell>
                        <cell>2.38</cell>
                        <cell>0.54</cell>
                    </row>
                    <row>
                        <cell>MSI-Net [82]</cell>
                        <cell>0.88</cell>
                        <cell>0.82</cell>
                        <cell>0.59</cell>
                        <cell>0.75</cell>
                        <cell>1.07</cell>
                        <cell>0.87</cell>
                        <cell>2.3</cell>
                        <cell>0.36</cell>
                    </row>
                    <row>
                        <cell>EML-NET [77]</cell>
                        <cell>0.87</cell>
                        <cell>0.79</cell>
                        <cell>0.59</cell>
                        <cell>0.75</cell>
                        <cell>1.05</cell>
                        <cell>0.88</cell>
                        <cell>2.38</cell>
                        <cell>0.96</cell>
                    </row>
                    <row>
                        <cell>DeepFix [66]</cell>
                        <cell>0.87</cell>
                        <cell>0.81</cell>
                        <cell>0.58</cell>
                        <cell>0.74</cell>
                        <cell>1.15</cell>
                        <cell>0.87</cell>
                        <cell>2.28</cell>
                        <cell>0.37</cell>
                    </row>
                    <row>
                        <cell>BMS [49]</cell>
                        <cell>0.85</cell>
                        <cell>0.84</cell>
                        <cell>0.59</cell>
                        <cell>0.61</cell>
                        <cell>1.95</cell>
                        <cell>0.67</cell>
                        <cell>1.67</cell>
                        <cell>0.83</cell>
                    </row>
                    <row>
                        <cell>eDN [62]</cell>
                        <cell>0.85</cell>
                        <cell>0.84</cell>
                        <cell>0.55</cell>
                        <cell>0.52</cell>
                        <cell>2.64</cell>
                        <cell>0.54</cell>
                        <cell>1.3</cell>
                        <cell>0.97</cell>
                    </row>
                    <row>
                        <cell>iSEEL [106]</cell>
                        <cell>0.84</cell>
                        <cell>0.81</cell>
                        <cell>0.59</cell>
                        <cell>0.62</cell>
                        <cell>1.78</cell>
                        <cell>0.66</cell>
                        <cell>1.67</cell>
                        <cell>0.92</cell>
                    </row>
                    <row>
                        <cell>Judd Model [61]</cell>
                        <cell>0.84</cell>
                        <cell>0.84</cell>
                        <cell>0.56</cell>
                        <cell>0.46</cell>
                        <cell>3.6</cell>
                        <cell>0.54</cell>
                        <cell>1.3</cell>
                        <cell>0.94</cell>
                    </row>
                    <row>
                        <cell>EYMOL [107]</cell>
                        <cell>0.83</cell>
                        <cell>0.76</cell>
                        <cell>0.51</cell>
                        <cell>0.61</cell>
                        <cell>1.91</cell>
                        <cell>0.72</cell>
                        <cell>1.78</cell>
                        <cell>1.67</cell>
                    </row>
                    <row>
                        <cell>LDS [108]</cell>
                        <cell>0.83</cell>
                        <cell>0.79</cell>
                        <cell>0.56</cell>
                        <cell>0.58</cell>
                        <cell>2.09</cell>
                        <cell>0.62</cell>
                        <cell>1.54</cell>
                        <cell>0.79</cell>
                    </row>
                    <row>
                        <cell>FES [109]</cell>
                        <cell>0.82</cell>
                        <cell>0.76</cell>
                        <cell>0.54</cell>
                        <cell>0.57</cell>
                        <cell>2.24</cell>
                        <cell>0.64</cell>
                        <cell>1.61</cell>
                        <cell>2.1</cell>
                    </row>
                    <row>
                        <cell>Aboudib Magn [110]</cell>
                        <cell>0.81</cell>
                        <cell>0.77</cell>
                        <cell>0.55</cell>
                        <cell>0.58</cell>
                        <cell>2.1</cell>
                        <cell>0.64</cell>
                        <cell>1.57</cell>
                        <cell>1.41</cell>
                    </row>
                    <row>
                        <cell>GBVS [28]</cell>
                        <cell>0.8</cell>
                        <cell>0.79</cell>
                        <cell>0.58</cell>
                        <cell>0.51</cell>
                        <cell>2.99</cell>
                        <cell>0.5</cell>
                        <cell>1.23</cell>
                        <cell>0.8</cell>
                    </row>
                    <row>
                        <cell>Context-Aware saliency [111]</cell>
                        <cell>0.77</cell>
                        <cell>0.76</cell>
                        <cell>0.6</cell>
                        <cell>0.5</cell>
                        <cell>3.09</cell>
                        <cell>0.42</cell>
                        <cell>1.07</cell>
                        <cell>1.04</cell>
                    </row>
                    <row>
                        <cell>IttiKoch2 [4]</cell>
                        <cell>0.77</cell>
                        <cell>0.76</cell>
                        <cell>0.59</cell>
                        <cell>0.48</cell>
                        <cell>3.44</cell>
                        <cell>0.42</cell>
                        <cell>1.06</cell>
                        <cell>0.92</cell>
                    </row>
                    <row>
                        <cell>AWS [112]</cell>
                        <cell>0.76</cell>
                        <cell>0.75</cell>
                        <cell>0.61</cell>
                        <cell>0.49</cell>
                        <cell>3.36</cell>
                        <cell>0.42</cell>
                        <cell>1.09</cell>
                        <cell>0.94</cell>
                    </row>
                    <row>
                        <cell>AIM [41]</cell>
                        <cell>0.76</cell>
                        <cell>0.75</cell>
                        <cell>0.6</cell>
                        <cell>0.44</cell>
                        <cell>3.69</cell>
                        <cell>0.36</cell>
                        <cell>0.89</cell>
                        <cell>1.13</cell>
                    </row>
                    <row>
                        <cell>WMAP [113]</cell>
                        <cell>0.75</cell>
                        <cell>0.69</cell>
                        <cell>0.6</cell>
                        <cell>0.47</cell>
                        <cell>3.28</cell>
                        <cell>0.38</cell>
                        <cell>1.01</cell>
                        <cell>1.65</cell>
                    </row>
                    <row>
                        <cell>Torralba saliency [51]</cell>
                        <cell>0.72</cell>
                        <cell>0.71</cell>
                        <cell>0.58</cell>
                        <cell>0.45</cell>
                        <cell>3.44</cell>
                        <cell>0.33</cell>
                        <cell>0.85</cell>
                        <cell>1.6</cell>
                    </row>
                    <row>
                        <cell>Murray model [72]</cell>
                        <cell>0.7</cell>
                        <cell>0.7</cell>
                        <cell>0.59</cell>
                        <cell>0.43</cell>
                        <cell>3.79</cell>
                        <cell>0.3</cell>
                        <cell>0.77</cell>
                        <cell>1.14</cell>
                    </row>
                    <row>
                        <cell>SUN saliency [54]</cell>
                        <cell>0.7</cell>
                        <cell>0.69</cell>
                        <cell>0.57</cell>
                        <cell>0.43</cell>
                        <cell>3.42</cell>
                        <cell>0.3</cell>
                        <cell>0.77</cell>
                        <cell>2.22</cell>
                    </row>
                    <row>
                        <cell>Achanta [36]</cell>
                        <cell>0.57</cell>
                        <cell>0.55</cell>
                        <cell>0.52</cell>
                        <cell>0.33</cell>
                        <cell>4.46</cell>
                        <cell>0.11</cell>
                        <cell>0.29</cell>
                        <cell>2.31</cell>
                    </row>
                    <row>
                        <cell>IttiKoch [4]</cell>
                        <cell>0.56</cell>
                        <cell>0.53</cell>
                        <cell>0.52</cell>
                        <cell>0.34</cell>
                        <cell>4.66</cell>
                        <cell>0.09</cell>
                        <cell>0.25</cell>
                        <cell>6.71</cell>
                    </row>
                </table>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"
                    coords="15,166.10,97.74,382.41,225.61">
                <head>Table 4 .</head>
                <label>4</label>
                <figDesc>
                    <div>
                        <p>
                            <s coords="15,200.26,97.74,243.73,8.63">Performance of the dynamic models over the DHF1K dataset.</s>
                        </p>
                    </div>
                </figDesc>
                <table coords="15,176.39,118.89,372.12,204.46">
                    <row>
                        <cell/>
                        <cell>Model Name</cell>
                        <cell>AUC-Judd</cell>
                        <cell>sAUC</cell>
                        <cell>CC</cell>
                        <cell>NSS</cell>
                        <cell>SIM</cell>
                    </row>
                    <row>
                        <cell>Static</cell>
                        <cell>DVA [78]</cell>
                        <cell>0.86</cell>
                        <cell>0.595</cell>
                        <cell>0.358</cell>
                        <cell>2.013</cell>
                        <cell>0.262</cell>
                    </row>
                    <row>
                        <cell>Models</cell>
                        <cell>SALICON [69]</cell>
                        <cell>0.857</cell>
                        <cell>0.59</cell>
                        <cell>0.327</cell>
                        <cell>1.901</cell>
                        <cell>0.232</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>JuntingNet [71]</cell>
                        <cell>0.855</cell>
                        <cell>0.592</cell>
                        <cell>0.331</cell>
                        <cell>1.775</cell>
                        <cell>0.201</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>Shallow-Net [71]</cell>
                        <cell>0.833</cell>
                        <cell>0.529</cell>
                        <cell>0.295</cell>
                        <cell>1.509</cell>
                        <cell>0.182</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>GBVS [28]</cell>
                        <cell>0.828</cell>
                        <cell>0.554</cell>
                        <cell>0.283</cell>
                        <cell>1.474</cell>
                        <cell>0.186</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>ITTI [4]</cell>
                        <cell>0.774</cell>
                        <cell>0.553</cell>
                        <cell>0.233</cell>
                        <cell>1.207</cell>
                        <cell>0.162</cell>
                    </row>
                    <row>
                        <cell>Dynamic</cell>
                        <cell>ACLNet [92]</cell>
                        <cell>0.89</cell>
                        <cell>0.601</cell>
                        <cell>0.434</cell>
                        <cell>2.354</cell>
                        <cell>0.315</cell>
                    </row>
                    <row>
                        <cell>Models</cell>
                        <cell>OM-CMM [90]</cell>
                        <cell>0.856</cell>
                        <cell>0.583</cell>
                        <cell>0.344</cell>
                        <cell>1.911</cell>
                        <cell>0.256</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>Two-stream [85]</cell>
                        <cell>0.834</cell>
                        <cell>0.581</cell>
                        <cell>0.325</cell>
                        <cell>1.632</cell>
                        <cell>0.197</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>FANG [114]</cell>
                        <cell>0.819</cell>
                        <cell>0.537</cell>
                        <cell>0.273</cell>
                        <cell>1.539</cell>
                        <cell>0.198</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>RUDOY [115]</cell>
                        <cell>0.769</cell>
                        <cell>0.501</cell>
                        <cell>0.285</cell>
                        <cell>1.498</cell>
                        <cell>0.214</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>STRA [88]</cell>
                        <cell>0.895</cell>
                        <cell>0.663</cell>
                        <cell>0.458</cell>
                        <cell>2.588</cell>
                        <cell>0.355</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>AWS-D [116]</cell>
                        <cell>0.703</cell>
                        <cell>0.513</cell>
                        <cell>0.174</cell>
                        <cell>0.94</cell>
                        <cell>0.157</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>PQFT [117]</cell>
                        <cell>0.699</cell>
                        <cell>0.562</cell>
                        <cell>0.137</cell>
                        <cell>0.749</cell>
                        <cell>0.139</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>OBDL [118]</cell>
                        <cell>0.638</cell>
                        <cell>0.5</cell>
                        <cell>0.117</cell>
                        <cell>0.495</cell>
                        <cell>0.171</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>SEO [119]</cell>
                        <cell>0.635</cell>
                        <cell>0.499</cell>
                        <cell>0.07</cell>
                        <cell>0.334</cell>
                        <cell>0.142</cell>
                    </row>
                </table>
            </figure>
            <figure
                    xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"
                    coords="16,166.39,211.02,394.54,135.11">
                <head/>
                <label/>
                <figDesc>
                    <div>
                        <p>
                            <s coords="16,456.72,211.02,102.56,9.58;16,189.46,223.58,369.82,9.58;16,189.46,236.13,204.15,9.58;16,166.39,248.68,7.47,9.58;16,189.46,248.68,370.02,9.58;16,189.46,261.24,125.95,9.58">[79]rot et al. [122]used audio data to help video prediction.The shared attention proposed by Gorji et al.[79]could effectively improve model performance.3.Visualization: The black box model of deep learning is difficult to present in a manner that humans can understand.</s>
                            <s coords="16,318.42,261.24,240.85,9.58;16,189.18,273.79,69.81,9.58">However, saliency prediction itself is a representation of visual concepts.</s>
                            <s coords="16,262.08,273.79,298.86,9.58">Visualized CNNs have many benefits for understanding models, in-</s>
                        </p>
                    </div>
                </figDesc>
                <table/>
                <note coords="16,189.46,286.34,369.82,9.58;16,189.46,298.90,370.02,9.58;16,189.46,311.45,369.82,9.58;16,189.16,324.00,234.61,9.58;16,166.39,336.55,7.47,9.58">
                    <div>
                    <p>
                        <s coords="16,189.46,286.34,281.74,9.58">cluding the meaning of filters, visual patterns, or visual concepts.</s>
                        <s coords="16,474.30,286.34,84.97,9.58;16,189.46,298.90,370.02,9.58;16,189.46,311.45,37.85,9.58">Bylinskii et al. [123] designed a visual dataset and found that a specific type of database may be better for training.</s>
                        <s coords="16,230.40,311.45,328.88,9.58;16,189.16,324.00,234.61,9.58">Visualization can help us better understand a model, and it also brings the possibility of proposing better models and databases.</s>
                        <s coords="16,166.39,336.55,7.47,9.58">4.</s>
                    </p>
                    </div>
                </note>
            </figure>
        </body>
        <back>
            <div type="funding">
                <div>
                    <p>Funding: This research was funded by
                        <rs type="funder">National Key Research and Development Program</rs>
                        (
                        <rs type="grantNumber">2019YFB2101902</rs>),
                        <rs type="funder">National Natural Science Foundation of China</rs>
                        (
                        <rs type="grantNumber">62176268</rs>),
                        <rs type="funder">Non-profit Central Research Institute Fund of Chinese Academy of Medical Sciences</rs>
                        (
                        <rs type="grantNumber">2020-JKCS-008</rs>),
                        <rs type="funder">Major Science and Technology Project of Zhejiang Province Health Commission</rs>
                        (
                        <rs type="grantNumber">WKJ-ZJ-2112</rs>), and the
                        <rs type="funder">Fundamental Research Funds for the Central Universities</rs>
                        (
                        <rs type="grantNumber">FRF-BD-20-11AFRF-DF-20-05</rs>).Institutional Review Board Statement: Not
                        applicable.Informed Consent Statement: Not applicable.
                    </p>
                </div>
            </div>
            <listOrg type="funding">
                <org type="funding" xml:id="_WGAwppF">
                    <idno type="grant-number">2019YFB2101902</idno>
                </org>
                <org type="funding" xml:id="_3va5pvx">
                    <idno type="grant-number">62176268</idno>
                </org>
                <org type="funding" xml:id="_3KxAEYx">
                    <idno type="grant-number">2020-JKCS-008</idno>
                </org>
                <org type="funding" xml:id="_Ec4ErJA">
                    <idno type="grant-number">WKJ-ZJ-2112</idno>
                </org>
                <org type="funding" xml:id="_vp8nUyd">
                    <idno type="grant-number">FRF-BD-20-11AFRF-DF-20-05</idno>
                </org>
            </listOrg>
            <div type="availability">
                <div
                        xmlns="http://www.tei-c.org/ns/1.0">
                    <p>
                        <s coords="17,166.39,752.49,179.50,8.63">Data Availability Statement: Not applicable.</s>
                    </p>
                </div>
            </div>
            <div type="annex">
                <div
                        xmlns="http://www.tei-c.org/ns/1.0">
                    <p>
                        <s coords="17,166.04,599.71,394.35,8.63;17,166.39,611.42,394.37,8.63;17,166.39,623.14,289.12,8.63">Author Contributions: Conceptualization, F.Y., C.C. and R.X.; investigation, F.Y. and P.X.; resources, S.Q. and C.C.; writing-original draft preparation, F.Y.; writing-review and editing, R.X.; supervision, Z.W.; project administration, R.X.; funding acquisition, R.X. and Z.W.</s>
                        <s coords="17,457.75,623.14,101.52,8.63;17,166.39,634.86,199.89,8.63">All authors have read and agreed to the published version of the manuscript.</s>
                    </p>
                </div>
                <div
                        xmlns="http://www.tei-c.org/ns/1.0">
                    <head coords="17,166.39,770.26,82.92,8.41">Conflicts of Interest:</head>
                    <p>
                        <s coords="17,252.09,770.19,165.99,8.63">The authors declare no conflict of interest.</s>
                    </p>
                </div>
            </div>
            <div type="references">
                <listBibl>
                    <biblStruct coords="18,39.08,113.10,490.67,8.74" xml:id="b0">
                        <analytic>
                            <title level="a" type="main" coords="18,121.33,113.22,175.50,8.63">Computational modelling of visual attention</title>
                            <author>
                                <persName>
                                    <forename type="first">Laurent</forename>
                                    <surname>Itti</surname>
                                </persName>
                            </author>
                            <author>
                                <persName>
                                    <forename type="first">Christof</forename>
                                    <surname>Koch</surname>
                                </persName>
                            </author>
                            <idno type="DOI">10.1038/35058500</idno>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,303.38,113.10,72.07,8.55">Nature Reviews Neuroscience</title>
                            <title level="j" type="abbrev">Nat Rev Neurosci</title>
                            <idno type="ISSN">1471-003X</idno>
                            <idno type="ISSNe">1471-0048</idno>
                            <imprint>
                                <biblScope unit="volume">2</biblScope>
                                <biblScope unit="issue">3</biblScope>
                                <biblScope unit="page" from="194" to="203"/>
                                <date type="published" when="2001-03-01">2001</date>
                                <publisher>Springer Science and Business Media LLC</publisher>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,39.08,124.61,456.65,8.74" xml:id="b1">
                        <analytic>
                            <title level="a" type="main" coords="18,108.13,124.73,181.75,8.63">Some studies in the speed of visual perception</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <forename type="middle">C</forename>
                                    <surname>Sziklai</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,296.72,124.61,83.02,8.55">IRE Trans. Inf. Theory</title>
                            <imprint>
                                <biblScope unit="volume">76</biblScope>
                                <biblScope unit="page" from="125" to="128"/>
                                <date type="published" when="1956">1956</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,39.08,136.12,521.31,8.74;18,56.95,147.63,184.43,8.74" xml:id="b2">
                        <analytic>
                            <title level="a" type="main" coords="18,403.72,136.23,130.95,8.63">How Much the Eye Tells the Brain</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Koch</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Mclean</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Segev</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <forename type="middle">A</forename>
                                    <surname>Freed</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">I</forename>
                                    <forename type="middle">I</forename>
                                    <surname>Michael</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">V</forename>
                                    <surname>Balasubramanian</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Sterling</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j"
                                   coords="18,541.28,136.12,19.11,8.55;18,56.95,147.63,16.69,8.55">Curr. Biol</title>
                            <imprint>
                                <biblScope unit="volume">16</biblScope>
                                <biblScope unit="page" from="1428" to="1434"/>
                                <date type="published" when="2006">2006</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,39.08,159.13,483.10,8.74" xml:id="b3">
                        <analytic>
                            <title level="a" type="main" coords="18,83.16,159.25,262.61,8.63">A model of saliency-based visual attention for rapid scene analysis</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Itti</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,352.36,159.13,44.33,8.55">IEEE Trans</title>
                            <imprint>
                                <biblScope unit="volume">20</biblScope>
                                <biblScope unit="page" from="1254" to="1259"/>
                                <date type="published" when="1998">1998</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,39.08,170.64,520.19,8.74;18,56.98,182.15,189.79,8.74" xml:id="b4">
                        <analytic>
                            <title level="a" type="main" coords="18,213.70,170.75,261.80,8.63">Unsupervised extraction of visual attention objects in color images</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Han</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <forename type="middle">N</forename>
                                    <surname>Ngan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,482.63,170.64,76.64,8.55;18,56.98,182.15,73.26,8.55">IEEE Trans. Circuits Syst. Video Technol</title>
                            <imprint>
                                <biblScope unit="volume">16</biblScope>
                                <biblScope unit="page" from="141" to="145"/>
                                <date type="published" when="2005">2005</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,39.08,193.77,521.68,8.63;18,57.23,205.16,374.29,8.74" xml:id="b5">
                        <analytic>
                            <title level="a" type="main"
                                   coords="18,124.57,193.77,436.19,8.63;18,57.23,205.28,14.31,8.63">A Unified Spectral-Domain Approach for Saliency Detection and Its Application to Automatic Object Segmentation</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Jung</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Kim</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,77.90,205.16,228.13,8.55">IEEE Trans. Image Process. A Publ. IEEE Signal Process. Soc</title>
                            <imprint>
                                <biblScope unit="volume">21</biblScope>
                                <biblScope unit="page" from="1272" to="1283"/>
                                <date type="published" when="2012">2012</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,39.08,216.67,499.16,8.74" xml:id="b6">
                        <analytic>
                            <title level="a" type="main" coords="18,129.82,216.78,214.76,8.63">Biologically Inspired Mobile Robot Vision Localization</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Siagian</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Itti</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,351.28,216.67,70.43,8.55">IEEE Trans. Robot</title>
                            <imprint>
                                <biblScope unit="volume">25</biblScope>
                                <biblScope unit="page" from="861" to="873"/>
                                <date type="published" when="2009">2009</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,39.08,228.17,521.32,8.74;18,57.23,239.80,33.62,8.63" xml:id="b7">
                        <analytic>
                            <title level="a" type="main" coords="18,141.94,228.29,313.26,8.63">Shifts in Selective Visual Attention: Towards the Underlying Neural Circuitry</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Koch</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Ullman</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,462.72,228.17,64.22,8.55">Hum. Neurobiol</title>
                            <imprint>
                                <biblScope unit="volume">4</biblScope>
                                <biblScope unit="page" from="219" to="227"/>
                                <date type="published" when="1987">1987</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,39.08,251.19,521.32,8.74;18,57.23,262.69,65.01,8.74" xml:id="b8">
                        <analytic>
                            <title level="a" type="main" coords="18,280.92,251.30,217.80,8.63">A Spatiotemporal Saliency Model for Video Surveillance</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Tong</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <forename type="middle">A</forename>
                                    <surname>Cheikh</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Guraya</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Konik</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Trémeau</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,505.36,251.19,55.03,8.55">Cogn. Comput</title>
                            <imprint>
                                <biblScope unit="volume">3</biblScope>
                                <biblScope unit="page" from="241" to="263"/>
                                <date type="published" when="2011">2011</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,274.20,517.21,8.74;18,57.23,285.71,122.71,8.74" xml:id="b9">
                        <analytic>
                            <title level="a" type="main" coords="18,83.29,274.32,369.53,8.63">Automatic foveation for video compression using a neurobiological model of visual attention</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Itti</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,459.41,274.20,97.33,8.55">IEEE Trans. Image Process</title>
                            <imprint>
                                <biblScope unit="volume">13</biblScope>
                                <biblScope unit="page" from="1304" to="1318"/>
                                <date type="published" when="2004">2004</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,297.21,516.09,8.74;18,56.95,308.72,282.36,8.74" xml:id="b10">
                        <analytic>
                            <title level="a" type="main" coords="18,148.03,297.33,335.53,8.63">Perceptual Image Hashing Via Feature Points: Performance Evaluation and Tradeoffs</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">V</forename>
                                    <surname>Monga</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <forename type="middle">L</forename>
                                    <surname>Evans</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,490.26,297.21,69.02,8.55;18,56.95,308.72,156.87,8.55">IEEE Trans. Image Process. A Publ. IEEE Signal Process. Soc</title>
                            <imprint>
                                <biblScope unit="volume">15</biblScope>
                                <biblScope unit="page" from="3452" to="3465"/>
                                <date type="published" when="2006">2006</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,320.34,516.09,8.63;18,57.23,331.74,341.95,8.74" xml:id="b11">
                        <analytic>
                            <title level="a" type="main"
                                   coords="18,246.74,320.34,312.54,8.63;18,57.23,331.85,63.09,8.63">Inferring Salient Objects from Human Fixations. Inferring salient objects from human fixations</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Shen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Dong</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Borji</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,126.72,331.74,146.97,8.55">IEEE Trans. Pattern Anal. Mach. Intell</title>
                            <imprint>
                                <biblScope unit="volume">42</biblScope>
                                <biblScope unit="page" from="1913" to="1927"/>
                                <date type="published" when="2019">2019</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,343.24,516.09,8.74;18,56.91,354.75,195.72,8.74" xml:id="b12">
                        <analytic>
                            <title level="a" type="main" coords="18,248.95,343.36,226.76,8.63">Paying Attention to Video Object Pattern Understanding</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Shen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Lu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <forename type="middle">C H</forename>
                                    <surname>Hoi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Ling</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,483.19,343.24,76.09,8.55;18,56.91,354.75,70.22,8.55">IEEE Trans. Pattern Anal. Mach. Intell</title>
                            <imprint>
                                <biblScope unit="volume">43</biblScope>
                                <biblScope unit="page" from="2413" to="2428"/>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,366.26,516.09,8.74;18,56.91,377.76,195.72,8.74" xml:id="b13">
                        <analytic>
                            <title level="a" type="main" coords="18,167.74,366.37,309.77,8.63">A Deep Network Solution for Attention and Aesthetics Aware Photo Cropping</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Shen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Ling</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,484.78,366.26,74.49,8.55;18,56.91,377.76,70.22,8.55">IEEE Trans. Pattern Anal. Mach. Intell</title>
                            <imprint>
                                <biblScope unit="volume">41</biblScope>
                                <biblScope unit="page" from="1531" to="1544"/>
                                <date type="published" when="2018">2018</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,389.27,515.87,8.74" xml:id="b14">
                        <analytic>
                            <title level="a" type="main" coords="18,182.79,389.39,169.29,8.63">Salient Bundle Adjustment for Visual SLAM</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Ma</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Lu</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,359.80,389.27,101.94,8.55">IEEE Trans. Instrum. Meas</title>
                            <imprint>
                                <biblScope unit="volume">70</biblScope>
                                <biblScope unit="page" from="1" to="9"/>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,400.89,517.66,8.63;18,57.23,412.28,114.33,8.74" xml:id="b15">
                        <monogr>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Aksoy</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Yazc</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Kasap</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>See</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2002.11020</idno>
                            <title level="m" coords="18,191.46,400.89,365.31,8.63">Attend and Brake: An Attention-based Saliency Map Prediction Model for End-to-End Driving</title>
                            <imprint/>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,423.79,517.21,8.74;18,57.23,435.41,33.62,8.63" xml:id="b16">
                        <analytic>
                            <title level="a" type="main" coords="18,194.58,423.91,211.75,8.63">Hierarchical co-attention for visual question answering</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Lu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Batra</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Parikh</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j"
                                   coords="18,413.38,423.79,110.97,8.55">Adv. Neural Inf. Process. Syst</title>
                            <imprint>
                                <biblScope unit="volume">29</biblScope>
                                <biblScope unit="page" from="289" to="297"/>
                                <date type="published" when="2016">2016</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,446.92,516.09,8.63;18,57.23,458.31,371.35,8.74" xml:id="b17">
                        <analytic>
                            <title level="a" type="main"
                                   coords="18,383.67,446.92,175.61,8.63;18,57.23,458.43,220.76,8.63">Atypical Visual Saliency in Autism Spectrum Disorder Quantified through Model-Based Eye Tracking</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Jiang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Duchesne</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Laugeson</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Kennedy</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Adolphs</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Q</forename>
                                    <surname>Zhao</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,284.85,458.31,27.74,8.55">Neuron</title>
                            <imprint>
                                <biblScope unit="volume">88</biblScope>
                                <biblScope unit="page" from="604" to="616"/>
                                <date type="published" when="2015">2015</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,469.93,516.35,8.63;18,57.23,481.32,149.26,8.74" xml:id="b18">
                        <monogr>
                            <title level="m" type="main"
                                   coords="18,267.79,469.93,291.75,8.63;18,57.23,481.44,28.13,8.63">SalientSleepNet: Multimodal Salient Wave Detection Network for Sleep Staging</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Jia</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Lin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Xie</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2105.13864</idno>
                            <imprint/>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,492.83,517.21,8.74;18,56.95,504.34,211.90,8.74" xml:id="b19">
                        <analytic>
                            <title level="a" type="main" coords="18,228.94,492.95,279.79,8.63">Salient Object Detection in the Deep Learning Era: An In-depth Survey</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Q</forename>
                                    <surname>Lai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Fu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Shen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,515.72,492.83,44.68,8.55;18,56.95,504.34,99.86,8.55">IEEE Trans. Pattern Anal. Mach. Intell</title>
                            <imprint>
                                <biblScope unit="page" from="1448" to="1457"/>
                                <date type="published" when="2021">2021</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,515.96,516.09,8.63;18,57.23,527.47,350.91,8.63" xml:id="b20">
                        <analytic>
                            <title level="a" type="main"
                                   coords="18,220.71,515.96,338.57,8.63;18,57.23,527.47,63.83,8.63">An Iterative and Cooperative Top-Down and Bottom-Up Inference Network for Salient Object Detection</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Shen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Cheng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Shao</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="18,138.36,527.47,105.78,8.63">Proceedings of the CVPR19</title>
                            <meeting>the CVPR19
                                <address>
                                    <addrLine>Long Beach, CA, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2019-06">June 2019</date>
                                <biblScope unit="page" from="16" to="20"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,538.98,516.09,8.63;18,57.23,550.48,219.18,8.63" xml:id="b21">
                        <analytic>
                            <title level="a" type="main" coords="18,233.18,538.98,260.82,8.63">Salient Object Detection With Pyramid Attention and Salient Edges</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Zhao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Shen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Hoi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Borji</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="18,511.50,538.98,47.78,8.63;18,57.23,550.48,55.18,8.63">Proceedings of the CVPR19</title>
                            <meeting>the CVPR19
                                <address>
                                    <addrLine>Long Beach, CA, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2019-06">June 2019</date>
                                <biblScope unit="page" from="16" to="20"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,561.87,516.09,8.74;18,57.23,573.50,91.17,8.63" xml:id="b22">
                        <monogr>
                            <title level="m" type="main" coords="18,295.74,561.99,236.06,8.63">Uncertainty-Aware Deep Calibrated Salient Object Detection</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Dai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Yu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Harandi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>Barnes</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Hartley</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2012.06020</idno>
                            <imprint/>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,585.00,517.66,8.63;18,57.23,596.39,225.90,8.74" xml:id="b23">
                        <analytic>
                            <title level="a" type="main" coords="18,224.21,585.00,332.59,8.63">Looking for the Detail and Context Devils: High-Resolution Salient Object Detection</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Zeng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Lei</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Lu</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,57.23,596.39,96.76,8.55">IEEE Trans. Image Process</title>
                            <imprint>
                                <biblScope unit="volume">30</biblScope>
                                <biblScope unit="page" from="3204" to="3216"/>
                                <date type="published" when="2021">2021</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,607.90,454.59,8.74" xml:id="b24">
                        <analytic>
                            <title level="a" type="main" coords="18,166.34,608.02,158.10,8.63">A feature-integration theory of attention</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Treisman</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Gelade</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,330.99,607.90,54.75,8.55">Cogn. Psychol</title>
                            <imprint>
                                <biblScope unit="volume">12</biblScope>
                                <biblScope unit="page" from="97" to="136"/>
                                <date type="published" when="1980">1980</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,619.41,513.40,8.74" xml:id="b25">
                        <analytic>
                            <title level="a" type="main" coords="18,109.57,619.52,188.67,8.63">Feature binding, attention and object perception</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Treisman</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j"
                                   coords="18,305.08,619.41,121.54,8.55">Philos. Trans. R. Soc. B Biol. Sci</title>
                            <imprint>
                                <biblScope unit="volume">353</biblScope>
                                <biblScope unit="page" from="1295" to="1306"/>
                                <date type="published" when="1998">1998</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,630.91,451.88,8.74" xml:id="b26">
                        <analytic>
                            <title level="a" type="main" coords="18,102.84,631.03,201.75,8.63">Guided Search 2.0 A revised model of visual search</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Wolfe</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,311.28,630.91,71.74,8.55">Psychon. Bull. Rev</title>
                            <imprint>
                                <biblScope unit="volume">1</biblScope>
                                <biblScope unit="page" from="202" to="238"/>
                                <date type="published" when="1994">1994</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,642.54,516.09,8.63;18,57.23,654.04,307.48,8.63" xml:id="b27">
                        <analytic>
                            <title level="a" type="main" coords="18,179.41,642.54,115.07,8.63">Graph-Based Visual Saliency</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Harel</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Koch</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Perona</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="18,314.14,642.54,245.13,8.63;18,57.23,654.04,124.92,8.63">Proceedings of the IEEE Conference on Advances in Neural Information Processing Systems</title>
                            <meeting>the IEEE Conference on Advances in Neural Information Processing Systems
                                <address>
                                    <addrLine>Vancouver, BC, Canada</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2006-12-09">4-9 December 2006</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,665.55,516.08,8.63;18,57.23,677.06,270.27,8.63" xml:id="b28">
                        <analytic>
                            <title level="a" type="main" coords="18,90.25,665.55,250.88,8.63">Contrast-based image attention analysis by using fuzzy growing</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <forename type="middle">F</forename>
                                    <surname>Ma</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="18,358.75,665.55,200.52,8.63;18,57.23,677.06,104.11,8.63">Proceedings of the 11th Annual ACM International Conference on Multimedia</title>
                            <meeting>the 11th Annual ACM International Conference on Multimedia
                                <address>
                                    <addrLine>Berkeley, CA, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2003-11-08">2-8 November 2003</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,688.45,517.21,8.74;18,57.23,699.96,119.54,8.74" xml:id="b29">
                        <analytic>
                            <title level="a" type="main" coords="18,323.98,688.56,132.14,8.63">Learning to detect a salient object</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Yuan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Sun</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>Zheng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Tang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H.-Y</forename>
                                    <surname>Shum</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="18,462.57,688.45,97.83,8.55;18,57.23,699.96,47.27,8.55">IEEE Trans. Pattern Anal. Mach. Intell</title>
                            <imprint>
                                <biblScope unit="volume">33</biblScope>
                                <biblScope unit="page" from="353" to="367"/>
                                <date type="published" when="2010">2010</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,711.58,516.27,8.63;18,56.91,723.09,311.11,8.63" xml:id="b30">
                        <analytic>
                            <title level="a" type="main" coords="18,120.08,711.58,246.22,8.63">Exploiting local and global patch rarities for saliency detection</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Borji</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Itti</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="18,383.48,711.58,175.98,8.63;18,56.91,723.09,153.50,8.63">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
                            <meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)
                                <address>
                                    <addrLine>Providence, RI, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2012-06">June 2012</date>
                                <biblScope unit="page" from="16" to="21"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,734.59,516.08,8.63;18,57.23,746.10,225.37,8.63" xml:id="b31">
                        <analytic>
                            <title level="a" type="main" coords="18,139.70,734.59,176.89,8.63">Saliency Detection: A Boolean Map Approach</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Sclaroff</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="18,334.31,734.59,224.97,8.63;18,57.23,746.10,66.42,8.63">Proceedings of the 2013 IEEE International Conference on Computer Vision</title>
                            <meeting>the 2013 IEEE International Conference on Computer Vision
                                <address>
                                    <addrLine>Sydney, Australia</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2013-12-08">1-8 December 2013</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="18,43.19,757.61,516.09,8.63;18,57.23,769.11,343.68,8.63" xml:id="b32">
                        <analytic>
                            <title level="a" type="main" coords="18,131.15,757.61,290.22,8.63">Visual attention detection in video sequences using spatiotemporal cues</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Zhai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Shah</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="18,439.69,757.61,119.59,8.63;18,57.23,769.11,157.80,8.63">Proceedings of the 14th ACM International Conference on Multimedia</title>
                            <meeting>the 14th ACM International Conference on Multimedia
                                <address>
                                    <addrLine>Santa Barbara, CA, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2006-10">October 2006</date>
                                <biblScope unit="page" from="23" to="27"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,98.72,516.09,8.63;19,57.23,110.23,274.64,8.63" xml:id="b33">
                        <analytic>
                            <title level="a" type="main" coords="19,175.14,98.72,155.04,8.63">Salient object detection by composition</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Wei</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Jie</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Tao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Jian</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,347.93,98.72,211.35,8.63;19,57.23,110.23,113.16,8.63">Proceedings of the IEEE International Conference on Computer Vision, ICCV 2011</title>
                            <meeting>the IEEE International Conference on Computer Vision, ICCV 2011
                                <address>
                                    <addrLine>Barcelona, Spain</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2011-11-13">6-13 November 2011</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,121.74,516.08,8.63;19,57.23,133.25,277.71,8.63" xml:id="b34">
                        <analytic>
                            <title level="a" type="main" coords="19,208.05,121.74,112.65,8.63">What Makes a Patch Distinct</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Margolin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Tal</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Zelnik-Manor</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,337.69,121.74,221.58,8.63;19,57.23,133.25,126.14,8.63">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
                            <meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
                                <address>
                                    <addrLine>Portland, OR, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2013-06">June 2013</date>
                                <biblScope unit="page" from="23" to="28"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,144.75,516.08,8.63;19,57.23,156.26,424.69,8.63" xml:id="b35">
                        <analytic>
                            <title level="a" type="main" coords="19,264.15,144.75,162.09,8.63">Frequency-tuned salient region detection</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Achanta</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Hemami</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Estrada</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Su Sstrunk</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,443.46,144.75,115.81,8.63;19,57.23,156.26,224.01,8.63">Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
                            <meeting>the 2009 IEEE Conference on Computer Vision and Pattern Recognition
                                <address>
                                    <addrLine>Miami, FL, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2009-06">June 2009</date>
                                <biblScope unit="page" from="1597" to="1604"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,167.77,516.09,8.63;19,57.23,179.27,365.71,8.63" xml:id="b36">
                        <analytic>
                            <title level="a" type="main" coords="19,295.35,167.77,187.66,8.63">Global Contrast Based Salient Region Detection</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Cheng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <forename type="middle">X</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Mitra</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Huang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Hu</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,500.40,167.77,58.88,8.63;19,57.23,179.27,179.17,8.63">Proceedings of the Computer Vision and Pattern Recognition</title>
                            <meeting>the Computer Vision and Pattern Recognition
                                <address>
                                    <addrLine>Colorado Springs, CO, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2011-06">June 2011</date>
                                <biblScope unit="page" from="20" to="25"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,190.66,516.09,8.74;19,56.69,202.17,111.04,8.74" xml:id="b37">
                        <analytic>
                            <title level="a" type="main" coords="19,214.18,190.78,211.56,8.63">Superpixel-Based Spatiotemporal Saliency Detection</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Zhi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Luo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">O</forename>
                                    <forename type="middle">L</forename>
                                    <surname>Meur</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="19,433.84,190.66,125.43,8.55;19,56.69,202.17,29.80,8.55">IEEE Trans. Circuits Syst. Video Technol</title>
                            <imprint>
                                <biblScope unit="volume">24</biblScope>
                                <biblScope unit="page" from="1522" to="1540"/>
                                <date type="published" when="2014">2014</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,213.79,516.09,8.63;19,57.23,225.30,394.98,8.63" xml:id="b38">
                        <analytic>
                            <title level="a" type="main" coords="19,201.75,213.79,342.05,8.63">Improved saliency detection based on superpixel clustering and saliency propagation</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Ren</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Hu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <forename type="middle">T</forename>
                                    <surname>Chia</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Rajan</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,57.23,225.30,254.31,8.63">Proceedings of the Acm International Conference on Multimedia</title>
                            <meeting>the Acm International Conference on Multimedia
                                <address>
                                    <addrLine>Firenze, Italy</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2010-10">October 2010</date>
                                <biblScope unit="page" from="25" to="29"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,236.81,517.65,8.63;19,57.23,248.20,216.69,8.74" xml:id="b39">
                        <analytic>
                            <title level="a" type="main" coords="19,185.86,236.81,370.44,8.63">Unsupervised video co-segmentation based on superpixel co-saliency and region merging</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Huang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Pun</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Lin</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="19,57.23,248.20,82.23,8.55">Multimed. Tools Appl</title>
                            <imprint>
                                <biblScope unit="volume">76</biblScope>
                                <biblScope unit="page" from="12941" to="12964"/>
                                <date type="published" when="2016">2016</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,259.82,516.09,8.63;19,57.23,271.33,268.74,8.63" xml:id="b40">
                        <analytic>
                            <title level="a" type="main" coords="19,164.00,259.82,174.75,8.63">Saliency Based on Information Maximization</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <forename type="middle">D B</forename>
                                    <surname>Bruce</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <forename type="middle">K</forename>
                                    <surname>Tsotsos</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,356.37,259.82,202.90,8.63;19,57.23,271.33,86.77,8.63">Proceedings of the Advances in Neural Information Processing Systems 18</title>
                            <meeting>the Advances in Neural Information Processing Systems 18
                                <address>
                                    <addrLine>Vancouver, BC, Canada</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2005-12-08">5-8 December 2005</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,282.83,516.08,8.63;19,57.23,294.34,364.91,8.63" xml:id="b41">
                        <analytic>
                            <title level="a" type="main" coords="19,89.16,282.83,252.88,8.63">Dynamic visual attention: Searching for coding length increments</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Hou</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,359.07,282.83,200.20,8.63;19,57.23,294.34,100.82,8.63">Proceedings of the Advances in Neural Information Processing Systems (NIPS</title>
                            <meeting>the Advances in Neural Information Processing Systems (NIPS
                                <address>
                                    <addrLine>Vancouver, BC, Canada</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2008-08-10">2008. 8-10 December 2008</date>
                                <biblScope unit="page" from="681" to="688"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,305.85,517.66,8.63;19,57.23,317.36,465.13,8.63" xml:id="b42">
                        <analytic>
                            <title level="a" type="main" coords="19,277.68,305.85,279.21,8.63">A Rarity-Based Visual Attention Map-Application to Texture Description</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Mancas</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Mancas-Thillou</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Gosselin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Macq</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,67.71,317.36,298.61,8.63">Proceedings of the International Conference on Image Processing, ICIP 2006</title>
                            <meeting>the International Conference on Image Processing, ICIP 2006
                                <address>
                                    <addrLine>Atlanta, GA, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2006-10">October 2006</date>
                                <biblScope unit="page" from="8" to="11"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,328.86,516.27,8.63;19,57.23,340.37,484.30,8.63" xml:id="b43">
                        <analytic>
                            <title level="a" type="main" coords="19,139.31,328.86,250.26,8.63">Nonparametric bottom-up saliency detection by self-resemblance</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Seo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Milanfar</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,406.14,328.86,153.32,8.63;19,57.23,340.37,301.03,8.63">Proceedings of the 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
                            <meeting>the 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops
                                <address>
                                    <addrLine>Miami, FL, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2009-06-25">20-25 June 2009</date>
                                <biblScope unit="page" from="45" to="52"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,351.76,517.65,8.74;19,57.23,363.38,83.73,8.63" xml:id="b44">
                        <analytic>
                            <title level="a" type="main" coords="19,210.03,351.88,250.92,8.63">The effect of background color on asymmetries in color search</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Rosenholtz</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <forename type="middle">L</forename>
                                    <surname>Nagy</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <forename type="middle">R</forename>
                                    <surname>Bell</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="19,468.54,351.76,22.17,8.55">J. Vis</title>
                            <imprint>
                                <biblScope unit="volume">4</biblScope>
                                <biblScope unit="page" from="224" to="240"/>
                                <date type="published" when="2004">2004</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,374.89,516.26,8.63;19,56.91,386.40,282.35,8.63" xml:id="b45">
                        <analytic>
                            <title level="a" type="main" coords="19,135.48,374.89,201.48,8.63">Saliency Detection: A Spectral Residual Approach</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Hou</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,356.46,374.89,202.99,8.63;19,56.91,386.40,113.32,8.63">Proceedings of the IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
                            <meeting>the IEEE Conference on Computer Vision &amp; Pattern Recognition
                                <address>
                                    <addrLine>Minneapolis, MN, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2007-06">June 2007</date>
                                <biblScope unit="page" from="17" to="22"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,397.90,516.09,8.63;19,57.23,409.41,473.46,8.63" xml:id="b46">
                        <analytic>
                            <title level="a" type="main" coords="19,171.87,397.90,368.72,8.63">Spatio-temporal Saliency detection using phase spectrum of quaternion fourier transform</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Guo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Qi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,57.23,409.41,312.91,8.63">Proceedings of the IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
                            <meeting>the IEEE Conference on Computer Vision &amp; Pattern Recognition
                                <address>
                                    <addrLine>Anchorage, AK, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2008-06">June 2008</date>
                                <biblScope unit="page" from="23" to="28"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,420.92,516.09,8.63;19,57.23,432.42,307.13,8.63" xml:id="b47">
                        <analytic>
                            <title level="a" type="main" coords="19,255.12,420.92,152.10,8.63">Salient Edges: A Multi Scale Approach</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Holtzman-Gazit</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Zelnik-Manor</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">I</forename>
                                    <surname>Yavneh</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,425.23,420.92,134.05,8.63;19,57.23,432.42,125.93,8.63">Proceedings of the 11th European Conference on Computer Vision</title>
                            <meeting>the 11th European Conference on Computer Vision
                                <address>
                                    <addrLine>Crete, Greece</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2010-09-11">5-11 September 2010</date>
                                <biblScope unit="page">4310</biblScope>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,443.82,510.37,8.74" xml:id="b48">
                        <analytic>
                            <title level="a" type="main" coords="19,99.05,443.93,303.86,8.63">Exploiting Surroundedness for Saliency Detection: A Boolean Map Approach</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Sclaroff</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="19,410.36,443.82,70.93,8.55">IEEE Comput. Soc</title>
                            <imprint>
                                <biblScope unit="volume">38</biblScope>
                                <biblScope unit="page" from="889" to="902"/>
                                <date type="published" when="2016">2016</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,455.32,516.09,8.74" xml:id="b49">
                        <analytic>
                            <title level="a" type="main" coords="19,118.77,455.44,174.46,8.63">State-of-the-Art in Visual Attention Modeling</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Borji</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Itti</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="19,300.44,455.32,144.16,8.55">IEEE Trans. Pattern Anal. Mach. Intell</title>
                            <imprint>
                                <biblScope unit="volume">35</biblScope>
                                <biblScope unit="page" from="185" to="207"/>
                                <date type="published" when="2013">2013</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,466.94,516.08,8.63;19,57.23,478.45,428.33,8.63" xml:id="b50">
                        <analytic>
                            <title level="a" type="main" coords="19,281.72,466.94,213.35,8.63">Top-down control of visual attention in object detection</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Oliva</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Torralba</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <forename type="middle">S</forename>
                                    <surname>Castelhano</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Henderson</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,511.89,466.94,47.38,8.63;19,57.23,478.45,205.51,8.63">Proceedings of the International Conference on Image Processing</title>
                            <meeting>the International Conference on Image Processing
                                <address>
                                    <addrLine>Barcelona, Spain</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2003-09-18">14-18 September 2003</date>
                                <biblScope unit="page" from="253" to="256"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,489.96,516.08,8.63;19,57.23,501.35,212.30,8.74" xml:id="b51">
                        <analytic>
                            <title level="a" type="main"
                                   coords="19,278.22,489.96,281.06,8.63;19,57.23,501.47,50.22,8.63">Modelling search for people in 900 scenes: A combined source model of eye guidance</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <forename type="middle">A</forename>
                                    <surname>Ehinger</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Hidalgo-Sotelo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Torralba</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Oliva</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="19,114.53,501.35,38.48,8.55">Vis. Cogn</title>
                            <imprint>
                                <biblScope unit="volume">17</biblScope>
                                <biblScope unit="page" from="945" to="978"/>
                                <date type="published" when="2009">2009</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,512.86,517.21,8.74;19,56.98,524.36,96.09,8.74" xml:id="b52">
                        <analytic>
                            <title level="a" type="main" coords="19,161.79,512.97,183.14,8.63">Bayesian Saliency via Low and Mid Level Cues</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Xie</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Lu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <forename type="middle">H</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="19,352.10,512.86,208.30,8.55;19,56.98,524.36,14.86,8.55">IEEE Trans. Image Process. A Publ. IEEE Signal Process. Soc</title>
                            <imprint>
                                <biblScope unit="volume">22</biblScope>
                                <biblScope unit="page" from="1689" to="1698"/>
                                <date type="published" when="2013">2013</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,535.87,517.21,8.74;19,56.59,547.38,103.66,8.74" xml:id="b53">
                        <analytic>
                            <title level="a" type="main" coords="19,322.04,535.99,227.14,8.63">A Bayesian framework for saliency using natural statistics</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Tong</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Marks</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Tim</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Shan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Cottrell</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Sun</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j"
                                   coords="19,555.22,535.87,5.18,8.55;19,56.59,547.38,14.03,8.55">J. Vis</title>
                            <imprint>
                                <biblScope unit="volume">8</biblScope>
                                <biblScope unit="page">32</biblScope>
                                <date type="published" when="2008">2008</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="19,43.19,559.00,516.09,8.63;19,57.23,570.51,502.04,8.63;19,57.23,582.01,62.64,8.63"
                            xml:id="b54">
                        <analytic>
                            <title level="a" type="main" coords="19,154.84,559.00,263.45,8.63">Discriminant Saliency for Visual Recognition from Cluttered Scenes</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Gao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>Vasconcelos</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,435.53,559.00,123.74,8.63;19,57.23,570.51,372.32,8.63">Proceedings of the Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, NIPS 2004</title>
                            <meeting>the Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, NIPS 2004
                                <address>
                                    <addrLine>Vancouver, BC, Canada</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2004-12-18">12-18 December 2004</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,593.52,516.26,8.63;19,57.23,604.91,328.38,8.74" xml:id="b55">
                        <analytic>
                            <title level="a" type="main"
                                   coords="19,158.54,593.52,400.91,8.63;19,57.23,605.03,145.48,8.63">Decision-Theoretic Saliency: Computational Principles, Biological Plausibility, and Implications for Neurophysiology and Psychophysics</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Gao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>Vasconcelos</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="19,209.80,604.91,54.77,8.55">Neural Comput</title>
                            <imprint>
                                <biblScope unit="volume">21</biblScope>
                                <biblScope unit="page" from="239" to="271"/>
                                <date type="published" when="2014">2014</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,616.42,517.21,8.74;19,56.78,628.04,33.62,8.63" xml:id="b56">
                        <analytic>
                            <title level="a" type="main" coords="19,202.11,616.53,168.19,8.63">Spatiotemporal saliency in dynamic scenes</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Kim</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Kim</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <forename type="middle">Y</forename>
                                    <surname>Sim</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <forename type="middle">S</forename>
                                    <surname>Kim</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="19,377.03,616.42,146.96,8.55">IEEE Trans. Pattern Anal. Mach. Intell</title>
                            <imprint>
                                <biblScope unit="volume">32</biblScope>
                                <biblScope unit="page" from="171" to="177"/>
                                <date type="published" when="2015">2015</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,639.55,516.09,8.63;19,57.23,651.06,482.97,8.63" xml:id="b57">
                        <analytic>
                            <title level="a" type="main" coords="19,169.59,639.55,313.62,8.63">Generating Sequence of Eye Fixations Using Decision-theoretic Attention Model</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Gu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <forename type="middle">I</forename>
                                    <surname>Badler</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,501.08,639.55,58.20,8.63;19,57.23,651.06,325.79,8.63">Proceedings of the IEEE Computer Society Conference on Computer Vision &amp; Pattern Recognition</title>
                            <meeting>the IEEE Computer Society Conference on Computer Vision &amp; Pattern Recognition
                                <address>
                                    <addrLine>San Diego, CA, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2005-06">June 2005</date>
                                <biblScope unit="page" from="20" to="26"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,662.56,516.09,8.63;19,57.23,673.95,184.65,8.74" xml:id="b58">
                        <analytic>
                            <title level="a" type="main"
                                   coords="19,288.44,662.56,270.84,8.63;19,57.23,674.07,58.15,8.63">Center-surround patterns emerge as optimal predictors for human saccade targets</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Kienzle</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <forename type="middle">O</forename>
                                    <surname>Franz</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Scholkopf</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <forename type="middle">A</forename>
                                    <surname>Wichmann</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="19,121.79,673.95,21.50,8.55">J. Vis</title>
                            <imprint>
                                <biblScope unit="volume">9</biblScope>
                                <biblScope unit="page" from="1" to="15"/>
                                <date type="published" when="2009">2009</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,685.58,517.65,8.63;19,57.23,697.08,492.41,8.63" xml:id="b59">
                        <analytic>
                            <title level="a" type="main" coords="19,129.76,685.58,427.32,8.63">Beyond bottom-up: Incorporating task-dependent influences into a computational model of spatial attention</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Peters</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Itti</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,67.71,697.08,312.91,8.63">Proceedings of the IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
                            <meeting>the IEEE Conference on Computer Vision &amp; Pattern Recognition
                                <address>
                                    <addrLine>Minneapolis, MN, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2007-06">June 2007</date>
                                <biblScope unit="page" from="17" to="22"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="19,43.19,708.59,516.09,8.63;19,57.23,720.10,407.00,8.63" xml:id="b60">
                        <analytic>
                            <title level="a" type="main" coords="19,249.12,708.59,168.79,8.63">Learning to Predict Where Humans Look</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Judd</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Ehinger</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Durand</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Torralba</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,439.44,708.59,119.84,8.63;19,57.23,720.10,226.36,8.63">Proceedings of the IEEE 12th International Conference on Computer Vision, ICCV 2009</title>
                            <meeting>the IEEE 12th International Conference on Computer Vision, ICCV 2009
                                <address>
                                    <addrLine>Kyoto, Japan</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2009-10-04">27 September-4 October 2009</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="19,43.19,731.60,516.09,8.63;19,57.23,743.11,502.05,8.63;19,57.08,754.62,40.32,8.63"
                            xml:id="b61">
                        <analytic>
                            <title level="a" type="main" coords="19,165.08,731.60,376.17,8.63">Large-Scale Optimization of Hierarchical Features for Saliency Prediction in Natural Images</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Vig</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Dorr</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Cox</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="19,57.23,743.11,381.98,8.63">Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
                            <meeting>the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
                                <address>
                                    <addrLine>Columbus, OH, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2014-06">June 2014</date>
                                <biblScope unit="page" from="23" to="28"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,98.61,516.09,8.74;20,57.23,110.23,86.69,8.63" xml:id="b62">
                        <monogr>
                            <title level="m" type="main" coords="20,202.74,98.72,328.11,8.63">Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Kümmerer</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Theis</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Bethge</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1411.1045</idno>
                            <imprint/>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,121.62,517.21,8.74;20,56.95,133.13,175.60,8.74" xml:id="b63">
                        <analytic>
                            <title level="a" type="main" coords="20,229.32,121.74,260.72,8.63">Imagenet classification with deep convolutional neural networks</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Krizhevsky</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">I</forename>
                                    <surname>Sutskever</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <forename type="middle">E</forename>
                                    <surname>Hinton</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="20,498.00,121.62,62.40,8.55;20,56.95,133.13,50.10,8.55">Adv. Neural Inf. Process. Syst</title>
                            <imprint>
                                <biblScope unit="volume">25</biblScope>
                                <biblScope unit="page" from="1097" to="1105"/>
                                <date type="published" when="2012">2012</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,144.75,516.09,8.63;20,57.23,156.26,437.42,8.63" xml:id="b64">
                        <analytic>
                            <title level="a" type="main" coords="20,247.17,144.75,202.91,8.63">ImageNet: A large-scale hierarchical image database</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Jia</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Wei</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Socher</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Kai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <forename type="middle">F</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="20,467.30,144.75,91.97,8.63;20,57.23,156.26,245.71,8.63">Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
                            <meeting>the 2009 IEEE Conference on Computer Vision and Pattern Recognition
                                <address>
                                    <addrLine>Miami, FL, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2009-06">June 2009</date>
                                <biblScope unit="page" from="248" to="255"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,167.77,517.65,8.63;20,57.23,179.16,225.90,8.74" xml:id="b65">
                        <analytic>
                            <title level="a" type="main" coords="20,249.17,167.77,307.75,8.63">A Fully Convolutional Neural Network for Predicting Human Eye Fixations</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Kruthiventi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Ayush</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <forename type="middle">V</forename>
                                    <surname>Babu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Deepfix</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="20,57.23,179.16,96.76,8.55">IEEE Trans. Image Process</title>
                            <imprint>
                                <biblScope unit="volume">26</biblScope>
                                <biblScope unit="page" from="4446" to="4456"/>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,190.66,517.65,8.74" xml:id="b66">
                        <monogr>
                            <title level="m" type="main" coords="20,170.85,190.78,274.70,8.63">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Simonyan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Zisserman</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1409.1556</idno>
                            <imprint/>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,202.17,517.21,8.74;20,57.23,213.79,68.75,8.63" xml:id="b67">
                        <monogr>
                            <title level="m" type="main" coords="20,244.53,202.29,266.31,8.63">II: Reading fixations from deep features trained on object recognition</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Kümmerer</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Wallis</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Bethge</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Deepgaze</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1610.01563</idno>
                            <imprint/>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,225.30,517.21,8.63;20,57.23,236.81,135.28,8.63" xml:id="b68">
                        <analytic>
                            <title level="a" type="main" coords="20,194.40,225.30,117.65,8.63">SALICON: Saliency in Context</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Ming</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Huang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Duan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Qi</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="20,329.20,225.30,227.12,8.63">Proceedings of the Computer Vision &amp; Pattern Recognition</title>
                            <meeting>the Computer Vision &amp; Pattern Recognition
                                <address>
                                    <addrLine>Boston, MA, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2015-06">June 2015</date>
                                <biblScope unit="page" from="7" to="12"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="20,43.19,248.31,516.09,8.63;20,57.23,259.71,503.16,8.74;20,57.23,271.33,71.84,8.63"
                            xml:id="b69">
                        <analytic>
                            <title level="a" type="main"
                                   coords="20,274.69,248.31,284.59,8.63;20,57.23,259.82,74.65,8.63">A Benchmark of Computational Models of Saliency to Predict Human Fixations in Videos</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Azam</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <forename type="middle">O</forename>
                                    <surname>Gilani</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Jeon</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Yousaf</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J.-B</forename>
                                    <surname>Kim</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="20,149.49,259.71,95.91,8.55">VISIGRAPP (4: VISAPP)</title>
                            <meeting>
                                <address>
                                    <addrLine>Setúbal, Portugal</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <publisher>SCITEPRESS-Science and Technology Publications, Lda</publisher>
                                <date type="published" when="2016">2016</date>
                                <biblScope unit="page" from="134" to="142"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,282.83,516.44,8.63;20,57.23,294.23,160.16,8.74" xml:id="b70">
                        <monogr>
                            <title level="m" type="main"
                                   coords="20,328.98,282.83,230.65,8.63;20,57.23,294.34,39.13,8.63">Shallow and Deep Convolutional Networks for Saliency Prediction</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Pan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Mcguinness</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Sayrol</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>O'connor</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Giro-I-Nieto</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1603.00845</idno>
                            <imprint/>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,305.85,516.09,8.63;20,57.23,317.36,439.95,8.63" xml:id="b71">
                        <analytic>
                            <title level="a" type="main" coords="20,172.24,305.85,273.18,8.63">End-to-end saliency mapping via probability distribution prediction</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Jetley</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>Murray</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Vig</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="20,462.79,305.85,96.48,8.63;20,57.23,317.36,224.01,8.63">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
                            <meeting>the IEEE Conference on Computer Vision and Pattern Recognition
                                <address>
                                    <addrLine>Las Vegas, NV, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2016-06">June 2016</date>
                                <biblScope unit="page" from="5753" to="5761"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,328.75,517.20,8.74;20,57.23,340.25,306.51,8.74" xml:id="b72">
                        <analytic>
                            <title level="a" type="main" coords="20,120.60,328.86,386.87,8.63">A Deep Spatial Contextual Long-Term Recurrent Convolutional Network for Saliency Detection</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Han</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="20,514.99,328.75,45.40,8.55;20,57.23,340.25,181.02,8.55">IEEE Trans. Image Process. A Publ. IEEE Signal Process. Soc</title>
                            <imprint>
                                <biblScope unit="volume">27</biblScope>
                                <biblScope unit="page" from="3264" to="3274"/>
                                <date type="published" when="2018">2018</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,351.88,516.09,8.63;20,57.23,363.38,343.20,8.63" xml:id="b73">
                        <analytic>
                            <title level="a" type="main" coords="20,251.03,351.88,213.64,8.63">A Deep Multi-Level Network for Saliency Prediction</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Cornia</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Baraldi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Serra</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Cucchiara</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="20,483.86,351.88,75.42,8.63;20,57.23,363.38,189.95,8.63">Proceedings of the International Conference on Pattern Recognition</title>
                            <meeting>the International Conference on Pattern Recognition
                                <address>
                                    <addrLine>Cancun, Mexico</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2016-12-08">4-8 December 2016</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,374.89,517.66,8.63;20,57.23,386.28,181.64,8.74" xml:id="b74">
                        <analytic>
                            <title level="a" type="main" coords="20,245.37,374.89,310.76,8.63">Predicting Human Eye Fixations via an LSTM-based Saliency Attentive Model</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Marcella</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Lorenzo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Giuseppe</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Rita</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="20,57.23,386.28,96.76,8.55">IEEE Trans. Image Process</title>
                            <imprint>
                                <biblScope unit="volume">27</biblScope>
                                <biblScope unit="page" from="5142" to="5154"/>
                                <date type="published" when="2016">2016</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,397.90,516.09,8.63;20,56.88,409.29,207.85,8.74" xml:id="b75">
                        <monogr>
                            <title level="m" type="main"
                                   coords="20,383.30,397.90,175.97,8.63;20,56.88,409.41,86.12,8.63">Visual Saliency Prediction with Generative Adversarial Networks</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Pan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Canton</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Mcguinness</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <forename type="middle">E</forename>
                                    <surname>O'connor</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Giro-I-Nieto</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Salgan</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1701.01081</idno>
                            <imprint/>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,420.80,502.03,8.74" xml:id="b76">
                        <monogr>
                            <title level="m" type="main" coords="20,161.09,420.92,263.11,8.63">NET:An Expandable Multi-Layer NETwork for Saliency Prediction</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Jia</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <forename type="middle">D B</forename>
                                    <surname>Bruce</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Eml</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1805.01047</idno>
                            <imprint/>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,432.31,463.58,8.74" xml:id="b77">
                        <analytic>
                            <title level="a" type="main" coords="20,188.78,432.42,129.65,8.63">Deep Visual Attention Prediction</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Wang</forename>
                                    <forename type="middle">;</forename>
                                    <surname>Wenguan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">;</forename>
                                    <surname>Jianbing</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Shen</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="20,325.13,432.31,96.76,8.55">IEEE Trans. Image Process</title>
                            <imprint>
                                <biblScope unit="volume">27</biblScope>
                                <biblScope unit="page" from="2368" to="2378"/>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="20,43.19,443.93,516.09,8.63;20,57.23,455.44,503.16,8.63;20,57.23,466.94,144.15,8.63"
                            xml:id="b78">
                        <analytic>
                            <title level="a" type="main"
                                   coords="20,133.61,443.93,425.67,8.63;20,57.23,455.44,102.31,8.63">Attentional Push: A Deep Convolutional Network for Augmenting Image Salience with Shared Attention Modeling in Social Scenes</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Gorji</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Clark</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="20,177.11,455.44,378.58,8.63">Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
                            <meeting>the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
                                <address>
                                    <addrLine>Honolulu, HI, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2017-07">July 2017</date>
                                <biblScope unit="page" from="21" to="26"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,478.34,517.21,8.74;20,57.23,489.96,86.85,8.63" xml:id="b79">
                        <analytic>
                            <title level="a" type="main" coords="20,141.62,478.45,273.81,8.63">Visual Saliency Prediction Using a Mixture of Deep Neural Networks</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Dodge</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Karam</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="20,422.87,478.34,97.38,8.55">IEEE Trans. Image Process</title>
                            <imprint>
                                <biblScope unit="volume">27</biblScope>
                                <biblScope unit="page" from="4080" to="4090"/>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,501.47,516.09,8.63;20,57.23,512.86,286.73,8.74" xml:id="b80">
                        <analytic>
                            <title level="a" type="main"
                                   coords="20,176.51,501.47,382.77,8.63;20,57.23,512.97,62.80,8.63">DeepFeat: A bottom-up and top-down saliency model based on deep features of convolutional neural networks</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Mahdi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Qin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Crosby</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="20,127.19,512.86,109.22,8.55">IEEE Trans. Cogn. Dev. Syst</title>
                            <imprint>
                                <biblScope unit="volume">12</biblScope>
                                <biblScope unit="page" from="54" to="63"/>
                                <date type="published" when="2019">2019</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,524.36,517.21,8.74;20,57.23,535.99,33.62,8.63" xml:id="b81">
                        <analytic>
                            <title level="a" type="main" coords="20,192.01,524.48,270.07,8.63">Contextual encoder-decoder network for visual saliency prediction</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Aka</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <surname>Msa</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Kd</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Rgab</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="20,468.86,524.36,45.85,8.55">Neural Netw</title>
                            <imprint>
                                <biblScope unit="volume">129</biblScope>
                                <biblScope unit="page" from="261" to="270"/>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="20,43.19,547.49,516.08,8.63;20,57.23,559.00,502.05,8.63;20,57.23,570.51,307.48,8.63"
                            xml:id="b82">
                        <analytic>
                            <title level="a" type="main" coords="20,221.02,547.49,272.28,8.63">The discriminant center-surround hypothesis for bottom-up saliency</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Gao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">V</forename>
                                    <surname>Mahadevan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>Vasconcelos</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m"
                                   coords="20,510.38,547.49,48.89,8.63;20,57.23,559.00,502.05,8.63;20,57.23,570.51,124.92,8.63">Proceedings of the Advances in Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference on Neural Information Processing Systems</title>
                            <meeting>the Advances in Neural Information Processing Systems 20, the Twenty-First Annual Conference on Neural Information Processing Systems
                                <address>
                                    <addrLine>Vancouver, BC, Canada</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2007-12-06">3-6 December 2007</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,582.01,516.09,8.63;20,57.23,593.52,297.86,8.63" xml:id="b83">
                        <analytic>
                            <title level="a" type="main" coords="20,145.72,582.01,241.37,8.63">Using local regression kernels for statistical object detection</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Seo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Milanfar</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="20,406.21,582.01,153.06,8.63;20,57.23,593.52,127.03,8.63">Proceedings of the IEEE International Conference on Image Processing</title>
                            <meeting>the IEEE International Conference on Image Processing
                                <address>
                                    <addrLine>San Diego, CA, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2008-10">October 2008</date>
                                <biblScope unit="page" from="12" to="15"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,604.91,517.21,8.74;20,57.23,616.42,122.71,8.74" xml:id="b84">
                        <analytic>
                            <title level="a" type="main" coords="20,213.52,605.03,256.75,8.63">Spatio-temporal saliency networks for dynamic saliency prediction</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Bak</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Kocak</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Erdem</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Erdem</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="20,476.77,604.91,83.63,8.55">IEEE Trans. Multimed</title>
                            <imprint>
                                <biblScope unit="volume">20</biblScope>
                                <biblScope unit="page" from="1688" to="1698"/>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,628.04,516.09,8.63;20,57.23,639.55,497.04,8.63" xml:id="b85">
                        <analytic>
                            <title level="a" type="main" coords="20,237.39,628.04,306.23,8.63">Transfer learning with deep networks for saliency prediction in natural vide</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Chaabouni</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Benois-Pineau</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <forename type="middle">B</forename>
                                    <surname>Amar</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="20,57.23,639.55,324.35,8.63">Proceedings of the 2016 IEEE International Conference on Image Processing (ICIP)</title>
                            <meeting>the 2016 IEEE International Conference on Image Processing (ICIP)
                                <address>
                                    <addrLine>Phoenix, AZ, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2016-09">September 2016</date>
                                <biblScope unit="page" from="25" to="28"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="20,43.19,651.06,516.09,8.63;20,57.23,662.56,502.04,8.63;20,57.23,674.07,54.06,8.63"
                            xml:id="b86">
                        <analytic>
                            <title level="a" type="main"
                                   coords="20,334.10,651.06,225.18,8.63;20,57.23,662.56,76.27,8.63">Learning Gaze Transitions from Depth to Improve Video Saliency Estimation</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Leifman</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Rudoy</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Swedish</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Bayro-Corrochano</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Raskar</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="20,150.88,662.56,327.39,8.63">Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV)</title>
                            <meeting>the 2017 IEEE International Conference on Computer Vision (ICCV)
                                <address>
                                    <addrLine>Venice, Italy</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2017-10">October 2017</date>
                                <biblScope unit="page" from="22" to="29"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,685.46,517.21,8.74;20,57.23,696.97,221.04,8.74" xml:id="b87">
                        <analytic>
                            <title level="a" type="main" coords="20,197.90,685.58,309.87,8.63">Video Saliency Prediction using Spatiotemporal Residual Attentive Networks</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Q</forename>
                                    <surname>Lai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Sun</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Shen</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="20,515.28,685.46,45.11,8.55;20,57.23,696.97,49.65,8.55">IEEE Trans. Image Process</title>
                            <imprint>
                                <biblScope unit="volume">29</biblScope>
                                <biblScope unit="page" from="1113" to="1126"/>
                                <date type="published" when="2019">2019</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,708.47,517.21,8.74;20,57.23,720.10,68.75,8.63" xml:id="b88">
                        <monogr>
                            <title level="m" type="main" coords="20,218.88,708.59,290.92,8.63">Recurrent Mixture Density Network for Spatiotemporal Visual Attention</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Bazzani</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Larochelle</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Torresani</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1603.08199</idno>
                            <imprint/>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="20,43.19,731.49,516.09,8.74;20,57.23,743.11,91.17,8.63" xml:id="b89">
                        <monogr>
                            <title level="m" type="main" coords="20,165.06,731.60,365.23,8.63">Predicting Video Saliency with Object-to-Motion CNN and Two-layer Convolutional LSTM</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Jiang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Xu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1709.06316</idno>
                            <imprint/>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,43.19,98.72,516.09,8.63;21,57.23,110.23,503.16,8.63;21,56.78,121.74,64.98,8.63"
                            xml:id="b90">
                        <analytic>
                            <title level="a" type="main" coords="21,137.46,98.72,403.81,8.63">Going from Image to Video Saliency: Augmenting Image Salience with Dynamic Attentional Push</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Gorji</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Clark</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="21,57.23,110.23,399.51,8.63">Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
                            <meeting>the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
                                <address>
                                    <addrLine>Salt Lake City, UT, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2018-06">June 2018</date>
                                <biblScope unit="page" from="18" to="23"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,43.19,133.25,516.09,8.63;21,57.23,144.75,503.16,8.63;21,56.78,156.26,64.98,8.63"
                            xml:id="b91">
                        <analytic>
                            <title level="a" type="main" coords="21,262.84,133.25,280.68,8.63">Revisiting Video Saliency: A Large-Scale Benchmark and a New Model</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Shen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Fang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Cheng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Borji</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="21,57.23,144.75,399.51,8.63">Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
                            <meeting>the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
                                <address>
                                    <addrLine>Salt Lake City, UT, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2018-06">June 2018</date>
                                <biblScope unit="page" from="18" to="23"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="21,43.19,167.65,517.21,8.74;21,57.23,179.27,77.88,8.63" xml:id="b92">
                        <analytic>
                            <title level="a" type="main" coords="21,100.10,167.77,310.41,8.63">A Spatial-Temporal Recurrent Neural Network for Video Saliency Prediction</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="21,418.30,167.65,100.27,8.55">IEEE Trans. Image Process</title>
                            <imprint>
                                <biblScope unit="volume">30</biblScope>
                                <biblScope unit="page" from="572" to="587"/>
                                <date type="published" when="2020">2020</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="21,43.19,190.78,516.08,8.63;21,57.23,202.17,266.75,8.74" xml:id="b93">
                        <analytic>
                            <title level="a" type="main"
                                   coords="21,228.35,190.78,330.92,8.63;21,57.23,202.29,33.84,8.63">Saliency Prediction on Omnidirectional Image With Generative Adversarial Imitation Learning</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Xu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Tao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Duan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="21,98.08,202.17,96.76,8.55">IEEE Trans. Image Process</title>
                            <imprint>
                                <biblScope unit="volume">30</biblScope>
                                <biblScope unit="page" from="2087" to="2102"/>
                                <date type="published" when="2021">2021</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="21,43.19,213.79,516.09,8.63;21,57.23,225.30,307.38,8.63" xml:id="b94">
                        <analytic>
                            <title level="a" type="main" coords="21,252.82,213.79,215.21,8.63">Saliency Detection via Graph-Based Manifold Ranking</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Lu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Ruan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <forename type="middle">H</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="21,485.83,213.79,73.45,8.63;21,57.23,225.30,156.29,8.63">Proceedings of the Computer Vision &amp; Pattern Recognition</title>
                            <meeting>the Computer Vision &amp; Pattern Recognition
                                <address>
                                    <addrLine>Portland, OR, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2013-06">June 2013</date>
                                <biblScope unit="page" from="23" to="28"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="21,43.19,236.69,481.61,8.74" xml:id="b95">
                        <monogr>
                            <title level="m" type="main" coords="21,161.08,236.81,242.42,8.63">A Large Scale Fixation Dataset for Boosting Saliency Research</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Borji</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Itti</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Cat</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1505.03581</idno>
                            <imprint>
                                <date type="published" when="2000">2000</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="21,43.19,248.31,516.08,8.63;21,57.23,259.82,385.21,8.63" xml:id="b96">
                        <analytic>
                            <title level="a" type="main" coords="21,226.63,248.31,268.56,8.63">Analysis of Scores, Datasets, and Models in Visual Saliency Prediction</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Borji</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <forename type="middle">R</forename>
                                    <surname>Tavakoli</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <forename type="middle">N</forename>
                                    <surname>Sihite</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Itti</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="21,511.89,248.31,47.38,8.63;21,57.23,259.82,226.12,8.63">Proceedings of the IEEE International Conference on Computer Vision</title>
                            <meeting>the IEEE International Conference on Computer Vision
                                <address>
                                    <addrLine>Columbus, OH, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2014-06">June 2014</date>
                                <biblScope unit="page" from="23" to="28"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,43.19,271.33,516.09,8.63;21,57.23,282.83,502.04,8.63;21,57.23,294.34,20.17,8.63"
                            xml:id="b97">
                        <analytic>
                            <title level="a" type="main" coords="21,249.07,271.33,293.01,8.63">Emotional Attention: A Study of Image Sentiment and Visual Attention</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Fan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Shen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Ming</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <forename type="middle">L</forename>
                                    <surname>Koenig</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Qi</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="21,57.23,282.83,357.59,8.63">Proceedings of the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
                            <meeting>the 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition
                                <address>
                                    <addrLine>Salt Lake City, UT, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2018-06">June 2018</date>
                                <biblScope unit="page" from="18" to="23"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,43.19,305.85,517.65,8.63;21,56.94,317.24,154.68,8.74;21,35.72,328.86,11.77,8.63"
                            xml:id="b98">
                        <analytic>
                            <title level="a" type="main" coords="21,257.62,305.85,298.67,8.63">Clustering of Gaze During Dynamic Scene Viewing is Predicted by Motion</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <forename type="middle">K</forename>
                                    <surname>Mital</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Smith</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <forename type="middle">L</forename>
                                    <surname>Hill</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Henderson</surname>
                                </persName>
                            </author>
                            <idno>100</idno>
                        </analytic>
                        <monogr>
                            <title level="j" coords="21,56.94,317.24,56.08,8.55">Cogn. Comput</title>
                            <imprint>
                                <biblScope unit="volume">3</biblScope>
                                <biblScope unit="page" from="5" to="24"/>
                                <date type="published" when="2011">2011</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,328.75,511.79,8.74;21,56.69,340.25,250.80,8.74;21,35.72,351.88,11.77,8.63"
                            xml:id="b99">
                        <analytic>
                            <title level="a" type="main" coords="21,165.84,328.86,367.43,8.63">Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for Visual Recognition</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Mathe</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Sminchisescu</surname>
                                </persName>
                            </author>
                            <idno>101</idno>
                        </analytic>
                        <monogr>
                            <title level="j" coords="21,540.17,328.75,19.11,8.55;21,56.69,340.25,125.30,8.55">IEEE Trans. Pattern Anal. Mach. Intell</title>
                            <imprint>
                                <biblScope unit="volume">37</biblScope>
                                <biblScope unit="page" from="1408" to="1424"/>
                                <date type="published" when="2015">2015</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,351.88,511.79,8.63;21,57.23,363.38,443.30,8.63;21,35.72,374.89,11.77,8.63"
                            xml:id="b100">
                        <analytic>
                            <title level="a" type="main" coords="21,230.91,351.88,262.50,8.63">Deepvs: A deep learning based video saliency prediction approach</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Jiang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Xu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Qiao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="21,511.02,351.88,48.26,8.63;21,57.23,363.38,223.61,8.63">Proceedings of the European Conference on Computer Vision (ECCV)</title>
                            <meeting>the European Conference on Computer Vision (ECCV)
                                <address>
                                    <addrLine>Munich, Germany</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2018-09-14">8-14 September 2018</date>
                                <biblScope unit="page">102</biblScope>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,374.77,511.79,8.74;21,57.23,386.40,274.29,8.63;21,35.72,397.90,11.77,8.63"
                            xml:id="b101">
                        <monogr>
                            <title level="m" type="main" coords="21,191.72,374.77,293.63,8.55">A Benchmark of Computational Models of Saliency to Predict Human Fixations</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Judd</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Durand</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Torralba</surname>
                                </persName>
                            </author>
                            <idno>MIT-CSAIL-TR-2012-001</idno>
                            <imprint>
                                <date type="published" when="2012">2012</date>
                                <biblScope unit="page">103</biblScope>
                                <pubPlace>Cambridge, MA, USA</pubPlace>
                            </imprint>
                            <respStmt>
                                <orgName>MIT Libraries</orgName>
                            </respStmt>
                        </monogr>
                        <note type="report_type">Technical Report</note>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,397.90,511.80,8.63;21,57.23,409.29,235.49,8.74;21,35.72,420.92,11.77,8.63"
                            xml:id="b102">
                        <analytic>
                            <title level="a" type="main"
                                   coords="21,171.21,397.90,388.07,8.63;21,57.23,409.41,20.62,8.63">Quantitative Analysis of Human-Model Agreement in Visual Saliency Modeling: A Comparative Study</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Borji</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <forename type="middle">N</forename>
                                    <surname>Sihite</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Itti</surname>
                                </persName>
                            </author>
                            <idno>104</idno>
                        </analytic>
                        <monogr>
                            <title level="j" coords="21,84.76,409.29,96.76,8.55">IEEE Trans. Image Process</title>
                            <imprint>
                                <biblScope unit="volume">22</biblScope>
                                <biblScope unit="page" from="55" to="69"/>
                                <date type="published" when="2013">2013</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,420.80,513.36,8.74;21,57.23,432.42,41.48,8.63;21,35.72,443.93,11.77,8.63"
                            xml:id="b103">
                        <analytic>
                            <title level="a" type="main" coords="21,202.20,420.92,237.69,8.63">Components of bottom-up gaze allocation in natural images</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Peters</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Iyer</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Itti</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Koch</surname>
                                </persName>
                            </author>
                            <idno>105</idno>
                        </analytic>
                        <monogr>
                            <title level="j" coords="21,447.02,420.80,32.22,8.55">Vis. Res</title>
                            <imprint>
                                <biblScope unit="volume">45</biblScope>
                                <biblScope unit="page" from="2397" to="2416"/>
                                <date type="published" when="2005">2005</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,443.82,512.91,8.74;21,57.23,455.44,73.40,8.63;21,35.72,466.94,11.77,8.63"
                            xml:id="b104">
                        <analytic>
                            <title level="a" type="main" coords="21,200.42,443.93,239.83,8.63">The Earth Mover's Distance as a Metric for Image Retrieval</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Rubner</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Tomasi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Guibas</surname>
                                </persName>
                            </author>
                            <idno>106</idno>
                        </analytic>
                        <monogr>
                            <title level="j" coords="21,447.67,443.82,74.97,8.55">Int. J. Comput. Vis</title>
                            <imprint>
                                <biblScope unit="volume">40</biblScope>
                                <biblScope unit="page" from="99" to="121"/>
                                <date type="published" when="2000">2000</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,466.94,511.79,8.63;21,56.96,478.34,298.86,8.74;21,35.72,489.96,11.77,8.63"
                            xml:id="b105">
                        <analytic>
                            <title level="a" type="main"
                                   coords="21,249.56,466.94,309.72,8.63;21,56.96,478.45,119.04,8.63">Exploiting inter-image similarity and ensemble of extreme learners for fixation prediction using deep features</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <forename type="middle">R</forename>
                                    <surname>Tavakoli</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Borji</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Laaksonen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Rahtu</surname>
                                </persName>
                            </author>
                            <idno>107</idno>
                        </analytic>
                        <monogr>
                            <title level="j" coords="21,182.54,478.34,61.78,8.55">Neurocomputing</title>
                            <imprint>
                                <biblScope unit="volume">244</biblScope>
                                <biblScope unit="page" from="10" to="18"/>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,489.96,512.91,8.63;21,57.23,501.47,101.47,8.63;21,35.72,512.97,11.77,8.63"
                            xml:id="b106">
                        <analytic>
                            <title level="a" type="main" coords="21,134.56,489.96,222.31,8.63">Variational Laws of Visual Attention for Dynamic Scenes</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Zanca</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Gori</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="21,374.15,489.96,113.93,8.63">Proceedings of the NIPS 2017</title>
                            <meeting>the NIPS 2017
                                <address>
                                    <addrLine>Long Beach, CA, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2017-12">December 2017</date>
                                <biblScope unit="page">108</biblScope>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,512.97,513.36,8.63;21,57.23,524.36,224.58,8.74;21,35.72,535.99,11.77,8.63"
                            xml:id="b107">
                        <analytic>
                            <title level="a" type="main" coords="21,224.49,512.97,332.36,8.63">Learning Discriminative Subspaces on Random Contrasts for Image Saliency Analysis</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Shu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Jia</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Tian</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Huang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j"
                                   coords="21,57.23,524.36,143.35,8.55">IEEE Trans. Neural Netw. Learn. Syst</title>
                            <imprint>
                                <biblScope unit="volume">28</biblScope>
                                <biblScope unit="page">109</biblScope>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,535.99,511.79,8.63;21,57.23,547.49,436.42,8.63;21,35.72,559.00,11.77,8.63"
                            xml:id="b108">
                        <analytic>
                            <title level="a" type="main" coords="21,201.69,535.99,342.83,8.63">Fast and efficient saliency detection using sparse sampling and kernel density estimation</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <forename type="middle">R</forename>
                                    <surname>Tavakoli</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Rahtu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Heikkilä</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="21,57.23,547.49,250.30,8.63">Proceedings of the Scandinavian Conference on Image Analysis</title>
                            <meeting>the Scandinavian Conference on Image Analysis
                                <address>
                                    <addrLine>Ystad, Sweden</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2011-05-27">23-27 May 2011</date>
                                <biblScope unit="page">110</biblScope>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,559.00,511.79,8.63;21,57.23,570.51,375.49,8.63;21,35.72,582.01,11.77,8.63"
                            xml:id="b109">
                        <analytic>
                            <title level="a" type="main" coords="21,202.45,559.00,265.69,8.63">A model of bottom-up visual attention using cortical magnification</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Aboudib</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">V</forename>
                                    <surname>Gripon</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Coppin</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="21,485.57,559.00,73.71,8.63;21,57.23,570.51,171.46,8.63">Proceedings of the IEEE International Conference on Acoustics</title>
                            <meeting>the IEEE International Conference on Acoustics
                                <address>
                                    <addrLine>South Brisbane, QLD, Australia</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2015-04-24">19-24 April 2015</date>
                                <biblScope unit="page">111</biblScope>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,581.90,512.91,8.74;21,56.78,593.52,86.85,8.63;21,35.72,605.03,11.77,8.63"
                            xml:id="b110">
                        <analytic>
                            <title level="a" type="main" coords="21,220.70,582.01,133.71,8.63">Context-aware saliency detection</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Goferman</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Zelnik-Manor</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Tal</surname>
                                </persName>
                            </author>
                            <idno>112</idno>
                        </analytic>
                        <monogr>
                            <title level="j" coords="21,363.27,581.90,157.44,8.55">IEEE Trans. Pattern Anal. Mach. Intell</title>
                            <imprint>
                                <biblScope unit="volume">34</biblScope>
                                <biblScope unit="page" from="1915" to="1926"/>
                                <date type="published" when="2011">2011</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,605.03,511.79,8.63;21,57.23,616.42,281.86,8.74;21,35.72,628.04,11.77,8.63"
                            xml:id="b111">
                        <analytic>
                            <title level="a" type="main"
                                   coords="21,292.15,605.03,267.12,8.63;21,57.23,616.53,159.09,8.63">On the relationship between optical variability, visual saliency, and eye fixations: A computational approach</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Garcia-Diaz</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">V</forename>
                                    <surname>Leboran</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <forename type="middle">R</forename>
                                    <surname>Fdez-Vidal</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Pardo</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="21,223.48,616.42,21.50,8.55">J. Vis</title>
                            <imprint>
                                <biblScope unit="volume">12</biblScope>
                                <biblScope unit="page">113</biblScope>
                                <date type="published" when="2012">2012</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,628.04,512.14,8.63;21,57.23,639.43,313.03,8.74;21,35.72,651.06,11.77,8.63"
                            xml:id="b112">
                        <analytic>
                            <title level="a" type="main"
                                   coords="21,292.10,628.04,267.52,8.63;21,57.23,639.55,177.87,8.63">Scene recognition through visual attention and image features: A comparison between sift and surf approaches</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Lopez-Garcia</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <forename type="middle">R</forename>
                                    <surname>Fdez-Vidal</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Pardo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Dosil</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="21,242.21,639.43,56.46,8.55">Object Recognit</title>
                            <imprint>
                                <biblScope unit="volume">4</biblScope>
                                <biblScope unit="page">114</biblScope>
                                <date type="published" when="2011">2011</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,651.06,511.79,8.63;21,57.23,662.56,419.11,8.63;21,35.72,674.07,11.77,8.63"
                            xml:id="b113">
                        <analytic>
                            <title level="a" type="main" coords="21,162.79,651.06,306.37,8.63">Video Saliency Incorporating Spatiotemporal Cues and Uncertainty Weighting</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Fang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Lin</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="21,486.67,651.06,72.61,8.63;21,57.23,662.56,271.23,8.63">Proceedings of the 2013 IEEE International Conference on Multimedia and Expo (ICME)</title>
                            <meeting>the 2013 IEEE International Conference on Multimedia and Expo (ICME)
                                <address>
                                    <addrLine>San Jose, CA, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2013-07">July 2013</date>
                                <biblScope unit="page">115</biblScope>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,674.07,511.79,8.63;21,57.23,685.58,492.36,8.63;21,35.72,697.08,11.77,8.63"
                            xml:id="b114">
                        <analytic>
                            <title level="a" type="main" coords="21,270.17,674.07,274.47,8.63">Learning Video Saliency from Human Gaze Using Candidate Selection</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Rudoy</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">B</forename>
                                    <forename type="middle">G</forename>
                                    <surname>Dan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Shechtman</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Zelnik-Manor</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="21,57.23,685.58,341.28,8.63">Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
                            <meeting>the 2013 IEEE Conference on Computer Vision and Pattern Recognition
                                <address>
                                    <addrLine>Portland, OR, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2013-06">June 2013</date>
                                <biblScope unit="page">116</biblScope>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,696.97,512.91,8.74;21,57.23,708.47,113.75,8.74;21,35.72,720.10,11.77,8.63"
                            xml:id="b115">
                        <analytic>
                            <title level="a" type="main" coords="21,293.04,697.08,112.10,8.63">Dynamic whitening saliency</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">V</forename>
                                    <surname>Leboran</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Garcia-Diaz</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <forename type="middle">R</forename>
                                    <surname>Fdez-Vidal</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Pardo</surname>
                                </persName>
                            </author>
                            <idno>117</idno>
                        </analytic>
                        <monogr>
                            <title level="j" coords="21,411.67,696.97,148.72,8.55">IEEE Trans. Pattern Anal. Mach. Intell</title>
                            <imprint>
                                <biblScope unit="volume">39</biblScope>
                                <biblScope unit="page" from="893" to="907"/>
                                <date type="published" when="2016">2016</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,720.10,511.79,8.63;21,56.88,731.49,293.92,8.74;21,35.72,743.11,11.77,8.63"
                            xml:id="b116">
                        <analytic>
                            <title level="a" type="main"
                                   coords="21,253.28,720.10,306.00,8.63;21,56.88,731.60,183.30,8.63">A Novel Multiresolution Spatiotemporal Saliency Detection Model and Its Applications in Image and Video Compression</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <forename type="middle">F</forename>
                                    <surname>Dedieu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Gazin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Rigolet</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Galibert</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="21,247.52,731.49,36.04,8.55">Oncogene</title>
                            <imprint>
                                <biblScope unit="volume">3</biblScope>
                                <biblScope unit="page">118</biblScope>
                                <date type="published" when="1988">1988</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="21,47.48,743.11,511.79,8.63;21,57.23,754.62,424.16,8.63;21,35.72,766.12,11.77,8.63"
                            xml:id="b117">
                        <analytic>
                            <title level="a" type="main" coords="21,284.18,743.11,211.06,8.63">How many bits does it take for a stimulus to be salient</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <forename type="middle">H</forename>
                                    <surname>Khatoonabadi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>Vasconcelos</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">I</forename>
                                    <forename type="middle">V</forename>
                                    <surname>Bajic</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <forename type="middle">Y</forename>
                                    <surname>Shan</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="21,511.89,743.11,47.38,8.63;21,57.23,754.62,282.48,8.63">Proceedings of the 2015 IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
                            <meeting>the 2015 IEEE Conference on Computer Vision &amp; Pattern Recognition
                                <address>
                                    <addrLine>Boston, MA, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2015-06">June 2015</date>
                                <biblScope unit="page">119</biblScope>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="21,47.48,766.01,474.27,8.74;22,35.72,98.72,11.77,8.63" xml:id="b118">
                        <analytic>
                            <title level="a" type="main" coords="21,142.00,766.12,261.84,8.63">Static and space-time visual saliency detection by self-resemblance</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Seo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Milanfar</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="21,410.63,766.01,21.50,8.55">J. Vis</title>
                            <imprint>
                                <biblScope unit="volume">9</biblScope>
                                <biblScope unit="page">120</biblScope>
                                <date type="published" when="2009">2009</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="22,47.48,98.72,511.79,8.63;22,57.23,110.12,236.19,8.74;22,35.72,121.74,11.77,8.63"
                            xml:id="b119">
                        <analytic>
                            <title level="a" type="main"
                                   coords="22,299.82,98.72,259.46,8.63;22,57.23,110.23,81.86,8.63">On computational modeling of visual saliency: Examining what's right, and what's left</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <forename type="middle">D B</forename>
                                    <surname>Bruce</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Wloka</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>Frosst</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Rahman</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <forename type="middle">K</forename>
                                    <surname>Tsotsos</surname>
                                </persName>
                            </author>
                            <idno>121</idno>
                        </analytic>
                        <monogr>
                            <title level="j" coords="22,144.89,110.12,32.01,8.55">Vis. Res</title>
                            <imprint>
                                <biblScope unit="volume">116</biblScope>
                                <biblScope unit="page" from="95" to="112"/>
                                <date type="published" when="2015">2015</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="22,47.48,121.74,511.79,8.63;22,57.23,133.25,413.45,8.63;22,35.72,144.75,11.77,8.63"
                            xml:id="b120">
                        <analytic>
                            <title level="a" type="main" coords="22,231.65,121.74,263.10,8.63">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Sun</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Shrivastava</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Singh</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Gupta</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="22,511.74,121.74,47.53,8.63;22,57.23,133.25,276.27,8.63">Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV)</title>
                            <meeting>the 2017 IEEE International Conference on Computer Vision (ICCV)
                                <address>
                                    <addrLine>Venice, Italy</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2017-10">October 2017</date>
                                <biblScope unit="page">122</biblScope>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="22,47.48,144.64,507.89,8.74;22,35.72,156.26,11.77,8.63" xml:id="b121">
                        <analytic>
                            <title level="a" type="main" coords="22,156.98,144.75,280.54,8.63">How saliency, faces, and sound influence gaze in dynamic social scenes</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Coutrot</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>Guyader</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="22,444.24,144.64,21.50,8.55">J. Vis</title>
                            <imprint>
                                <biblScope unit="volume">14</biblScope>
                                <biblScope unit="issue">5</biblScope>
                                <biblScope unit="page">123</biblScope>
                                <date type="published" when="2014">2014</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="22,47.48,156.26,511.79,8.63;22,57.23,167.65,286.62,8.74;22,35.72,179.27,11.77,8.63"
                            xml:id="b122">
                        <monogr>
                            <title level="m" type="main"
                                   coords="22,444.16,156.26,115.12,8.63;22,57.23,167.77,165.60,8.63">Understanding Infographics through Textual and Visual Tag Prediction</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Bylinskii</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Alsheikh</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Madan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Recasens</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Zhong</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">H</forename>
                                    <surname>Pfister</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Durand</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Oliva</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1709.09215.124</idno>
                            <imprint/>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="22,47.48,179.16,512.91,8.74;22,56.59,190.66,142.22,8.74;22,35.72,202.29,11.77,8.63"
                            xml:id="b123">
                        <analytic>
                            <title level="a" type="main" coords="22,249.69,179.27,232.62,8.63">Overt attention in natural scenes: Objects dominate features</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Stoll</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">M</forename>
                                    <surname>Thrun</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Nuthmann</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Einhäuser</surname>
                                </persName>
                            </author>
                            <idno>125</idno>
                        </analytic>
                        <monogr>
                            <title level="j" coords="22,488.79,179.16,71.60,8.55;22,56.59,190.66,30.18,8.55">Vis. Res. An. Int. J. Vis. Sci</title>
                            <imprint>
                                <biblScope unit="volume">107</biblScope>
                                <biblScope unit="page" from="36" to="48"/>
                                <date type="published" when="2015">2015</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="22,47.48,202.29,511.97,8.63;22,57.23,213.79,502.05,8.63;22,56.78,225.30,218.03,8.63;22,35.72,236.81,11.77,8.63"
                            xml:id="b124">
                        <analytic>
                            <title level="a" type="main"
                                   coords="22,270.56,202.29,288.89,8.63;22,57.23,213.79,166.52,8.63">Saliency Prediction via Multi-Level Features and Deep Supervision for Children with Autism Spectrum Disorder</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Wei</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Huang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Nebout</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">O</forename>
                                    <forename type="middle">L</forename>
                                    <surname>Meur</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="22,242.20,213.79,317.08,8.63;22,56.78,225.30,82.37,8.63">Proceedings of the 2019 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)</title>
                            <meeting>the 2019 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW)
                                <address>
                                    <addrLine>Shanghai, China</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2019-07-12">8-12 July 2019</date>
                                <biblScope unit="page">126</biblScope>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="22,47.48,236.81,512.14,8.63;22,57.23,248.20,267.54,8.74;22,35.72,259.82,11.77,8.63"
                            xml:id="b125">
                        <monogr>
                            <title level="m" type="main"
                                   coords="22,268.13,236.81,291.49,8.63;22,57.23,248.31,102.39,8.63">Neonatal seizure detection from raw multi-channel EEG using a fully convolutional architecture</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>O'shea</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Lightbody</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">G</forename>
                                    <surname>Boylan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Temko</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:2105.13854</idno>
                            <idno>127</idno>
                            <imprint/>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="22,47.48,259.71,512.91,8.74;22,57.23,271.33,68.75,8.63;22,35.72,282.83,11.77,8.63"
                            xml:id="b126">
                        <monogr>
                            <title level="m" type="main" coords="22,249.99,259.82,257.59,8.63">Faster gaze prediction with dense networks and Fisher pruning</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Theis</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">I</forename>
                                    <surname>Korshunova</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Tejani</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Huszár</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1801.05787.128</idno>
                            <imprint/>
                        </monogr>
                    </biblStruct>
                    <biblStruct
                            coords="22,47.48,282.83,512.91,8.63;22,57.23,294.34,163.22,8.63;22,35.72,305.85,11.77,8.63"
                            xml:id="b127">
                        <analytic>
                            <title level="a" type="main" coords="22,233.47,282.83,188.96,8.63">Inferring Shared Attention in Social Scene Videos</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Fan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Wei</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <forename type="middle">C</forename>
                                    <surname>Zhu</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="22,439.50,282.83,115.70,8.63">Proceedings of the IEEE CVPR</title>
                            <meeting>the IEEE CVPR
                                <address>
                                    <addrLine>Salt Lake City, UT, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2018-06">June 2018</date>
                                <biblScope unit="page">129</biblScope>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="22,47.48,305.85,511.79,8.63;22,57.23,317.24,160.18,8.74" xml:id="b128">
                        <monogr>
                            <title level="m" type="main"
                                   coords="22,260.49,305.85,298.79,8.63;22,57.23,317.36,39.36,8.63">Understanding Human Gaze Communication by Spatio-Temporal Graph Reasoning</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Fan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">W</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Huang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">X</forename>
                                    <surname>Tang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <forename type="middle">C</forename>
                                    <surname>Zhu</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1909.02144</idno>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                                <pubPlace>arVix</pubPlace>
                            </imprint>
                        </monogr>
                    </biblStruct>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>