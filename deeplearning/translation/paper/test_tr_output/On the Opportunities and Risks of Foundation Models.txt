On the Opportunities and Risks of Foundation Models
AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) trained on broad data (generally using self-supervision at scale) that can be adapted to a wide range of downstream tasks. 
We call these models foundation models to underscore their critically central yet incomplete character. 
This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotic manipulation, reasoning, human interaction) and technical principles (e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). 
Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities, and their effectiveness across so many tasks incentivizes homogenization. 
Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. 
Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. 
To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature. 
1 INTRODUCTION
This report investigates an emerging paradigm for building artificial intelligence (AI) systems based on a general class of models which we term foundation models. 2 
A foundation model is any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks; current examples include BERT [Devlin et al. 2019] , GPT-3 [Brown et al. 2020] , and CLIP [Radford et al. 2021] . 
From a technological point of view, foundation models are not new -they are based on deep neural networks and self-supervised learning, both of which have existed for decades. 
However, the sheer scale and scope of foundation models from the last few years have stretched our imagination of what is possible; for example, GPT-3 has 175 billion parameters and can be adapted via natural language prompts to do a passable job on a wide range of tasks despite not being trained explicitly to do many of those tasks [Brown et al. 2020] . 
At the same time, existing foundation models have the potential to accentuate harms, and their characteristics are in general poorly understood. 
Given their impending widespread deployment, they have become a topic of intense scrutiny [Bender et al. 2021 ]. 
1.1 Emergence and homogenization
The significance of foundation models can be summarized by two words: emergence and homogenization. 
Emergence means that the behavior of a system is implicitly induced rather than explicitly constructed; it is both the source of scientific excitement and anxiety about unanticipated consequences. 
Homogenization indicates the consolidation of methodologies for building machine learning systems across a wide range of applications; it provides strong leverage towards many tasks but also creates single points of failure. 
To better appreciate emergence and homogenization, let us reflect on their rise in AI research over the last 30 years. 
Fig. 1 . 
The story of AI has been one of increasing emergence and homogenization. 
With the introduction of machine learning, how a task is performed emerges (is inferred automatically) from examples; with deep learning, the high-level features used for prediction emerge; and with foundation models, even advanced functionalities such as in-context learning emerge. 
At the same time, machine learning homogenizes learning algorithms (e.g., logistic regression), deep learning homogenizes model architectures (e.g., Convolutional Neural Networks), and foundation models homogenizes the model itself (e.g., GPT-3). 
Machine learning. 
Most AI systems today are powered by machine learning, where predictive models are trained on historical data and used to make future predictions. 
The rise of machine learning within AI started in the 1990s, representing a marked shift from the way AI systems were built previously: rather than specifying how to solve a task, a learning algorithm would induce it based on data -i.e., the how emerges from the dynamics of learning. 
Machine learning also represented a step towards homogenization: a wide range of applications could now be powered by a single generic learning algorithm such as logistic regression. 
Despite the ubiquity of machine learning within AI, semantically complex tasks in natural language processing (NLP) and computer vision such as question answering or object recognition, where the inputs are sentences or images, still required domain experts to perform "feature engineering" -that is, writing domain-specific logic to convert raw data into higher-level features (e.g., SIFT [Lowe 1999 ] in computer vision) that were more suitable for popular machine learning methods. 
Deep learning. 
Around 2010, a revival of deep neural networks under the moniker of deep learning [LeCun et al. 2015 ] started gaining traction in the field of machine learning. 
Deep learning was fueled by larger datasets, more computation (notably, the availability of GPUs), and greater audacity. 
Deep neural networks would be trained on the raw inputs (e.g., pixels), and higher-level features would emerge through training (a process dubbed "representation learning"). 
This led to massive performance gains on standard benchmarks, for example, in the seminal work of AlexNet [Krizhevsky et al. 2012 ] on the ImageNet dataset [Deng et al. 2009] . 
Deep learning also reflected a further shift towards homogenization: rather than having bespoke feature engineering pipelines for each application, the same deep neural network architecture could be used for many applications. 
Foundation models. 
Foundation models have taken shape most strongly in NLP, so we focus our story there for the moment. 
That said, much as deep learning was popularized in computer vision but exists beyond it, we understand foundation models as a general paradigm of AI, rather than specific to NLP in any way. 
By the end of 2018, the field of NLP was about to undergo another seismic change, marking the beginning of the era of foundation models. 
On a technical level, foundation models are enabled by transfer learning [Thrun 1998 ] and scale. 
The idea of transfer learning is to take the "knowledge" learned from one task (e.g., object recognition in images) and apply it to another task (e.g., activity recognition in videos). 
Within deep learning, pretraining is the dominant approach to transfer learning: a model is trained on a surrogate task (often just as a means to an end) and then adapted to the downstream task of interest via fine-tuning. 
Transfer learning is what makes foundation models possible, but scale is what makes them powerful. 
Scale required three ingredients: (i) improvements in computer hardware -e.g., GPU throughput and memory have increased 10× over the last four years ( §4.5: systems); (ii) the development of the Transformer model architecture [Vaswani et al. 2017 ] that leverages the parallelism of the hardware to train much more expressive models than before ( §4.1: modeling); and (iii) the availability of much more training data. 
The importance of the availability of data and the ability to harness it cannot be underestimated. 
Transfer learning with annotated datasets has been common practice for at least a decade, for example, pretraining on the ImageNet dataset [Deng et al. 2009] for image classification in the computer vision community. 
However, the non-trivial cost of annotation imposes a practical limit on the benefits of pretraining. 
In self-supervised learning on the other hand, the pretraining task is derived automatically from unannotated data. 3 
For example, the masked language modeling task used to train BERT [Devlin et al. 2019 ] is to predict a missing word in a sentence given its surrounding context (e.g., I like sprouts). 
Self-supervised tasks are not only more scalable, only depending on unlabeled data, but they are designed to force the model to predict parts of the inputs, making them richer and potentially more useful than models trained on a more limited label space. 
There had been considerable progress in self-supervised learning dating back to word embeddings [Turian et al. 2010; Mikolov et al. 2013; Pennington et al. 2014] , which associated each word with a context-independent vector, provided the basis for a wide range of NLP models. 
Shortly thereafter, self-supervised learning based on autoregressive language modeling (predict the next word given the previous words) [ Dai and Le 2015] became popular. 
This produced models that represented words in context, such as GPT [Radford et al. 2018] , ELMo [Peters et al. 2018] , and ULMFiT [Howard and Ruder 2018 ]. 4 
he next wave of developments in self-supervised learning -BERT [Devlin et al. 2019 ] GPT-2 [Radford et al. 2019] , RoBERTa [Liu et al. 2019] , T5 [Raffel et al. 2019] , BART [Lewis et al. 2020a ]quickly followed, embracing the Transformer architecture, incorporating more powerful deep bidirectional encoders of sentences, and scaling up to larger models and datasets. 
While one can view this last wave of technical developments purely through the lens of selfsupervised learning, there was a sociological inflection point around the introduction of BERT. 
Before 2019, self-supervised learning with language models was essentially a subarea in NLP, which progressed in parallel to other developments in NLP. 
After 2019, self-supervised learning with language models became more of a substrate of NLP, as using BERT has become the norm. 
The acceptance that a single model could be useful for such a wide range of tasks marks the beginning of the era of foundation models. 
Foundation models have led to an unprecedented level of homogenization: Almost all state-ofthe-art NLP models are now adapted from one of a few foundation models, such as BERT, RoBERTa, BART, T5, etc. 
While this homogenization produces extremely high leverage (any improvements in the foundation models can lead to immediate benefits across all of NLP), it is also a liability; all AI systems might inherit the same problematic biases of a few foundation models [Bolukbasi et al. 2016; Caliskan et al. 2017; Abid et al. 2021, inter alia] ) -see §5.1: fairness, §5.6: ethics for further discussion. 
We are also beginning to see a homogenization across research communities. 
For example, similar Transformer-based sequence modeling approaches are now applied to text [Devlin et al. 2019; Radford et al. 2019; Raffel et al. 2019] , images [Dosovitskiy et al. 2020; Chen et al. 2020d] , speech [Liu et al. 2020d ], tabular data [Yin et al. 2020] , protein sequences [Rives et al. 2021] , organic molecules [Rothchild et al. 2021] , and reinforcement learning [Chen et al. 2021b; Janner et al. 2021] . 
These examples point to a possible future where we have a unified set of tools for developing foundation models across a wide range of modalities [Tamkin et al. 2021b] . 
Besides the homogenization of approaches, we also see the homogenization of actual models across research communities in the form of multimodal models -e.g., foundation models trained on language and vision data [Luo et al. 2020; Kim et al. 2021a; Cho et al. 2021; Ramesh et al. 2021; Radford et al. 2021] . 
Data is naturally multimodal in some domains-e.g., medical images, structured data, clinical text in healthcare ( §3.1: healthcare). 
Thus, multimodal foundation models are a natural way of fusing all the relevant information about a domain, and adapting to tasks that also span multiple modes (Figure 2 ). 
Foundation models have also led to surprising emergence which results from scale. 
For example, GPT-3 [Brown et al. 2020] , with 175 billion parameters compared to GPT-2's 1.5 billion, permits in-context learning, in which the language model can be adapted to a downstream task simply by providing it with a prompt (a natural language description of the task), an emergent property that was neither specifically trained for nor anticipated to arise. 
Homogenization and emergence interact in a potentially unsettling way. 
Homogenization could potentially provide enormous gains for many domains where task-specific data is quite limited -see the opportunities presented in several such domains (e.g., §3.1: healthcare, §3.2: law, §3.3: education); on the other hand, any flaws in the model are blindly inherited by all adapted models ( §5.1: fairness, §5.6: ethics). 
Since the power of foundation models comes from their emergent qualities rather than their explicit construction, existing foundation models are hard to understand ( §4.4: evaluation, §4.10: theory, §4.11: interpretability) and they have unexpected failure modes ( §4.7: security, §4.8: robustness). 
Since emergence generates substantial uncertainty over the capabilities and flaws of foundation models, aggressive homogenization through these models is risky business. 
Derisking is the central challenge in the further development of foundation models from an ethical ( §5.6: ethics) and AI safety ( §4.9: ai-safety) perspective. 
1.1.1 Naming.
We introduce the term foundation models to fill a void in describing the paradigm shift we are witnessing; we briefly recount some of our reasoning for this decision. 
Existing terms (e.g., pretrained model, self-supervised model) partially capture the technical dimension of these models, but fail to capture the significance of the paradigm shift in an accessible manner for those beyond machine learning. 
In particular, foundation model designates a model class that are distinctive in their sociological impact and how they have conferred a broad shift in AI research and deployment. 
In contrast, forms of pretraining and self-supervision that technically foreshadowed foundation models fail to clarify the shift in practices we hope to highlight. 
 3 . Before reasoning about the social impact of foundation models, it is important to understand that they are part of a broader ecosystem that stretches from data creation to deployment. 
At both ends, we highlight the role of people as the ultimate source of data into training of a foundation model, but also as the downstream recipients of any benefits and harms. 
Thoughtful data curation and adaptation should be part of the responsible development of any AI system. 
Finally, note that the deployment of adapted foundation models is a decision separate from their construction, which could be for research. 
Additionally, while many of the iconic foundation models at the time of writing are language models, the term language model is simply too narrow for our purpose: as we describe, the scope of foundation models goes well beyond language. 
We also considered terms such as general-purpose model and multi-purpose model that capture the important aspect that these models can serve multiple downstream tasks, but both fail to capture their unfinished character and the need for adaptation. 
Terms such as task-agnostic model would capture the manner of training, but fail to capture the significant implication to downstream applications. 
We chose the new term foundation models to identify the models and the emerging paradigm that are the subject of this report. 
In particular, the word "foundation" specifies the role these models play: a foundation model is itself incomplete but serves as the common basis from which many task-specific models are built via adaptation. 
We also chose the term "foundation" to connote the significance of architectural stability, safety, and security: poorly-constructed foundations are a recipe for disaster and well-executed foundations are a reliable bedrock for future applications. 
At present, we emphasize that we do not fully understand the nature or quality of the foundation that foundation models provide; we cannot characterize whether the foundation is trustworthy or not. 
Thus, this is a critical problem for researchers, foundation model providers, application developers who rely on foundation models, policymakers, and society at large to address. 
1.2 Social impact and the foundation models ecosystem
Foundation models are scientifically interesting due to their impressive performance and capabilities, but what makes them critical to study is the fact that they are quickly being integrated into realworld deployments of AI systems with far-reaching consequences on people. 
For example, Google search, which boasts 4 billion users, now depends on foundation models like BERT [Devlin et al. 2019] as one of its signals. 5 
e must thus pause and ask: What is the nature of this social impact? 
In this report, we address many aspects of this question: the potential exacerbation of social inequities ( §5.1: fairness), the economic impact due to increased capabilities ( §5.5: economics), the environmental impact due to increased computation demands ( §5.3: environment), potential concerns of amplifying disinformation ( §5.2: misuse), legal ramifications due to powerful generative capabilities ( §5.4: legality), ethical issues resulting from homogenization, and the broader political economy in which foundation models are developed and deployed ( §5.6: ethics) . 
Given the protean nature of foundation models and their unmapped capabilities, how can we responsibly anticipate and address the ethical and societal considerations they raise? 
A recurring theme is that it is easier to reason about the social impact of specific systems deployed to specific users than it is to reason about the social impact of foundation models, which could be adapted to any number of unforeseen downstream systems. 
Before attempting to answer these questions, we need to lay some groundwork. 
First, let us distinguish between research on foundation models and deployment of foundation models. 
Most of what is publicly known is foundation models research -through academic papers, demonstrations, and progress on leaderboards. 
While the production of knowledge can play a vital role in shaping the future, the direct social impact is through the actual deployment of these models, which is governed by proprietary practices on often private data. 
Sometimes the deployment is through new products -e.g., GitHub's Copilot 6 based on OpenAI's Codex model [Chen et al. 2021f ], but often, it is through upgrades to existing products (e.g., Google search using BERT). 
Research models are often not extensively tested and might have unknown failure modes; warning labels should be placed on research models that are not fit to deploy. 
On the other hand, deployed foundation models that actually affect people's lives should be subject to much more rigorous testing and auditing. 
To further understand the research and deployment of foundation models, we must zoom out and consider the full ecosystem that these foundation models inhabit, from data creation to actual deployment. 
It is important to note that the foundation model is only one component (though an increasingly important component) of an AI system. 
Simplifying, we can think about the ecosystem of a foundation model in terms of sequence of stages, extending the training and adaptation stages from before. 7 
Appropriately, as we're interested in social impact, people occupy both ends of the pipeline. 
This ecosystem view allows us to see that different questions about foundation models (e.g., whether a foundation model is ethical) should actually be answered with respect to different stages. 
(1) Data creation: Data creation is fundamentally a human-centric process: all data is created by people and most data is at least implicitly about people. 
Sometimes data is created by people for other people in the form of emails, articles, photos, etc., and sometimes it is a measurement of people (e.g., genomic data) or a measurement of the environment people live in (e.g., satellite images). 
It is important to note that all data has an owner and is created with a purpose (where that purpose may or may not include training a foundation model). 
(2) Data curation: Data is then curated into datasets. 
There is no single natural distribution of data; even the most permissive Internet crawl requires some selection and post-filtering. 
Ensuring data relevance and quality while respecting legal and ethical constraints is critical but challenging. 
While this is recognized in industry, it is underappreciated in AI research ( §4.6: data). 
(3) Training: Training foundation models on these curated datasets 8 is the celebrated centerpiece in AI research, though it is only one of many stages. 
(4) Adaptation: In the context of machine learning research, adaptation is about creating a new model based on the foundation model that performs some task (e.g., document summarization). 
For deployment, adaptation is about creating a system, which requires potentially many different modules, custom rules (e.g., restrictions on the output space) or classifiers (e.g., for toxicity classification), and combination with other complementary signals (e.g., a question answering model's generated answers would be validated against relevant documents). 
For example, a problematic model capable of generating toxic content might be tolerable if appropriate precautions are taken downstream. 
The extra application-specific logic is crucial for mitigating harms. 
(5) Deployment: The direct social impact of an AI system occurs when it is deployed to people. 
Though we would not want to deploy potentially harmful foundation models trained on questionable data, there might still be value in permitting them in research to advance scientific understanding, though one must still exercise caution. 
More generally, it is standard practice in large-scale deployments to conduct gradual releases, where deployment happens to an increasing fraction of users; this can partially mitigate any potential harms. 
While this report is about foundation models, it is important to note that many of the impacts come from decisions made in other stages in the pipeline, and thoughtful monitoring and intervention is needed at every stage. 
While large organizations might own the entire pipeline, each stage could be performed by a different organization, e.g., a company which specializes in creating custom foundation models for various domains that application-developers can use. 
Think ecosystem, act model. 
While the social impact depends on the whole ecosystem, it is still important to be able to reason about the social implications of a foundation model, given that many researchers' and practitioners' purview is restricted to the training stage. 
This is difficult because foundation models are unfinished intermediate objects that can be adapted to many downstream applications, sometimes by an entirely different entity for unforeseen purposes. 
What we need are two things: (i) surrogate metrics for a representative set of potential downstream evaluation ( §4.4: evaluation), and (ii) a commitment to documenting these metrics [Mitchell et al. 2019] similar to data sheets for materials such as metals and plastics, which can be adapted to many downstream use cases. 
Characterizing the potential downstream social impact of foundation models is challenging and demands a deep understanding of both the technological ecosystem and of society. 
One cannot fully assess the harms ( §5.1: fairness) of a foundation model without recognizing how it will be deployed, and one cannot just define automatic metrics without considering the rich social and historical context. 
1.3 The future of foundation models
Foundation models have demonstrated raw potential, but we are still in the early days. 
Despite their deployment into the real world, these models are very much research prototypes that are poorly understood. 
Even the professional norms -what Robert Merton calls the ethos of science [Merton 1979 ] -around foundation models are underdeveloped. 
For example, there is lack of agreement on basic questions such as when models are "safe" to release or how the community should react in response to methodological misconduct. 
Given that the future of foundation models is thus filled with uncertainty, a big question is: who will determine this future? 
Disciplinary diversity. 
The technology behind foundation models is based on decades of research in machine learning, optimization, NLP, computer vision, and other fields. 
These technical contributions have come from both academia and industrial research labs. 
However, research on building foundation models themselves has occurred almost exclusively in industry -big tech companies such as Google, Facebook, Microsoft, or Huawei, or startups such as OpenAI or AI21 Labs, though AI2 is a notable exception [Peters et al. 2018; Zellers et al. 2019b] . 
The furious pace of technological progress and the entrenchment due to centralization raise powerful concerns that demand the attention of humanists and social scientists in addition to technologists. 
We should not rely on post-hoc audits of ethical and social consequences, conducted only after the technical architecture and deployment decisions have been made. 
We instead need to infuse social considerations and ethical design deeply into the technological development of foundation models and their surrounding ecosystem from the start. 
Academic institutions are unique in that they host the widest set of disciplines under one roof, thus bringing together computer scientists, social scientists, economists, ethicists, legal scholars, etc. 
Given the importance of disciplinary diversity in understanding and solving problems that combine technical, ethical, legal, social, and political dimensions [Hong and Page 2004; Solomon 2006; Steel et al. 2018] , we therefore see academia as playing a crucial role in developing foundation models in such a way to promote their social benefit and mitigate their social harms, as well as determining the contexts under which actions in each of the stages of the ecosystem ( §1.2: ecosystem) ranging from data curation to deployment should be strictly prohibited. 
Incentives. 
The political economy in which foundations models are designed, developed, and deployed provides an inevitable incentive structure for decision-making at every stage. 
How people and institutions respond to incentives is an elementary lesson of economics. 
Market-driven commercial incentives can align well with social benefit: making foundation models more accurate, reliable, safe, and efficient while searching for a wide variety of potential use cases can produce a great deal of social utility. 
However, commercial incentives can also lead to market failures and underinvestment in domains where shareholders are unable to capture the value of innovation. 
Just as the pharmaceutical industry has little incentive to devote significant resources to the research and development of malaria treatments, because poor people cannot afford medications, 9 the tech industry has little incentive to devote significant resources to technologies designed for improving the condition of poor and marginalized people [Reich et al. 2021 ]. 
What's more, commercial incentives can lead companies to ignore social externalities [Acemoglu 2021; Reich et al. 2021 ] such as the technological displacement of labor, the health of an informational ecosystem required for democracy, the environmental cost of computing resources, and the profit-driven sale of technologies to non-democratic regimes. 
Finally, there is little incentive for any given company to create an open, decentralized ecosystem for developing foundation models that encourages broad participation. 
In contrast, the long-standing and deeply-seated research mission of universities is the production and dissemination of knowledge and creation of global public goods [Kerr 2001; Rhoten and Calhoun 2011; Nussbaum 2010] . 
We believe that academia is distinctively positioned to shape the development of foundation models to ensure that we capture directions with potentially large social benefit that might not otherwise be prioritized by industry. 
Loss in accessibility. 
Unfortunately, academia has not been able to participate in the fullest way possible due to the loss in accessibility. 
One of the often overlooked effects of the deep learning revolution was the increase in reproducibility and open science: it increasingly became the norm to publicly release code and datasets, and packages such as TensorFlow [Abadi et al. 2016] and PyTorch [Paszke et al. 2019 ] made it much easier for people to collaborate and build off of each other's work. 
Initiatives like the ML Reproducibility Challenge 10 as well as reproducibility checklists adopted by major conferences [Pineau et al. 2020] , alongside platforms like CodaLab Worksheets 11 helped advance community standards for reproducibility. 
This resulted in a surge in technological innovation and progress. 
Foundation models start to roll back this positive trend. 
Some models (e.g., are not released at all (only API access to a limited pool of people). 
Even datasets (e.g., for GPT-2) are not released. 
While trained models may be available (e.g., BERT), the actual training of foundation models is unavailable to the vast majority of AI researchers, due to the much higher computational cost and the complex engineering requirements. 
Some meaningful research can still be done by training smaller models within reach of an academic budget, and indeed the surprisingly regularity predicted by scaling laws [Kaplan et al. 2020 ] make this a viable strategy for cases where the differences due to scale are quantitative (e.g., accuracy goes up). 
However, due to the emergent nature of these foundation models, some functionalities like in-context learning have only been demonstrated in models of sufficient size, so scale is needed to even ask the right questions. 
It is also possible to productively study pre-existing models that have been released; indeed, this has led to a large subcommunity within NLP for probing these models [Rogers et al. 2020; Manning et al. 2020] . 
Having access to existing models can be useful for powering downstream applications or identifying defects (e.g., bias), but this might not be enough for us to design better architectures or training objectives for foundation models that can fix these defects (e.g., mitigate the bias). 
It is worth reflecting on how much of NLP research today is based on BERT, a particular (and somewhat arbitrary) foundation model. 
Given the need to infuse social awareness and ethical design into the construction of these models, it is possible that we need to build foundation models that look quite different from what exists today. 
This will demand intense experimentation at scale. 
Community efforts such as EleutherAI 12 and Hugging Face's BigScience project 13 are attempting to train large foundation models, but the gap between the private models that industry can train and the ones that are open to the community will likely remain large if not grow. 
Further, today startups (OpenAI, Anthropic, AI21 Labs, etc.) are much more well-resourced than academia and can therefore still afford to train the largest foundation models (e.g., OpenAI's GPT-3). 
However, big tech companies are on a completely different level in terms of resources, especially in terms of the infrastructure, users, and data that come from their market position. 
The fundamental centralizing nature of foundation models means that the barrier to entry for developing them will continue to rise, so that even startups, despite their agility, will find it difficult to compete, a trend that is reflected in the development of search engines [Radinsky 2015 ]. 
One way to close the resource gap is for the government to invest in public infrastructure. 
We can look to Big Science projects such as the Hubble Space Telescope and the Large Hadron Collider as inspiration, where substantial investment made possible fundamental scientific discoveries which wouldn't have been possible. 
One can imagine a similar infrastructure for computing, from which academic research on foundation models would greatly benefit. 
In the US, the nascent National Research Cloud initiative 14 is a step in this direction. 
Another complementary approach is to rely on volunteer computing, in which any of the billions of computing devices (nodes) can connect to a central server and contribute computation. 
The Folding@home project has successfully implemented this approach for simulating protein dynamics [Beberg et al. 2009] . 
Recently, the Learning@home project is attempting to harness volunteer computing for training foundation models [Ryabinin and Gusev 2020] . 
The high latency connections between nodes and the high bandwidth requirements for training foundation models make this an open technical challenge. 
Summary. 
There are tremendous economic incentives to push the capabilities and scale of foundation models, so we anticipate steady technological progress over the coming years. 
But the suitability of a technology relying largely on emergent behavior for widespread deployment to people is unclear. 
What is clear that we need to be cautious, and that now is the time to establish the professional norms that will enable the responsible research and deployment of foundation models. 
Academia and industry need to collaborate on this: industry ultimately makes concrete decisions about how foundation models will be deployed, but we should also lean on academia, with its disciplinary diversity and non-commercial incentives around knowledge production and social benefit, to provide distinctive guidance on the development and deployment of foundation models that is both technically and ethically grounded. 
1.4 Overview of this report
In March 2021, we created an informal community at Stanford University of students, faculty, and researchers interested in some aspect of foundation models. 15 
From the very beginning, the community included not just AI researchers, but those eager to apply foundation models to their domain (e.g., healthcare and law), as well as those who were interested in societal concerns (e.g., ethics and economics). 
As discussions progressed, we noticed that there were many gaps in mutual understanding -how the technology worked, how industry develops foundation models, how to think about the ethical concerns, etc., and existing literature only covered bits and pieces. 
We wanted to therefore provide a fuller picture of foundation models, identify opportunities and risks, and establish a constructive vision for the future responsible development of foundation models. 
The writing of this report was an experiment: we had over 100 people from different backgrounds come together to write a single report covering a wide range of aspects of foundation models. 
A large part of this report is a survey of existing work, but through many discussions, we have unified it in one report to highlight all the interdisciplinary connections. 
Structure. 
The report is divided into 26 sections, each discussing one aspect of foundation models. 
The sections are grouped into four parts: capabilities ( §2: capabilities), applications ( §3: applications), technology ( §4: technology), and society ( §5: society), although there are many connections across sections. 
These connections highlight an integrated approach in which the technologies and capabilities are developed in a way that is sensitive to real societal concerns, while being inspired by and grounded out in applications. 
While we have sought to capture most of the important topics surrounding foundation models, this report will inevitably be incomplete, especially as the field evolves quickly. 
For example, many applications (e.g., natural sciences, music, finance, agriculture) are not included, though they are as likely to be affected as the applications we have chosen to discuss. 
It would also be interesting to 
Paper Roadmap
Fig. 4 . 
This report is divided into four parts: capabilities, applications, technology, and society, where each part contains a set of sections, and each section covers one aspect of foundation models. 
study how foundation models relate to research in neuroscience, cognitive science, and psychology to explain intelligence and aid efforts in computational social science to understand society. 
Author Contributions Percy Liang initiated and conceptualized the framing and structure of the overall report. 
He and Rishi Bommasani worked together to lead the decentralized writing effort and provided guidance on individual sections. 
Drew A. Hudson created all the figures in the report, discussing their structure and content with the authors of each section. 
Each of the 26 sections of this report was written by a subset of authors, whose names are listed at the beginning of each section. 
There were, however, many discussions that spanned multiple sections, so the actual contributions to each section generally came from a broader set. 
Finally, we note that not all the views expressed in this report are held by all the authors. 
1.4.1 Overview of capabilities.
Foundation models acquire various capabilities that can power applications. 
We have chosen to discuss five potential capabilities: the ability to process different modalities (e.g., language, vision), to affect the physical world (robotics), to perform reasoning, and to interact with humans (interaction). 
Finally, we conclude with a philosophical discussion of potential limits on their capabilities. 
§2.1: Language. 
NLP as a field has blazed the trail for foundation models. 
While these models dominate standard benchmarks, there is a clear gap between the capabilities these models acquire currently and those that characterize language as a complex system for human communication and thought. 
In response to this, we emphasize the full range of linguistic variation (e.g., different styles, dialects, languages), which poses an opportunity and challenge given some variants are data-limited. 
Further, child language acquisition is more sample efficient than the training of foundation models; we examine how signals beyond text and grounding may help to bridge this gap. 
Both of these characteristics of language provide clear directions for future foundation models research. 
§2.2: Vision. 
Computer vision led the adoption of deep learning in AI [Russakovsky et al. 2015] , demonstrating that models pretrained on large annotated datasets can transfer to numerous downstream settings. 
Now, pretraining on web-scale raw data instead of curated datasets, foundation models are on the rise in computer vision [e.g., Radford et al. 2021 ]. 
These models have shown promising results for standard tasks in the field, like image classification and object detection, and training on multimodal and embodied data beyond images may enable progress on significant challenges (e.g., 3D geometric and physical understanding, commonsense reasoning). 
We also discuss some of the key challenges in modeling (e.g., the ability to scale effectively to videos) and evaluation (e.g., the measurement of higher-order capabilities) along with the applications (e.g., ambient intelligence for healthcare) and societal considerations (e.g., surveillance) that will determine the impact of foundation models for computer vision going forward. 
§2.3: Robotics. 
A longstanding goal of robotics research is to develop "generalist" robots capable of performing myriad tasks across physically diverse environments. 
Unlike language and vision, which have led the way with foundation models both due to the abundance of raw data to train these models on and the availability of virtual applications to apply these models to, robotics faces fundamental challenges due to being anchored to the physical world. 
The principal challenge in developing new types of foundation models for robotics -different in nature than their language and vision counterparts -is acquiring sufficient data of the right form that is conducive to learning: we explore how plentiful data (e.g., generic videos of humans, amongst others) that is not specific to particular environments and across modalities (e.g., language, vision) may help to bridge this gap. 
These new robotic foundation models could allow for easier task specification and learning, ushering in new applications (e.g., better robotic assistance for household tasks) and heightening the importance of robustness and safety (e.g., formal safety evaluation). 
§2.4: Reasoning and search. 
Reasoning and search problems such as theorem proving and program synthesis have been long-standing challenges in AI. 
The combinatorial search space renders traditional search-based methods intractable. 
However, humans are known to operate intuitively even in the most mathematical of domains [Lakoff and Núñez 2000] , and indeed existing work such as AlphaGo have already shown that deep neural networks can be effective in guiding the search space. 
But humans also transfer knowledge across tasks, facilitating much more efficient adaptation and the ability to reason more abstractly. 
Foundation models offer the possibility of closing this gap: their multi-purpose nature along with their strong generative and multimodal capabilities offer new leverage for controlling the combinatorial explosion inherent to search. 
§2.5: Interaction. 
Foundation models show clear potential to transform the developer and user experience for AI systems: foundation models lower the difficulty threshold for prototyping and building AI applications due to their sample efficiency in adaptation, and raise the ceiling for novel user interaction due to their multimodal and generative capabilities. 
This provides a synergy we encourage going forward: developers can provide applications that better fit the user's needs and values, while introducing far more dynamic forms of interaction and opportunities for feedback. 
§2.6: Philosophy of understanding. 
What could a foundation model come to understand about the data it is trained on? 
Focusing on the case of natural language, we identify different positions on the nature of understanding and explore their relevance for our central question. 
Our tentative conclusion is that skepticism about the capacity of future foundation models to understand natural language may be premature, especially where the models are trained on multi-modal data. 
1.4.2 Overview of applications.
At present, foundation model research is largely confined to computer science and AI, with the impact of foundation models and the applications they support largely being centered in the tech industry. 
Moving forward, foundation models present clear potential to transform and extend the reach of AI across many sectors beyond the tech industry, suggesting a more pervasive effect on people's lives. 
While there is a multitude of applications and domains to consider, we we have chosen three applications -healthcare, law, and education -because they represent foundational pillars of our society. 
For foundation models to significantly contribute to these application domains, models will require specific capabilities ( §2: capabilities) as well as technical innovation ( §4: technology) to account for the unique considerations in each domain. 
Further, since these domains are critical to societal function ( §5: society), applying foundation models in these domains requires engaging with deeply sociotechnical matters such as those those pertaining to data ( §4.6: data), privacy ( §4.7: security), interpretability ( §4.11: interpretability), fairness ( §5.1: fairness) and ethics ( §5.6: ethics). 
§3.1: Healthcare and biomedicine. 
Healthcare tasks (e.g., patient care via disease treatment) and biomedical research (e.g., scientific discovery of new therapies) require expert knowledge that is limited and expensive. 
Foundation models present clear opportunities in these domains due to the abundance of data across many modalities (e.g., images, text, molecules) to train foundation models, as well as the value of improved sample efficiency in adaptation due to the cost of expert time and knowledge. 
Further, foundation models may allow for improved interface design ( §2.5: interaction) for both healthcare providers and patients to interact with AI systems, and their generative capabilities suggest potential for open-ended research problems like drug discovery. 
Simultaneously, they come with clear risks (e.g., exacerbating historical biases in medical datasets and trials). 
To responsibly unlock this potential requires engaging deeply with the sociotechnical matters of data sources and privacy as well as model interpretability and explainability, alongside effective regulation of the use of foundation models for both healthcare and biomedicine. 
§3.2: Law. 
Legal applications require that attorneys read and produce long coherent narratives that incorporate shifting contexts and decipher ambiguous legal standards. 
Foundation models may provide benefits in this domain: ample data exists in the form of legal documents and their generative capabilities are well-suited to the many generative tasks required in law, but significant improvements are required for foundation models to be able to reliably reason over various sources of information to generate truthful long-form documents. 
As is the care in healthcare ( §3.1: healthcare), the sample efficiency of adaptation for foundation models is of heightened value given the costs of expert time and knowledge in the legal domain, which may allow for the re-allocation of expertise towards pressing problems of justice and government service. 
The responsible development of foundation models for law will require specific consideration of privacy, and highlights core limitations of existing foundation models that will require fundamental advances with respect to provenance for their behavior and guarantees for the factuality of their generation. 
§3.3: Education. 
Education is a complex and subtle domain; effective teaching involves reasoning about student cognition and should reflect the learning goals of students. 
The nature of foundation models presents promise here that has yet to be realized in the sphere of AI for education: while certain many streams of data in education are individually too limited to train foundation models, the ability to leverage relevant data from outside the domain (e.g., the Internet) and make use of data across multiple modalities (e.g., textbooks, mathematical formula, diagrams, video-based tutorials) jointly offers hope for foundation models that are broadly applicable to educational tasks. 
If foundation models lead to a significant improvement in education-relevant capabilities, there is clear potential for new applications that align with the open-ended generative (e.g., problem generation) and interactive (e.g., feedback to teachers) aspects of foundation models; the sample efficient adaptation of foundation models suggests greater ability for adaptive and personalized learning. 
In this event, renewed consideration is required of hallmarks of applying technology to education (e.g., student privacy), along with certain concerns becoming more critical (e.g., inequity in access to technology in education, technology-aided plagiarism). 
1.4.3 Overview of technology.
Now we discuss the technology behind building better model architectures, training and adaptation procedures, and of course scaling up the systems. 
One crucial but often overlooked topic is datawhere does it come from and what is its composition? 
In addition, we want foundation models to be robust to distribution shifts and secure against attackers. 
Finally, we wish to understand why foundation models work from both a mathematical perspective as well as an empirical perspective. 
§4.1: Modeling. 
What structural properties give rise to a foundation model? 
In the modeling section, we explore the underlying architectures behind foundation models and identify 5 key attributes. 
First, we start by discussing expressivity of the computational model -to capture and assimilate real-world information, and scalability -to adeptly handle large quantities of highdimensional data. 
These properties are successfully realized by existing architectures such as the transformer network [Vaswani et al. 2017 ] that underpins most foundation models to date. 
We then proceed to attributes may be essential for the next generation of models, including: multimodallityto consume, process and potentially produce content from different sources and domains, memory capacity -to effectively store and retrieve the acquired knowledge, and finally, compositionality, to foster successful generalization to novel settings and environments. 
We believe that realizing the full potential envisioned for foundation models will hinge on modelling advances to fulfill these desiderata. 
§4.2: Training. 
Training objectives mathematically specify how models should learn and acquire capabilities from their training data. 
The current status quo for training foundation models involves modality-specific objectives (e.g., masked language modeling [Devlin et al. 2019 ] for text and SimCLR [Chen et al. 2020c] for images) that are often chosen heuristically. 
We envision that future training objectives for foundation models will reflect two changes: principled selection derived from systematic evidence and evaluation ( §4.4: evaluation), and domain-generality to provide rich, scalable, and unified training signal across data sources and modalities. 
We also discuss important design trade-offs, including generative vs discriminative training, the choice of input data representation, and the potential of future training objectives that involve explicit representations of goals. 
§4.3: Adaptation. 
Foundation models are intermediary assets; they are unfinished and generally should not be used directly, instead requiring adaptation for specific downstream tasks. 
The de facto approach for adaptation has been fine-tuning, with recent work suggesting that lightweight fine-tuning alternatives and prompting-based methods may achieve favorable accuracy-efficiency tradeoffs. 
Moving forward, we envision a more expansive view of adaptation that goes beyond just specializing foundation models to perform the task of interest: adaptation will alleviate deficiencies of stand-alone foundation models (e.g., temporal adaptation to reflect changes over time in the world) or introduce constraints (e.g., GDPR compliance relating to the right to be forgotten; §4.7: security); this broader perspective on adaptation coincides with a need for new evaluation protocols ( §4.4: evaluation) that systematically evaluate adaptation methods while controlling for resources (e.g., runtime, memory) and access requirements involved in adaptation. 
§4.4: Evaluation. 
Evaluation offers context to foundation models by providing a means to track progress, understand models, and document their capabilities and biases. 
Foundation models challenge the ability of standard evaluation paradigms in machine learning to achieve these goals since they are one step removed from specific tasks. 
To envision new paradigms in evaluation that suit foundation models, we discuss (a) evaluating foundation models directly to measure their inherent capabilities and inform how foundation models are trained, (b) evaluating task-specific models by controlling for adaptation resources and access, and (c) broader evaluation design to provide richer context beyond measures of accuracy (e.g., robustness ( §4.8: robustness), fairness ( §5.1: fairness), efficiency ( §4.5: systems), environmental impact ( §5.3: environment)). 
Reform of evaluation practices will allow for evaluation that adequately serves both the diverse goals and stakeholders involved in the foundation model paradigm. 
§4.5: Systems. 
While the training data ( §4.6: data) determines the theoretical information available for foundation models, and model architectures ( §4.1: modeling) and training objectives ( §4.2: training) determine how much of this information can be extracted, computer systems determine what is practically achievable. 
Systems are a key bottleneck for scaling in terms of data and model size, both of which appear to reliably track with improvements in capabilities. 
To ensure that we can train the next generation of foundation models efficiently (with respect to time and cost), we will require the co-design of algorithms, models, software, and hardware. 
This co-design is already starting to happen to in various forms, from carefully tuned parallelism strategies to new architectures such as retrieval-based and mixture-of-expert models. 
Beyond training, we consider what will be required to deploy applications on top of foundation models (e.g., efficient inference). 
§4.6: Data. 
Data is the lifeblood of foundation models; the training data of these models largely determines what these capabilities these models can acquire. 
The centrality of data is not unique to foundation models; recent calls for data-centric AI [Press 2021 ; Ré 2021] indicate the pervasive importance of managing, understanding, and documenting data used to train machine learning models. 
For foundation models specifically, the current modus operandi is for training data to be selected using unspecified or unclear principles with a general lack of transparency regarding the nature of training data. 
We believe an alternative approach is needed to re-imagine the data ecosystem surrounding foundation models: we draw upon work on data visualization and management to propose a data hub for foundation models. 
We articulate how this proposal relates to many of the relevant data-centric considerations for foundation models: selection, curation, documentation, access, visualization and inspection, quality assessment, and legal regulation. 
§4.7: Security and privacy. 
Security and privacy for foundation models is largely uncharted at present. 
Fundamentally, foundation models are a high-leverage single point of failure, making them a prime target for attack: existing work demonstrates a variety of security vulnerabilities (e.g., adversarial triggers to generate undesirable outputs) or privacy risks (e.g., memorization of training data) for these models. 
Further, the generality of foundation models compounds these concerns, intensifying the risk for function creep or dual use (i.e., use for unintended purposes). 
For security, we view foundation models as akin to operating systems in traditional software systems; we discuss steps towards secure foundation models which, if achieved, would provide a strong abstraction layer to build upon for reliable ML applications. 
For privacy, by leveraging knowledge transfer from public data, foundation models may enable more sample efficient adaptation to sensitive data distributions, i.e., privacy-preserving applications may incur less degradation in accuracy when built using foundation models. 
§4.8: Robustness to distribution shifts. 
A major limitation of standard machine learning is that it produces models that are not robust to distribution shifts, where the training distribution does not match the test distribution (for the downstream task). 
Existing work shows that adapting a foundation model trained on a broad range of unlabeled data improves the robustness of adapted models across a wide variety of shifts. 
This opens a new set of promising directions for improving training and adaptation of foundation models for robustness. 
However, we do not believe that foundation models are a panacea for robustness -challenges such as extrapolation across time and spurious correlations are not likely to be fully addressed. 
§4.9: AI safety and alignment. 
Ensuring foundation models are reliable ( §4.5: systems), robust ( §4.8: robustness), and interpretable ( §4.11: interpretability) is increasingly important when considering the potential real-world applications of these models. 
In addition to critical and immediate considerations, we also consider the relationship between foundation models and larger-scale risks, hazards, and harms that have the potential for increased relevance as model capabilities continue to advance. 
For example, we consider the importance of aligning foundation models such that they are not deployed with misspecified goals or values. 
We also discuss the relevance of forecasting the emergent behaviors of foundation models (e.g., the ability to deceive or plan strategically), which may complicate attempts to adapt them to particular tasks, and may require new approaches for interpretability ( §4.11: interpretability) or evaluation ( §4.4: evaluation). 
§4.10: Theory. 
Learning theory provides a broad foundation for the variety of contexts encountered in applied machine learning; theory offers both understanding, principles, and guarantees to complement empirical findings. 
At present, the study of foundation models is largely empirical: the theory of standard supervised learning, while relatively mature, is inadequate to fully explain foundation models. 
Specifically, the discrepancy between the training phase and the adaptation phase within the foundation model regime pinpoints the insufficiency of existing theory, since these phases correspond to (potentially) completely different tasks and data distributions. 
Nevertheless, we endeavor that advances in theory to address this discrepancy, even in simple, limited settings, will provide useful insights. 
§4.11: Interpretability. 
Interpretability provides clarity to foundation models: the opacity of the deep neural networks that underpin foundation models, alongside the expected ubiquity of foundation models, heightens the need to understand these models and their capabilities. 
Interpretability methods at present generally are designed for interpreting and explaining the behavior of task-specific models; the nature of foundation models (i.e., the wide array of tasks these models are beneficial for and the unexpected emergent properties they acquire) introduces new challenges for interpretability research. 
To frame the discussion of interpretability for foundation models, we propose the one model-many models paradigm, which aims to determine the extent to which the one model (the foundation model) and its many models (its adapted derivatives) share decision-making building blocks. 
In addition to interpreting the decision-making components involved, we further discuss explainability in the context of foundation models (e.g., the validity ofpost hoc explanations generated by models) as well as the mechanisms that drive model behavior (which may clarify the extent to which understanding foundation models can extend to understanding their adapted derivatives). 
Given the critical role we ascribe interpretability in the study of foundation models, we conclude with an assessment of the societal impact of interpretability and non-interpretability. 
1.4.4 Overview of society.
We believe the rapid development of foundation models, adapted and deployed to various applications, will have wide-ranging consequences on the health of societies. 
What makes these models so exciting and also so troubling is their task agnosticity. 
Societal impact is easier (but still non-trivial) to understand and reason about when we talk about specific systems deployed to users, but how can we take into account the societal impact of all possible systems and use cases when developing foundation models? 
§5.1: Inequity and fairness. 
In many contexts, machine learning has been shown to contribute to, and potentially amplify, societal inequity. 
Foundation models may extend this trend, i.e., furthering the unjust treatment of people who have been historically discriminated against. 
However, understanding the relationship between inequity and foundation models requires reckoning with the abstraction of foundation models; foundation models are intermediary assets that are adapted for applications that impact users. 
Therefore, we delineate intrinsic biases, i.e., properties in foundation models that portend harm, and extrinsic harms, i.e., harms arising in the context of specific applications built using foundation models. 
We taxonomize various sources (e.g., training data, lack of diversity among foundation model developers, the broader sociotechnical context) that give rise to these biases and harms, emphasizing the importance, and technical difficulty, of source tracing to understand ethical and legal responsibility. 
We do not view unfairness as inevitable in the foundation model paradigm: to address unfair outcomes that arise from foundation models, we dually consider proactive interventions (e.g., technical methods like counterfactual data augmentation) and reactive recourse (e.g., mechanisms for feedback propagation and attribution of moral/legal responsibility). 
§5.2: Misuse. 
We define foundation model misuse as the use of foundation models as they are technically intended (e.g., to generate language or video), but with the goal of causing societal harm (e.g., to generate disinformation, to develop deepfakes for harassment). 
We argue that advances in foundation models will result in higher-quality machine-generated content that will be easier to create and personalize for misuse purposes. 
For example, disinformation actors may use them to quickly generate collections of articles targeted across different demographic groups (e.g., nationality, political party, religion, etc.). 
While these new capabilities may limit existing human detection methods for harmful content (e.g., tracking similar text across different sources), foundation models may themselves provide promising potential as automated misuse detectors. 
§5.3: Environment. 
Foundation models are the byproducts of computationally expensive training regimes, with the existing trajectory favoring even more intensive models; the energy required for this training coincides with the release of more carbon into the atmosphere and the degradation of the environment. 
At present, current discussion centers these enormous single-time training costs and the potential to amortize these costs across repeated use. 
We seek to clarify these discussions by identifying assumptions that shape the calculus of environmental impact for foundation models. 
Further, we envision that the ecosystem surrounding foundation models requires a multi-faceted approach: (a) more compute-efficient models, hardware, and energy grids all may mitigate the carbon burden of these models, (b) environmental cost should be a clear factor that informs how foundation models are evaluated ( §4.4: evaluation), such that foundation models can be more comprehensively juxtaposed with more environment-friendly baselines, and (c) the cost-benefit analysis surrounding environmental impact necessitates greater documentation and measurement across the community. 
§5.4: Legality. 
Foundation models rest on tenuous legal footings at present; how the law bears on both the development and use of these models is largely unclear. 
Legal and regulatory frameworks for foundation models specifically, alongside those for AI technology more generally, will be needed to influence, constrain, and even foster practices in research, development, and deployment. 
Centering on the legal landscape of the United States, where existing consideration of algorithmic tools remains broadly uncertain, we highlight the pertinent issues of liability for model predictions and protections from model behavior. 
With respect to both issues, we describe how legal standards will need to be advanced to address these given the intermediary status of foundation models (as opposed to that of user-facing task-specific models). 
§5.5: Economics. 
Foundation models are likely to have substantial economic impact due to their novel capabilities and potential applications in a wide variety of industries and occupations. 
We consider the implications of the development and use of foundation models for the future of the US and global economy with a focus on productivity, wage inequality, and concentration of ownership. 
§5.6: Ethics of scale. 
In addition to running the risk of increasing inequity, as discussed in §5.1: fairness, the widespread adoption of foundation models poses other ethical, political and social concerns. 
We discuss ethical issues related to the scale of application of foundation models, such as homogenization and the concentration of power, as well as the norms and release strategies appropriate to address them. 
2 CAPABILITIES
Foundation models acquire capabilities, some that surprisingly emerge from their learning process, that power downstream applications ( §3: applications). 
Specifically, we discuss linguistic ( §2.1: language) and visual ( §2.2: vision) capabilities alongside the ability to affect the physical world ( §2.3: robotics), perform reasoning and search ( §2.4: reasoning), and interact with humans ( §2.5: interaction). 
In addition, we discuss how self-supervision (the technical approach used to learn most current foundation models) philosophically relates to the ability to understand ( §2.6: philosophy). 
2.1 Language
Authors: Isabel Papadimitriou, Christopher D. Manning 2.1.1 
The nature of human language. 
Language is the basis of most human communication and interaction. 
However, it is not just a means for humans to achieve shared goals: language is central to human thought, to how social and emotional relations are formed, to how we identify ourselves socially and personally, and to how humans record knowledge and develop societal intelligence. 
Spoken or signed languages arise in every human society, and the languages of the world are both incredibly diverse in the ways that they express and structure the information they convey, while also exhibiting surprising concordance in the richness of what makes a language [Comrie 1989 ]. 
Languages are remarkably complex yet efficient systems, acquired consistently by children in a short amount of time, and which evolve and encompass the changing needs and conditions of linguistic communities. 
Due to this centrality of language in human activities, language understanding and generation is a critical element of research in artificial intelligence. 
Natural language processing (NLP) is the subfield of artificial intelligence concerned with language and, together with the related fields of automatic speech recognition (ASR) and text-to-speech (TTS), has the goal of giving computers the ability to understand and generate human language in much the same way human beings can. 
To date in 2021, NLP has been the field most profoundly affected by foundation models. 
The first generation of foundation models showcased an impressive variety of linguistic abilities, as well as a surprising amount of adaptability to a large range of linguistic situations. 
Since the introduction of the early foundation models ELMo [Peters et al. 2018] and BERT [Devlin et al. 2019 ] in 2018, the field of NLP has become largely centered around using and understanding foundation models. 
The field has shifted to using foundation models as the primary tool, moving towards more generalized language learning as a central approach and goal. 
In this section, we go over the recent successes of foundation models in NLP, detail how foundation models have changed the overall process and mentality for training machine learning models for language, and discuss some of the theoretical and practical challenges facing foundation models as they are applied to a broader set of languages and more realistic and complex linguistic situations. 
2.1.2 Impact of foundation models on NLP.
Foundation models have had a huge impact on the field of NLP, and are now central to most NLP systems and research. 
On a first level, many foundation models are skilled language generators: for example, Clark et al. [2021] demonstrate that non-experts have difficulty distinguishing short-form English text that was written by GPT-3 from that written by humans. 
However, the feature of foundation models that has been most impactful in NLP is not their raw generation abilities but their surprising generality and adaptability: a single foundation model can be adapted in different ways in order to achieve many linguistic tasks. 
The field of NLP has historically focused on defining and engineering systems for challenging linguistic tasks, with the vision that models that are good at these tasks will lead to competent language systems for downstream applications. 
NLP tasks include classification tasks for a whole sentence or document (e.g., sentiment classification, like predicting whether a movie review is positive or negative), sequence labeling tasks, in which we classify each word or phrase in a sentence or document (e.g., predicting if each word is a verb or a noun, or which spans of words refer to a person or an organization), span relation classification, (e.g., relation extraction or parsing, like whether a person and location are linked by a "current residence" relation, or a verb and a noun by a "subject-verb" relation) and generation tasks, producing new text that is conditioned There are over 6,000 languages in the world, with estimates varying due to the inherent uncertainty of what constitutes a separate language [Nordhoff and Hammarström 2011] . 
This map shows the languages of the world, with each dot representing one language and its color indicating the top-level language family. 
Data is from Glottolog [Hammarström et al. 2021 ]. 
We label a few of the languages on the map as examples. 
strongly on an input (e.g., producing a translation or summary of a text, recognizing or producing speech, or responding in a conversation) [Jurafsky and Martin 2009] . 
In the past, NLP tasks had distinct research communities that developed task-specific architectures, often based on pipelines of different models, each performing a linguistic sub-task such as token segmentation, syntactic parsing, or coreference resolution. 
By contrast, the dominant modern approach for performing each task is to use a single foundation model and adapt it slightly using relatively small amounts of annotated data specific to each task (sentiment classification, named entity tagging, translation, summarization) to create an adapted model. 
This has proved to be an extremely successful approach: for the vast majority of the tasks described above, a foundation model that is slightly adapted for a task greatly outperforms previous models or pipelines of models that were built specifically to perform that one task. 
To take just one example, the best system for answering open-ended science questions in 2018, before foundation models, could get 73.1% on the NY Regents 8th grade science exam. 
A year later in 2019, an adapted foundation model scored 91.6% [Clark et al. 2019] . 
The emergence of foundation models that are largely trained to generate language has constituted an important shift in the role of language generation in NLP. 
Until around 2018, the problem of generating general-purpose language was considered very difficult and essentially unapproachable except through other linguistic sub-tasks [Paris et al. 2013 ]. 
Instead, NLP research was mostly focused on linguistically analyzing and understanding text. 
Now, it is possible to train highly coherent foundation models with a simple language generation objective, like "predict the next word in this sentence". 
These generative models now constitute the primary vehicle through which machine learning for language is done -including the analysis and understanding tasks that were once considered prerequisites for generation. 
The successful generation exhibited by foundation models has also led to a flowering of research for language generation tasks like summarization and dialogue generation. 
The rise of the foundation model paradigm has begun to play a similar role in spoken language as well as written. 
Modern automatic speech recognition (ASR) models like wav2vec 2.0 are trained on large datasets of speech audio alone, and then adapted on audio with associated transcriptions for the task of ASR [Baevski et al. 2020] . 
Due to the changes brought about by the foundation model paradigm, the focus of research and practice in NLP has shifted from making bespoke architectures for different tasks to exploring how to best leverage foundation models. 
Research into adaptation methods has blossomed (see §4.3: adaptation for a detailed look at adaptation), and the surprising successes of foundation models have also caused a shift in research interest towards analyzing and understanding foundation models (see §4.11: interpretability for interpretability and analysis of foundation models). 
2.1.3 Language variation and multilinguality.
Though foundation models are surprisingly versatile with the linguistic knowledge they obtain from pretraining, there are limits to this adaptability: it is not clear how successful current foundation models are at handling language variation. 
Language varies greatly. 
Apart from the fact that there are thousands of different languages in the world, language varies even within one language or within one speaker. 
To point out a few examples, informal conversation manifests differently from written language, the grammatical constructions that people reach for when speaking to friends are very different from those used when speaking to someone with authority, and communities of speakers within a language use different dialects. 
Social and political factors are embedded in how language variation is viewed and valued, and in how much different varieties are represented in NLP research (see for example Blodgett and O'Connor [2017] on the failures of NLP for African American English, and §5.1: fairness for a deeper discussion on inequities in foundation models). 
Due to their large capacity for learning linguistic information and flexibly adapting that knowledge, foundation models hold promise for expanding NLP to encompass more linguistic diversity. 
It remains an open research question to understand whether it is possible to make foundation models that robustly and equitably represent language with both its major and subtle variations, giving equal weight and acuity to what makes each linguistic variety distinct [research posing and addressing this question includes Ponti et al. 2019; Bender 2011; Joshi et al. 2020] . 
Following the success of foundation models for English, multilingual foundation models have been released to extend that success to non-English languages. 
For most of the over 6,000 languages in the world, the text data available is not enough to train a large-scale foundation model. 
To give one example, there are over 65 million speakers of Fula, a West African language, but few if any resources available for NLP in Fula [Nguer et al. 2020 ]. 
Multilingual foundation models address this by jointly training on multiple languages simultaneously. 
The multilingual foundation models to date (mBERT, mT5, XLM-R) are each trained on around 100 languages [Devlin et al. 2019; Goyal et al. 2021; Xue et al. 2020] . 
Joint multilingual training relies on the reasonable assumption that the shared structures and patterns between languages can lead to sharing and transfer from the high-resource languages to the low-resource ones, making foundation models possible for languages where we could not train a stand-alone model. 
Experiments using and analyzing multilingual foundation models have shown that there is indeed a surprising amount of transfer between and parallel encoding of the different languages in multilingual foundation models [Wu and Dredze 2019; Choenni and Shutova 2020; Pires et al. 2019; Libovickỳ et al. 2019; Chi et al. 2020; Papadimitriou et al. 2021; Cao et al. 2019] . 
However, the extent to which these models are robustly multilingual is still an open question. 
It remains unclear how much models trained on this data can represent aspects of languages that are drastically different from English or for which few language resources are available [Wu and Dredze 2020] , and whether their apparent multilingual performance relies more on assimilation [Lauscher et al. 2020; Virtanen et al. 2019; Artetxe et al. 2020] . 
Multilingual models show better performance in languages that are similar to the highest-resource languages in their training data, and it has been shown that languages in multilingual models compete for model parameters, making it unclear how much variation can fit in a single model [Wang et al. 2020d] . 
A salient issue stems from the data that we use to train multilingual foundation models: in many multilingual corpora, English data is not only orders of magnitude more abundant than that of lower-resource languages, but it is often cleaner, broader, and contains examples showcasing more linguistic depth and complexity [Caswell et al. 2021 ] (see Nekoto et al. [2020] on building participatory and robust multilingual datasets). 
However, the answer does not simply lie in creating more balanced corpora: there are so many axes of language variation that it would be infeasible to create a corpus that is balanced and representative in all regards. 
The future, versatility, and equity of foundation models all depend on robustly handling language variation despite unbalanced data [e.g., Oren et al. 2019] . 
Current multilingual foundation models in their raw form, and naive unsupervised multilingual training as a method, may not model the subtleties of languages and language varieties to their full extent. 
Nevertheless, they remain useful for some multilingual applications, for example through adapting multilingual models for low-resource languages not in their original training set [Wang et al. 2020b ]. 
Moreover, the results for the (non-public) GShard neural machine translation model show the largest gains over monolingual baselines for the lowest resource languages, with the gains increasing with model size [Lepikhin et al. 2021 ]. 
The research community should critically examine how foundation models deal with language variation, understand the limits of foundation models in bringing equity and representation to NLP, and not settle on promoting foundation models that erase language variation and mostly conform to the linguistic majority in their training data. 
2.1.4 Inspiration from human language acquisition.
Though foundation models have constituted a huge source of progress in creating NLP systems that act more like humans, there are still significant ways in which the linguistic system that they acquire, as well as the learning process, differ from human language. 
Understanding the implications of this gap between machine and human language learning is a necessary part of developing a research community informed about the linguistic limits and possibilities of foundation models. 
Human language acquisition is very efficient: foundation models like GPT-3 are trained on around three to four orders of magnitude more language data than most humans will ever hear or read, and certainly much more than children have been exposed to by the time they are mostly linguistically competent. 
One salient difference between foundation models and human language acquisition is that human language is grounded to the real world [Saxton 2017 ]. 
For example babies and caretakers point to objects during language development [Colonnesi et al. 2010] , and babies learn the grounded meanings of words that refer to common objects before they learn a lot of the other aspects of the linguistic system [Bergelson and Swingley 2012] . 
Most foundation models used in NLP, on the other hand, learn from the distributional information of raw, ungrounded text, and (in contrast to human learners) Zhang et al. [2021] show that RoBERTa models express abstract syntactic features before usable meaning. 
Powerful ungrounded statistical learning is indeed also present in babies [Saffran et al. 1996] , so it is no doubt an important factor in acquisition. 
Nevertheless, advancing grounded language learning for foundation models remains an important direction for approaching human acquisition efficiency [Dupoux 2018; Tan and Bansal 2020; Zellers et al. 2021a , inter alia] (see §2.2: vision and §2.3: robotics for the multimodal potential of foundation models, and §2.6: philosophy for a discussion of whether foundation models can understand language without grounding). 
Another important direction is examining the inductive biases in foundation Fig. 6 . 
Language Acquisition for humans and foundation models. 
While there are certainly different inductive biases between the human brain and foundation models, the ways that they learn language are also very different. 
Most saliently, humans interact with a physical and social world in which they have varied needs and desires, while foundation models mostly observe and model data produced by others. 
models and how they relate to the inductive biases in the human mind, both those specific to language learning and those general to human cognition [Linzen and Baroni 2021] . 
Though the human brain may be more architecturally specialized for efficient language acquisition, foundation models are not blank-slate learners [Baroni 2021 ], and understanding and aligning these linguistic inductive biases is an important future direction for research in foundation models. 
A significant factor in the efficiency of language acquisition is the fact that humans acquire a systematic and generalizable language system. 
Though there are many differing theories about what types of theoretical abstractions the human language system makes [e.g., Comrie 1989; Chomsky 2014; Croft 2001; Jackendoff 2011] , it is generally agreed that humans learn language in a way that allows them to easily slot new knowledge into existing abstractions and productively create new grammatical sentences. 
For example, a ten-year-old child has acquired a lot of the abstractions about how their language works, though the actual words and constructions that they produce will change drastically over the next ten years. 
Foundation models, on the other hand, often do not acquire the systematic abstractions that we expect from humans. 
For example, when a foundation model produces a linguistic construction accurately one time there is no guarantee that future uses of that construction will be mostly consistent, especially after a significant domain shift in the subject matter [examples of work examining limitations of foundation models in systematicity include Lake and Baroni 2018; Kim and Linzen 2020; Bahdanau et al. 2018; Chaabouni et al. 2021] . 
NLP faces the challenge of developing some sort of systematicity in acquisition for foundation models, without regressing to systems that rely too heavily on rigid linguistic rules. 
Language learning continues for a speaker's whole lifetime: the grammar of human languages evolves, and humans flexibly adapt to novel linguistic situations [Sankoff 2018 ]. 
For example, as new terms and concepts arise in an adult's life, they can use them relatively easily in grammatical sentences, and humans often adapt their grammatical patterns to fit in with different social groups [Rickford et al. 1994 ]. 
On the other hand, the linguistic system of foundation models is mostly set by the training data, and is relatively static [Lazaridou et al. 2021; Khandelwal et al. 2020] . 
Though adaptation methods can prime foundation models for different tasks (see §4.3: adaptation), it still remains unclear how to change the more basic linguistic foundation of a foundation model without a large amount of training. 
Making adaptable models that naturally mirror human-like linguistic accommodation and language evolution is an important research area for the future of foundation models. 
Foundation models have drastically changed the research and practice of NLP. 
Foundation models have given rise to many new research directions for the community: understanding generation as a fundamental aspect of language, studying how to best use and understand foundation models, understanding the ways in which foundation models may increase inequities in NLP, examining whether foundation models can satisfactorily encompass linguistic variation and diversity, and finding ways to draw on human language learning dynamics. 
Most of the complex NLP tasks that the research community focused on before foundation models are now best handled, to an almost-human level, using one of a few publicly released foundation models. 
Nevertheless, there remain significant gaps between this performance and the needs for useful and safe deployment of foundation models in complex downstream settings. 
2.2 Vision
Authors: Shyamal Buch, Drew A. Hudson, Frieda Rong, Alex Tamkin, Xikun Zhang, Bohan Wu, Ehsan Adeli, Stefano Ermon, Ranjay Krishna, Juan Carlos Niebles, Jiajun Wu, . 
By harnessing self-supervision at scale, foundation models for vision have the potential to distill raw, multimodal sensory information into visual knowledge, which may effectively support traditional perception tasks and possibly enable new progress on challenging higher-order skills like temporal and commonsense reasoning ( §2.2.1: vision-capabilities). 
These inputs can come from a diverse range of data sources and application domains, suggesting promise for applications in healthcare and embodied, interactive perception settings ( §2.2.2: vision-challenges). 
Image credits [Zamir et al. 2018; Haque et al. 2020 ]. 
Vision underlies one of the primary modes through which a living organism understands its environment. 
The ability to see enables the near-constant, long-range gathering of dense signals, a critical capability developed over an evolutionary time-scale in a diverse range of life forms [Parker 2003; Zhang and Shu 2021] . 
For a skill executed effortlessly by even simple living creatures, transferring the same abilities to machines has proved remarkably challenging, leading computer vision and robotics researcher Hans Moravec in 1988 to observe a paradox: in AI, (what were considered) hard problems are easy and likewise easy problems are hard, and among the "easiest" problems of them all is the visual acuity which we use each day to continually interpret complex scenes in a matter of milliseconds [Moravec 1988; Thorpe et al. 1996; Fei-Fei et al. 2007 ]. 
On the other end of this formidable challenge is the substantial scope of transformative applications which computer vision holds the key to: self-driving cars that can free commuters from gridlock ( §2.3: robotics), life-saving AI tools that can assist overworked specialists by detecting rare medical events ( §3.1: healthcare), next-generation tools for multimedia creation and editing ( §2.5: interaction), among others. 
Reflecting on the applications and settings where human perception is instrumental offers a sense of the potential areas where computer vision can assist and transform. 
The field of computer vision and the challenges we define draw inspiration in many ways from human perception capabilities. 
Several classical theories [e.g., Biederman 1972; McClelland and Rumelhart 1981; Marr 1982] suggested that humans may perceive real world scenes by contextualizing parts as a larger whole, and pointed the way for computer vision techniques to progressively model the physical world with growing levels of abstractions [Lowe 1992; Girshick et al. 2014] . 
 Gibson [1979] suggested that human vision is inherently embodied and interactive ecological environments may play a key role in its development. 
These ideas continue to motivate the ongoing development of computer vision systems, iterating towards a contextual, interactive, and embodied perception of the world. 
In the context of computer vision, foundation models translate raw perceptual information from diverse sources and sensors into visual knowledge that may be adapted to a multitude of downstream settings (Figure 7 ). 
To a large extent, this effort is a natural evolution of the key ideas that have emerged from the field over the last decade. 
The introduction of ImageNet [Deng et al. 2009 ] and the advent of supervised pretraining led to a deep learning paradigm shift in computer vision. 
This transition marked a new era, where we moved beyond the classic approaches and task-specific feature engineering of earlier days [Lowe 2004; Bay et al. 2006; Rosten and Drummond 2006] towards models that could be trained once over large amounts of data, and then adapted for a broad variety of tasks, such as image recognition, object detection, and image segmentation [Krizhevsky et al. 2012; Szegedy et al. 2015; He et al. 2016a; Simonyan and Zisserman 2015] . 
This idea remains at the core of foundation models. 
The bridge to foundation models comes from the limitations of the previous paradigm. 
Traditional supervised techniques rely on expensive and carefully-collected labels and annotations, limiting their robustness, generalization and applicability; in contrast, recent advances in self-supervised learning [Chen et al. 2020c; He et al. 2020 ] suggest an alternative route for the development of foundation models that could make use of large quantities of raw data to attain a contextual understanding of the visual world. 
Relative to the broader aims of the field, the current capabilities of vision foundation models are currently early-stage ( §2.2.1: vision-capabilities): we have observed improvements in traditional computer vision tasks (particularly with respect to generalization capability) [Radford et al. 2021; Ramesh et al. 2021] and anticipate that the near-term progress will continue this trend. 
However, in the longer-term, the potential for foundation models to reduce dependence on explicit annotations may lead to progress on essential cognitive skills (e.g., commonsense reasoning) which have proven difficult in the current, fully-supervised paradigm [Zellers et al. 2019a; Martin-Martin et al. 2021] . 
In turn, we discuss the potential implications of foundation models for downstream applications, and the central challenges and frontiers that must be addressed moving forward ( §2.2.2: vision-challenges). 
2.2.1 Key capabilities and approaches.
At a high-level, computer vision is the core sub-field of artificial intelligence that explores ways to endow machines with the capacity to interpret and understand the visual world. 
It encompasses a multitude of tasks, sub-domains and downstream applications, where the community has made continual progress over the last several decades [Zamir et al. 2018 ]. 
A selection of example tasks 16 : (1) semantic understanding tasks, which aim to discover the properties and relations among entities within visual scenes; these include image classification, object detection, semantic segmentation, action recognition, and scene graph generation, among others [e.g., Krizhevsky et al. 2012; He et al. 2016a; Krishna et al. 2017; Russakovsky et al. 2015; Krizhevsky et al. 2009; Kay et al. 2017; Lin et al. 2014] . 
(2) geometric, motion and 3D tasks, seeking to represent the geometry, pose and structure of still or moving objects, and include tasks of depth estimation, structure-from-motion, surface normal detection, curvature line and keypoint estimation, to name a few [e.g., Laina et al. 2016; Agarwal et al. 2011; Wang et al. 2015a; Zamir et al. 2018; Ullman 1979 ]. 
(3) multimodal integration tasks, combining semantic and geometric understanding with other modalities such as natural language; these include, for instance, visual question answering, image captioning, and instruction following [e.g., Antol et al. 2015; Chen et al. 2015b; Anderson et al. 2018; Goyal et al. 2017b; Hudson and Manning 2019b; Johnson et al. 2017; Luo et al. 2020; Akbari et al. 2021; Huang et al. 2021c; Tsimpoukelli et al. 2021 ]. 
We highlight a subset of traditional core tasks in Figure 7 . 
The predominant paradigm for addressing these tasks, driven by the emergence of ImageNet [Deng et al. 2009 ] during the early 2010s, tends to center around a familiar core idea: First, pretrain a model on a large collection of carefully annotated data [Russakovsky et al. 2015 ] with a fully supervised training task, like image classification. 
Then, adapt the model downstream on taskspecific datasets and domains [Lin et al. 2014; Chen et al. 2015b; Antol et al. 2015 ] by fine-tuning to reach state-of-the-art performance [Krizhevsky et al. 2012; Simonyan and Zisserman 2015; He et al. 2016a; Xu and Saenko 2016] . 
This notion of pretraining followed by adaptation persists in the definitions we consider now for foundation models ( §1: introduction). 
The limitations of this fully supervised paradigm motivate the transition to foundation models: the reliance on external supervised annotations constrains the upper bound capability of previous approaches to capture the diverse spectrum of visual inputs in a scalable, robust and generalizable manner. 
Recent developments in the domain of visual synthesis and unsupervised learning offer a compelling alternative. 
GANs, for instance, learn to generate visual content of high fidelity, realism and diversity, by featuring two competing networks of a generator and a discriminator that can supervise one another from image collections alone [e.g., Goodfellow et al. 2014; Hudson and Zitnick 2021] . 
Other neural models infer the visual properties of objects and scenes without explicitly annotated supervision, by employing variational auto-encoding, contrastive learning or other self-supervised techniques [e.g., Kingma and Welling 2014; Chen et al. 2020c; He et al. 2020 ]. 
For instance, He et al. [2021] build upon prior work on representation learning with masked image encoding [e.g., Pathak et al. 2016; Vincent et al. 2008 ] by, in part, combining recent advancements in flexible architectures (e.g., vision transformers [Dosovitskiy et al. 2021; Zhai et al. 2021] ) with increased scaling. 
With foundation models, the development of such self-supervision techniques has enabled training at greater scales of visual data [Changpinyo et al. 2021] , both in terms of its scope as well as its potential diversity. 
Accordingly, we have seen early indicators of progress on traditional vision tasks in terms of both standard accuracy metrics and few-shot generalization. 
For image classification and object detection, self-supervised techniques have reported competitive performance to prior fully-supervised approaches [He et al. 2019; Chen et al. 2020c; Radford et al. 2021; Hénaff et al. 2021] , without explicit annotations during training and greater sample efficiency during adaptation. 
For visual synthesis, notable examples include DALL-E [Ramesh et al. 2021] and CLIP-guided generation [Radford et al. 2021; Galatolo et al. 2021] , where researchers leverage multimodal language and vision input to render compelling visual scenes. 
In the short-term, we anticipate that the capabilities of these foundation models will continue to improve along these directions, as training objectives are refined [Chen et al. 2020a; Hénaff et al. 2021; Selvaraju et al. 2021] and architectures are designed to incorporate additional modalities [Jaegle et al. 2021b ]. 
Notably, current foundation models for computer vision are nascent relative to their NLP counterparts ( §2.1: language): promising early efforts are still largely centered on RGB image inputs and a subset of core traditional vision tasks. 
However, the field continues to progress on broader challenges centered on embodied and interactive perception settings (critical for foundation models for robotics [Bohg et al. 2017, §2.3: robotics] ). 
We note a subset of these higher-order goals in Figure 7 , including physical scene understanding, reasoning over visual commonsense and temporal events, and perception for social affordances. 
Each of these have been goals for fully-supervised systems, but have proven challenging in part due to the difficulty of annotating these tasks at scale. 
For instance, standard systems for visual-question answering struggle to answer questions that require commonsense understanding, since these questions often require external knowledge beyond what is present in the pixels alone [Zellers et al. 2019a ]. 
Perceiving human gaze and social affordances in a robust manner remain ongoing challenges for embodied vision systems in interactive agents [Martin-Martin et al. 2021] . 
By reducing the dependence on explicit annotations, foundation models may enable further progress towards these goals than was previously feasible. 
Related progress in language foundation models ( §2.1: language), which have been able to capture a degree of commonsense over language events [Brown et al. 2020 ], also suggests a potential avenue towards achieving similar capability over multimodal visual inputs. 
While the exact roadmap for how to achieve these capabilities in foundation models remains an open problem, a combination of new efficient and flexible architectures ( §4.1: modeling), large-scale training ( §4.5: systems), self-supervision techniques ( §4.2: training) and few-shot adaptation schemes ( §4.3: adaptation) may open the door towards capabilities that have been difficult to reach so far. 
2.2.2 Central research challenges.
Our discussion of research challenges is motivated by the downstream application domains where foundation models may further the integration and impact of vision models. 
We highlight a few such areas: (1) ambient intelligence for healthcare and home environments: building upon existing approaches for ambient intelligence in these settings [Haque et al. 2017; Lyytinen and Yoo 2002; Hong and Landay 2004] , foundation models may offer the potential for better detection of finegrained human activities and medical events, as well as improved assistive interaction for clinicians, patients, and everyday consumers (see also §3.1: healthcare). 
(2) mobile and consumer applications: foundation models with stronger multimodal grounding may enable more capable interactivity of services in mobile settings, and fundamental improvements in generation capability from vision and language inputs can benefit computational photography and content editing applications [Delbracio et al. 2021; Ramesh et al. 2021; Park et al. 2019 ] (see also §2.5: interaction). 
(3) embodied, interactive agents: perception models have already proven effective as both inputs [Sermanet et al. 2018 ] and reward functions [Chen et al. 2021c; Shao et al. 2020] in robotics settings; foundation models trained on large collections of egocentric (real/simulated, human/robotic) visual data [Damen et al. 2018; Chen et al. 2021e ] may potentially further this progress by capturing a wider distribution of visual scenes, objects, and actions (see also §2.3: robotics) . 
The extent to which foundation models may further impact these application settings hinges on the degree to which the capabilities outlined in §2.2.1: vision-capabilities are realized. 
To bridge the significant gaps between present, short-term, and long-term anticipated capabilities, we must address current limitations of foundation models for vision, including their training and evaluation. 
Below, a subset of corresponding key challenges: Semantic systematicity and perceptual robustness. 
Humans have a remarkable capacity for generalizing visual understanding to unseen compositions, and reasoning about the physical and geometric properties of novel objects and scenes [Lake et al. 2015] . 
While current foundation models have shown promising capability for image synthesis and early results for generalization to fine-grained language inputs, these models still struggle to generalize to compositions of simple shapes and colors [Ramesh et al. 2021; Radford et al. 2021; Rong 2021] . 
Generalizability goes beyond semantics as well; visual scenes and objects have a natural regularity to their physical dynamics and geometric properties. 
Foundation models have shown early indications of understanding scene and object geometry [Ramesh et al. 2021] . 
Further, early efforts towards physical scene and geometric understanding in perception models may provide guidance for ongoing foundation model development [Yi et al. 2019; Bakhtin et al. 2019; Li et al. 2020b ]. 
Indeed, the continued incorporation of multiple modalities (e.g., audio) in foundation models may prove beneficial towards these aims [Zhang et al. 2017; Gao et al. 2020b; Jaegle et al. 2021a ]. 
However, the specific techniques to enable generalizing the initial observed capabilities robustly to a wide range of natural scenes and objects at the level of humans remains an open research challenge for foundation models. 
Computational efficiency and dynamics modeling. 
Humans are surprisingly efficient at processing the continuous visual stream of objects, scenes, and events necessary to support an understanding of event dynamics [Zacks et al. 2001; Tversky and Zacks 2013] . 
Foundation models in language ( §2.1: language) have shown initial steps towards modeling longer-term coherence of events; the analogous ability to capture long-range temporal correlations and causal coherence in visual input would stand to benefit downstream settings like robotics [Dai et al. 2019; Alyamkin et al. 2019; Goel et al. 2020b; Feng et al. 2019, §2.3: robotics] . 
However, relative to word token-level inputs in language, low-level computer vision inputs are extremely high-dimensional: a single 1080p frame contains over 2 million pixels. 
In this context, modeling the richer event dynamics in long-range video sequences seems like a daunting endeavor, especially with additional modalities (e.g., speech, optical flow, etc.) and increasing resolutions. 
Understandably, a naïve approach to fully processing every individual pixel is likely prohibitive. 
Current vision models [e.g., Radford et al. 2021; Sun et al. 2019a; Tan and Bansal 2019; Kim et al. 2021a] often address this by processing embeddings that summarize image patches or even groups of frames altogether, but this has the potential drawback of losing fine-grained details [Ramesh et al. 2021 ]. 
In addition to considerations of the raw input space, foundation models for vision may need to revisit the design of fundamental architecture primitives ( §4.1: modeling) for efficient and effective modeling: alternatives to 3D convolutions may better address its cubic complexity [Fan et al. 2020; Sitzmann et al. 2019 ], while particle-based representations may prove more effective for modeling physical dynamics [Bear et al. 2021] . 
Further, deployment of these vision models to downstream application settings will also necessitate advancements in systems design ( §4.5: systems). 
Taken together, the bottleneck of efficient and effective modeling for larger-scale, dynamic vision inputs remains a multi-faceted research direction that must be addressed going forward. 
Training, environments, and evaluation. 
Equally critical to realizing the potential of foundation models are the supporting elements for training and evaluating them. 
Current foundation models for vision have largely focused on a small subset of modalities shown in Figure 7 (e.g., datasets of RGB images and text), since these are perhaps the most readily accessible [Changpinyo et al. 2021; Radford et al. 2021 ]. 
This motivates the development and use of additional large-scale training datasets which contain a diverse collection of inputs across a broad spectrum of modalities. 
While additional annotations may not strictly be necessary, the input quality impacts the learning efficiency of the models; techniques that leverage foundation models of other types (e.g., language) to help improve quality are a promising route forward [Zellers et al. 2021b ]. 
We also want to consider settings beyond static datasets: classic studies have suggested that perceptual understanding in humans is linked to its embodiment and interactive, ecological settings [Gibson 1979 ]. 
As stepping stones towards longer-term capabilities of embodiment and interaction ( §2.3: robotics), ongoing development of simulation environments that capture physical, visual, and ecological realism with multiple modalities and viewpoints may play an important role in providing scalable and high-fidelity visual inputs for this goal [Kolve et al. 2017a; Savva et al. 2019b; Gan et al. 2020; Shen et al. 2021a; Srivastava et al. 2021] . 
Finally, there is the question of metrics: how do we evaluate the faithfulness of generative foundation model outputs with respect to semantics? 
Standard metrics like Fréchet Inception Distance, suffer from known flaws [Bińkowski et al. 2018] ; such issues parallel ones in natural language processing (e.g., metrics like BLEU do not correlate with causal judgements from humans). 
Having human judgements as part of evaluation may be one route, but incurs significant cost and may not be as scalable [Zhou et al. 2019; Khashabi et al. 2021 ]. 
The outstanding and open challenges surrounding the training ( §4.2: training), data ( §4.6: data), and evaluation ( §4.4: evaluation) settings for vision foundation models are indeed quite nuanced, and will be a central area of research going forward. 
Concluding remarks. 
In this section, we explored foundation models in the context of computer vision, from identifying roots in previous computer vision paradigms, to contextualizing its current and anticipated capabilities, to proposing research directions moving forward. 
We conclude with a brief discussion of some broader societal implications of foundation models for computer vision and their continued development (see also §5: society). 
The ubiquity of cameras in our society means that advances in computer vision techniques have great potential for disruptive impact; this carries a corresponding burden of responsibility for careful consideration of its risks. 
There is a well-documented history of learned bias in computer vision models, resulting in lower accuracies and correlated errors for underrepresented groups, with consequently inappropriate and premature deployment to some real-world settings [e.g., Buolamwini and Gebru 2018, §5.1: fairness] . 
Many of the same underlying issues continue to persist in current foundation models [Agarwal et al. 2021] . 
As data from additional sensor modalities (e.g., wearable or ambient sensors, Figure 7 ) become incorporated in these foundation models, concerns surrounding privacy and surveillance become paramount (see §5.6: ethics). 
Furthermore, generated deepfake images and misinformation pose greater risks as the semantic and generative capability of vision foundation models continues to grow [Dolhansky et al. 2020; Ramesh et al. 2021, §5.2: misuse] . 
While the intriguing open challenges and opportunities ahead for computer vision and foundation models are significant, addressing these and related risks concurrently remains essential. 
2.3 Robotics
Authors: Siddharth Karamcheti, Annie Chen, Suvir Mirchandani, Suraj Nair, Krishnan Srinivasan, Kyle Hsu, Jeannette Bohg, Dorsa Sadigh, Chelsea Finn Fig. 8 . Building new types of foundation models for robotics will require massive datasets spanning diverse environments and behaviors. 
Simulation, robotic interaction, videos of humans, and natural language descriptions could all be useful data sources for these models. 
Despite the challenges of acquiring data, developing new foundation models for robotics has tremendous potential for a variety of problem formulations in task specification and robot learning. 
Image credits: [Finn et al. 2016b; Szot et al. 2021] . 
A longstanding challenge of robotics research is to endow robots with the ability to handle the myriad conditions they will encounter in real-world settings. 
In this section, we discuss how the ideas underlying foundation models can potentially help bring about "generalist" robots that can, for example, cook a new meal in a new house, with a new kitchen. 
To make progress towards this goal, existing foundation models will not suffice. 
We need new types of models trained on a multitude of data sources, spanning grounded robotic interaction data to videos of humans performing tasks, amongst others. 
We focus on how such foundation models can apply to the problem of a robot controlling its own physical embodiment to successfully perform different tasks. 
This is a high-dimensional and closed-loop decision-making problem: the actions that a robot takes directly influence what it perceives next, which in turn influences the next robot action. 
This closed-loop aspect is not traditionally studied in language and computer vision, where large offline datasets are dominant and foundation models have already seen success. 
We focus on how the demonstrated benefits of foundation models -large-scale, self-supervised learning -can be leveraged in this new closed-loop data regime. 
The promise of a new type of robotic foundation model is in its ability to amplify the potential of robots to improve key facets of daily life ranging from manufacturing [Nof 1999; Sanneman et al. 2020] , construction [Khoshnevis 2004; Bock 2007 ], autonomous driving [Thorpe et al. 1988; Badue et al. 2020] , to household aid [Thrun and Mitchell 1995; Brooks 2002; Dillmann 2004; Goodrich and Schultz 2007; Gupta et al. 2018; Shridhar et al. 2020 ] and personal assistance [Dragan and Srinivasa 2013; Javdani et al. 2018 ], amongst others. 
Our discussion in this section primarily focuses on mobile manipulation robots for household tasks, but we expect its essence to be broadly applicable to the other use-cases of robotics listed above. 
On the critical path towards building new types of foundation models for robotics is embracing opportunities in task specification and task learning, coupled with tackling challenges in data acquisition and safety and robustness. 
Consider the following robot learning paradigm: starting with a description of a task capturing what a user might like the robot to do (e.g., "make breakfast") -learn a corresponding policy to generate the desired robot actions. 
While policies can be parameterized in different ways, a common choice is that of a function that maps the task representation and environment observation (e.g., a scene image from a fixed or egocentric camera, or inputs from alternative sensors like LIDAR) to robot actions [Andrychowicz et al. 2017; Nair et al. 2018 ]. 
As the robot acts in a task-conditioned manner, the subsequent states are fed back to the policy, generating more actions until the task has been satisfied. 
Yet, implementing such a paradigm in practice is difficult. 
To begin, what is the right interface for describing one's goals? 
For a given user in one context, "make breakfast" carries an implication of a full breakfast that consists of fried eggs, toast, and a glass of orange juice; for another user, "make breakfast" may imply idlis with sambar and a tumbler of filter coffee. 
In general, high-level context-dependent goals like these do not stand alone and can introduce a multitude of ambiguities. 
How does one specify a goal (and corresponding subgoals) with enough clarity to both resolve these ambiguities, and in so doing, allow a robot to make progress on the given task? 
Additionally, how might we craft general task representations that might aid generalization to similar objectives (e.g., fetching a glass of milk instead of orange juice). 
Going a step further, how do we build methods that aid robots in learning policies for new tasks and new environments (in this case, a brand new kitchen with new utensils, appliances, layouts, etc.)? 
Recent breakthroughs in applying foundation models for language and vision ( §2.1: language and §2.2: vision) suggest several potential benefits of large-scale, self-supervised pretraining for improving generalization. 
The ability to tap into diverse streams of data to learn meaningful representational priors (akin to those learned by models such as BERT and GPT-3) holds promise for learning powerful robotic foundation models for task specification. 
Diverse robotic interaction data can be used for learning action-conditional dynamics models or policies indexing general and semantically meaningful skills thereby holding promise for task learning. 
Yet while these opportunities exist, the key stumbling block is collecting the right data. 
Unlike language and vision data, robotics data is neither plentiful nor representative of a sufficiently diverse array of embodiments, tasks, and environments -we (as a field) still have not converged on the kinds of data that would be maximally useful for enabling generalist robotics (e.g., offline demonstrations, thirdperson recordings of humans, egocentric videos, autonomous experience, etc.) Coupled with issues in obtaining the right scale and diversity of data are questions of ensuring safety and robustness: how do we behave in a new environment without causing damage? 
Building new types of foundation models for robotics thus consists of a dichotomy of opportunities and challenges: opportunities for task specification and learning balanced against challenges of data collection and safe deployment. 
This section explores both by presenting a picture of how robotic foundation models might help us develop generalist robots, in a way that not only meaningfully addresses the challenges associated with building such systems, but that also embraces the potential of multi-modality -incorporating perception, actuation, and language -as well as human-robot interaction for specification and learning. 
2.3.1 Opportunities.
Robotic foundation models could take a variety of forms: problems in robotics do not easily conform to a one-size-fits-all model, since different problems have different input-output signatures -a contrast to domains like NLP where many problems can be cast into a general "text-in, text-out" signature. 
We focus on opportunities in generalizable task specification and learning across tasks, environments, and robot embodiments. 
Foundation models for task specification. 
Before robots can learn how to solve tasks in a general purpose way, they must understand what the desired task is: for example, to be useful in a new kitchen, a robot needs to know what we would like it to cook, as well as behaviors we would like it to avoid. 
Therefore, a necessary first step towards developing generalist robots is building a new type of foundation models for reliable task specification, i.e., the intuitive and effective communication of task objectives, preferences, and constraints. 
We formalize task specification as a process that transforms a human-provided task description into a quantitative metric that measures a robot's task completion and progress -e.g., a reward function. 
This signal is crucial for optimizing robot behavior, diagnosing failures, and prompting human feedback. 
As the most natural way to describe a task can vary depending on the user, environment, or task, robotic foundation models for task specification should accept a variety of description modalities, such as goal states [Fu et al. 2018; Singh et al. 2019 ], natural language [MacGlashan et al. 2015; Karamcheti et al. 2017; Misra et al. 2017b; Co-Reyes et al. 2019; Shao et al. 2020] , videos of humans [Shao et al. 2020; Chen et al. 2021c; Liu et al. 2018] , pairwise or ranking comparisons [Biyik and Sadigh 2018] , interactive corrections [Co-Reyes et al. 2019; Karamcheti et al. 2020 ] and physical feedback [Ross et al. 2011; Bajcsy et al. 2017 ]. 
An important requirement of general purpose models for task specification is the ability to transfer to new environments and tasks. 
Reliably transforming task descriptions into generalizable reward signals for robot learning remains an open problem [Taylor et al. 2016 ] -one that robotic foundation models will arguably be well suited for. 
When applied to task specification, such models should provide more robust ( §4.8: robustness) reward signals by learning from large and broad datasets -even leveraging multiple of the description modalities listed above. 
One possible instantiation of a new foundation model for task specification might be to learn a mapping from arbitrary (language, current observation) pairs to reward signals by training on diverse language and vision datasets [Bahdanau et al. 2019; Fu et al. 2019; Chen et al. 2021c] . 
By learning informative priors from these broad, diverse datasets, such a model may be able to generalize to unseen language instructions and observations in unseen environments. 
In general, the potential for new foundation models to be able to deftly bridge modalities and generalize broadly make them appealing for general purpose task specification. 
Foundation models for task learning. 
In addition to enabling more general task specification, robotic foundation models could make learning to solve new tasks more efficient and reliable. 
In this context, these new types of foundation models might take the form of a joint distribution over actions, sensor observations, rewards, and other properties of interest. 
Conditioning on different dimensions of this joint distribution recovers different inference problems, each corresponding to a different signature: 
• Dynamics modeling: (future observations | actions, past observations) [Finn and Levine 2017; Hafner et al. 2019; Wu et al. 2021d ]. 
• Policy learning: (actions | observations, goal) [Kaelbling 1993; Schaul et al. 2015; Ding et al. 2019 ]. 
• Inverse reinforcement learning: (reward function | observations, actions) [Ng and Russell 2000; Ziebart et al. 2008; Finn et al. 2016a ]. 
A plausible training objective for a robotic foundation model is to predict the different elements of the joint distribution described above in an autoregressive fashion [Janner et al. 2021; Chen et al. 2021b, §4.1: modeling] . 
However, these are not the only options. 
In particular, robot datasets contain large amounts of unlabeled data consisting of synchronized observations from many different sensor modalities (e.g., RGB and depth cameras, haptic sensors, microphones, etc.) and a sequence of actions that the robot has performed to generate these observations. 
Beyond the objectives above, a robotic foundation model could be trained to predict observations of one sensor modality from another or to predict whether two streams of sensory observations are from the same segment of time. 
These kinds of self-supervised objectives can leverage multi-modal correspondences to produce low-dimensional representations of high-dimensional data, and can even be combined with the above objectives to yield models, policies, and rewards on top of those representations.These objectives may facilitate the training of powerful robotic foundation models from unlabeled dataas long as the data exhibits diverse, meaningful behavior. 
§2.3.2: 
robotics-challenges discusses the challenges of collecting such data further. 
In language and vision, foundation models have demonstrated the capability to learn broadly applicable priors from large, diverse datasets, that can be subsequently adapted to downstream tasks ( §2.1: language, §2.2: vision). 
Robotic foundation models have the potential to similarly enable few-shot adaptation of perception and control to new environments, tasks, and embodiments though leveraging different data, self-supervised objectives, and modalities than have been studied with existing language and vision models. 
Consider our running kitchen example. 
To cook in a new kitchen, a robot needs to adapt to the specific environment -its spatial layout, the available equipment, etc. Priors learned from offline videos of humans, robotic interaction, text, and/or simulation might encode general aspects of kitchens, such as the fact that stoves are usually against walls and must be turned on in order to produce heat. 
Such commonsense knowledge, physical priors, and visual priors could make adaptation to new environments more sample efficient. 
Similarly, developing a new foundation model for robot task learning might enable the use of a large number of cooking videos in its training dataset to adapt a policy for a common skill, such as "fry an egg, " to a specific user's preferences from a low number of demonstrations -allowing for sample efficient adaptation. 
Finally, with their potential to learn the cross-modal representations described earlier, robotic foundation models could help enable adaptation to new embodiments. 
This aspect of adaptation is crucial to make these models widely useful. 
2.3.2 Challenges and risks.
Despite this exciting vision, multiple challenges need to be overcome. 
To enable the generalization discussed above, we must collect robotic datasets of sufficient size and diversity. 
Additionally, we need mechanisms to ensure that we can deploy learned behaviors safely in the real world. 
Data needs & challenges. 
Learning a policy for a robot that perceives the state of its environment via sensors and takes actions to accomplish tasks traditionally requires large datasets of the robot interacting in the real world. 
On the other hand, many learning tasks in computer vision and natural language processing rely on large and diverse offline datasets that can easily be scraped from the web. 
Motivated by the advances of existing foundation models in language and vision, we are excited by the possibility of leveraging large offline data sources for training new foundation models for robotics. 
One path towards this goal is collecting large datasets for offline learning, for example using teleoperation [Mandlekar et al. 2019] , kinesthetic teaching [Sharma et al. 2018] , or autonomous methods [Pinto and Gupta 2016; Gupta et al. 2018; Levine et al. 2018; Dasari et al. 2019; Kalashnikov et al. 2021; Chen et al. 2021d ], which have shown some promising indications on generalization. 
While scaling up robot data collection to the size of vision and language datasets [Deng et al. 2009; Krishna et al. 2017; Raffel et al. 2019; Gao et al. 2020a ] remains an open challenge, the increasing scale and quality of robotic datasets suggests they can play an important role in learning foundation models for robotics. 
Moreover, because robots have the ability to actively and autonomously shape their environment, they should be able to generate targeted unlabeled data at scale. 
Given the challenging closed-loop nature of learning control, it is possible that collecting datasets of size comparable to those used in vision and language is insufficient for robotics. 
One exciting option is to additionally leverage external, non-robotic sources of data such as videos of humans or existing vision and natural language datasets. 
Such data is diverse and exists in large quantities on the web [Deng et al. 2009; Lee et al. 2012; Heilbron et al. 2015; Goyal et al. 2017a; Damen et al. 2018; Gao et al. 2020a; Grauman et al. 2021] , affording the possibility of broad generalization if properly leveraged. 
Elegantly addressing the gap between the robot's domain and those found in videos or language on the web remains an open challenge; however, recent progress in domain adaptation [Smith et al. 2019; Schmeckpeper et al. 2020 ] and using pretrained video and language models in robotics [Lynch and Sermanet 2020; Shao et al. 2020; Chen et al. 2021c ] present promising directions towards closing this gap. 
Finally, simulation presents a boundless source of rich interactive data that robots can learn from, with a range of sensor modalities like rendered visuals, point-clouds, and simulated touch/audio. 
However, a major challenge lies in bridging the gap between simulation and the real world, both in the underlying physics and in the semantic distribution of environments and tasks. 
Recent work has shown that by using extensive domain randomization, tasks ranging from flight [Sadeghi and Levine 2017] to contact-rich manipulation [Mahler et al. 2017; OpenAI et al. 2019] and locomotion [Peng et al. 2020; Hwangbo et al. 2019 ] skills learned in simulation can be transferred to real robots with some success, and that the semantic and visual distribution of the real world can be simulated by scanning the real world into a simulation [Chang et al. 2017; Kolve et al. 2017b; Savva et al. 2019a; Szot et al. 2021; Shen et al. 2021a] . 
While these are promising steps towards closing the sim-to-real gap, effective and general sim-to-real learning of manipulation and locomotion skills remains an open challenge. 
Simulation data, real robot data, videos of humans, and natural language data could all be essential to learning robotic foundation models. 
Safety & robustness. 
Further complicating the development of new foundation models for robotics is ensuring their safety and robustness when training or deploying them in the real world. 
We can expect the safety risks from these models for robotics to be different from their language counterparts given that embodied agents are empowered to manipulate and interact with their surroundings directly in the physical world. 
One core safety challenge for learning-based systems is the chicken-and-egg problem of needing to specify system constraints for safety prior to collecting data, after which unforeseen unsafe behaviors requiring additional constraints may emerge. 
For instance, an agent adapting to a new kitchen outside of the training distribution requires sufficient safety guarantees to ensure safe data collection, which may either adversely affect task performance or cause the agent to fail in novel ways. 
One way to resolve this is restricting the complexity of the environment or increasing the complexity of the robot such that irrecoverable states or unsafe actions are avoided by construction. 
The robot can also be tasked with autonomously resetting the environment to facilitate uninterrupted learning (or adaptation) from large-scale data collection [Eysenbach et al. 2017; Gupta et al. 2021b ]. 
This would either mean ensuring that nothing in the kitchen is breakable, or ensuring and replacing the items the agent may break while it attempts to collect data. 
To address risks posed by robotic foundation models that fail to generalize or produce unexpected behaviors to new stimuli, potential future directions include developing a causal analysis of agents [Déletang et al. 2021] , new formal safety evaluation tools, and realistic simulation environments [Corso et al. 2020; Dreossi et al. 2017; Julian and Kochenderfer 2019] . 
Finally, deriving formal safety guarantees for robotic foundation models, e.g., Hamilton-Jacobi reachability of safe-sets [Chow et al. 2018; Fisac et al. 2019; Herbert et al. 2021] or developing safety boundaries for learning that are interpretable ( §4.11: interpretability) to human operators, could help reduce risks posed by such models [Berkenkamp et al. 2017] . 
As the development and study of these new types of foundation models progresses, solutions to these challenges will be crucial. 
Conclusion.
While the promise of robotic foundation models are many -spanning multiple levels of the robotics pipeline from task specification to task learning -the challenges are significant. 
Collecting data in the physical world that covers diverse environments and embodiments at scale is a sizable hurdle, and ensuring the safety and robustness of such systems is equally exigent. 
Despite this, our optimism prevails; tackling these challenges now, before developing models offers us the chance to identify ways to collect the right data, from the right sources, at the right scale to build safe and reliable robotic foundation models with the capabilities we desire. 
Underpinning this section has been a theme of multimodality. 
Robotic foundation models -in all possible instantiations -have and will continue to benefit from work in other subfields of AI such as language and vision ( §2.1: language, §2.2: vision). 
Yet as we consider incorporating these extensions from other fields, there are interdisciplinary challenges on the horizon that touch other aspects of foundation models: systems innovation for training and deploying such models for real-time robotics ( §4.5: systems), innovation in interfaces for robust human-robot interaction ( §2.5: interaction), and lessons to incorporate as we better grasp the safety and robustness of such models ( §4.9: ai-safety, §4.8: robustness). 
Building a reliable ecosystem and thoughtful research practices around foundation models in general, and robotic foundation models in particular, is key to realizing these goals. 
2.4 Reasoning and search
Authors: Yuhuai Wu, Frieda Rong, Hongyu Ren, Sang Michael Xie, Xuechen Li, Andy Shih, Drew A. Hudson, Omar Khattab Fig. 9 . Multimodality can allow foundation models to not only reason with formal symbolic language, but also exploit visual aspects of the problem, such as equivalence, symmetry, and Euclidean geometry, to prune the infinite search space and find promising constructions for a solution ( §2.4.1: reasoning-tasks), mimicking the way humans reason about geometry problems. 
Reasoning and search have been a central theme throughout the history of AI. 
Classic tests of intellect, from strategy games to abstract mathematical discovery, served as inspirational goal posts that pushed the limits of "machine intelligence" through a need to devise ever smarter ways of searching for winning solutions. 
In the early days, symbolic methods were the dominant approach for reasoning [Russell and Norvig 2020] , but the involved engineering effort and the need to formalize heuristics to tackle intractable search spaces quickly proved cumbersome. 
More recently, data-driven methods using neural networks have shown encouraging results -e.g., defeating the best humans in Go [Silver et al. 2016 ], a board game with a much larger space of actions than the classic challenge of chess -by exploiting statistical structures and learning useful heuristics. 
This section outlines existing reasoning tasks, ones that require scaling to ever-larger search spaces and understanding the world broadly ( §2.4.1: reasoning-tasks). 
We then argue in §2.4.2: reasoningrole that foundation models should play a central role towards general reasoning as vehicles for capturing the statistical regularities of unbounded search spaces (generativity), allowing positive transfer across tasks and scenarios (universality), and exploiting the grounding of knowledge in multi-modal environments (grounding). 
2.4.1 What are the current tasks?
Many reasoning problems pose unbounded search spaces, where systems must deal with numerous kinds of open-ended alternatives. 
Consider trying to prove that the angles ∠ and ∠ are equal for an isosceles triangle △ with = (Figure 9 ). 
A system can perform any number of actions at each step of reasoning. 
For instance, the system could add a new auxiliary point with an arbitrary construction, say a perpendicular line, a parallel line, or a tangent circle, and the search space only grows larger as the diagram grows more complicated. 
One way to prove this theorem is to draw a line that is the angle bisector of , and use the congruence of the two triangles △ and △ to show ∠ = ∠, but how can systems find this without extensive search? 
More generally, a mathematician is not confined with searching in diagram constructions and Euclidean theorems: mathematicians can apply a vast number of theorems from various branches of mathematics, make high-level conjectures, formalize new mathematical concepts, or find counterexamples. 
This contrasts with more structured AI challenges such as the game of Go, whose search space is considered much smaller. 17 
esides theorem proving, many real-world problems deal with unbounded search spaces, such as program synthesis [Gulwani et al. 2017] , drug discovery [Drews 2000 ], chemical synthesis [Segler et al. 2018] , computer-aided design [Haigh 1985 ], combinatorial optimization [Bengio et al. 2021 ], and more. 
These reasoning problems tend to exhibit similar structure, like the bijection between retrosynthesis in drug discovery and theorem proving in propositional logic, illustrated in Figure 10 : in both problems, one is building a tree of synthesis, whose nodes are chemical products on the one side and propositions on the other, and the leaf nodes are the products on the one side, and end axioms on the other. 
In these problems, a simulated environment is often provided, which allows a solver to run several search threads towards building the solution tree. 
The simulator often provides intermediate feedback, say, informing the solver with the remaining propositions to establish before the proof is considered complete. 
The solver in turn needs to select the most promising search thread and proceed based on the intermediate feedback. 
Recently, there has been a surge of interest in applying learning-based approaches to tackle reasoning problems. 
To overcome the unbounded search space challenge, researchers first started with a constrained search space to make the problem tractable [Huang et al. 2018; Bansal et al. 2019 ]. 
But such approaches suffered from the limited kinds of actions the solver could issue. 
For example, the solver could only apply theorems from a known database to prove the target theorem, instead of synthesizing novel theorems and lemmas. 
Because large language models offered a generic way of modeling the output space as a sequence, they quickly became a more favorable choice, allowing the generation of arbitrary kinds of actions. 
Researchers have applied these language model-based approaches to various applications, such as predicting protein structures [Senior et al. 2020] , proving formal theorems [Polu and Sutskever 2020; Han et al. 2021] , conjecturing theorems [Urban and Jakubuv 2020; Rabe et al. 2021; Li et al. 2021b ], synthesizing programs from natural language [Chen et al. 2021f; Ling et al. 2016] , repairing, generating and understanding code [Yasunaga and Liang 2021; Lu et al. 2021b; Guo et al. 2020; Svyatkovskiy et al. 2020; Kim et al. 2021b; Zügner et al. 2021 ]. 
It has also been shown that scaling model size significantly improves reasoning capabilities [Polu and Sutskever 2020] , and furthermore standard techniques from language modelling, such as pretraining, can also greatly improve performance on these tasks [Rabe et al. 2021; Polu and Sutskever 2020] . 
2.4.2 
What's the role of foundation models? 
Generativity. 
We believe that the generative capabilities of foundation models are essential for effective reasoning. 
Due to the unbounded search space, it becomes intractable to enumerate all kinds of possibilities. 
Instead, with foundation models, one can model the distribution of the optimal decisions, and generate suitable candidates to proceed to the next step. 
In particular, as foundation models offer a generic way of modeling the output space as a sequence, the next decision generation is entirely unconstrained and hence universal. 
Such flexibility is essential for many of the reasoning challenges we discussed, to allow creative generation in domains such as mathematical conjecturing [Li et al. 2021b ] and synthesizing novel programs [Chen et al. 2021f ]. 
As one scales up foundation models, the capabilities of capturing such statistical structures also grow immensely [Polu and Sutskever 2020] . 
Universality. 
As we mentioned in the last section, many reasoning problems exhibit similar latent structures. 
We believe that the unifying framework imposed by a foundation model can transfer and share significant heuristics across tasks, ranging from generalizing low-level techniques that work well for one task to new scenarios all the way to directly finding meta-techniques that work well across numerous kinds of problems. 
In addition, since a foundation model is trained across many domains, it can positively transfer meta-knowledge encoded in the foundation models' weights across tasks and domains [Papadimitriou and Jurafsky 2020; Wu et al. 2021f; Lu et al. 2021a ]. 
The foundation model training and adaptation framework encourage a separation of concerns, where foundation model training learns meta-knowledge such as the shared search tree structure between drug retrosynthesis and propositional logic proofs, and the adaptation phase can focus on learning the task specific vocabulary. 
Thus, foundation models can reduce the complexity of the learning problem in the adaptation phase, improving sample complexity and generalization. 
Grounding. 
Reasoning problems are often easily expressed in symbolic languages (e.g., mathematics, code, SMILE representation of molecules). 
However, these symbols have deep underlying semantic meanings -saying "isosceles triangle" paints a vivid image in the human mind. 
Foundation models can enable deep groundings and semantic meanings. 
First, grounding representations in other modalities, such as visual or physical, are essential to grasp abstract concepts in reasoning tasks and endow them with concrete meaning [Larkin and Simon 1987; Jamnik 2001 ]. 
Since the models may be trained on multiple modalities, foundation models can assist in understanding a range of data sources (e.g., images, texts). 
Hence, in the geometry example case, with its understanding of geometrical shapes learned in natural images, a foundation model could effectively utilize the diagrammatic representation of the problem. 
However, aligned multi-modal data in reasoning is scarce, and it remains an open question whether foundation models can discover connections between different modalities in an unsupervised manner (e.g., discovering of commutative diagram with the corresponding algebraic equations). 
Furthermore, even within the symbolic domain, symbols can have various levels of interpretation. 
For example, high-level programming languages can be translated to low-level assembly codes. 
Foundation models can learn a shared representation that encompasses these various views. 
Past works have shown that self-supervised tasks [Han et al. 2021; Peng et al. 2021; Li et al. 2021a ] allow the model to understand the inner workings behind the high-level code scripts, and further assist downstream tasks. 
2.4.3 Future challenges in reasoning.
Due to the intrinsic difficulty of these problems, high-quality annotated data is scarce and harder to collect compared to raw images and text. 
There have been several attempts towards alleviating this issue. 
In mathematics, researchers proposed to generate synthetic theorems in the hope of generalizing to realistic theorems [Wang and Deng 2020; Wu et al. 2021a; Firoiu et al. 2021; Zhou et al. 2021c ]. 
Another approach is to design self-supervised tasks to augment datasets [Yasunaga and Liang 2020; Ren et al. 2020; Han et al. 2021; Rozière et al. 2021; Yasunaga and Liang 2021] , or better pretraining objectives [Wu et al. 2021f ]. 
However, we still lack general principled approaches in designing self-supervised tasks, as most of the existing works are tailored to specific problem setups [Yasunaga and Liang 2020; Ren and Leskovec 2020; Han et al. 2021] . 
Building a foundation model will encourage a unifying framework of constructing a suite of self-supervised tasks that can be applied to all reasoning problems. 
In addition, interactivity ( §2.5: interaction) could, with enough scalability, alleviate the data scarcity problem by bringing humans into the loop to minimally guide the learning curriculum or data augmentation process, for example, in selecting axioms to add or conjectures to explore, while interactive tools themselves are a motivating use of foundation models for reasoning [Han et al. 2021; Chen et al. 2021f ] in assisting people with the most cognitively demanding or laborious aspects. 
Interpretation-friendly interactive tools could find further applications in education by assisting humans in learning with the help of highly capable foundation models ( §3.3: education). 
Improving the high-level reasoning capabilities is a core challenge for existing foundation models. 
Humans perform abstract reasoning and high-level planning in tackling difficult problem-solving tasks [Miller et al. 1960] . 
For example, when building a software tool or proving a theorem, we often start with a high-level sketch before delving into the low-level details [Koedinger and Anderson 1990] . 
Existing foundation models are not trained to generate such high-level plans. 
Instead, they often focus solely on predicting the next low-level steps [Polu and Sutskever 2020; Han et al. 2021; Chen et al. 2021f ]. 
Unfortunately, to train foundation models to emulate human-like reasoning, we again face a data collection challenge. 
Although such data does exist in limited settings [Li et al. 2021b] , in general, data for high-level reasoning is scarce and difficult to collect. 
One line of research is to let abstract and modular hierarchy to emerge by itself during learning [Ellis et al. 2021; Hong et al. 2021 ], but it still remains an open question how to scale these approaches to more general and realistic settings. 
Aside from these challenges, there exist many open questions that are also essential to topics discussed in other sections. 
What constitutes a good architecture for reasoning reliably ( §4.1: modeling)? 
How can we understand and interpret these models theoretically ( §4.10: theory and practically §4.11: interpretability)? 
Can we train robust reasoning models that could generalize to out-of-domain problems ( §4.8: robustness and §4.3: adaptation)? 
We believe research about foundation models on each of these fronts can greatly broaden their impact for the field of reasoning. 
2.5 Interaction
Authors: Joon Sung Park, Chris Donahue, Mina Lee, Siddharth Karamcheti, Dorsa Sadigh, Michael S. Bernstein Fig. 11 . 
Foundation models will bring significant opportunities to developers by lowering the difficulty threshold for building AI-infused applications, and to the application users by raising the ceiling for what types of interactions are achievable. 
In some cases, the line between developers and users will start to blur, and users may be able to easily develop their own AI applications, for instance with natural language. 
The early forms of foundation models such as GPT-3 [Brown et al. 2020 ] and DALL•E [Ramesh et al. 2021] have demonstrated a high level of versatility both in terms of their ability to let even non-ML experts to prototype powerful AI-infused applications, and their ability to seamlessly integrate modalities ranging from texts to images. 
As the development of foundation models matures, the models' capacity will continue to expand and their versatility may ultimately lead to fundamental changes in how we interact with AI by allowing us to rapidly prototype and build highly dynamic and generative AI-infused applications. 
In this section, we discuss the opportunities that these changes present from the perspectives of two important stakeholders: (1) applications developers who will interact with foundation models to design user experience, and (2) end-users who will use or be affected by the AI-infused applications powered by foundation models. 
Finally, we consider scenarios in which the line that rigidly separates developers and end-users today may start to blur, affording new opportunities for creating AI-infused applications that more closely satisfy users' needs and values. 
2.5.1 Impact on AI-infused application developers' development process.
How will foundation models transform the way developers create AI-infused applications? 
Despite the monumental progress in machine learning algorithms and systems infrastructure, some point out that designing novel and positive forms of human-AI interaction remains difficult [Dove et al. 2017; Cooper et al. 2014] . 
The vast amount of data, computing resources, and skills needed to create a powerful task-specific model is frequently in conflict with the iterative prototyping process necessary to elicit and satisfy users' needs and values [Yang et al. 2016 ]. 
This challenge is further compounded by the fact that AI responses can be unpredictable, and models can produce a vast generative output space, making it difficult for people to build effective mental models of their performance. 
There has already been some progress on tackling these challenges in the form of work on interactive machine learning (e.g., Crayon [Fails and Olsen 2003] , Regroup [Amershi et al. 2012] ) and design frameworks for conveying uncertainty in AI to end-users (e.g., principles of mixedinitiative [Horvitz 1999] ). 
However, more work is still needed to overcome these obstacles [Yang et al. 2020] . 
Foundation models pose important opportunities to address many of the challenges mentioned above. 
For instance, language-based foundation models' ability to take natural language as input, and to generalize to many downstream tasks, could significantly lower the difficulty "threshold" [Myers et al. 2000] for application development, i.e., by enabling the development of sophisticated models without having to collect significant amounts of data and train large models from scratch. 
This could enable even non-ML experts to quickly prototype AI-infused applications. 
At the same time, the powerful generative and potentially multi-modal capabilities of foundation models could offer a far higher "ceiling" [Myers et al. 2000 ] of what types of interactions are achievable both in terms of their quality and diversity as we will discuss below. 
However, how successfully we can leverage these capacities will depend on how effectively we can wrangle foundation models into forms that will be more manageable by application developers. 
Unfortunately, the same generalizability and high ceiling that give foundation models their edge can also make these models difficult to work with, as they may be even more unpredictable and complex than single-purpose AI models. 
Indeed, recent work has shown that it can be difficult to make models like GPT-3 consistently perform the intended task [Reynolds and McDonell 2021] , while understanding what it is capable of is still an active area of research [Hendrycks et al. 2021a] . 
In an effort to improve the reliability and trustworthiness of AI-infused applications, we recommend that future work should continue to investigate how to achieve more predictable and robust behaviors from foundation models (e.g., through fine-tuning, or in cases where the main mode of interaction is natural language prompt, through prompt-engineering [Reynolds and McDonell 2021; Liu et al. 2021d] , calibrating [Zhao et al. 2021] , or pre-formatting a task-specific endpoint. 18 
Please see §4.8: robustness for more details). 
2.5.2 Impact on end-user interaction with AI-infused applications.
Beyond the new ways developers might create AI-infused applications, what changes will foundation models bring to the experience for end-users interacting with these applications? 
Existing design frameworks for developing user-facing AI applications focus on augmenting (rather than replacing) users' abilities as described by Douglas Engelbart [Engelbart 1963 ] -we expect that these frameworks should and will remain relevant for the development of future AI-infused applications. 
For instance, maintaining users' agency and reflecting their values will continue to be a central theme for foundation model-powered applications. 
Additionally, the benefits of allowing AI agents to take initiatives and automate users' routines versus the benefits of waiting for users' direct manipulation [Shneiderman and Maes 1997] will need to be carefully weighed [Horvitz 1999 ]. 
Moreover, users' values should be directly gathered and reflected through processes such as participatory [Lee et al. 2019 ] and value-sensitive design [Smith et al. 2020 ] that advocate for actively involving all stakeholders during the designing of the AI-infused applications. 
These issues may become especially salient with foundation models because the model may behave in ways that surprise and disappoint users and communities. 
Generative capabilities might expose biases or points of view that are counter to the communities' goals, or more insidiously, draw on such associations in their behavior without the community being aware. 
This will place a large burden on the groups utilizing foundation models to monitor their models' behavior, and to the extent possible, adapt them to act in appropriate ways. 
While the design frameworks for thinking about AI-infused applications to augment users' abilities should remain the same, the actual forms of interactions that are attainable may dramatically diversify due to foundation models' powerful generative and multi-modal capacities. 
Already, early generations of what can be considered foundation model-powered software tools for multimedia creation and editing have started to drive a new frontier that empowers even novice content creators to generate high-quality multimedia from coarse, intuitive specifications (e.g., collaborative authoring for writers [Lee et al. 2022 ], text-to-image generation for digital artists, 19 mastering for musicians, 20 and code completion for programmers). 21 
Improved foundation models might enable even more ambitious tools (e.g., a fan might provide thematic material for a song which will then be generated in the style of their favorite band, or a business owner might provide simple descriptions of their product which will be used to create a full website). 
Moreover, foundation models will be used to enrich static multimedia (e.g., automatically remastering legacy multimedia content into new formats, or generating unique experiences for each player in new video games) and may even lead to new forms of multi-modal interactions using interfaces that themselves mix different modalities, such as visual and gesture-based interaction. 
We are starting to see glimpses of how foundation models might materialize into concrete interactions in applications ranging from AI Dungeon 22 to Microsoft PowerApps 23 and CoPilot. 24 
s we start to envision new forms of interactions, it is of increasing importance for us to think critically about the potential implications these interactions will have on individual users and society to maximize their positive impact. 
For example, how will foundation model-powered applications change the way we communicate with one another? 
Will a powerful model write emails in our stead and if so, how will this reshape people's trust, credibility, and identity knowing that the writers may not have written the emails themselves, and how will this alter our writing styles [Hancock et al. 2020 ]? 
Who will own the authorship of the model-generated content and how could the shifting responsibilities and ownership of the consent be misused [Weiner 2018 ] (see §5.5: economics for a more in-depth discussion)? 
What are the long-term implications that foundation models will have on our work, language and culture [Hancock et al. 2020; Buschek et al. 2021 ]? 
Of particular relevance to this last question is the fact that foundation models are trained on observed data and do not necessarily inform us about causality. 
Hence, how can we ensure that the use of foundation models leads us to a desired future and not a repetition of the past? 
Though these issues are not necessarily unique to foundation models, they will be amplified and become more prevalent as foundation models accelerate the creation of effective AI-infused applications. 
2.5.3 Blurring the line between developers and end-users.
Today, the line that separates the developers of AI models and end-users is rigid -it is rarely the case that an end-user has the data, computing resources, and expertise to be able to develop a new model that suits one's values and needs well. 
While a generic model (i.e., one that is not specific to a specific user or community) could be sufficient in some cases, recent years have seen an increasing number of scenarios in which such models fail to serve users. 
For instance, a text classification model designed to identify problematic comments for one online community might work well for that community but will fail in others whose norms and cultures may differ significantly (e.g., NSFW communities on Reddit might be more tolerant of certain content, while science communities might reject seemingly mundane anecdotes that are not based on scientific research) [Chandrasekharan et al. 2018] . 
In another example, AI-powered sensors and robotics tools designed for one target population may fail without the ability to quickly adapt in-context for users with different abilities and needs [Karamcheti et al. 2021] . 
While recent work has presented promising avenues for future research on how end-users may be able to co-create AI models by manually providing models' parameters or datasets (e.g., WeBuildAI [Lee et al. 2019] ), the results are still preliminary and often focus on rudimentary models. 
If foundation models can sufficiently lower the difficulty threshold for building AI-infused applications, they could present an important opportunity to more tightly couple users' needs and values with the models' behaviors by allowing users to actively partake in the development process of the models. 
Recent work has shown that GPT-3, for example, can robustly perform classification tasks in a few-shot or even in zero-shot fashion when given an adequate task description in its natural language prompt [Brown et al. 2020 ]. 
An online community trying to moderate its own content might be able to leverage such a capability to create bespoke AI classifiers that filter content based on classification task descriptions that the community has agreed on (of course, this power could also be instead misused to silence the voices of certain members within the community -we point to §5.2: misuse for further discussion on this topic). 
In addition, the powerful in-context learning capabilities that foundation models will exhibit may allow foundation model-powered applications to more effectively optimize their interfaces on a per-user basis. 
This could open doors to tackling many salient problems in human-computer and robot interaction such as balancing the power of users' direct manipulation and automation in mixed-autonomy settings. 
Of course, there will still be important challenges that we would need to overcome to truly realize this potential for blurring the line between users and developers. 
These challenges include mitigating existing biases in foundation models, as well as making the models' behavior more robust and manageable even for non-ML experts (compared to ML experts, it could be even more difficult for non-ML experts to understand the full capacities and mechanisms of foundation models, which can lead to unexpected pitfalls in the development cycle [Yang et al. 2018] ). 
Future work should explore how foundation models could be situated in the context of interactive machine learning and study how we can support even those with limited experience with machine learning to leverage these models in a robust manner. 
Nonetheless, the ability for end-users to be involved in developing AI-infused applications is an exciting opportunity that could introduce a new paradigm for how we will interact with these applications in the future. 
2.6 Philosophy of understanding
Authors: Christopher Potts, Thomas Icard, Eva Portelance, Dallas Card, Kaitlyn Zhou, John Etchemendy What could a foundation model come to understand about the data it is trained on? 
An answer to this question would be extremely informative about the overall capacity of foundation models to contribute to intelligent systems. 
In this section, we focus on the case of natural language, since language use is a hallmark of human intelligence and central to the human experience. 
The best foundation models at present can consume and produce language with striking fluency, but they invariably lapse into the sort of incoherence that suggests they are merely "stochastic parrots" [Bender et al. 2021] . 
Are these lapses evidence of inherent limitations, or might future foundation models truly come to understand the symbols they process? 
Our aim in this section is to clarify these questions, and to help structure debates around them. 
We begin by explaining what we mean by foundation model, paying special attention to how foundation models are trained, since the training regime delimits what information the model gets about the world. 
We then address why it is important to clarify these questions for the further development of such models. 
Finally, we seek to clarify what we mean by understanding, addressing both what understanding is (metaphysics) and how we might come to reliably determine whether a model has achieved understanding (epistemology). 
Ultimately, we conclude that skepticism about the capacity of future models to understand natural language may be premature. 
It is by no means obvious that foundation models alone could ever achieve understanding, but neither do we know of definitive reasons to think they could not. 
2.6.1 What is a foundation model?
There is not a precise technical definition of foundation model. 
Rather, this is an informal label for a large family of models, and this family of models is likely to grow and change over time in response to new research. 
This poses challenges to reasoning about their fundamental properties. 
However, there is arguably one defining characteristic shared by all foundation models: they are self-supervised. 
Our focus is on the case where self-supervision is the model's only formal objective. 
In self-supervision, the model's sole objective is to learn abstract co-occurrence patterns in the sequences of symbols it was trained on. 
This task enables many of these models to generate plausible strings of symbols as well. 
For example, many foundation models are structured so that one can prompt them with a sequence like "The sandwich contains peanut" and ask them to generate a continuation -say, "butter and jelly". 
Other models are structured so that they are better at filling in gaps; you might prompt a model with "The sandwich contains __ and jelly" and expect it to fill in "peanut butter". 
Both capabilities derive from these models' ability to extract co-occurrence patterns from their training data. 
There is no obvious sense in which this kind of self-supervision tells the model anything about what the symbols mean. 
The only information it is given directly is information about which words tend to co-occur with which other words. 
On the face of it, knowing that "The sandwich contains peanut" is likely to be continued with "butter and jelly" says nothing about what sandwiches are, what jelly is, how these objects will be combined, etc. 
This might seem to suggest an inherent limitation on what a foundation model could achieve. 
However, we need not restrict the model to seeing only textual input. 
A foundation model might be trained on a wide range of different symbols: not just language but also computer code, database files, images, audio, and sensor readings. 
As long as it is just learning co-occurrence patterns of the sequences it is exposed to, then it counts as a foundation model by our definition. 
As part of this learning, the model might come to represent strong associations between a given piece of text and a particular sensor reading, or between a sequence of pixel values and a database entry. 
These associations might reflect important aspects of the world we inhabit and the language we use to talk about it. 
2.6.2 What is at stake?
Before considering analyses of what understanding is, it is worth reflecting on why we might care about the question of whether a foundation model could achieve it. 
These models are poised to be deployed for numerous purposes with various functionalities. 
Some of our goals in deployment may only be met to the extent that the model is capable of understanding. 
Here we list a few such goals: 
• Trust: One might argue that we cannot trust a system's linguistic behavior unless it understands the language it is using. 
Of course, we currently trust engineered systems to do things (e.g., manufacturing auto parts) without the question of understanding even arising, but language might be special in this regard, since it is uniquely human. 
In addition, language can be used to deceive and misrepresent, so understanding alone clearly does not imply trust. 
On the whole, then, understanding might be taken as a necessary condition for trust in the context of language use. 
• Interpretability: If genuine natural language understanding in some way involves maintaining and updating an internal model of the world (including, e.g., the speech context), and if we (as engineers) are able to analyze how linguistic input and output interface with this internal model, that could afford substantial gains in interpretability, predictability, and control of these systems. 
• Accountability: Not unrelated to the previous points, in the future we may find it desirable to hold artificial agents in some way accountable for the language they produce [The HAI Adaptive Agents Group 2021]. 
Depending on how we think about concepts like accountability, responsibility, agency, and the like, language understanding may emerge as a prerequisite. 
The mere possibility that understanding will play an indispensable role in any of these matters provides strong motivation to develop a framework for theorizing about it. 
2.6.3 What is understanding?
Our central question is whether a foundation model could come to understand a natural language. 
With the above, we can now sharpen it: is self-supervision sufficient for understanding, keeping in mind that there are no constraints on the data used for this supervision? 
In order to address this question, we first need to define what we mean by understanding. 
As a start, we find it helpful to make explicit a distinction that is sometimes conflated in discussions of the topic. 
The distinction is between the metaphysics and the epistemology of understanding. 
Metaphysics concerns what it would mean ("in principle") for an agent to achieve understanding. 
Epistemology, by contrast, concerns how ("in practice") we could ever come to know that an agent has achieved the relevant type of understanding. 
In short, metaphysics is more about our ultimate target, whereas epistemology is more about how (if at all) we could know when we have reached it. 
Our epistemology thus depends to some extent on our metaphysics. 
Metaphysics of understanding. Philosophy of language offers a number of alternatives for
what it is to understand natural language. 25 
Simplifying the landscape for the sake of brevity, the following three broad classes of views all have connections with research lines in AI and NLP: 26 
• Internalism: Language understanding amounts to retrieval of the right internal representational structures in response to linguistic input. 
Thus, language understanding is not even a possibility without a rich internal conceptual repertoire of the right kind. 
• Referentialism: Roughly, an agent understands language when they are in a position to know what it would take for different sentences in that language to be true (relative to a context). 
That is, words have referents and (declarative) utterances are truth-evaluable, and understanding involves a capacity to evaluate them relative to presentation of a situation or scenario. 
• Pragmatism: Understanding requires nothing in the way of internal representations or computations, and truth and reference are not fundamental. 
Rather, what matters is that the agent be disposed to use language in the right way. 
This might include dispositions toward inference or reasoning patterns, appropriate conversational moves, and so on. 
Crucially, the relevant verbal abilities constitute understanding. 27 
hile this is a simplified picture of the space of possibilities, we already see how they relate in quite different ways to the goals mentioned above. 
On the pragmatist view, for instance, achieving language understanding does not imply anything about our ability to trust or interpret the system, insofar as it guarantees nothing about the agent's internal structure or its relation to the (non-linguistic) world. 
On the internalist view, by contrast, a fairly robust kind of internal/causal interpretability is at least strongly suggested. 
The question of whether or not a foundation model could understand language in principle takes on a very different character depending on which of these metaphysical characterizations we adopt. 
Internalism and referentialism can both be cast as defining a mapping problem: to associate a linguistic sign with a "meaning" or a "semantic value". 
For internalism this will be a representation or concept, a program for computing a value, or some other type of internal object. 
For referentialism, it might be a mapping from a word to an external referent, or a mapping from a situation to a truth value (all relative to a context). 
Could self-supervision suffice for achieving the desired mapping in a foundation model? 
Here, the nature of the training examples might be relevant. 
If the model receives only linguistic inputs, then its capacity to learn this mapping might be fundamentally limited in ways that prevent it from learning to refer in the relevant sense. 
(Indeed, Merrill et al. [2021] identify some theoretical limits, albeit under very strong assumptions about what it means to learn the meaning of a symbol.) 
However, if the input symbol streams include diverse digital traces of things in the world -images, audio, sensors, etc. -then the co-occurrence patterns might contain enough information for the model to induce high-fidelity proxies for the required mapping. 28 
For referentialism, there is still a further question of how these proxies relate to the actual world, but the same question arises for human language users as well. 
 Bender and Koller [2020] give an interesting argument that combines referentialism with pragmatism. 
They imagine an agent O that intercepts communications between two humans speaking a natural language L. O inhabits a very different world from the humans and so does not have the sort of experiences needed to ground the humans' utterances in the ways that referentialism demands. 
Nonetheless, O learns from the patterns in the humans' utterances, to the point where O can even successfully pretend to be one of the humans. 
Bender and Koller then seek to motivate the intuition that we can easily imagine situations in which O's inability to ground L in the humans' world will reveal itself, and that this will in turn reveal that O does not understand L. The guiding assumption seems to be that the complexity of the world is so great that no amount of textual exchange can fully cover it, and the gaps will eventually reveal themselves. 
In the terms we have defined, the inability to refer is taken to entail that the agent is not in the right dispositional state for understanding. 
Fundamentally, the scenario Bender and Koller describe is one in which some crucial information for understanding is taken to be missing, and a simple behavioral test reveals this. 
We can agree with this assessment without concluding that foundation models are in general incapable of understanding. 
This again brings us back to the details of the training data involved. 
If we modify Bender and Koller's scenario so that the transmissions include digitally encoded images, audio, and sensor readings from the humans' world, and O is capable of learning associations between these digital traces and linguistic units, then we might be more optimistic -there might be a practical issue concerning O's ability to get enough data to generalize, but perhaps not an in principle limitation on what O can achieve. 29 
e tentatively conclude that there is no easy a priori reason to think that varieties of understanding falling under any of our three positions could not be learned in the relevant way. 
With this possibility thus still open, we face the difficult epistemological challenge of clarifying how we could hope to evaluate potential success. 
Epistemology of understanding.
A positive feature of pragmatism is that, by identifying success with the manifestation of concrete behaviors, there is no great conceptual puzzle about how to test for it. 
We simply have to convince ourselves that our limited observations of the system's behavior so far indicate a reliable disposition toward the more general class of behaviors that we took as our target. 
Of course, agreeing on appropriate targets is very difficult. 
When concrete proposals are made, they are invariably met with objections, often after putative success is demonstrated. 
The history of the Turing Test is instructive here: although numerous artificial agents have passed actual Turing Tests, none of them has been widely accepted as intelligent as a result. 
Similarly, in recent years, a number of benchmark tasks within NLP have been proposed to evaluate specific aspects of understanding (e.g., answering simple questions, performing commonsense reasoning). 
When systems surpass our estimates of human performance, the community's response is generally that the test was flawed, not that the target was reached. 
There may be some suite of behaviors that is our real target, but it is just hard to circumscribe or turn into a practical test. 30 
Then again, this might reveal that internalism or referentialsm are what we had in mind all along. 
If we take internalism or referentialism as the ultimate target -our gold standard for what understanding is -then behavioral tests will always be at best imperfect as a means of assessing whether understanding has been achieved. 
The imperfections are two-fold. 
First, behavioral tests will always have gaps that could allow unsophisticated models to slip through. 
Second, a system might have achieved the mapping that these views require, but we may be unable to show this with behavioral testing. 
Recent experiences with the model GPT-3 show how challenging this might become: depending on the prompt one uses, one can see surprisingly coherent outputs or utter nonsense, and so prompt engineering requires deep expertise [Rong 2021 ]. 
Thus, both internalism and referentialism call for structural evaluation methods that allow us to study their internal representations, probing them for information [Tenney et al. 2019; Manning et al. 2020] , studying their internal dynamics [Sundararajan et al. 2017] , and perhaps actively manipulating them according to specific experimental protocols supporting causal inference [Vig et al. 2020; Geiger et al. 2020 ]. 
There may be fundamental limitations on what we can learn from practical experiments about the inner workings of a complex foundation model, but it is clear that these methods will be useful whenever our target aligns with internalism or referentialism. 
2.6.4 Moving the discussion forward.
It seems clear that there are no easy answers to the question of whether foundation models will ever understand language. 
To even begin to address the question, one must resolve a difficult metaphysical question about which there are a number of substantively distinct views. 
The metaphysical question then feeds into an epistemological question that poses many practical challenges. 
Nonetheless, the above discussion does invite one practical conclusion: if foundation models are pursued as a path to language understanding in artificial agents, then multimodal training regimes may well be the most viable strategy, as they would seem the most likely to provide the model with the requisite information. 
Whether self-supervision then suffices is a completely open question. 
3 APPLICATIONS
The capabilities ( §2: capabilities) of foundation models indicate that they have the potential to transform various sectors and industries, extending the role AI plays in society ( §5: society). 
Among the myriad applications where foundation models may be applied, we will focus on three disciplines -healthcare ( §3.1: healthcare), law ( §3.2: law), and education ( §3.2: law) -that are all foundational to societal function. 
Within each, we discuss the opportunities that foundation models pose for this domain alongside challenges (e.g., interpretability; §4.11: interpretability) and concerns (e.g., privacy; §4.7: security). 
3.1 Healthcare and biomedicine
Authors: Michihiro Yasunaga, Jing Huang, Camilo Ruiz, Yuhui Zhang, Giray Ogut, Saahil Jain, William Wang, Yusuf Roohani, Hongyu Ren, Antoine Bosselut, Ehsan Adeli, Jure Leskovec, Russ Altman Fig. 12 . Foundation models in healthcare and biomedicine. 
We visualize an interactive framework where foundation models enable various tasks across healthcare and biomedicine when trained on multimodal data generated by various sources in the healthcare ecosystem. 
The first column lists several sources of data, including care providers, payers, institutions (universities, non-profits, and governments), pharma, wearables, and medical publications/forums. 
The second column shows several data modalities generated by the data sources. 
They include images (e.g., chest X-rays), videos (such as ultrasounds), graphs of chemical compounds, tables for electronic health records (EHRs), text such as clinical notes, time series such as ECGs, and genetic data. 
The third column visualizes a foundation model trained on such data and then applied to healthcare and biomedicine downstream tasks listed in the fourth column. 
This process can generate new data that will further improve the foundation model, hence the bidirectional relation between the foundation models and the tasks. 
Healthcare and biomedicine are an enormous application area in society, for instance, with expenditures accounting for 17% of gross domestic product (GDP) in the US [Swensen et al. 2011; van Hartskamp et al. 2019; Keehan et al. 2020] . 
Both healthcare (which focuses on the delivery of care to patients via diagnosis, treatment, and health administration) and biomedical research (which focuses on the scientific understanding of disease and the discovery of new therapies) demand significant expenses, time, and comprehensive medical knowledge [Yu et al. 2018; Korngiebel and Mooney 2021] . 
We envision that foundation models can be a central storage of medical knowledge that is trained on diverse sources/modalities of data in medicine [Krumholz et al. 2016; Soltanian-Zadeh 2019; Suresh et al. 2020 ] (Figure 12 left), and can be queried/updated interactively by medical professionals (e.g., healthcare providers and biomedical researchers access published findings and upload new publications) [Ionescu et al. 2020 ] and queried by the public. 
As foundation models have strong adaptation capabilities (e.g., fine-tuning, prompting [Brown et al. 2020] ), they can be efficiently adapted to various individual tasks in healthcare and biomedicine (e.g., question answering app used by patients [Klasnja and Pratt 2012; Zhu et al. 2019; Daniel et al. 2019; Liu et al. 2020a] , clinical trial matching system [Ni et al. 2015; Harrer et al. 2019; Beck et al. 2020] accessed by researchers and patients; Figure 12 right). 
This way, foundation models can be a central interface that supports various interactions between data, tasks, and people in healthcare and biomedicine, thereby advancing the efficiency and accuracy of healthcare/biomedical applications [Elbattah et al. 2021 ]. 
We elaborate these opportunities in §3.1.1: 
healthcare-tasks and §3.1.2: 
biomed-tasks. 
At the same time, healthcare/biomedical applications pose unique challenges that motivate further research in foundation models, such as integrating multimodal data in healthcare/biomedicine [Miura et al. 2021; Liu et al. 2021a ] and observing ethical and legal regulations in medicine (privacy, safety and explainability) [Guan 2019; Xu et al. 2019 ]. 
We elaborate these challenges in §3.1.3: 
healthcare-biomed-challenge. 
3.1.1 Opportunities in healthcare.
Foundation models may improve the delivery of care to patients through healthcare providers and hospitals. 
Currently, healthcare cost increases every year [Keehan et al. 2020] , and studies estimate that 30% of healthcare spending may be wasteful due to administrative inefficiency and preventable medical errors [Kocher 2021]. 
Moreover, as the demand for healthcare increases, the society faces a serious shortage in healthcare providers [Kirch and Petelle 2017] . 
This inefficiency and shortage in healthcare necessitate developing fast and accurate interfaces for healthcare providers and patients, such as automated aid systems for diagnosis/treatment, summarization of patient records, and answering of patient questions [Davenport and Kalakota 2019; Nie et al. 2018; Wang et al. 2021b ]. 
In particular, in an urgent pandemic crisis such as COVID-19, fast diagnosis/screening (e.g., automatic analysis of chest X-ray images) as well as automated question answering for patients (e.g., symptom checking and care) and the public (e.g., disease prevention) are vital to reduce the spread of diseases and allocate healthcare resources for critical patients, saving more lives [Lalmuanawma et al. 2020] . 
As foundation models have a strong capability to serve as an integrated knowledge reservoir, they can be queried and adapted to various individual tasks in healthcare. 
Below are examples of important tasks in healthcare that would benefit from foundation models. 
Interface for healthcare providers. 
Foundation models can improve the efficiency and accuracy of care by providers. 
Healthcare providers spend unnecessary time editing electronic heath records (EHRs) [Kocher 2021], and preventable medical errors (e.g., hospital readmissions, surgical errors) cause wastes in healthcare [Shrank et al. 2019; Shah et al. 2020] . 
Foundation models can be adapted as an efficient and accurate interface into EHRs (clinical notes, lab value histories and imaging files) [Li et al. 2020c; Steinberg et al. 2021; Percha 2021] , helping healthcare providers create summaries of patient visitation [Krishna et al. 2020] , retrieving relevant cases and literature, and suggesting lab tests, diagnosis, treatments and discharges [Zhang et al. 2019b; Rasmy et al. 2021] . 
Foundation models can also be adapted to help a surgical robot monitor and achieve accurate surgeries [Diana and Marescaux 2015; Agrigoroaie and Tapus 2016; Yu et al. 2019 ]. 
See §2.3: robotics for more discussions on foundation models for robotics. 
Interface for patients. 
Foundation models can be adapted to serve as an interface to patients, providing relevant information about clinical appointments [Bates 2019 ], answering patient questions related to preventive care [Demner-Fushman et al. 2020] , along with relevant medical explanatory information (e.g., text and graphics that explain conditions) [Chaix et al. 2019] , and helping assistive-care robots for patients [Jeong et al. 2015; Abdi et al. 2018 ]. 
See §2.5: interaction for more discussion on foundation models for user interaction. 
Foundation models can also serve as an interface with the general public to answer questions related to public health and pandemic prevention (such as the COVID-19 case) [Bharti et al. 2020; Herriman et al. 2020] . 
At the same time, we note that the interface must guarantee factual accuracy to ensure public trust in medical advice [Kreps and Kriner 2020] (see §3.1.3: 
healthcare-biomed-challenge). 
3.1.2 Opportunities in biomedicine.
Foundation models may facilitate biomedical research such as discovery of drugs and understanding of diseases, which ultimately translates to improved healthcare solutions [Hanney et al. 2015 ]. 
Currently, biomedical discovery requires significant human resources, experimental time and financial costs. 
For instance, drug development involves a complex process, from basic drug research of protein target identification and potent molecule discovery to clinical development (e.g., clinical trials) to the final drug approval, which typically takes over 10 years and costs more than one billion dollars [Wouters et al. 2020 ]. 
Facilitating and accelerating biomedical discovery using existing data and published findings is an imperative problem in biomedicine [Yu et al. 2018] . 
In particular, a novel disease outbreak such as COVID-19 costs millions of lives and trillions of dollars [Lalmuanawma et al. 2020; McKibbin et al. 2020 ]; if we can speed up drug development for new diseases, that would be very helpful. 
Foundation models can be particularly helpful for biomedical discovery in two aspects. 
First, foundation models have a strong generative capability (e.g., coherent text generation in GPT-3), which can help generative tasks in biomedical research such as generating experimental protocols (clinical trials) and designing molecules that work (drug discovery) given existing data [Kadurin et al. 2017; Harrer et al. 2019 ]. 
Second, foundation models have a potential to integrate diverse data modalities in medicine, which enables investigating biomedical concepts (e.g., disease) from multiple scales (using molecule-, patient-and population-level data) and multiple knowledge sources (using imaging, textual and chemical descriptions). 
This facilitates biomedical discoveries that are difficult to obtain if using single-modality data [Lanckriet et al. 2004; Aerts et al. 2006; Kong et al. 2011; Ribeiro et al. 2012; Wang et al. 2014 Wang et al. , 2015c;; Ruiz et al. 2020; Wu et al. 2021h ]. 
Foundation models also enable transfer knowledge across modalities. 
 Lu et al. [2021a] showed how a transformer model trained on natural language (a data-rich modality) could be adapted for other sequence-based tasks such as protein fold prediction, which is a long-studied predictive task in biomedicine [Jumper et al. 2020] . 
Below are examples of important tasks in biomedicine that will benefit from foundation models. 
Drug discovery. 
To discover a drug or a therapeutic that treats a disease, researchers must first identify a target (e.g., proteins, genes, RNA causally implicated in the disease) and must then search for molecules (e.g., chemical compounds, antibodies) that bind to the target and treat the disease. 
 [Whirl-Carrillo et al. 2012; Tatonetti et al. 2012; Gerstung et al. 2017; Grinfeld et al. 2018; Adam et al. 2020] . 
Foundation models are uniquely powerful in their ability to integrate multimodal patient data ranging from the EHR [Rajkomar et al. 2018 ] to medical imaging [Bera et al. 2019; Ouyang et al. 2020 ] to drug and molecular measurements [Gottlieb et al. 2011; Ruiz et al. 2020] to make an optimal prediction. 
Clinical image [Chaitanya et al. 2020] , gene [Ji et al. 2021] , protein [Jumper et al. 2020] ), and do not jointly learn from diverse modalities. 
To learn the inter-modality and cross-modality information from these diverse multimodal medical data, we need to investigate both feature-level and semantic-level fusion strategies in the training of foundation models. 
If done effectively, this has a potential to unify biomedical knowledge and facilitate discoveries as discussed in §3.1.2: 
biomed-tasks. 
Explainability. 
Explainability -providing evidence and logical steps for decision making -is crucial in healthcare and biomedicine [Holzinger et al. 2019] , and is made obligatory under the General Data Protection Regulation (GDPR). 
For instance, in diagnosis and clinical trials, patient symptoms and temporal relevance must be explained as evidence. 
This helps the resolution of potential disagreement between the system and human experts. 
Explainability is also needed for informed consent in healthcare [Amann et al. 2020 ]. 
However, current foundation models' training objectives do not include explainability, requiring future research in this direction [Linardatos et al. 2021] . 
Incorporation of knowledge graphs may be a step to further improve model explainability [Roberts et al. 2020; Xu et al. 2020; Jin et al. 2021] . 
Readers are refered to §4.11: interpretability for more discussion on explainability. 
Legal and ethical regulations. 
Healthcare applications must observe legal and ethical regulations with guarantees, such as patient safety, privacy and fairness. 
For instance, regarding safety, predictions made by foundation models must be factually accurate with established medical knowledge, and must quantify uncertainty or choose to defer to an expert when uncertain [Challen et al. 2019; Mozannar and Sontag 2020] . 
For privacy, the use of patient health records must observe the privacy laws, such as HIPAA [Act 1996 ] in the case of the US. 
Federated learning is one potential solution to keeping the raw, sensitive data private in the training of foundation models [Chamikara et al. 2021 ]. 
For fairness, researchers will need to be mindful of common pitfalls or otherwise risk exacerbating existing social inequalities [Chen et al. 2019; Wiens et al. 2019; Chen et al. 2020b ]. 
They must ensure that the training and evaluation data for foundation models is sufficiently representative of different sexes, races, ethnicities and socioeconomic backgrounds; an area where medical datasets and clinical trials have had a long history of bias [Martinez-Martin et al. 2020; Kaushal et al. 2020] . 
Research is also needed to debias and regularize models to ensure fairness when representative data is scarce [Zhao et al. 2020a ]. 
Foundation model developers also need to consult with ethics and law researchers, and observe regulations in the specific circumstances (e.g., country, region) where they are deployed. 
We also refer readers to §4.7: security, §4.8: robustness, §5.1: fairness, §5.4: legality for details on privacy, robustness, fairness and legality. 
Extrapolation. 
The process of biomedical discovery involves extrapolation. 
For instance, foundation models must be able to quickly adapt to new experimental technologies (e.g., new assays, new imaging techniques such as high resolution microscopy) or new settings (e.g., new target diseases such as COVID-19) [Jaroch et al. 2018; Benam et al. 2019 ]. 
The ability to leverage existing datasets and extrapolate to new settings is a key machine learning challenge in biomedicine [Snell et al. 2017; Ma et al. 2021b ]. 
While GPT-3 exhibits some extrapolation behaviors (e.g., generating new text not seen before), its mechanism is unclear and still in its infancy. 
Further research is needed for improving the extrapolation capability of foundation models, especially when considering the diverse range of data modalities and tasks that is inherent to healthcare and biomedicine but is not commonly studied in current GPT-3 and related models. 
Also see §4.8: robustness. 
3.2 Law
Authors: Peter Henderson, Lucia Zheng, Jenny Hong, Neel Guha, Mark Krass, Julian Nyarko, Daniel E. Ho What role might foundation models play in the law? 32 
A major promise is that foundation models can improve access to justice and government services by leveling procedural and financial barriers to legal services. 
The challenges posed by legal applications can, in turn, motivate basic research questions for foundation models. 
Many legal applications pose unique challenges to computational solutions. 
Legal language is specialized and legal outcomes often rely on the application of ambiguous and unclear standards to varied and previously unseen fact patterns. 
At the same time, due to its high costs, labeled training data is scarce. 
Depending on the specific task, these idiosyncrasies can pose insurmountable obstacles to the successful deployment of traditional models. 
In contrast, their flexibility and capability to learn from few examples suggest that foundation models could be uniquely positioned to address the aforementioned challenges. 
Throughout this section, foundation models may take as context many modalities as evidence: audio during trial proceedings, video and images during discovery, and text in conducting legal research. 
Yet, the majority of legal tasks in which reliance on foundation models will be beneficial involve text-based inputs and outputs. 
As such, we mainly focus on text-based domains while only briefly discussing others. 
To ground the discussion, Figure 13 describes the stages of a civil lawsuit in the United States and where foundation models might come into play in this process. 
Figure 14 shows the logic flow required to generate just part of one paragraph of a legal brief, which might serve as a concrete example of a task that foundation models might one day be used for. 
An Important Consideration. 
Before proceeding, we note that the ethical, legal, and fairness considerations expanded on in §5.6: ethics, §5.4: legality, and §5.1: fairness are particularly important to examine before using foundation models in an applied legal or government context, as these applications often have important, real-world consequences to those affected [Surden 2020 ]. 
Foundation models must also be thoroughly scrutinized before deployment, as discussed in §4.4: evaluation. 
For example, the legal system places particular emphasis on -and may even mandate -transparency, accountability, and explainability. 
Consequently, it is questionable whether current models are positioned to solve many of the most pressing, legal problems. 
Nonetheless, the need to expand and improve access to legal and government services provides a worthy goal for foundation models. 
3.2.1 Opportunities in law.
Legal applications can range from the use of machine learning in government contexts [Engstrom et al. 2020 ; Coglianese and Ben Dor 2020; Re and Solow-Niederman 2019] to aiding lawyers in their provision of legal services [Zheng et al. 2021; Huang et al. 2021b; Ostendorff et al. 2021; Vold and Conrad 2021] . 
We note that prior work has also surveyed machine learning-assisted legal tasks in text-based domains [Zhong et al. 2020; Chalkidis et al. 2020] , although it has been noted that recent legal AI research has focused on geographic regions outside of the U.S. [Zheng et al. 2021] . 
While many of the topics we discuss here may be applicable to different legal systems, due to the expertise of our team we focus primarily on the U.S. In particular, we concentrate on three broad categories of legal applications that may benefit from foundation models in the U.S. legal system: private law or civil justice (claims between private individuals, arising out of, for instance, contracts, property or torts), criminal law (i.e., the prosecution of individuals for criminal behavior), and (non-criminal) public law (e.g., the regulation of private behavior by government agencies). 
Civil law. 
In U.S. civil proceedings, parties must typically find and pay attorneys to be represented. 
As a result, many individuals, especially those with low income, struggle to secure adequate legal representation [Rhode 2004] . 
Foundation models have the potential to improve access to justice by reducing the cost, improving the quality, and extending the reach of legal services. 
In Figure 13 , we describe the process by which a civil lawsuit is filed in a U.S. court and where foundation models may play a role in aiding both attorneys and judges. 
Even before an attorney is involved in the legal process, clients may benefit from the deployment of foundation models. 
Recent work has used machine learning models to identify the relevant legal issues contained in a plain-language description of facts presented by a client. 33 
Tools like these can help provide a recommendation for the type of legal action needed to address the issue at hand or to recommend a specialized attorney. 
A number of other similar efforts have sought to increase access to justice by providing information tailored to a client's particular needs [Cabral et al. 2012; Brescia et al. 2014; Queudot et al. 2020; Westermann et al. 2019] . 
Once a client speaks with an attorney, prior to civil litigation, the attorney may seek to avoid a costly trial. 
At this stage, they can rely on foundation models to evaluate contracts, review terms of service, find relevant patents, and conduct other pre-litigation processes in order to ensure that their clients are at an advantage [Betts and Jaep 2017; Elwany et al. 2019; Lippi et al. 2019; Lee and Hsiang 2019; Hendrycks et al. 2021c; Hegel et al. 2021] . 
Notably, recent work has both described the challenges and benefits of using foundation models for contract review [Leivaditi et al. 2020; Hegel et al. 2021; Hendrycks et al. 2021c ]. 
In addition to reviewing and drafting legal documents, client interactions and documents can be translated to reduce costs and barriers to the provision of legal services [Cuéllar 2019 ]. 
But translation of legal documents requires precision and an understanding of highly technical language, which makes collecting training data costly. 
Additionally, translating client statements or trial proceedings often requires an understanding of local dialects and language. 
This, too, makes it difficult to collect enough ground truth translation data to train on. 
As a result, traditional supervised methods rarely achieve the level of accuracy required in the legal domain [Vieira et al. 2020] . 
Foundation models may improve performance in this area over fully supervised mechanisms by adapting quickly in these low-resource contexts. 
During litigation, foundation models can help lawyers to conduct legal research, draft legal language, or assess how judges evaluate their claims [Zheng et al. 2021; Huang et al. 2021b; Ostendorff et al. 2021; Vold and Conrad 2021; Chalkidis et al. 2020 Chalkidis et al. , 2019]] . 
This could potentially reduce the costs of and improve legal services. 
For example, recent work has utilized pretrained models for the recommendation of relevant citations and holding statements when writing legal texts [Zheng et al. 2021; Huang et al. 2021b; Ostendorff et al. 2021 ]. 
Other work uses pretrained models for improved legal question answering to power commonly used legal search engines and help lawyers conduct legal research [Vold and Conrad 2021] . 
A wide variety of work has also examined automated contract drafting and review, a task that could similarly benefit from foundation models [Hendrycks et al. 2021c; Betts and Jaep 2017] . 
Perhaps most compelling, foundation models may help assist lawyers generate legal briefs (written arguments). 
The models might find novel arguments or identify problems in attorney-written portions of the brief. 
For example, Tippett et al. [2021] predict the outcome of a legal proceeding based on features extracted from the filed briefs. 
Foundation models can be leveraged to use raw language as inputs rather than extracted features. 
This might provide attorneys with more informative recommendations as to how their brief could be improved to ensure a favorable outcome. 
After opening and reply briefs are filed, parties then begin the discovery process, which has already used simple machine learning models for the better part of a decade [Grossman and Cormack 2010] . 
Attorneys use these systems to label whether a document should be produced to the opposing party. 
The documents are multi-modal in nature, often containing video, images, audio, and text. 
Current systems are costly because they used supervised learning and active learning to label the documents as responsive [Grossman and Cormack 2010; Oard et al. 2018; Yang et al. 2021] . 
Instead, few-shot or zero-shot document retrieval capabilities that might be possible with foundation models would help ease concerns about the large costs of the current process. 34 
To avoid the possibilities of gamesmanship in the discovery process, Cui [2018] has proposed a zero-shot (or few-shot) adaptation process that can only be operationalized through the use of foundation models. 
After discovery, once the trial begins, foundation models could help parties prepare for trial by predicting what the judge might focus on during questioning [Dickinson 2018], adapting to the current context from judges' prior published opinions. 
In the courtroom, foundation models might be used to examine audio and video of courtroom proceedings to determine if outcomes were biased against the defendant because of their race or dialect. 35 
nce the trial concludes, foundation models could help judges and law clerks to properly evaluate legal claims from both parties using similar technologies, or the use of contextual embeddings from foundation models might assist in statutory interpretation [Nyarko and Sanga 2020; Choi 2020 ]. 
Recent work (without reliance on foundation models or NLP) has examined whether an appeals decision can be predicted from a set of extracted features, like citation counts and the appearance of key words [Katz et al. 2017; Boniol et al. 2020] . 
It is possible that such models could be improved using foundation models and applied to help judges draft decisions by flagging obvious mistakes in their opinion, as has been discussed in the context of adjudicative agencies [Engstrom et al. 2020; Ray and Lubbers 2014] . 
They can also be used to identify racial biases in legal opinions and help judges revise their opinions accordingly [Rice et al. 2019 ]. 
Criminal law. 
One particularly contentious area has been the use of risk scores in government settings, particularly in criminal law. 
Some may want to use language-based foundation models to aid in making charging decisions or parole decisions based on a given text-based narrative of the events. 
Careful consideration must be taken before using foundation models for risk scoring due to the potential for biases, especially when language data is included [Bender et al. 2021; Berk et al. 2021; Laufer 2020 ]. 
But foundation models may play a role in many other dimensions of criminal justice. 
The same tools as in civil litigation, above, can also be used by prosecutors and defense attorneys. 
This can help appointed attorneys perform their job more efficiently and reduce unnecessary overhead. 
As a result, they may be able to balance already heavy caseloads more effectively. 
For example, public defenders are often viewed as being overworked and underfunded, which would lead to avoidable procedural errors. 36 
Foundation models can help reduce some of these resource constraints by identifying errors and automating simple tasks. 
However, they are not a solution on their own. 
In other areas, foundation models can act as an oversight mechanism to reduce structural inequities. 
Pretrained models have been used for processing parole hearing transcripts to find instances of anomalous outcomes [Bell et al. 2021] . 
Recent work has also removed linguistic cues for a suspect's race in police reports to promote race-blind charging decisions and avoid racially biased prosecutions [Chohlas-Wood et al. 2020] . 
Other work has helped identify disrespectful police communications [Voigt et al. 2017] . 
In these contexts, it is very costly to label data since annotators must be given access to sensitive data and appropriate background checks are often required. 
To reduce these costs, foundation models can be used to pretrain and adapt quickly to downstream tasks where labels are scarce. 
Public law. 
Government agencies regulate vast parts of society, and foundation models have wide potential applicability across public law. 
This includes: analyzing public comments in the noticeand-comment process, assisting patent examination, retrieving relevant documents in response to Freedom of Information Act requests, aiding in mass adjudication, among many others. 
Recent work has surveyed these government applications in a variety of contexts and we refer the reader to the relevant sources for in-depth discussion [Engstrom et al. 2020; Coglianese and Ben Dor 2020] . 
In many of these applications, foundation models can improve the quality, efficiency, utility, and accessibility of government services: labels are scarce, resources are constrained, and contexts are constantly shifting. 
As such, the adaptability and flexibility of foundation models are often required to improve efficiency and performance. 
To give an illustrative example of just one such application, existing work has leveraged NLP for facilitative moderation in public comment forums. 
In this use case, predictive models help lay-users improve arguments and identify misstatements in their comments. 
Such a system has already been deployed in the U.S. Department of Transportation rulemaking process [Park et al. 2012] , although it can likely be improved through the linguistic reasoning capabilities of foundation models. 
But government agencies must comply with constitutional, statutory, and administrative obligations (see §5.4: legality), so additional care is needed in these settings. 
3.2.2 How can foundation models uniquely help?
The above examples of legal applications are unique in several ways. 
First, the cost of annotating data is very high. 
Often, the expertise to create high-quality labels can only be found in attorneys, who may charge hundreds of dollars per hour. 
Even after labels are obtained, certain data may be sensitive and cannot be pooled together to training a large language model. 
Given recent progress in few-shot learning [Brown et al. 2020] , foundation models are among the most promising paths for learning models with limited annotations. 
Second, legal decision-making requires context at various scales: knowledge of all historical decisions and standards, knowledge of the case law that remains relevant in the present, and knowledge of the nuances of the individual case at hand. 
Foundation models are uniquely poised to have the potential to learn shared representations of historical and legal contexts, as well as have the linguistic power and precision for modeling an individual case. 
3.2.3 What are foundation models lacking that requires more research?
To illustrate the deficiencies current foundation models need to overcome in order to be realistically deployed, we consider as an example the automatic creation of a legal brief to submit to a court. 
A brief lays out the arguments to a judge before a hearing. 
Once a party has filed an opening brief, the opposing party files a response. 
The judge then evaluates the briefs and asks questions of both parties at a hearing before making a decision. 
Figure 14 visualizes the structure of such a legal brief with some of its characteristic features. 
An automated brief generation mechanism might take as context relevant documents and facts of a case (as specified by an attorney) as well as a rough sketch of the desired outcome. 
It would then generate a legal brief with complex legal arguments to submit to the court. 
Long Documents and Narratives. 
To achieve this goal, the model must be able to read long contexts and produce long narratives. 
Legal documents tend to be far longer than documents in any other context. 
The average U.S. Supreme Court opinion contains around 4,700 words, 37 a brief on the merits to the Supreme Court can have as many as 15,000 words, 38 a law review article often contains 20,000 to 30,000 words, 39 parole transcripts can be hundreds of pages long [Bell et al. 2021] , and trial records can be even longer. 
Current foundation models have struggled with such long contexts and outputs (see §4.1: modeling for more discussion). 
Retrieval, Concept Drift, Argument Formation, and Logical Reasoning. 
In addition to reading casespecific documents, the foundation model must retrieve the relevant case law and understand which case law is still valid and which has been overruled, taking into account potential concept drift since it was trained. 
More work in editing grounded information in foundation models will be required as case law evolves [De Cao et al. 2021] . 
Using retrieved legal standards, the foundation model must then understand how to weave them into a persuasive argument. 
Emerging research has studied ways of using foundation models to measure, detect, and generate persuasive texts [Duerr and Gloor 2021; Li et al. 2020a; Longpre et al. 2019] , which may provide stepping stones towards this goal. 
But legal briefing also requires the ability to identify a relevant legal rule and determine how it applies to a new situation. 
For example, Holzenberger et al. [2020] provide a dataset and methodology for evaluating this ability in current models based on the tax code. 
Given a set of tax code rules and a description of someone's situation, the model must determine how the rules apply to the individual in order to determine what their tax liability is. 40 
Holzenberger et al. [2020] found that domain experts can express the relevant tax law as a prolog solver to determine an individual's tax liability with perfect accuracy. 
However, they found that simply using a BERT model, even when pretrained on in-domain legal text, was not sufficient to allow the model to deduce and apply the relevant rules, a process also referred to as "legal reasoning. 
" 
These challenges can further be illustrated through a real exchange with GPT-3, demonstrating that current models are unable to perform even comparatively simple tasks involving legal reasoning. 
Legal Reasoning with GPT-3. 
"Liquidated damages" are a form of monetary compensation for a breach of contract that the parties specify in their agreement. 
In general, courts in the U.S. will not enforce liquidated damages provisions if the amount stipulated in the contract is so high that it acts as a form of punishment, rather than to be merely compensatory. 
First, we see if the model encoded the correct rule about the enforcement of liquidated damages provisions. 
q: 
Are liquidated damages clauses enforceable? 
a: 
Liquidated damages clauses are generally enforceable unless the sum stipulated is exorbitant or unconscionable. 
The rule appears correct. 
But then we apply the rule to a set of facts. 
Here, the model needs to infer that one million in damages is exorbitant for a Toyota Corolla and thus, the liquidated damages provision should not be enforced. 
q: 
X and Y contract over the sale of a Toyota Corolla. 
Their contract specifies that X has to pay Y one million dollars in damages if X does not deliver the car on time. 
X does not deliver the car on time. 
Is Y entitled to damages? 
If so, in what amount? 
a: 
Yes, Y is entitled to $1,000,000 in damages. 
Damages are meant to compensate the injured party for the loss suffered due to the breach. 
Y has suffered a loss of $1,000,000 because X has not delivered the car on time. 
The model is unable to infer that $1M for a Toyota Corolla is exorbitant. 
Even if the model is provided with both the abstract rule and its precise translation to the context in the prompt -and the application should thus be trivial -GPT-3 is still unable to conduct the logical reasoning required. 
q: 
X and Y contract over the sale of a Toyota Corolla. 
Their contract specifies that X has to pay Y the exorbitant amount of one million dollars in damages if X does not deliver the car on time. 
X does not deliver the car on time. 
The law requires that liquidated damages clauses are enforced unless the sum stipulated is exorbitant or unconscionable. 
Is Y entitled to damages? 
If so, in what amount? 
a: 
Yes, Y is entitled to the liquidated damages. 
The amount in this case is not exorbitant since the price was set by the parties themselves. 
Precision. 
The foundation model also cannot create fake facts in the process, a problem that has come up in current models [Gretz et al. 2020; Zellers et al. 2019b ]. 
Specificity and truthfulness are of heightened important in legal contexts, where imprecise statements can have drastic, unanticipated consequences, and false statements can lead to sanctions against attorneys. 
Few-shot Learning. 
Some challenges facing foundation models beyond those described in the above example include few-shot learning, which is still in its infancy [Perez et al. 2021 ]. 
Research on few-shot learning techniques and increasing access to legal corpora can work in tandem. 
Because foundation models need to be precise, not only in factual truth, as discussed above, but also in technical legal language, it remains unclear to what extent information obtained from one corpus can be utilized in another corpus of a different domain. 
Few-shot learning thus remains important. 
Adaptation. 
Some gains have been observed from domain-adaptive pretraining on unlabeled legal corpora. 
These gains appear to be most pronounced when the pretraining corpus is highly relevant to the downstream task and labeled training data is limited (a setting which is common in the law) [Zheng et al. 2021 ]. 
It has not yet been comprehensively studied whether this extends to a diverse set of legal tasks, but leveraging unlabeled domain-specific corpora for self-supervised training of foundation models may provide complementary improvements to few-shot methods. 
Access to Clean In-Domain Data. 
Some recent efforts have sought to create large labeled datasets for more challenging legal benchmark tasks through automation [Zheng et al. 2021] or manual annotation by volunteer legal experts [Hendrycks et al. 2021c ]. 
These efforts have demonstrated that larger language models that are pretrained on more data achieve performance gains on certain challenging tasks, compared to more limited gains observed in other settings [Chalkidis et al. 2020; Elwany et al. 2019; Zhong et al. 2020 ]. 
This work suggests that larger legal benchmark datasets may be necessary to observe further gains from applying transfer learning techniques to foundation models. 
However, creating benchmark datasets for tasks that are legally meaningful and difficult from an NLP perspective can itself be challenging, as human expert annotation can be costly and automated methods that utilize conventional tokenization and sentence segmentation techniques can fail to account for unique aspects of legal text, such as the structure of legal citations [Bommarito et al. 2018; Savelka et al. 2017 ]. 
As a consequence of these challenges, many existing legal domainspecific labeled datasets are small, not publicly available, or reflect simpler tasks that have been solved by methods often pre-dating the development of foundation models. 41 
uch available legal data may also be unrepresentative. 
Since only a fraction of cases end up in legal opinions, it is unclear whether the disputes in publicly available data are representative of the typical disputes presented to a model in practice [Priest and Klein 1984] . 
Costly training data for more representative scenarios may be concentrated in the biggest law firms. 
These law firms may have the ability to retain and accumulate data across many cases and clients. 
One concern then is that foundation models could concentrate power even more among the few actors that have the resources to train models on in-domain data -unless the models can generalize sufficiently well. 
Reliability. 
Finally, we again note that even if foundation models could successfully perform all tasks in the legal domain, deployment remains a major challenge: a failure of a foundation model in the law will have real, damaging consequences to both clients and attorneys (see also discussion on fairness, legality, and ethics in §5.1: fairness, §5.4: legality, and §5.6: ethics). 
For this reason machine translation software has already been deemed unreliable for use as evidence in some courts, 42 although it continues to be relied upon in others. 43 
iven all of these complexities, legal briefing and reasoning is likely beyond the capabilities of current models, but appears to be within the future realm of possibilities. 
As such, these serve as a potential lode star for the ongoing development of foundation models. 
3.3 Education
Authors: Ali Malik, Dorottya Demszky, Pang Wei Koh, Moussa Doumbouya, Drew A. Hudson, Allen Nie, Hamed Nilforoshan, Alex Tamkin, Emma Brunskill, Noah Goodman, Chris Piech Fig. 15 . 
Foundation models in education could be trained on multiple data sources to learn the capabilities necessary for education: an understanding of various subject matter and different pedagogical techniques. 
These foundation models can be applied in a general-purpose way across a range of tasks and goals such as understanding students, assisting teachers, and generating educational content. 
In the year 2000, the largest gathering of world leaders convened at the United Nations Millennial Summit to reflect on an ideal vision for the future. 
Delegates concluded that a primary focus should be education, declaring it "a foundation for human fulfillment, peace, sustainable development, economic growth, decent work, gender equality and responsible global citizenship. 
" This discussion was ultimately recodified into the United Nations Sustainable Development goal to "ensure inclusive and quality education for all and promote lifelong learning" [United Nations General Assembly 2015]. 
However, providing high quality, inclusive education at a large scale poses difficult societal and economic challenges. 
The price of education per student is growing faster than economy-wide costs [Bowen 2012], limiting the resources available to support student learning. 
In the United States, one symptom is that private education debt held by students has reached $1.6 trillion, surpassing total credit card debt [Friedman 2020 ]. 
Considering the rising need to provide adult retraining, the gap between the demand for education and our ability to provide it is alarmingly large and has concerning achievement disparities across protected demographics. 
With the advent of the digital age and the rapid growth in digital learning, computational approaches to education have shown promise in increasing the effectiveness of learners and teachers. 
Several core directions have emerged as potentially impactful applications of AI for education [Woolf et al. 2013] , such as systems that can provide meaningful feedback to students [Malik et al. 2021] , help teachers improve [Jensen et al. 2020; Demszky et al. 2021; Suresh et al. 2021] , or even create personalised and adaptive learning experiences that tailor the learning process to individual students' needs and dispositions [Connor 2019] . 
Despite this potential, building technical solutions to effectively scale inclusively and quality of education has proven to be exceptionally difficult. 
One particular challenge is that existing work has focused on custom solutions to highly specific tasks for which large amounts of training data has to be collected from scratch. 
Due to the difficulty and cost of creating large datasets, using this approach to solve every educational task independently is fundamentally limited. 
Instead, is it possible to create general-purpose approaches that are reusable across various tasks and subjects? 
Foundation models have already started to boost the performance of some specific flagship tasks in education. 
Recent examples include using MathBERT [Shen et al. 2021b ] to power "knowledge tracing" -the challenge of tracking a student's understanding over time given their past responsesand the "feedback challenge", where an algorithm has to interpret a student's answer to a structured open-ended task, such as a coding question [Wu et al. 2021e ]. 
Can foundation models lead to even more transformative changes in this domain? 
And what are the known and imagined risks of foundation models applied to education? 
In this section, we first frame the conversation around the ethical considerations. 
We then ground our discussion in two concrete tasks: (1) understanding student misconceptions, and (2) improving student understanding through instruction. 
3.3.1 Important concerns for centering foundation models in education research.
The future of AI for education is exciting, especially in the context of foundation models. 
However, we caution the reader to be especially thoughtful about the impact of any AI research applied to education. 44 
The goal of education are deeply interwoven with complex, long term social impact. 
While we actively work to improve digital education, it is imperative that we put in substantial thought to try and imagine the complexities of any disruption in this space [Piech and Einstein 2020] . 
Ethical challenges range from issues such as data bias, legal constraints, and the impact of digital socialization. 
These issues are not unique to foundation models, but they are worth reflecting on regularly as research makes substantial progress in AI for education. 
Reflection on impact is especially important when research starts by asking "what can new AI technology afford?" 
Many of the issues in §5.6: ethics apply to education. 
For example, as in many other domains, small biases in foundation model training data could be hard to track down [Dixon et al. 2018; Bolukbasi et al. 2016 ], but have important implications for equity of educational access. 
Moreover, these systems may experience a high degree of "feedback", where the collected data continually reinforces the model's decisions. 
This issue of bias goes beyond what data is collected and includes concerns over the applications that researchers choose to work on. 
Below, we discuss other education-specific issues. 
Many of the issues revolve around the question: "who benefits?" 
and for whom is new technology created? 
Removing teachers from the loop One of the goals of digital education, especially based on AI, is to increase the productivity of the learning experience so that more learning happens per unit time or unit cost. 
One can imagine that decision makers could use this increased productivity to remove human teachers from the loop. 
The long term implications of such decisions are hard to know a priori. 
Could interacting with an education system optimized to maximize "learning" have adverse effects on socioemotional skill development? 
Could it create fewer opportunities for Fig. 16 . 
The figure illustrates a system that embeds signals from various modalities (image, speech, sign, text) and languages into a universal feature space. 
Such a feature space allows ideas to be linked across modalities and languages. 
Pedagogically relevant link types include analogies (similarities across languages) and contrasts (distinct concepts across languages), both of which can occur in the same modality or across different modalities. 
interacting with others? 
Loneliness is on the rise in younger generations [Cigna 2018 ], and teachers are a modulating force for pressures that AI researchers might not envision. 
Was this work done by the learner or a foundation model? 
Another challenge is how to effectively teach students who have access to foundation-model-based tools. 
For example, it will be much more complex for teachers to understand the extent of a student's contribution if the student worked together with a powerful generative model, or to regulate ineffective collaborations and detect plagiarism. 
Visual Studio has recently released GitHub CoPilot, an AI pair-programmer built upon GPT-3 [Chen et al. 2021f ]. 
How will this change computer science education? 
Many challenges for beginner programmers might be trivial to CoPilot or its technical successors, which could undermine the learning experience for novices. 
It would be instructive to study other examples of technological advances that disrupted education for certain subjects, such as calculators in math classrooms and Google Translate in language courses, both of which now coexist with traditional instruction. 
Privacy and security. 
One important ethical issue in the use of AI in education is highlighted by the strict legal guidelines concerning privacy in student work. 
For example, in the United States, student information is protected by the Family Education Rights and Privacy Act (FERPA). 
These laws and regulations are especially important for children under 13, who have their data privacy and security additionally protected by the Children's Online Privacy Protection Act. 
Among other things, FERPA limits teachers from sharing personally identifiable student work. 
This could directly impact initiatives to share data used both for training and for evaluating foundation models. 
Moreover, there is an open question as to whether the weights of a foundation model could somehow leak the (possibly private) data it was trained upon [Nasr et al. 2018; Song et al. 2017 ]. 
These issues, and their corresponding approaches, are similar to the challenges described in §3.1: healthcare. 
This list is not exhaustive and the ethical considerations will vary from project to project. 
3.3.2 Foundation models of student thought.
When building AI tools for inclusive, and joyful education, there are many tasks where foundation models could be useful. 
Many of those tasks require us to first understand the learners whom we are trying to help, especially in the context of open ended work. 
What would it take for a foundation model to be able to reason about student understanding? 
It is easy to imagine a foundation model which has been adapted to answer a math question correctly, but it is less clear how to build a model that can diagnose mistakes in student understanding based on the student's answers. 
To explore this theme, we consider the case study of providing feedback to students who are working on open-ended tasks such as writing a short paragraph, drawing a physics diagram, or writing code. 
This "feedback challenge" exemplifies how foundation models can be helpful off-the-shelf for learners, and also demonstrates open areas for foundation model research. 
To effectively provide feedback to students, two central capabilities are required: (1) understanding the subject matter of the task (e.g., physics or coding), and (2) the diagnostic ability to "notice": a technical term in education for inferring why a student made a mistake. 
For typical student interactions in a typical classroom, there is not enough data for an AI model to learn, from scratch, both of these central capabilities. 
Even for massive courses with millions of students, supervised algorithms barely understand the complex student reasoning behind even short, four-line programs [Malik et al. 2021 ]. 
As such, the feedback task inherently requires a transfer of understanding from external data and experience. 
Foundation models, as they currently exist, are directly helpful for the first of these capabilities: understanding a specific subject matter. 
For example, when learning to provide feedback on short programming questions, a foundation model such as GPT-3 can efficiently understand what fluent code looks like with a few examples. 
Some research in this direction has already started exploring foundation models that can quickly adapt to questions in new subject matter domains [Wu et al. 2021e; Condor et al. 2021 ]. 
Similarly, foundation models could also integrate multiple modes of information such as the text of a task's prompt, diagrams in the question, or even the content of a grading rubric provided to teaching assistants. 
This unified representational ability can help foundation models comprehend a subject matter through richer sources of information. 
As a concrete case study, many of these insights were leveraged as core components of an algorithm which was able to grade an introductory Computer Science midterm at Stanford University, with the same effectiveness as human teaching assistants [Wu et al. 2021e ]. 
In this case, subject matter encoding was built on a foundation model that had been adapted on GitHub code and a corresponding small dataset for each question's subject matter. 
In general, we can imagine leveraging various sources of data to adapt foundation models to different subject matter. 
For example, math adaptation could use mathematical websites or textbooks [Shen et al. 2021b ] or historical student answers on platforms such as Gradescope; spoken language understanding could leverage radio archives or podcasts; and domains like creative writing could look to large digital archives like Project Gutenberg. 
In contrast to subject matter, adapting a foundation model to the task of mapping observed mistakes to flaws in a student's thought processes is much less well-explored. 
The ability for an instructor to "notice" the reasons behind why a student makes a specific mistake is a critical component of the feedback challenge. 
Imagine, for example, a student learning two digit addition who answers the question "what is 26 + 19?" with the response "315." 
Take a moment and try to guess why they gave that answer and what misconceptions they have. 45 
. 
This ability to notice could be posed as an adaptation task for foundation models ( §4.3: adaptation) or perhaps even as a reasoning task ( §2.4: reasoning). 
While difficult, training an AI system to notice is an achievable goal. 
Across classrooms, and across learning tasks in a given domain, there are generalizable patterns in how students arrive at their answers. 
The labeled data that can directly be used for this adaptation task, such as instructorwritten feedback to student work in [Wu et al. 2021e] , are often held privately by instructors in disparate datasets. 
However, publicly accessible data, such as StackOverflow interactions, might also be creatively used to adapt a foundation model to notice. 
Some research has also explored effective ways of extracting, from instructors, generative descriptions of how students make mistakes [Malik et al. 2021; Gulwani and Singh 2013 ] -these hand-written generative models could also be used to generate adaptation data to help foundation models diagnose student mistakes. 
3.3.3 Foundation models for instruction.
Reasoning about student understanding is an essential step towards a second objective: provide inclusive, high quality instruction. 
Computational approaches to instruction focus on different tasks like content personalization [Connor 2019], question generation [Guo et al. 2016; Willis et al. 2019; Srivastava and Goodman 2021] , adaptive curriculum design [Mandel et al. 2014; Doroudi et al. 2017] , and predicting instructor intervention [Chandrasekaran and Kan 2019; Alrajhi et al. 2021] . 
In this subsection, we discuss how foundation models could be useful in the act of teaching students. 
Since effective teaching requires reasoning about student understanding, the previous discussions on understanding subject matter and "noticing" are extremely relevant. 
However, providing effective instruction requires an additional capability: that of understanding pedagogy [McKenzie 2003 ]. 
This encapsulates an effective understanding of techniques to guide a student, such as asking Socratic questions or providing analogies/contrasting cases; using encouraging or supportive language; tailoring the difficulty of questions to the student; and generating examples that are relevant to a student's interests and background. 
How can foundation models be adapted to understand good pedagogy for instruction? 
One idea is to consider adaptation using data source where instruction is the primary role. 
For example, data from question answering forums like StackOverflow could potentially be used to build a tutor which can parrot common Socratic questions. 
Similarly, a foundation model adapted on encyclopedias such as Wikipedia might be able to give answers to student questions which are (often) factually correct. 
There are also public data sources like textbooks, lecture videos, lesson plans, and graded feedback that collectively contain important pedagogical behaviours which could be adapted by foundation models (Figure 15 ). 
Another adaptation challenge for instruction based on foundation model is to learn how to speak to students like teachers. 
The language used by teachers is often different from the language used by the general population. 
Teachers are ideally trained to speak to students with respect and in a way that intentionally helps them form a positive identity with the subject being learned [Truax 2018] . 
Cautionary examples like Microsoft's 2016 Twitter bot "Tay, " a chatbot that started generating hate speech within 24 hours of being deployed live, show us the importance of explicitly accounting for this factor in education. 
To train a language model which is more heavily influenced by professional teachers in classrooms, we could perhaps adapt foundation models to data sources like lecture videos or recorded office hour videos. 
The adaptation problem above is compounded by the fact that different education contexts vary significantly in the kind of language that would be appropriate: for example, effective instruction in a 5th-grade science class would look quite different from that in a college physics class, much less a college literature class. 
This presents technical challenges beyond what would be faced in typical NLP domain shift settings (e.g., question answering based on news articles vs. Reddit posts), as the foundation model would need to be fluidly adaptable in terms of its tone and language, and not just the factual content that it generates. 
Beyond sound pedagogical techniques and instructional language, how might foundation models provide even more insightful forms of instruction? 
§2.1: language of this paper highlights the fact that remarkably complex language can be acquired by babies in a short amount of time. 
As the authors point out, a salient difference between foundation model training and human language acquisition is that "human language is grounded to the real world: for example, a baby's caretakers point to objects while they talk about them." 
This same insight can also inspire ideas as to how foundation models can be used for generative education. 
Humans seem to learn well when presented with real-world analogies and contrasts which may be cross-cutting between their current context and past experiences. 
For example, when teaching sign language, an instructor might use an analogy such as "the hand shapes for the word 'morning' looks like the sun rising" or note that "the hand shape you just made look very similar to another word, so let us focus on the differences." 
As another example, when teaching Swahili to a learner who already knows Arabic and English, an instructor could point out that the Swahili word for 8 (pronounced nane) is a "false friend" that is phonetically similar to English word for 9 (pronounced nine). 
Foundation models that can integrate multi-modal data have the potential to make these kinds of rich analogies and comparisons that are typical in childhood language learning (Figure 16 ). 
4 TECHNOLOGY
The technological foundations of foundation models give rise to the capabilities ( §2: capabilities) that determine their potential. 
To understand the technology used in development, we consider the data ( §4.6: data), model architectures ( §4.1: modeling) and systems ( §4.5: systems) used to train ( §4.2: training), and further adapt, ( §4.3: adaptation) these models alongside the theory ( §4.10: theory) that should be developed to understand this paradigm. 
To then understand the resulting models, we discuss how to evaluate ( §4.4: evaluation) and interpret ( §4.11: interpretability) alongside the importance of robustness ( §4.8: robustness), security and privacy ( §4.7: security), and long-term AI safety ( §4.9: ai-safety) for ensuring the reliability of these models when deployed in society ( §5: society). 
4.1 Modeling
Authors: Drew A. Hudson, Antoine Bosselut, Alex Tamkin, Omar Khattab, Jared Quincy Davis, Jiaxuan You, Trevor Gale The emerging paradigm of foundation models has attained impressive achievements in AI over the last few years, as models such as BERT [Devlin et al. 2019 ] shine at a wide spectrum of language understanding tasks: from textual classification and entailment to question answering and reading comprehension, while GPT-3 composes rich and fluent tales about unicorns [Brown et al. 2020 ] and DALL-E shows signs of visual creativity, generating from scratch strikingly-realistic pictures of avocado chairs [Ramesh et al. 2021 ]. 
These and other instances of recent foundation models not only achieve remarkable performance across a multitude of diverse downstream tasks and applications [Rajpurkar et al. 2018; Wang et al. 2019a ], but also manifest noteworthy behaviors of interpretability [Karras et al. 2020] , robustness [Devlin et al. 2019] , controllability [Patashnik et al. 2021] and generalization [Brown et al. 2020 ]. 
What does it take for a model to demonstrate such qualities? 
What architectures are capable of consuming large quantities of potentially multimodal information and translate them into rich knowledge of the world? 
And overall, what desirable properties should a network possess to give rise to a foundation model? 
Here, we identify and discuss five such properties, spanning expressivity, scalability, multimodality, memory capacity, and compositionality, that we believe are essential for a foundation model in order to: (1) distill and accumulate knowledge from various sources and domains, (2) organize it in an effective and scalable representation, and (3) flexibly generalize it towards novel contexts. 
For each of these properties, we motivate their necessity, provide examples of contemporary models that incorporate them, and explore key challenges and promising avenues for future research and development. 
See Figure 17 for an overview diagram. 
4.1.1 Expressivity.
Expressivity concerns with the theoretical and practical capacity of a network to model the data distribution it is trained over and represent it in a flexible manner. 
Prior works have proposed formal expressivity measures to characterize the complexity of functions a network can compute, or more precisely, approximate, which is essentially affected by its depth, width, connectivity, and structural patterns [Raghu et al. 2017] . 
As the No Free Lunch theorem suggests, there is no single model or algorithm that suits best for all cases [Wolpert and Macready 1997], and so, for our purposes, we are particularly interested in identifying which models could effectively capture the facets of natural information, such as human language or real-world images [Goodfellow et al. 2016 ]. 
These modalities are either continuous (as in vision) or discrete (as in language), are distinctly hierarchical and high-dimensional, and present a complex set of relations and interactions among their constituent elements, whether these are pixels, words or physical objects. 
Indeed, recent breakthroughs in generative modeling provide strong evidence for the high expressivity of neural networks, as they successfully express distributions of textual [Brown et al. 2020; Devlin et al. 2019; Lieber et al. 2021; Wang and Komatsuzaki 2021] , auditory [van den Oord et al. 2016] , and visual [Karras et al. 2020; Brock et al. 2018 ] domains, and generate samples of high fidelity, diversity and realism. 
Inductive Biases. 
Much of the success of neural networks over the last decade in modeling natural data is owed to the networks' high depths, as could be roughly measured by the number of stacked non-linear layers they are composed of, or the number of computational steps they take during their chain-of-reasoning. 
Great depths play a crucial role in enhancing networks' expressivity, allowing them to form powerful hierarchical and distributed representations that could generalize from the training data to new unseen examples [He et al. 2016b; Levine et al. 2020] . 
The universal approximation theorem [Lu et al. 2019b ] indeed states that even simple multilayer perceptrons (MLPs) can represent a broad set of functions, while different inductive biases, as those implemented in Recurrent Neural Networks (RNNs) or Convolutional Neural Networks (CNNs) [Goodfellow et al. 2016] , can improve the learning efficiency and enhance the capacity of a given network to model different forms of information: sequential data, common to language, speech and time-series, for the former, or spatially-invariant information, as in images or videos, for the latter. 
Transformer Networks & Attention. 
Meanwhile, transformer networks [Vaswani et al. 2017] , introduced more recently, demonstrate the importance of capturing long-range dependencies and pairwise or higher-order interactions between elements. 
They build on the self-attention mechanism [Vaswani et al. 2017; Bahdanau et al. 2014 ] that enables shorter computation paths and provides direct means to compare elements far-across the input data (such as a pronoun and its antecedent in a sentence, or two sentences that refer to the same topic). 
From another perspective, the multiplicative interaction embodied in both attention as well as gating structures (as in LSTMs [Hochreiter and Schmidhuber 1997] or Mixture-of-Experts [Shazeer et al. 2017] ) offers a more flexible alternative to the rigid fixed-weight computation of MLPs and CNNs, dynamically adapting the computation to the input at hand. 
This proves especially useful for language modeling, where, for instance, given a sentence like "She ate the ice-cream with the X", while a feed-forward network would always process it in the very same manner, an attentionbased model could adapt its computation to the input -updating the contextual representation of the word "ate" if the prepositional phrase (PP) attachment X is "spoon", or instead link it to the "ice-cream" if X refers e.g., to "strawberries" [Zavrel et al. 1997 ]. 
General-Purpose Computation. 
A final notable advantage of attention over prior architectures stems from its stronger generality, where it is not strongly tied to a particular task or domain, as is the case for the local receptive field of convolution or the sequential assumption of recurrent networks, both reflecting inherent properties specific to the vision and language modalities respectively. 
We hypothesize that the general-purpose nature of attention and transformers contributes to their broad applicability for a wide range of research problems and applications [Liu et al. 2019; Dosovitskiy et al. 2020; Hudson and Zitnick 2021] . 
This contrast captures a more general trade-off between task-specialization and expressivity: models with stronger structural priors can leverage them to improve sample efficiency on the particular tasks that benefit from these assumptions; while conversely, models that integrate weaker inductive biases learn more slowly, but can in turn scale to higher volumes of data and adapt to a diverse set of domains, since they do not rely on restrictive or task-specific suppositions. 
As both data and compute turn more accessible, we observe that the exploration of models with a minimal set of inductive biases that can "let the data speak for itself" seems to serve as a more promising approach for future research in the field. 
Challenges & Future Directions. 
Notwithstanding the stellar progress and accomplishments of neural networks in general, and foundation models in particular, in terms of expressivity, notable challenges still remain. 
Leading approaches [Choromanski et al. 2020; Dosovitskiy et al. 2020] keep struggling with modeling of extremely long-range dependencies, such as those occurring in books, movies, or even DNA sequences, which may be attributed to the quadratic computation of contemporary transformer-based approaches [Wang et al. 2020c; Lin et al. 2021] . 
This challenge essentially reflects the trade-off between efficiency and expressivity: where explicit modeling of long-distance interactions through short and direct computation paths improves expressivity on the one hand, but comes at the expense of scalability due to computation entailed by the increased connectivity on the other [Child et al. 2019; Kitaev et al. 2020; Choromanski et al. 2020 ]. 
Models such as the GANformer [Hudson and Zitnick 2021] and the Perceiver [Jaegle et al. 2021b,a] explore ways to balance these two properties and propose transformers with linear complexity that rely on bipartite or bottleneck attention, so to improve computational efficiency while maintaining high-expressivity. 
We believe that identifying an effective equilibrium between these two objectives offers an interesting avenue for future research. 
Another important research direction relates to the expansion of foundation models, which, so far, have mainly focused on the language domain [Peters et al. 2018; Devlin et al. 2019; Brown et al. 2020] , to different modalities, such as the structural [Scarselli et al. 2008; Veličković et al. 2017] and perceptual [Tolstikhin et al. 2021; Jaegle et al. 2021b; Tan and Le 2021] , each involving a unique set of associated challenges. 
Likewise, we believe that exploring architectures for reasoning ( §2.4: reasoning), which demands iterative computation chains and interaction with symbolic information, constitutes a valuable goal for future foundation models research. 
4.1.2 Scalability.
Closely connected to model's expressivity is the notion of scalability. 
As rich data from varied sources becomes more readily available, and computational resources get stronger and more efficient ( §4.5: systems), we should look for ways to match this rate of progress and harness it to improve AI competency and versatility. 
For foundation models to effectively fit the complex and high-dimensional distribution of images or text, they should thereby be scalable across all dimensions: including both models' depth and width as well as their training time, number of parameters, and amount of data they could process. 
Optimization. 
Specifically, foundation models should both be: (1) easy-to-train ( §4.2: training), by being resilient to noise or imperfections in the data, and robust against instabilities like vanishing [Helfrich et al. 2018; Glorot and Bengio 2010] or exploding gradients [Hochreiter and Schmidhuber 1997; Nair and Hinton 2010 ], but also (2) easy-to-adapt ( §4.3: adaptation), by overcoming phenomena of catastrophic forgetting [Kirkpatrick et al. 2017 ] and supporting few-shot learning [Sung et al. 2018 ]. 
We are still in the early days of understanding the principles that drive the scalability of learning algorithms, and while recent works have started to shed some light on these themes [Liu et al. 2020c; Kuditipudi et al. 2019; Nakkiran et al. 2019] , much work remains to be done. 
Hardware Compatibility. 
Moving beyond aspects of robustness and optimization, foundation models should also be practically efficient ( §4.5: systems), and take advantage of contemporary and future hardware [Hooker 2020 ]. 
One example of that is parallelizablity, an important property that characterizes the computation supported by GPUs. 
Indeed, much of the transformers' great success over the previously dominating recurrent approach was driven by their higher degree of parallelism. 
Looking forward, given the fast-pace progress of systems development, we should further ensure that models are designed to co-adapt to future hardware advances. 
Consequently, foundation models should ideally be amenable to schemes such as distributed training, which is gaining popularity, as is the case for e.g., Mixture-of-Experts, and possibly leverage properties such as sparsity of the computation or representation, as is the case for the Longformer [Beltagy et al. 2020] , BigBird [Zaheer et al. 2020] , and Sparse Transformer [Child et al. 2019 ] approaches, and which likely will become more central in future hardware and processors. 
4.1.3 Multimodality.
Traditionally, the fields of computer vision, robotics, and NLP have made progress in an independent manner, with separate communities developing specific approaches suitable for each modality. 
A conducive consequence the rise of deep learning has brought about was the bridges it helped forming among the various communities and research areas within AI, as seemingly different problems could now be tackled by closely-related approaches, and studies of originally remote topics began converging to a common ground. 
This breakthrough opened up a new range of possibilities, fostering pioneering exploration into the theme of multimodality, encompassing areas as varied as language grounding [Lynch and Sermanet 2020] , visual semantics [Conser et al. 2019 ], embodied environments [Savva et al. 2019a ] and interactive agents [Gray et al. 2019] . 
Essentially, multimodality serves as a key component of intelligence, and is a crucial factor for the development of both thorough and broad comprehension of the world. 
Concretely, language learning is more effective when occurring in a grounded environment rather than in a vacuum. 
And inversely, from the vision perspective, language encourages the emergence of abstractions that link between low-level perceptual signals and statistics to semantic concepts of objects, properties, agents and motivations, thereby enriching and elevating visual representations. 
In light of these observations, we argue that foundation models should ideally connect together the different modalities, distill their embodied information into a shared multifaceted representation, and capture the full range of inter-connections and relations among them so as to furnish a wide range of capabilities (see §2.1: language, §2.2: vision, §2.3: robotics, §2.4: reasoning). 
Generality and Specialization. 
An important design choice for multimodal foundation models is the degree of specialization, or the structural sharing between the modules responsible for each modality. 
Naturally, data of different domains exhibits diverse kinds of structures and propertieswhere, for instance, language is discrete while vision is continuous. 
At first sight, this variation hints that specialized inductive biases tailored for each modality could be of aid. 
Yet, as training scales upwards and models are provided with the opportunity to base their learning less on structural priors and more on the data itself, general approaches that maintain only a handful of broad general assumptions prove in fact a lot more successful than task-specific alternatives. 
And so, as corroborated by recent success of general-purpose models like transformers across different modalities -both linguistic [Liu et al. 2019; Lan et al. 2019] and visual [Dosovitskiy et al. 2020; Hudson and Zitnick 2021] , we see that generality is critical for improving AI capabilities. 
Multimodal Interactions. 
Another key consideration for multimodal models relates to weight sharing: do the various modalities benefit from using the same or different parameters for their respective components? 
Prior works have shown that fruitful transfer could certainly occur across modalities, but the ideal degree of sharing remains unclear, so is the existence of principled ways for discovering it. 
Finally, a major design question concerns with the forms of the multimodal interactions supported by the model, which vary widely between concrete cases and examples: Cross-modal or late-fusion models such as ConVIRT [Zhang et al. 2020a] and CLIP [Radford et al. 2021 ] maintain fully separate encoders for each data source, and compare their spaces only at the ultimate computation stage, using e.g., a simple dot product. 
Meanwhile, early-fusion models, such as ViLBERT [Lu et al. 2019a; Cho et al. 2021] , jointly reason over multiple modalities necessary for tasks of visual reasoning and question answering. 
Identifying the optimal stage and form for merging the respective vector spaces [Nagrani et al. 2021 ] remains an open research question. 
Overall, while there seems to be a consensus within the community about the importance of multimodality, models that go beyond shallow alignment of vision and language are yet to exist, and the theme of grounded language learning in embodied environments still has much room for exploration. 
4.1.4 Memory.
So far, we have discussed the foundation models' goal to gather and accumulate information from varied modalities at large scales. 
This knowledge encompasses both broad understanding of the world as well as specific mastery of niche subjects or particular facts. 
Representing such a large body of learned information is by no means trivial, and is leading to interesting questions about effective mechanisms for access, storage, retrieval and manipulation of particular items or memories. 
Explicit Storage. 
An important design principle that could achieve these desiderata is to separate out computation from memory [Weston et al. 2014; Graves et al. 2016; Hudson and Manning 2018, 2019a] in order to enhance models' ability to transfer knowledge by applying previously acquired abstract skills to new concrete settings. 
In this context, it is important to distinguish between explicit facts -that can be stored in an external memory storage, and implicit knowledge -that is reflected through the networks' trainable weights. 
Such decoupling of explicit and implicit knowledge enjoys multiple advantages compared to the alternative of implicitly encoding all information together through the network weights. 
The separation mitigates the inflation in models' size and number of parameters needed to store the growing quantities of knowledge [Guu et al. 2020] , improves models' trust and reliability by increasing their knowledge provenance [Cheney et al. 2009 ], and most notably, is key for memory update, manipulation or adaptation [Lewis et al. 2020b ] ( §4.3: adaptation), which could in turn enable generalization to novel contexts and downstream tasks. 
Indeed, disentanglement between memory and computation has been a recurring goal in deep learning and NLP research over the last years, including models such as Memory Networks [Weston et al. 2014; Sukhbaatar et al. 2015] , the Neural Turing Machine [Graves et al. 2014 [Graves et al. , 2016]] , the Neural State Machine [Hudson and Manning 2019a] , and MAC [Hudson and Manning 2018] . 
Furthermore, using key-value structures [Miller et al. 2016] for accessing external memories has been shown to be very effective for modeling long-term dependencies [Henaff et al. 2016; Bosselut et al. 2018; Lample et al. 2019] . 
Transformers, the celebrated architecture underlying most foundation models to date, likewise exhibits operations that involve key-value memory-access and computation among the contextual word representations they gradually build [Geva et al. 2020 ]. 
Information Retrieval. 
Once a model completes gathering the information after training, there are multiple ways to retrieve particular facts or memories necessary for downstream applications and tasks. 
Some employ explicit prompting techniques that query the model's knowledge through input sequences [Petroni et al. 2019; Kassner et al. 2021; Jiang et al. 2020 ] while other approaches involve implicit recollection and reshaping of the prior knowledge through an adaption phase [Bosselut et al. 2019; Hwang et al. 2021] . 
A third category of methods goes a step further and combines neural-based computation with symbolic aggregation and retrieval of information from either unstructured textual repositories [Karpukhin et al. 2020; Lewis et al. 2020b; Khattab et al. 2020] or even structured resources such as knowledge graphs [Zhang et al. 2019a; Peters et al. 2019; Liu et al. 2020e; Verga et al. 2020; Yasunaga et al. 2021] . 
However, there is trade-off between the strong memorization skills offered by retrieval mechanisms on the one hand and the richer representations learned when there is an information bottleneck on the other. 
Indeed, over-reliance on retrieval reduces the opportunities to learn how to represent information in compact and abstract manners, distill key insights and concepts out of the vast amounts of input information the model is exposed too, and, basically, separate the wheat from the chaff. 
For instance, the in-context learning abilities of GPT-3 possibly emerge as a by-product of enforcing the network to represent the input sequential data through its bounded memory architecture [Brown et al. 2020] . 
Overall, While they certainly have some merits [Guu et al. 2020 ], models that rely on external retrieval mechanisms may not learn to generalize as effectively as bounded, compact and abstract representations. 
Knowledge Manipulation. 
Finally, when considering large-scale learning over long durations, it is crucial to note the dynamic nature of knowledge, where facts' correctness and validity can change over time as the world keeps evolving -and what was true or relevant yesterday may not be so tomorrow. 
It is therefore crucial for a model to represent its knowledge in a manner that supports efficient update or manipulation of facts as part of its lifelong learning. 
4.1.5 Compositionality.
Compositionality can be defined as the principle according to which the meaning of the whole is derived from the meaning of its constituent parts, and the rules applied to combine them [Janssen and Partee 1997; Bottou 2014] . 
It is a crucial ingredient of human intelligence [Lake et al. 2017 ], underlying our capabilities to plan, reason and learn readily and efficiently from a handful of examples. 
Compositionality may hold the key to achieve out-of-distribution -or specificallycombinatorial generalization. 
Drawing on classic ideas from symbolic AI, it encourages and enhances desirable properties within neural networks, such as interpretability, controllability and dataefficiency [Lake et al. 2017] , and can take different forms, characterizing variety of elements: 
Model. 
Compositionality can be reflected at the model level, in terms of its architectural properties, structure, and degree of modularity -which can increase training and inference efficiency of large neural models [Shazeer et al. 2017] . 
It also links to themes of interpretability and multimodality, as it relates to the interfaces between the different modules the model is composed of, what modes of interactions they employ, and how transparent they are. 
Computation. 
Models such as Module Networks [Andreas et al. 2016 ] and Mixture-of-Experts [Shazeer et al. 2017 ] go further along this direction, exhibiting not only structural modularity, but also compositional computation, supported by the specialization of sub-networks to different operations, in a manner that adapts and tailors the model behavior to the input at hand. 
While some methods rely on concatenation of hand-engineered modules [Andreas et al. 2016] , alternative approaches enable the network specialization to naturally emerge through learning [Shazeer et al. 2017] . 
Other models, such as MAC [Hudson and Manning 2018] and Dynamic Memory Networks [Xiong et al. 2016 ] perform an explicit iterative computation, where a given task is decomposed into multiple reasoning steps, performed one by one, manifesting sequential progression from a set of initial facts to novel inferences and conclusions. 
Training & Data. 
Not only can the model or its computation be compositional, but so can be the data or training processes too [Andreas 2020 ]. 
Instead of training one model over a complete dataset, one could split, or decompose it into subsets, train different models on each one independently, and ultimately recombine them at test time through various ensemble techniques [Dietterich 2000]. 
Such approaches could have far-reaching implications on the training and deployment procedures of foundation models, in both practical and even societal regards. 
Representation. 
We have discussed compositionality of different elements, such as the model, the computation, the training schemes or the data. 
But most notably, the learned representation itself, which emerges over the course of the model training and adaptation, can also be compositional [Andreas 2019 ]. 
Indeed, a promising manner to represent knowledge is through structured, potentially graph-based, object-oriented representations [Zhang et al. 2019a; Wang et al. 2021a] , that center around identifying entities and event nodes and forming connections, analogies and relation edges among them. 
It reflects a natural way to organize information about the world, where inputs from different modalities can be channeled and aggregated around semantic multi-faceted concepts. 
Such representations could support multi-hop reasoning and inference [Washington et al. 1995; Sun et al. 2020b; Yu et al. 2020c] , and potentially also enable stronger out-of-distribution generalization through recombination. 
However, compositionality can also hinder the expressivity of the representation, and impede its capacity to account for idiosyncrasies, exceptions, and contextual correlations [Misra et al. 2017a ]. 
In other words, the whole can sometimes be greater than the sum of its parts, where for instance, red wine is not the same as red onion. 
But while many approaches that have dominated over the last decade tend to focus mostly on one end of the spectrum, and learn monolithic distributed representations, we believe that exploring manners to reach a better balance between contextuality and compositionality is a promising avenue for future research. 
4.1.6 Summary.
We have introduced five properties that we believe are essential for the next generation of foundation models, in order to effectively distill the large amounts of information around us so to successfully address downstream tasks: expressivity -to flexibly capture and assimilate real-world information, scalability -to adeptly handle high volumes of high-dimensional data, multimodality -to consume, process and potentially produce content from different sources and domains, memory capacityto effectively store and retrieve the acquired knowledge, and finally, compositionality, to foster successful generalization to novel tasks, settings and environments. 
We believe that the realization of the full potential of foundation models, as is envisioned and discussed in detail throughout this report, will rely on research of new architectural and modeling advances to fulfill these desiderata. 
4.2 Training
Authors: Alex Tamkin
Training objectives are mathematical functions describing how to transform a model architecture and large amount of broad data into a foundation model. 
For example, GPT-3 was trained with a language modeling objective, which rewards the model for predicting the next word correctly [Shannon 1948 ]. 
We begin by laying out some goals of these training approaches, describe important design trade-offs in current approaches, and outline important goals for the path ahead. 
4.2.1 Goals of training objectives.
Here we outline some key goals for training algorithms in light of the recent rapid progress in these methods and models. 46 
everaging broad data. 
The rise of self-supervised learning algorithms has unlocked the power of internet-scale datasets which would be intractable to annotate by hand. 
This kind of broad data comes in many forms, including images, audio recordings, and video ( §2.2: vision); robotic and sensor data ( §2.3: robotics); and text, either in isolation or paired with other modalities like images ( §2.1: language). 
Because this data lacks external annotations, a major focus for researchers is designing bespoke self-supervised algorithms that leverage the unique structure within each kind of data to produce a training signal for a foundation model. 
Domain completeness.
An important goal for foundation model training algorithms is to be domain complete, in the sense that solving the training task requires capabilities that are broadly useful for downstream tasks in the domain (see §2.1: language, §2.2: vision, §2.3: robotics). 
This property is crucial for the generality of a foundation model. 
For example, language modeling may require models to acquire capabilities as wide-ranging as coreference, sentiment and translation as the model learns to predict the next word in a document. 
In contrast, a supervised learning task like sentiment classification may lead to a more narrow set of capabilities (see §2.1: language). 
As important as this quality is, it is not obvious a priori what tasks will result in a domain complete capabilities, or even how to evaluate the full breadth of a model's capabilities (see §4.4: evaluation and §4.10: theory). 
Scaling and compute efficiency. 
Procedures for training foundation models must reliably convert data, a model architecture, and compute into a broadly capable model. 
To maximize the capability of a foundation model, we can identify the bottlenecks to this process and propose new training algorithms which remove them. 
The rise of self-supervised algorithms has made model size and compute resources increasingly salient bottlenecks [Kaplan et al. 2020; Henighan et al. 2020 ], leading to a shift where models are evaluated not solely on their capabilities but rather on the amount and kind of compute needed to reach those capabilities ( §4.4: evaluation). 
The efficiency of training objectives can vary tremendously, 47 laying in sharp relief how important the design of a training approach is to the emergence of powerful capabilities given a fixed compute budget. 
Thus, a major goal for training researchers is to design training objectives with a richer training signal, resulting in models which learn faster and attain stronger capabilities. 48 
One force aiding this development is the surprising predictability of how capabilities scale with different kinds of architectures, data sizes, and compute [Hestness et al. 2017; Kaplan et al. 2020] , a striking phenomenon which enables model developers to make choices based on clearer trends instead of more costly random searches. 
4.2.2 Design trade-offs in current SSL methods.
Current self-supervised learning (SSL) methods for training foundation models are diverse, but what unites them is that they produce prediction problems from unlabeled data without the need for human annotators. 
SSL objectives manufacture a rich training signal from this data through carefully-designed constraints, either on the data itself (e.g., redacting or noising) or on the way the model is able to represent or process the data (e.g., latent bottlenecks). 
At some level, these constraints "bake in" the kinds of capabilities desired when adapting models to downstream tasks ( §4.3: adaptation). 49 
 Here, we describe three important design choices that current models explore, along with their respective tradeoffs in terms of their resulting capabilities. 
At what level of abstraction should we model? 
A fundamental question is what the input representation of a foundation model should be. 
One option is to model the input at the level of raw bytes. 
However, this high dimensionality may cause the model to focus on predicting less semantic aspects of the input, 50 slowing the rate at which it acquires more generally-useful capabilities. 
These approaches also become intractable when training models like transformers [Vaswani et al. 2017] whose compute costs grow quadratically with the input size. 51 
Another option is to use domain knowledge to reduce the input space of a model -such strategies include patch embeddings [Dosovitskiy et al. 2020] as well as fixed or learned tokenization [Schuster and Nakajima 2012; Sennrich et al. 2016; Kudo and Richardson 2018; van den Oord et al. 2017; Ramesh et al. 2021 ]. 
These methods may alleviate some challenges facing generative approaches, but have the trade-off that they may jettison possibly-useful information in the input. 52 
The choice of a continuous vs discrete input also has trade-offs for adaptation ( §4.3: adaptation); more work is needed to capture the benefits of both approaches. 
Generative vs discriminative models Generative training approaches are conceptually elegant yet powerful -they train models to learn joint or conditional distributions over training inputs. 
Two major families of generative foundation models include autoregressive foundation models [van den Oord et al. 2016; Radford and Narasimhan 2018; Chen et al. 2020d; Yang et al. 2019; Ramesh et al. 2021] , which generate inputs piece by piece, and denoising foundation models [Devlin et al. 2019; Raffel et al. 2019 ] which corrupt and then recover the inputs. 
The specific kind of generation performed in the training process determines what kind of interactivity is available ( §4.6: data), which continues to be a major challenge for many domains, including robotics ( §2.3: robotics) and low-resource languages ( §2.1: language) 49 For example, the causal language modeling objective used to train GPT-3 [Brown et al. 2020 ] enabled conditioning it via prefixes. 
And the color jitter augmentations used during contrastive learning [Chen et al. 2020c ] encourage invariance to properties not thought to be useful for downstream tasks. 
Better understanding how the particular choice and structure of these constraints influences the capabilities acquired by the model is an important area for future work ( §4.10: theory). 50 
e.g., blades of grass, audio compression artifacts, or spellings of words 51 See §2.2: vision and §4.1: modeling for discussions of training costs for high-dimensional sequences, such as images and video 52 For example, tokenizing text may make it harder to learn rhymes, puns, or other tasks that benefit from character-level information [Branwen 2020] during adaptation 53 (see §2.5: interaction and §4.3: adaptation), and future models may enable an even richer set of interactions. 54 
 While generative training approaches have their benefits, several discriminative approaches have also recently gained traction. 
These methods do not enable generation-based interaction, yet they may enable more efficient learning for classification-or regression-based tasks in high-dimensional continuous settings like images, audio, and video. 
Most of these methods output vectors for (parts of) inputs, which are trained to be similar for different "views" of an input [Wu et al. 2018; van den Oord et al. 2018; Chen et al. 2020c; He et al. 2020; Grill et al. 2020; Caron et al. 2021; Zhang et al. 2020a; Radford et al. 2021] or used to predict whether parts of inputs are real or fake [Clark et al. 2020; Iida et al. 2021] . 
Better understanding the trade-offs between generative and discriminative training, as well as capturing the best of both approaches, remain interesting avenues for future study. 
Capturing multimodal relationships. 
Another increasingly important research area is capturing the relationships between multiple kinds of data. 
What this means may differ based on the context and the goals of a modeler. 
For example, CLIP [Radford et al. 2021] and ViLBERT [Lu et al. 2019a ] are both multimodal vision-language, but differ in the precise way they are multimodal. 55 
 The former encodes images and text separately into vectors, enabling users who have examples from a single modality to retrieve, score, or classify examples from the other modality. 
The second processes images and text jointly at an early stage of the model, enabling downstream applications like visual question answering where reasoning over pairs of related images and text (e.g., images and questions about them) are provided. 
Multimodal foundation models remain a nascent research area; much is still unexplored about the different ways a model can be multimodal as well as better understanding the capabilities these additional modalities bring. 
4.2.3 Paths forward.
We close with some important goals for the future of foundation model training. 
Out-of-the-box SSL Right now, SSL objectives are highly domain-specific: different methods currently prevail in natural language processing, computer vision, and speech processing. 
This has two major disadvantages: First, these different techniques make it challenging to grasp the common threads and scientific principles underlying why each of these methods work. 
Second, this domain-specificity requires developing new foundation model training methods from scratch for each new field, including medical, scientific, and new multimodal settings. 
A more general objective for efficiently training foundation models on any kind of data would represent a significant milestone for the foundation model training community [Tamkin et al. 2021b ]. 
Obtaining a rich training signal It is clear that not all training objectives are made equal -some are radically more efficient than others, translating into far more capable foundation models for a given compute budget. 
Are there training methods orders of magnitude more efficient than those currently known? 
If so, how can we find them? 
These investigations will be shaped by many forces, including what future software and hardware advances ( §4.5: systems) make possible. 
We also need not view data ( §4.6: data) and training algorithms as independent factors: not only does the 53 For example, autoregressive models like GPT-3 enable prefix-based conditioning, while denoising models like T5 or BERT facilitate the use of bidirectional context to replace arbitrary-length spans or fix typos. 54 
Other kinds of generative approaches less studied in a foundation modeling context include diffusion and score-based models [Sohl-Dickstein et al. 2015; Song and Ermon 2019; Ho et al. 2020] , VAEs [Kingma and Welling 2014], flow models [Dinh et al. 2015; Kingma and Dhariwal 2018] , and GANs [Goodfellow et al. 2014 ] -it remains to be seen whether these or other future approaches can also enable learning as diverse variety of capabilities as autoregressive or denoising approaches. 55 
 While foundation models provide a powerful general-purpose engine for processing multi-modal information, adapting a foundation model before use is necessary for some applications. 
Broadly, an adaptation procedure produces an adapted model by conditioning a foundation model on additional information, either by priming the foundation model through the inclusion of new data or a prompt in its input or by updating some or all of the foundation model's parameters to reflect the new information. 
For example, in text summarization, appending a prompt such as TL;DR to the input article can improve foundation model performance [Radford et al. 2019 ] by acting as a task specification for the foundation model. 
Alternatively, fine-tuning the parameters of a foundation model with an organization's internal, domain-specific data could improve the model's accuracy by adding information relevant to the organization's use case. 
In this section, we describe existing approaches to adaptation and several factors that determine whether a particular adaptation procedure is appropriate for a particular setting. 
We additionally describe various use cases for foundation model adaptation, including relatively well-studied settings such as specialization of a foundation model to a particular task or domain as well as more speculative settings like test-time data removal [Bourtoule et al. 2019 ] and editing model behavior on particular inputs [Sinitsin et al. 2020 ]. 
We conclude by presenting a long-horizon goal for future research in foundation model adaptation. 
4.3.1 Methods for foundation model adaptation.
Many methods for adapting foundation models have been proposed, making the decision of which adaptation procedure to use for a particular problem or compute environment difficult. 
We emphasize three factors of particular importance for practitioners to consider when selecting an adaptation procedure: (1) the compute budget (specifically storage and memory); (2) the amount of task-specific data available; and (3) extent of access to foundation model gradients. 
Factor 1: Compute budget. 
For foundation models with billions or trillions of parameters, finetuning all model parameters may demand prohibitively large memory. 
Also, separately fine-tuning for many tasks can incur unacceptable storage costs. 
There are many works that propose methods to reduce the storage for adapting foundation models, and we refer to this class of lightweight adaptation methods as low-storage adaptation. 
Typically, methods in this class freeze most of the pretrained foundation model parameters and only learn a relatively small number of task-specific parameters (either by fine-tuning some pretrained parameters or by adding altogether new modules), reducing the storage overhead for each task. 
The key design decision for such algorithms is the choice of parameters to be adapted. 
Perhaps the simplest approach is to tune only the final layer of the pretrained model, while other works tune only the model's bias vectors [Zaken et al. 2021] , low-rank residuals to model weight tensors [Hu et al. 2021] , or masks over model parameters [Zhao et al. 2020b] . 
Another line of research tunes "soft" prompts [Li and Liang 2021; Qin and Eisner 2021; Liu et al. 2021e; Lester et al. 2021; Hambardzumyan et al. 2021] , corresponding to sequences of arbitrary parameter vectors rather than embeddings of the model's vocabulary, and conditions the foundation model on these prompts by concatenation with input activations, either at the input layer or at all layers. 
Another approach freezes all model parameters and interleaves new MLP modules with trainable parameters between existing model layers [Houlsby et al. 2019] . 
While these lightweight adaptation techniques seem to trade off parameter efficiency and performance on downstream tasks, they sometimes achieve comparable performance to full fine-tuning, despite updating 1000× fewer parameters [Zaken et al. 2021; Li and Liang 2021; Hu et al. 2021 ]. 
 Lester et al. [2021] shows an instance when the performance gap between full fine-tuning and lightweight adaptation vanishes as the model size increases. 
We remain uncertain how lightweight adaptation techniques scale as model size increases [Aghajanyan et al. 2020] . 
Because GPU memory is typically a more limiting resource than disk storage, low-memory adaptation procedures are perhaps even more critical to democratizing foundation models than low-storage adaptation procedures. 
Various techniques for low-memory model training have been proposed, which can be directly applied to fine-tuning-based adaptation procedures ( §4.5: systems). 
However, some low-memory procedures such as gradient checkpointing [Chen et al. 2016 ] trade off computation and memory, potentially exacerbating the significant energy consumption of foundation models [Bender et al. 2021] . 
Factor 2: Data availability. 
Task specialization mostly demands task-specific labeled data as training signals. 57 
However, the cost of annotation varies greatly across tasks and languages; for example, annotating MRI data requires expert medical knowledge, whereas labeling sentiment for English texts requires only common sense judgement. 
When adaptation data is plentiful, we might apply traditional fine-tuning approaches or their lightweight counterparts. 
In low-resource language-based settings, combining prompting and fine-tuning has been shown to be a promising direction [Schick and Schütze 2021a,b; Gao et al. 2020c; Perez et al. 2021; IV et al. 2021; Min et al. 2021] . 
Le Scao and Rush [2021] shows that a well-tuned prompt can be worth around 100 training examples, and fine-tuning a carefully prompted foundation model is significantly more data-efficient than fine-tuning an unconditioned foundation model. 
Factor 3: Access to foundation model gradients. 
Despite the significant impact of foundation models on some research communities, little standardization of distribution practices exists for large scale foundation models (with more than 50 billion parameters). 
As we gradually become aware of the potential hazards from the misuse of foundation models (see §5.2: misuse), providing access to all of a foundation model's parameters for fine-tuning might result in ethical concerns. 
Moreover, most users do not have enough compute resources to exploit their full access. 
For example, the memory requirements of foundation models might preclude their direct fine-tuning for many organizations and institutions. 
Consequently, future foundation model providers would likely restrict access to the full parameters of the model and instead provide surrogate API access, as exemplified by an early foundation model, GPT-3. 
On one extreme, if a foundation model provider enables access only to the model output (e.g., the textual continuation of a prompt, the generated image, or a score evaluating the alignment between an image and a textual description), foundation models can be adapted using in-context learning [Brown et al. 2020 ]. 
In-context learning freezes the foundation model parameters, and steers the output of the foundation models by conditioning on a (typically natural language) prompt, which might be composed of task instructions or demonstrations. 
To improve the performance of in-context learning, the prompt needs to be carefully designed, either by manual search or an automated procedure [Jiang et al. 2020; Shin et al. 2020] , and validated on the adaptation data. 
At the other extreme, if the foundation model provider grants access to gradients with respect to model parameters, full fine-tuning can be applied, where all model parameters are updated to improve performance on a downstream task. 
As a middle ground, we might obtain gradient access only to foundation model inputs, 58 which are considerably lower dimensional than foundation model parameters. 
In this case, we could deploy lightweight adaptation techniques [Liu et al. 2021e; Li and Liang 2021; Lester et al. 2021] , which freeze the model parameters and optimize a continuous prefix or prompt for each task. 
4.3.2 Use cases for adaptation.
Adaptation is useful whenever the desired use case of a model differs from the relatively general training objective used for foundation model training ( §4.2: training). 
Most commonly considered is the case in which a foundation model is adapted to perform a specific task (e.g., text summarization or animal classification from images), narrowing the scope of the model. 
Indeed, the vast majority of existing approaches described earlier in this section have targeted this setting. 
However, other forms of adaptation are useful, such as making local model edits to correct undesirable predictions for particular inputs or adding privacy constraints to the trained foundation model, which are task-agnostic. 
In this subsection, we describe a variety of use cases for adaptation, the methods that are most applicable to them, and remaining challenges in addressing these settings. 
Task specialization. 
The most widely-studied case of foundation model adaptation is that of task specialization, in which a foundation model is adapted to optimize performance for a specific task or set of tasks. 
For example, specializing for summarization tasks would induce foundation model behavior to extract key ideas from the input document(s) and re-organize them in short summary sentences. 
Various adaptation procedures have proven effective for task specialization, showing significant improvement over performance of the unadapted model [Howard and Ruder 2018; Brown et al. 2020 ]. 
In addition to the relatively widely-studied setting of specializing foundation models to specific tasks, other task-agnostic adaptation problems become increasingly challenging (but no less important) for foundation models owing to their size and computational demands. 
For example, the cost of training foundation models makes continual training over time to keep a model's predictions up to date with current events particularly expensive. 
Additionally, the challenge of collecting massive anonymized datasets used to train foundation models ( §4.6: data) make the likelihood of personal information leakage into training sets non-trivial; mechanisms to efficiently remove training data from a foundation model post-facto are therefore desirable. 
Temporal adaptation. 
Ideally, foundation models store knowledge that closely represents the state of the world, independent of modality. 
However, the world is constantly changing; new heads of state are elected, clothing styles change, social norms and beliefs shift ( §5.6: ethics), and the use of language evolves, causing a shift in the input distribution, target predictive distribution, or both. 
This temporal shift presents a challenging statistical problem due to the induced distribution shift, as discussed in §4.8: robustness. 
For foundation models, temporal shift also presents a particularly difficult computational problem; due to the computationally demanding nature of training foundation models [Shoeybi et al. 2019; Brown et al. 2020] , frequent re-training from scratch might carry unacceptable financial or environmental impacts [Bender et al. 2021 ] ( §5.3: environment), or simply take too long to be a viable method for keeping models up to date. 
In visual domains, gradual self-training on unlabeled data across intermediate time points can bridge the temporal shift across a long time period, but remains an expensive retraining procedure [Kumar et al. 2020a ]. 
In the context of language models, temporally-partitioned diagnostic datasets help quantify the rate at which large language models become outdated [Lazaridou et al. 2021; Hombaiah et al. 2021; Dhingra et al. 2021] , showing that classic techniques like re-weighting training data and dynamic evaluation (updating model parameters with new data at production time [Mikolov et al. 2010] ) can partially alleviate, but not fully solve, this problem. 
Explicitly conditioning a language model on the time period it is intended to model is another technique that has shown some promise [Dhingra et al. 2021 ]. 
An alternative strategy to addressing temporal shift is to design retrievalbased (semi-parametric) models, which augment the model input with additional context retrieved from a large, human-interpretable database (e.g., Wikipedia articles) [Karpukhin et al. 2020; Lewis et al. 2020b; Guu et al. 2020; Khandelwal et al. 2020; Khattab et al. 2020 ]. 
For retrieval-based models, adaptation corresponds to updating individual units of information in the database (e.g., single paragraphs of encyclopedia articles) without re-training the model. 
While promising, challenges for retrieval-based approaches remain both in training an accurate retrieval mechanism and in accurately conditioning on the retrieved information [Lewis et al. 2020b ]. 
We revisit the problem of temporal adaptation in the more general context of continual learning later in the section. 
Domain specialization. 
In addition to task specialization, it is often necessary to specialize a foundation model to a particular domain (such as legal documents), without limiting the breadth of tasks the foundation model can accomplish. 
This specialization induces a mismatch between the foundation model training and adaptation data distributions ( §4.8: robustness) which may require new adaptation methods to handle. 
Prior works have found that diverse and general pretraining data can cause negative transfer for current adaptation methods. 
For example, Cole et al. [2021] shows that fine-tuning a model pretrained only on the iNaturalist animal classification dataset provides better downstream performance than fine-tuning a model pretrained on iNaturalist along with 750K other images; similarly LegalBERT [Chalkidis et al. 2020] , which is pretrained only on legal documents, improves over BERT [Devlin et al. 2019] , which is trained on a much more diverse training set on the downstream task of text classification and sequence tagging in legal documents. 
One approach to domain specialization is to include an intermediate adaptation step, where the foundation model continues training on unlabeled data from the specialized domain. 
For instance, this approach improves the downstream performance significantly for satellite images and specialized text topics [Reed et al. 2021; Gururangan et al. 2020 ]. 
However, continual foundation model training may perform worse than re-training from scratch in certain domains such as legal documents [Chalkidis et al. 2020 ]. 
Elucidating the scenarios in which continual training does or does not benefit performance is an important direction for future work. 
Local model editing. 
In some settings, it is useful to adapt a foundation model locally, meaning that the model's predictive distribution should be adapted only for a single input or a local neighborhood around a single input, without changing the model's behavior for unrelated inputs. 
For example, when a foundation model produces an especially problematic mistranslation for a particular input phrase and target language, it is desirable to correct this mistranslation without affecting the model's behavior for unrelated phrases. 
Past work has studied the problem of applying approximately localized updates to large neural networks through new pretraining objectives that enable easy editing with standard gradient descent [Sinitsin et al. 2020 ], higher-order networks that predict parameter edits for an underlying model [Cao et al. 2021; Mitchell et al. 2021; Hase et al. 2021] , and constrained fine-tuning procedures [Zhu et al. 2020 ]. 
However, existing methods vary in the reliability with which they can perform model edits without damaging global model performance. 
Furthermore, scaling these methods to massive foundation models is not straightforward due to their size and the computational cost of training objectives that require computing higher-order gradients [Sinitsin et al. 2020; Cao et al. 2021; Mitchell et al. 2021 ]. 
Applying constraints. 
There are settings in which foundation models need to be adapted to satisfy privacy constraints. 
For instance, Carlini et al. [2021] demonstrated that existing foundation models are able to memorize sensitive information in the training data and can regurgitate such data when queried via standard APIs. 
While this phenomenon calls for improved data curation, developing adaptation procedures which eliminate or reduce the influence of specific data examples on the trained model would be a complementary solution. 
Improved adaptation strategies (along with better pretraining methods) in this direction will also benefit institutions working with foundation models under the General Data Protection Regulation (GDPR), as the mandate gives users the right to be forgotten. 
While research on the topic of machine unlearning [Bourtoule et al. 2019; Cao and Yang 2015 ] has started to gain traction, the problem has not yet been studied in depth for foundation models. 
In addition, foundation models trained on less curated internet data have been shown to exhibit harmful biases targeting specific groups (e.g., gender and racial bias) [Bender et al. 2021; Basta et al. 2019; Kurita et al. 2019; Sheng et al. 2019 ] and can produce toxic outputs [Gehman et al. 2020] ( §5.2: misuse). 
While strategies such as further fine-tuning the foundation model on carefully curated datasets (for potentially multiple generations) [Solaiman and Dennison 2021] or applying controllable generation techniques [Keskar et al. 2019 ] have shown some success in mitigating harmful behavior, a framework for training equitable and safe foundation models ( §5.1: fairness) will likely require further research with a collective effort across the data collection, training, and adaptation phases as well as consultation with domain experts. 
4.3.3 A long-term goal for foundation model adaptation research.
To the extent that adaptation is concerned with efficiently integrating a model's existing knowledge with new data or objectives, a natural extension of adaptation is continual learning [McCloskey and Cohen 1989; Parisi et al. 2019 ] or continual adaptation. 
The ability to adapt a foundation model continually is desirable, whether to keep a model's knowledge continually up-to-date with world events or cultural developments, continually add data from completely new domains or modalities as they become available, or continually edit a model's memories to comply with privacy or legal constraints as a society's values or laws evolve. 
However, continual learning problems typically induce catastrophic forgetting [McCloskey and Cohen 1989; Ratcliff 1990; Kirkpatrick et al. 2017 ] in neural networks, where old tasks or data are rapidly forgotten as the training distribution changes. 
We consider continual adaptation of a foundation model as a grand challenge for future foundation model adaptation research. 
Tackling this challenge requires closing the performance gap between a foundation model trained continuously on a non-stationary stream of data from different tasks, domains, or time periods and the same foundation model trained from i.i.d. 
data from the aggregate mixture [Lopez-Paz and Ranzato 2017] . 
Innovations in both model architectures and training objectives are likely to be necessary in order to do so. 
For example, while memory mechanisms have long been speculated as key to successful continual learning [French 1999 ] and have shown some promise for foundation models [Lewis et al. 2020b; Guu et al. 2020; Borgeaud et al. 2021] , effectively leveraging updated memories remains a challenging problem [Zhang and Choi 2021 ]. 
In addition, techniques for localizing knowledge in a foundation model in order to make targeted parameter updates [Dai et al. 2021a ] or learning such update rules [Cao et al. 2021; Mitchell et al. 2021 ] may help prevent forgetting, but repeated application of such updates still induces significant forgetting [Hase et al. 2021 ]. 
Continual foundation model adaptation may also require new understanding of how the problem of catastrophic forgetting manifests at the scale of foundation models as well as developing new meta-learning techniques [Schmidhuber 1987; Santoro et al. 2016; Finn et al. 2017] to improve forward transfer of already-learned information to new settings. 
Continually training on experiences gathered by deployed foundation models, or even training on aggregated experiences gathered by many different models, may speed the rate of progress of foundation model development, but incur risks of feedback loops and eroded alignment of model behavior and stakeholder values. 
Despite the aforementioned challenges, continual foundation model adaptation holds the promise of more rapidly responding to shifts in socio-cultural values, better leveraging existing knowledge to learn new concepts, lessening the environmental impact and increasing the accessibility of foundation models by eliminating the computational burden of training from scratch, and reducing the extent that previously-learned concepts must be re-learned due to forgetting. 
4.4 Evaluation
Authors: Rishi Bommasani, Kawin Ethayarajh, Omar Khattab Evaluation gives context to machine learning models: it serves as a means for (1) tracking progresshow do we we measure the performance of models and how do we design improved models ( §4.1: modeling); (2) understanding -what behaviors do models exhibit ( §4.11: interpretability) and how do they perform on different slices of data ( §4.8: robustness); and (3) documentation -how do we efficiently summarize model behavior and communicate this to diverse stakeholders. 
For foundation models, each of these purposes for evaluation are critical but the nature of foundation models introduces new challenges that are not generally encountered in other AI or ML settings: 
(1) Tracking progress requires relative comparison, but comparing foundation models is complicated by the fact that foundation models must be adapted (potentially in different ways) to perform tasks. 
(2) Understanding requires specified in-advance knowledge (e.g., taxonomies) of what is being evaluated for, but foundation models acquire emergent skills (e.g., in-context learning) that will be difficult to anticipate in designing evaluations. 
(3) Documentation requires clear desiderata to meaningfully inform decision-making, but foundation models can be adapted for myriad applications, which makes comprehensive documentation challenging. 
To orient the discussion of evaluating foundation models, we distinguish two classes of evaluation that arise from the abstraction of foundation models: intrinsic evaluation of the foundation model, which is inherently divorced from a specific task due to the task-agnosticity of these models, and extrinsic evaluation of task-specific models, which is necessarily dependent on both the foundation model and the adaptation mechanism. 
Further, we recognize that due to the anticipated impact and scope of foundation models, a variety of stakeholders (e.g., foundation model providers and application developers, auditors and policymakers, practitioners and researchers) will require evaluation of both foundation models and task-specific derivatives, with these evaluations serving different purposes and involving different desiderata based on the stakeholder. 
With this in mind, standard paradigms for the evaluation of machine learning models are not designed explicitly for the setting of foundation models. 
Therefore, we emphasize intrinsic evaluation ( §4.4.2: evaluationintrinsic), the importance of adaptation in extrinsic evaluation ( §4.4.3: evaluation-adaptation), and evaluation design ( §4.4.4: evaluation-design) as clear steps towards an evaluation framework that is better suited to foundation models. 
This discussion contributes to broader dialogue surrounding the role of evaluation of machine learning systems [Galliers and Spärck Jones 1993; Lipton and Steinhardt 2019; Ribeiro et al. 2020; Linzen 2020; Kiela et al. 2021; Milli et al. 2021; Jacobs and Wallach 2021; Bowman and Dahl 2021; Dehghani et al. 2021; Ma et al. 2021a , inter alia] and, given the complexities of evaluation, may benefit from drawing upon theories of measurement and evaluation that exist beyond machine learning [Messick 1987; Jackman 2008; Loevinger 1957; Messick 1988; Hand 2010; Brewer and Crano 2014] . 
4.4.2 Intrinsic evaluation.
Evaluation of machine learning systems has traditionally been grounded in tasks, often ones that are envisioned as functions specifically useful for applications (e.g., translation, object recognition). 
In contrast, since foundation models are intermediary assets that must be further adapted or specialized to perform useful tasks, the standard evaluation paradigm must be altered to facilitate the direct understanding and comparison of foundation models. 
One approach is to evaluate foundation models in terms of the task associated with the training objective. 
For example, a language model like GPT-3, which was trained by predicting the next word given the preceding context, may be evaluated based on the probabilities it assigns words given their preceding context in held-out test data (i.e., perplexity on language modelling benchmarks like LAMBADA [Paperno et al. 2016] ). 
This approach has shown promise in NLP thus far, but we identify two fundamental limitations it exhibits. 
First, relying on the training objective for evaluation lacks generality: foundation models trained using different incompatible objectives cannot be readily compared or understood in a consistent frame. 
Second, evaluation in this way relies upon a proxy relationship to be meaningful, i.e., measurements in terms of the training objective should correlate with other more meaningful and intelligible quantities (e.g., the quality of content generated via a foundation model). 
While this proxy relationship has proven to be robust in the past in some contexts, it likely will break down when assessing more diverse capabilities of foundation models, their behavior in more diverse environments or domains, and considerations beyond in-domain accuracy (we discuss this more extensively in §4.4.4: evaluation-design). 
In light of these limitations, we anticipate that two approaches will need to be considered, offering complementary benefits. 
Imputing intrinsic evaluation from broad extrinsic evaluation. 
One route towards evaluating foundation models is to adapt them to a wide range of tasks and measure the performance of the resulting task-specific models. 
As the foundation model is the shared basis across all of these models, the performance in aggregate reflects on the nature, and quality, of this shared basis. 
At present, many subareas of AI have begun to construct meta-benchmarks, i.e., a single evaluation that consolidates individual evaluations across a number of different tasks or domains [Wang et al. 2019b,a; Hu et al. 2020; Santurkar et al. 2020; Gehrmann et al. 2021; Hendrycks et al. 2021b; Koh et al. 2021; Tamkin et al. 2021b ]. 
Given the growing adoption of this paradigm and its established strengths, here we note why it is likely insufficient to fully satisfy the goals of evaluations with respect to foundation models. 
Meta-benchmark evaluation requires adaptation (minimally to specialize the foundation model to each of the tasks in the meta-benchmark), which makes reasoning about the foundation model itself challenging given the addition process (i.e., adaptation) involved. 
Specifically, this complicates matters of progress, both in terms of tracking (e.g., is performance attributable to potent foundation models or well-designed adaption practices) and in terms of identifying improvements in the process used to learn foundation models (e.g., fundamental improvements in data selection ( §4.6: data), training objectives ( §4.2: training), and model architectures ( §4.1: modeling) may be difficult to identify by comparing the performance on a meta-benchmark between two foundation models). 
In addition, this evaluation paradigm makes it difficult to understand or document properties and capabilities specific to the foundation model, which may make it unwieldy to convey to certain stakeholders (e.g., SuperGLUE performance may not be sufficiently informative, or may be misleading, for policymakers) or use as grounds for anticipating their behavior for new tasks or domains. 
Direct evaluation of intrinsic properties. 
To complement the use of meta-benchmarks, we also argue for why measuring the properties (e.g., specific capabilities or biases) of foundations models directly is valuable, divorced from specific tasks. 59 
For example, we may endeavor to directly measure the linguistic capabilities of foundation models to identify syntactically valid and invalid sentences. 
To motivate the value of this approach, we return to the purposes for evaluation. 
Notably, 59 Strictly speaking, these direct evaluations may still involve formulation as a task and foundation model specialization to perform the task, but the objective is more akin to probing (see §4.11: interpretability) of trying to measure the foundation model as directly as possible. 
articulating the presence and intensity of capabilities, skills, and biases identifies concrete areas for improvement (progress), elucidates the current potential (understanding), and expresses relevant aspects efficiently (documentation). 
Such an approach also is in service of broadly comprehensible evaluation, i.e., evaluation that can be understood by both technical experts, non-technical experts (e.g., policymakers or social scientists) and the general purpose. 
For example, characterizing the persuasive or rhetorical capabilities of these models may especially intuitive for internalizing their potential for disinformation and misuse ( §5.2: misuse) [Buchanan et al. 2021] . 
Direct evaluation of properties also serves as an important pathway towards better handling of the emergent properties of foundation models; to demonstrate this, we take in-context learning as a case study. 
In particular, Brown et al. [2020] not only demonstrated GPT-3's signature capability of robust in-context learning, but also were the first to specifically identify in-context learning as a specific way to adapt and interact with models (through their exploration of GPT-3). 
Traditional task-based extrinsic evaluation does not provide a clear means by which in-context learning could have been identified; directly interacting with the foundation model appears to be necessary in this case. 
More generally, while it appears inevitable that many unanticipated phenomena like in-context learning will be recognized through the unstructured or loosely structured exploration of these models and their capabilities, we believe new approaches to evaluation should be sought out that structure this exploration or, more ambitiously, suggest new properties that can then be more rigorously tested for. 
Intrinsic evaluation may also lower the threshold for demonstrating the potential of foundation models; new approaches for foundation models may be sufficiently promising if they demonstrate improvements in intrinsic evaluation, even if they are not immediately accompanied by corresponding well-suited adaptation methods for eliciting these capabilities in extrinsic evaluation. 
There is a significant open question of how intrinsic evaluation should be implemented; the mechanics of such evaluation are unclear. 
We enumerate a few general principles and considerations that may help inform the design and execution of intrinsic evaluation. 
(1) Inspiration from evaluation of humans. 
Many of the relevant properties, capabilities, and biases we are interested in for foundation models are also of interest for humans, which suggests that methods for measuring these properties in humans may prove to be instructive, or even directly translatable, for evaluating foundation models. 
For example, psycholinguistic measures of human linguistic competencies can be modified to evaluate foundation model linguistic competencies [Levy 2008; Frank et al. 2013; Linzen et al. 2016 (2) Human-in-the-loop evaluation. 
Human-in-the-loop evaluation may prove to be critical to provide a more exploratory means for understanding foundation models, including assessing their generative or interactive capabilities. 
In particular, human interaction with foundation models directly may better identify their emergent capabilities and limitations and direct auditing of foundation models [e.g., Raji and Buolamwini 2019, §5.6: ethics] may advances goals for documentation and transparency. 
(3) Validity of intrinsic measures. 
While intrinsic measures allow for direct measurement at the source, i.e., measurement and evaluation of the properties of a foundation model independent of adaptation and specific tasks, they pose challenges for building trust in the validity [Messick 1987 [Messick , 1988] ] of the evaluation. 
In particular, extrinsic evaluation outcomes may also be important in validating intrinsic measure design, e.g., the predictive validity of intrinsic measures (i.e., their ability to (statistically) predicted related downstream outcomes) may prove to be a central criterion. 
4.4.3 Extrinsic evaluation and adaptation.
Evaluating task-specific models has historically involved reporting the performance (generally meaning the accuracy) of the model on a specific held-out test set. 
While this paradigm may partially suffice to understand or document a model, it often amounts to unfair comparisons between taskspecific models produced with different (and, potentially, unequal) resources, making it difficult to gauge how much progress has been made. 
The concern of unfair comparisons is exacerbated in the foundation model regime: different foundation models (e.g., BERT and GPT-3) may form the foundation for different task-specific models, and these foundation models may involve vastly different amounts of training data and computation. 
To account for the resources required to achieve specific levels of performance, Linzen [2020] argues that (pre)training resources should be acknowledged and tracked in evaluation. 
We believe this is a scientifically principled proposal; comparing different approaches for training foundation models without accounting for training resources is likely to be misleading. 
However, given that the process for creating foundation models is especially expensive (e.g., requiring significant human and financial capital), and often governed by societal factors (e.g., commercial incentives) in addition to scientific factors, it may be the case that the foundation models in practice will vary greatly in the training resources afforded, making controlled comparison difficult. 
Here, we consider an alternative, which may be more pervasively viable, to partially account for the resources involved to complement the proposal of Linzen [2020] . 
In particular, we consider why extrinsic evaluation should acknowledge adaptation resources, which is critical for ensuring that extrinsic evaluation is able to identify the most performant adaptation methods (which intrinsic evaluation, fundamentally, cannot do). 
We draw attention to the fact that adaptation resources often are construed as the data used to adapt models, but additional resources [e.g., data used to choose adaptation methods; Perez et al. 2021 ] and constraints (e.g., the level of access required to adapt the foundation model; see §4.3: adaptation and §5.6: ethics for further discussion) should also be accounted for. 
Accounting for adaptation resources. 
Accounting for the resources expended to adapt foundation models for specific tasks requires a complete understanding of what resources or constraints are used for different adaptation methods, i.e., evaluations that endeavor to account for these resources must evolve alongside developments in what resources are used in adaptation ( §4.3: adaptation). 
In existing task-specific evaluations, most evaluations specify the amount of data that can be used to adapt a (foundation) model to the task. 
However, Perez et al. [2021] identify a key nuance here that has been discounted in past work, in that this should encapsulate all data used to inform adaptation, i.e., both the data used to adapt the foundation model and the data used to choose the adaptation method. 
Further, in the foundation model regime, the notion of access requirements for different adaptation methods is also a new consideration that should be factored into evaluation. 
Concretely, some adaptation methods may generally outperform others but may require greater ability to access or modify the foundation model compared to others (e.g., fine-tuning requires foundation model gradients to modify a foundation model, whereas prompting may only require blackbox access in specifying inputs). 
Accounting for the resources involved in adaptation enriches what conclusions can be reasonably drawn from evaluation of task-specific models. 
At present, task-specific evaluation may provide sufficient clarity for certain types of understanding or documentation of particular task-specific artifacts (i.e., the exact models being evaluated) but do not provide clear signal for how different adaptation methods perform and how to select a specific adaptation method in a given context. 
In contrast, by accounting for the resources and access requirements involved in adaptation, evaluation better enables research to identify which adaptation methods or processes make best use of the resources provided, i.e., signal is offered not just for the specific artifacts being evaluated but the more general processes by which they were derived. 
The proposed evaluation protocol, therefore, clearly works towards identifying which adaptation methods should be used; we note that all of these conclusions should always be taken as specific to a given foundation model, as evaluation in this form does not provide sufficient evidence to conclude an adaptation method is uniformly the best across foundation models. 60 
4.4.4 Evaluation design.
In theory, the goal of evaluation is to measure and characterize various theoretical constructs (e.g., accuracy, robustness ( §4.8: robustness), fairness ( §5.1: fairness), efficiency ( §4.5: systems), environmental impact ( §5.3: environment)) in service of various purposes (i.e., progress, understanding, documentation). 
However, in practice, the utility of evaluation will be determined by how evaluations are designed and executed. 
For example, automated measurements of the generative capabilities of foundation models (e.g., their factual correctness) may poorly capture the nature of these qualities and, instead, human-in-the-loop evaluation may better contextualize these capabilities. 
In considering the evaluation design we envision for foundation models and their adapted derivatives, we begin with the mechanics of evaluation. 
Traditionally, the evaluation of machine learning models has involved a large training set that is used to learn the model, an optional validation set that is used to set hyperparameters, and a test set to evaluate the generalization of the learned model to held-out data [Bishop 2006 ]. 
As a result, creating benchmarks to evaluate models has historically required large amounts of data, most of which is allocated towards training, which complicates the design of certain diagnostic or nuanced evaluations when data is scarce or expensive to attain [Rogers 2020 [Rogers , 2021]] . 
In contrast, because the benefits of foundation models will often coincide with the sample efficiency of adaptation (i.e., few-shot or zero-shot capabilities) and the diversity of possible applications, we instead envision a regime where benchmarks for individual tasks are much smaller (since far less data needs to be provided as "training", i.e., adaptation, data) and are far more diverse (both to capture various capabilities in intrinsic evaluation and more strongly ground evaluation in ecologically valid ways [Bronfenbrenner 1977; de Vries et al. 2020] during extrinsic evaluation). 
This suggests that the nature of foundation models may cause a shift in nature of benchmarks (and the mentality of those constructing benchmarks), de-emphasizing quantity as a key priority in benchmarks as opposed to quality and diversity. 
The NLP community has begun to see the beginnings of such a regime with expansive and diverse benchmarks like BIG-Bench 61 and FLEX [Bragg et al. 2021] ; this paradigm lowers the barrier for benchmark design, thereby enabling the broader community to partake in evaluation design. 62 
 Alongside the mechanics of evaluation, the presentation of and interface to the evaluation results informs how these results will be used inform decision-making (e.g., new modelling approaches, model selection, auditing). 
Leaderboards have become the de facto paradigm in machine learning, whereby models are ranked by a specific and singular criterion (generally a form of accuracy). 
This approach has generally led to significant and rapid progress in system quality over time [e.g., Wang 60 Current results, instead, suggest that different adaptation methods are better-suited to different types of foundation models and training objectives [Liu et al. 2021e; Lester et al. 2021 ]. 
61 https://github.com/google/BIG-bench 62 
Traditionally, the design of benchmarks like ImageNet [Deng et al. 2009] and SQuAD [Rajpurkar et al. 2016] has been conducted by high-resourced research labs that can afford to pay for the creation of these datasets through crowdsourcing [Rogers 2020 ]. 
et al. 2019a], but significant concerns have been raised of whether this yields more general improvements [e.g., Linzen 2020; Bowman and Dahl 2021]. 63 
As is true for all machine learning models, it is rarely the case that the desiderata for foundation models and their derivatives will be singular; instead, we anticipate the breadth of their application and societal impact necessitates heightened consideration of criteria beyond accuracy (e.g., robustness, fairness, efficiency and environmental impact). 
To this end, we note that evaluation of foundation models should report measurements across these diverse fronts; existing benchmarks are increasingly designed to reflect more than just accuracy (e.g., robustness [Koh et al. 2021; Goel et al. 2021] , fairness [Nadeem et al. 2021; Nangia et al. 2020] , efficiency and environmental impact [Coleman et al. 2017] ). 
Further, we note that if the reporting of performance across this different categories is done in the form of a leaderboard, mechanisms to disambiguate potential trade-offs (to induce a ranking) will be especially necessary [Ethayarajh and Jurafsky 2020] . 
In particular, since different stakeholders will have different preferences (e.g., the weight they ascribe to different properties) and values [Birhane et al. 2020] , leaderboard design should allow stakeholders to interact and manipulate how the ranking is done to align with their values; Ma et al. [2021a] presents an early attempt to enable this by comparing the utility of models using an economic framing based on a user's specified utility function. 
4.4.5 Takeaways.
Evaluation performs several roles (i.e., progress, understanding, documentation) that are vital for all machine learning paradigms, including the foundation model paradigm. 
Foundation models introduce new challenges for existing evaluation frameworks; designing evaluations that directly target the foundation model regime will better serve not only the multiple purposes of evaluation, but also the myriad of stakeholders involved. 
(1) While machine learning evaluation traditionally has considered task-specific models, evaluating foundation models involves engaging with the fact that these models are not specific to a task. 
Evaluation of these models likely will involve integrating two complementary approaches: (a) imputing the properties of foundation models from broad evaluation of task-specific derivatives and (b) direct measurement of these properties in foundation models. 
(2) Existing evaluation frameworks often do not account for the resources required to create the models being evaluated, leading to unfair comparisons. 
For foundation models, we discuss an evaluation paradigm that emphasizes accounting for adaptation resources (e.g., all data used in adaptation, access requirements for the foundation model), which appears to lead to more informative evaluations that better shape how adaptation is conducted. 
(3) Existing evaluation design often is limited in the diversity of metrics considered and requires large adaptation datasets. 
For foundation models, we echo growing calls for evaluation to consider a broader range of desiderata (e.g., robustness, fairness, efficiency, environmental impact) to capture the wide range of stakeholder values/preferences, as well highlight how the sample efficiency of adapting adaption models may allow for more diverse evaluations by re-allocating resources involved in designing evaluations. 
4.5 Systems
Authors: Deepak Narayanan, Trevor Gale, Keshav Santhanam, Omar Khattab, Tianyi Zhang, Matei Zaharia Computer systems are one of the largest bottlenecks to developing foundation models. 
Foundation models are frequently too large to fit in the main memory of a single accelerator (e.g., GPU) and require an immense amount of computation to train (e.g., > 1000 petaFLOP/s-days for GPT-3 [Brown et al. 2020] ). 
Additionally, these models will likely get larger over time: for instance, the compute and memory requirements of state-of-the-art language models have grown by three orders of magnitude in the last three years, and are projected to continue growing far faster than hardware capabilities (Figure 19 ). 
Once trained, these large models are expensive to perform inference with and difficult to debug, monitor, and maintain in production applications. 
We believe that further advances in the performance and usability of foundation models will require careful co-design across algorithms, models, software, and hardware systems, as well as new interfaces for programming and deploying ML applications. 
In this section, we discuss the key computer systems challenges in developing and productionizing large-scale foundation models. 
4.5.1 Improving performance through co-design.
Today, training large-scale foundation models [Brown et al. 2020; Rae et al. 2021; NVIDIA and Microsoft 2021; Wang and Komatsuzaki 2021] can often require custom software systems such as Megatron, DeepSpeed, or Mesh Transformer JAX [Shoeybi et al. 2019; Rasley et al. 2020; Wang 2021] , built on top of standard frameworks like PyTorch, TensorFlow, and JAX [Paszke et al. 2019; Abadi et al. 2016; Bradbury et al. 2018] . 
These software systems rely on a number of innovations across the stack to train models efficiently at scale: new parallelization dimensions such as pipeline parallelism [Huang et al. 2019; Narayanan et al. 2019 ] that limit communication while keeping devices busy, state-sharding optimizers to reduce memory usage [Rajbhandari et al. 2020 ], just-intime (JIT) compilers to optimize the computation graph [PyTorch 2021], and optimized libraries like cuDNN and NCCL [NVIDIA 2021 ]. 
Megatron and DeepSpeed are efficient to a particular scale; for example, Megatron can extract up to 52% of the theoretical peak throughput of modern hardware with approximately 3000 GPUs on a model with a trillion parameters [Narayanan et al. 2021b ]. 
However, scaling to larger models with more GPUs still is challenging, since existing parallelization strategies break down at larger GPU counts. 
Data parallelism is limited by the batch size [Li et al. 2020e] , pipeline parallelism by the number of layers in the model [Huang et al. 2019; Narayanan et al. 2019] , and tensor model parallelism by the number of GPUs in a single server [Shoeybi et al. 2019] . 
While we will continue to realize performance gains from new hardware, growth in the resource requirements of large models far outstrips generational hardware improvements [Brown et al. 2020 ]. 
To facilitate the next major leap in model capacity and to democratize the advances in model quality, it will be increasingly critical to co-design training algorithms, models, software, and hardware, because many of the avenues to dramatically increase performance alter the semantics of the training computation. 
For example, executing operations in lower precision (such as fp16) can help increase throughput on modern hardware (e.g., the V100 and A100 GPUs have dedicated tensor core units for lower-precision matrix multiplication), but also affect the numerics of the optimization procedure [Micikevicius et al. 2017] . 
Similarly, exploiting weight sparsity can significantly improve training and inference times [Elsen et al. 2020; Gale et al. 2020 ] by only performing mathematical operations on the non-zeros in the model, but requires different training algorithms [Jayakumar et al. 2021; Evci et al. 2020; Dettmers and Zettlemoyer 2019] . 
Other examples of co-design include model architectures that map more efficiently to hardware [So et al. 2019; Child et al. 2019; Wang et al. 2020c; Lee-Thorp et al. 2021; Kitaev et al. 2020; Beltagy et al. 2020; Tay et al. 2020; Ren et al. 2021 ], efficient optimizers [Anil et al. 2020; Shazeer and Stern 2018] , novel tokenization alternatives [Xue et al. 2021; Tay et al. 2021] , specially architected hardware training platforms [Jouppi et al. 2017; Mudigere et al. 2021; Selene 2021] , and distributed parallelization strategies with relaxed weight update semantics [Narayanan et al. 2019 [Narayanan et al. , 2021a]] . 
Case study: efficient knowledge representation. 
As a concrete case study of successful codesign, retrieval-based models such as REALM, RAG, ColBERT-QA, and RETRO [Guu et al. 2020; Lewis et al. 2020b; Khattab et al. 2020; Borgeaud et al. 2021 ] take a different approach to model design than simply increasing the number of model parameters. 
Instead of trying to accumulate implicit knowledge from ever-larger datasets directly into a DNN model with billions of parameters (like GPT-3), retrieval-based models store knowledge outside the model parameters in the form of text passages, capturing knowledge within the passages with dense vector representations. 
These models then use scalable top- search mechanisms to extract knowledge pertinent to each input, while keeping the DNN model itself small ( §4. 1.4: modeling-memory) . 
This design improves computational efficiency as well as maintainability of the model in production: for example, developers can update the knowledge of the model just by replacing a text passage, without needing to retrain a large DNN. 
Retrieval-based models have achieved promising initial results by leveraging several new crossfunctional ideas, including backpropagating the loss through the retriever during training [Guu et al. 2020] (which requires approximating the gradient through a knowledge store consisting of millions of passages) and modeling fine-grained interactions between queries and passages [Khattab and Zaharia 2020; Khattab et al. 2020 ] (which requires decomposing the computation into vector-level nearest-neighbor search operations). 
These techniques allow retrieval-based models to be accurate and efficient, but demand functionality not readily supported by popular ML frameworks and nearest-neighbor indexes such as FAISS [Johnson et al. 2019 ]. 
4.5.2 Automated optimization.
Another important challenge in systems is to automate the application of optimizations that straddle algorithms, models, software, and hardware. 
While many optimizations and parallelization strategies are complementary, identifying the most effective combination of optimizations is challenging since the joint search space grows combinatorially and optimizations interact in non-trivial ways [Narayanan et al. 2021b ]. 
Foundation models heighten the need for automated optimization as manual experimentation is extremely expensive and time-consuming at the scale of thousands of GPUs. 
Recent work in this area has focused on systems targeting semantics-preserving optimizations. 
In particular, systems have been proposed to automatically discover mathematically-equivalent graph substitutions [Jia et al. 2019a; Wang et al. 2021c ], facilitate the distributed execution of operator graphs through both high-level APIs and low-level compilers [Rasley et al. 2020; Mandeep Baines 2021; Bradbury et al. 2018; Shazeer et al. 2018; Lepikhin et al. 2020] , and automate the selection of hybrid distribution strategies [Jia et al. 2019b; Santhanam et al. 2021 ]. 
These systems have helped deploy many foundation models in industry [Fedus et al. 2021; M2M-100 2020; Turing-NLG 2020] . 
Unfortunately, automated optimization becomes much harder when composing semanticsaltering optimizations ( §4.5.1: systems-co-design), as it is often unclear how to jointly model the statistical impacts of these techniques (e.g., how many training iterations are needed to reach a specific accuracy?). 
We will therefore need new software tools, libraries, and compilers to automatically identify compositions of optimizations that target comprehensive metrics like timeto-accuracy [Coleman et al. 2017; Mattson et al. 2020 ]. 
Building such tools will require tight collaboration between systems and machine learning experts. 
4.5.3 Execution and programming models.
The unique multi-task nature of foundation models provides an opportunity to amortize training and inference costs over many applications. 
In particular, paradigms such as adaptation mean more sharing across model instances [Raffel 2021 ]. 
For example, two models prefix-tuned [Li and Liang 2021] from the same pretrained model can share the same model "stem," reducing the storage footprint (the shared stem only needs to be stored once), while also making it possible for execution to be shared and batched across the prefix-tuned models [Shen et al. 2019; Narayanan et al. 2018] . 
Consequently, the specific adaptation mechanism used informs system optimization ( §4.3: adaptation). 
It is an open question as to what programming interface should be used to specify that various adapted models are derived from the same pretrained model (e.g., models and are derived from the same pretrained model ), or that various components of two models share parameters (e.g., two models and share the same stem up to layer ). 
Ludwig [Molino et al. 2019 ] and PyTorch's Module offer easy ways to compose functionality within a model, but no system today supports cross-model dependencies. 
Giving users the opportunity to provide annotations will allow training and inference systems to optimize and orchestrate computation more efficiently; without such annotations, systems will not have visibility into what computation and parameters can be shared across model instances. 
A model's "adaptation history" (what models is this particular model adapted from) can also be used for debugging: an adapted model's errors on particular types of inputs could originate from the pretrained model, pointing to issues in the pretraining process versus adaptation process. 
Frameworks like PyTorch, as well as software libraries for training foundation models such as HuggingFace Transformers [Wolf et al. 2020] , do not allow for fine-grained lineage information across entire model instances to be specified. 
Building and maintaining a cluster of thousands of accelerators also requires tremendous effort. 
New training paradigms like Learning@Home [Ryabinin and Gusev 2020; Diskin et al. 2021 ] explore leveraging volunteer compute over the internet to train foundation models collaboratively. 
Such fundamentally new execution models can decrease the cost of training for any one entity, but require collaboration across a number of different areas like security (to ensure that a malicious volunteer cannot significantly alter the training process), distributed systems (to deal with fault tolerance issues as volunteers drop), and crowdsourcing. 
4.5.4 Productionization of foundation models.
As the community continues to push the capabilities of foundation models, realizing their potential will require addressing the challenges associated with deploying these resource-intensive models in production. 
These challenges include performing model inference with tight latency targets, and ensuring that models and data are monitored in an automated way. 
For applications with strict cost and latency constraints, model compression techniques like distillation [Hinton et al. 2015; Li et al. 2020d; Sanh et al. 2019] , quantization [Polino et al. 2018; Gholami et al. 2021; Zhou et al. 2018] , pruning [LeCun et al. 1990; Gordon et al. 2020; McCarley et al. 2019; Wang et al. 2019c; Sajjad et al. 2020] , and sparsity [Gale et al. 2020; Elsen et al. 2020 ] could aid deployment by transforming larger models to obtain desired inference-time properties. 
These techniques were originally intended for smaller models (e.g., BERT-L) in low-memory environments (e.g., mobile phones), but are now necessary to handle the extreme scale of modern foundation models in datacenter deployments. 
Parallelization techniques like tensor model parallelism [Shoeybi et al. 2019] , traditionally used for training, might also be useful to reduce inference latency, and also provide additional memory capacity across GPUs to fit the model's parameters. 
In addition to these practical constraints, increases in the size and complexity of foundation models and the datasets used to train them pose new challenges to model and dataset lifecycle management. 
Since models with a large number of parameters are hard to manually inspect by humans, we need better systems for automated dataset curation ( §4.6: data) and model quality assurance. 
Techniques like behavioral testing [Ribeiro et al. 2020 ] and model assertions [Kang et al. 2020 ] facilitate easier model maintenance in production by providing analogs to unit tests, runtime monitoring (in the form of test-time assertions), and continuous model improvement (as new inputs come in) for models deployed in end applications. 
These tools can help address issues of fairness and bias ( §5.1: fairness), and reduce model mispredictions. 
4.6 Data
Authors: Laurel Orr, Simran Arora, Karan Goel, Avanika Narayan, Michael Zhang, Christopher Ré Foundation models signal a paradigm shift where increasingly massive quantities of data are being "fed" to these models for improved adaptation performance [Devlin et al. 2019; Radford et al. 2021; Tolstikhin et al. 2021] with the overarching rule-of-thumb being "the more data the better" [Kaplan et al. 2020] . 
As previous sections have mentioned, this focus on data curation has raised concerns around the foundation model data lifecycle including (1) managing the data at such a large scale ( §1: introduction), (2) integrating data across new modalities ( §2.3: robotics, §3.1: healthcare), (3) reasoning over licensing and governance regulations -especially when considering the massing web-crawls used in foundation models training -( §3.1: healthcare, §5.4: legality), and ( 4 ) understanding the data quality ( §4.4: evaluation). 
While foundation models add new and difficult facets to these challenges, we see parallels between these issues and core challenges in communities such as data management and data analytics as well as industrial ML pipelines. 
For example, data management has long studied scalable declarative systems for data analysis, versioning, provenance, and integration-addressing challenges ( 1 ) and ( 2 ) [Zaharia et al. 2012; Cudré-Mauroux et al. 2009; Stonebraker and Weisberg 2013; Stonebraker and Ilyas 2018; Hellerstein and Stonebraker 2005] . 
Industry has pipelines dealing with challenge (3) to manage diverse data licenses and help mitigate data violations. 
There is an entire ecosystem of research and systems tackling challenge ( 4 ) to support interactive data analytics and visualization [Hohman et al. 2020 ]. 64 
While these solutions are not necessarily "foundation model-ready", we believe a path to better management of the foundation model data lifecycle should take inspiration from these existing systems. 
In this section, we address managing the foundation model data lifecycle. 
We first outline four desiderata including data management at scale, support for heterogenous data sources, data governance, and data quality monitoring. 
We then envision how all of these requirements can be integrated into a holistic data management solution called a data hub. 
The data hub is simply a data management toolkit that can be used by the private or public sectors to better support the interactive management of the foundation model data lifecycle. 
4.6.1 Data Management Desiderata.
Current practices in foundation model development are generally ad-hoc across the entire lifecycle from data curation and data documentation to model monitoring and patching [Gebru et al. 2018; Bandy and Vincent 2021; Bender and Friedman 2018] . 
Research in the data management community has shown that well-defined data management platforms facilitate ML model development at scale through data ingestion, data versioning, data provenance, efficient analysis, and model monitoring [Hellerstein and Stonebraker 2005; Agrawal et al. 2019; Vartak et al. 2016; Ikeda and Widom 2010] . 65 
Taking inspiration from the data management community, we consider core desiderata when building a holistic data management platform for foundation models. 
(1) Scalability. 
Foundation models are being trained on increasingly massive quantities of data [Kaplan et al. 2020] with the WuDao 2.0 model being trained on 4.9 TB of multi-modal data. 66 
This scale is expected to increase as most recent models are trained largely on public facing datasets. 
Public data represents an extremely small fraction of data compared to the petabytes of business and personal data collected every day and used in industrial foundation model pipelines [Marr 2017 ]. 
There is therefore a growing need for highly scalable techniques that can handle multi-modal foundation model datasets. 
(2) Data integration. 
Recent work using foundation models demonstrates that leveraging integrated structured and unstructured data can help models better generalize to rare concepts [Orr et al. 2020 ] and improve factual knowledge recall [Orr et al. 2020; Logeswaran et al. 2019; Zhang et al. 2019a; Peters et al. 2019; Poerner et al. 2020] . 
Despite these recent successes, integrating datasets for foundation models remains a challenge. 
Many works use unstructured text data with structured entity knowledge or image data [Antol et al. 2015 ]. 
There is a growing need to integrate datasets across diverse modalities such as text, video, eye-tracking [Hollenstein et al. 2020] , and robotic simulations [Lynch and Sermanet 2021 ] (see §2.3: robotics). 
We need data-integration solutions that can be applied at an industrial scale to multiple modalities and to multiple domains, such as government, business, and science. 
(3) Privacy and governance controls. 
The training data used for foundation models may risk the violation of the privacy of data subjects; their data may be disclosed, collected, or used without their consent [Jo and Gebru 2020] or outside the context for which consent was originally given. 
The issue of consent and use is especially relevant for foundation models where downstream applications cannot always be anticipated. 
As explained in §5.4: legality, these issues are compounded with the prevalence of web scraped datasets for foundation model training. 
As there are still open legal questions about how web-crawled data will be governed and copyrighted, 67 the consequences of using web data remain unclear to foundation model providers in the public and private sector. 
We need tooling to help foundation model providers adapt to emerging regulations and guidelines to ensure safe and responsible data management. 
( 4 ) Understanding data quality. 
Data quality impacts model performance [Lee et al. 2021b] ; however, toolkits or methods to systematically and scalably understand the training data and relevant data subsets are still in their infancy. 
The data creation process can be messy, and the data can contain different types of biases [Blodgett et al. 2020; Bender et al. 2021 ] (see §5.1: fairness) and consist of poisoned, false, or duplicated information [Chang et al. 2020; Carlini and Terzis 2021; Buchanan et al. 2021; Lee et al. 2021b] . 
Data is also continuously updated and refined [Kiela et al. 2021 ] and may have emergent entities [Fetahu et al. 2015] , distribution shift [Chen et al. 2021a] , and concept meaning shift [Kenter et al. 2015; Lazaridou et al. 2021] . 
Further, once deployed, foundation models may present undesirable behavior on critical, fine-grained sub-populations of data that foundation model providers need to detect and mitigate [Goel et al. 2021; Hohman et al. 2018; Ré et al. 2019; Oakden-Rayner et al. 2019] . 
We need toolkits that can detect and potentially mitigate different types of undesirable data to improve model performance in an interactive and iterative fashion. 
Such toolkits also need to adapt to the dynamical nature of training data. 
4.6.2 Data Hub Solution.
Pulling on years of work from data management, data science, and data analytics, we envision a foundation model lifecycle data management solution, which we term a data hub. 
While examples 67 These issues have recently come to bear by the debate surrounding the use of GitHub data in Copilot's Codex tool to help developers code https://www.pwvconsultants.com/blog/questions-around-bias-legalities-in-githubs-copilot/ of ML-focused data hubs 68 as well as more traditional data management systems exist, 69 they either (1) do not treat data integration as a first class primitive, (2) do not natively support the end-to-end lifecycle with model predictions, or (3) do not allow for interaction-driven data curation and refinement, where foundation model providers can dynamically explore and update possible datasets subject to access control guidelines. 
We now discuss how the data hub addresses the four desiderata. 
Data scale. 
To address the management at scale challenge, the data hub will need standard data management solutions [Armbrust et al. 2009 ] such as infrastructure to store and maintain largescale datasets as they change over time and scalable interfaces to query, select, and filter datasets. 
The hub should support heterogenous compute as well as cloud infrastructure to support scalable solutions in different environments. 
Data integration. 
The hub should incorporate data integration as a first class citizen. 
It will need advanced data integration solutions [Stonebraker and Ilyas 2018; Abiteboul 1997; Dong et al. 2020; Rekatsinas et al. 2017a] 70 to allow for the merging of structured and unstructured knowledge across modalities and domains. 
Further, this implies the hub will need to support storing and querying over heterogeneous datasets and sources. 
Access control. 
Considering the access controls of the hub, the hub will need to support diverse documentation, e.g., dataset sheets [Gebru et al. 2018] or data statements [Bender and Friedman 2018] , to allow data curators to reflect on their processes and be transparent about the intended use cases, potential biases, and limitations of their dataset. 
The data hub will need to decide which documentation is required for data to be uploaded (e.g., the data source and data description) and which information is recommended (e.g., what tasks the data could be used for). 
Furthermore, documentation may need to be updated as datasets evolve [Goel et al. 2021] . 
Data sources are often associated with licenses, and the hub will need to integrate different sources with different legal concerns and conditions [Masur 2018 ]. 71 
Further, certain datasets have legal guidelines to protect the privacy of the data subjects. 
The hub will need methods to ensure a dataset does not release personally identifiable information (PII), 72 that the aggregation of anonymized or de-identified data does not release PII, 73 and that the data subjects have given informed consent for their data to be disseminated. 74 
 Pulling on ideas from data integration [Rekatsinas et al. 2017b ], the hub should support mechanisms to enable efficient and safe maintenance and sharing of data resources. 
Especially as the legality of certain public datasets (e.g., web dumps) are still being decided ( §5.4: legality), the hub critically needs tooling to help identify licensing violations and mitigate the impact of any governance violation. 
As certain violations will likely relate to model behavior, we need systems to support better understanding of model behavior, as we describe next. 
Data quality tooling. 
Drawing on the field of data analysis and exploration, as users interactively select, filter, and refine the data to use for training or adaptation, the hub will need tools to quickly 68 Some public data hubs include: https://data.world/, 
https://dataverse.harvard.edu/dataverse/harvard, 
https:// datacommons.org/, 
https://www.data.gov/, 
https://www.kaggle.com/, 
https://huggingface.co/datasets, https://www.ldc. 
upenn.edu/ 69 
Some traditional data management systems for foundation models include: https://aws.amazon.com/big-data/datalakesand-analytics/, 
https://eng.uber.com/michelangelo-machine-learning-platform/, 
https://kafka.apache. 
As central components in critical data-driven decision-making systems, machine learning models must address a variety of security and privacy threats. 78 
These threats can be characterized using the traditional "CIA triad" of computer security. 
ML systems should protect the Confidentiality of user data against inference and reconstruction attacks [Fredrikson et al. 2015; Shokri et al. 2017; Carlini et al. 2019 Carlini et al. , 2021]] . 
Moreover, the secrecy of trained models themselves can be at risk of model stealing attacks [Tramèr et al. 2016; Papernot et al. 2017 ]. 
The Integrity of ML systems can be compromised by adversarial examples [Biggio et al. 2013; Szegedy et al. 2014 ] and data poisoning attacks [Biggio et al. 2012; Chen et al. 2017 ]. 
Finally, resource-depletion attacks [Shumailov et al. 2020; Hong et al. 2020a ] can threaten the Availability of ML systems. 
In regard to these threats, we posit that the security role of foundation models in future machine learning systems will be akin to the role played by the operating system in traditional software systems. 
Due to its generality and ubiquity, a foundation model may become a single point of failure and thus a prime target for attacks against applications derived from this model. 
In turn however, a foundation model imbued with strong security and privacy properties could form the backbone for the design of a variety of secure and reliable ML applications. 
Of course, these applications may still have to be designed to enforce specific security and privacy guarantees (in the same way that software designers cannot rely on a secure operating system to protect against all security risks). 
4.7.1 Risks.
Single points of failure. 
A foundation model that is adapted to a variety of applications represents a single point of failure for these applications. 
For example, data poisoning attacks on a foundation model, where an adversary inserts malicious examples into the training data, might impact all adapted applications as well. 
Similarly, adversarial examples against a foundation model (i.e., small input perturbations that cause the model to output very different features) could more easily transfer to adapted applications. 
 Wallace et al. [2019] even find that a single adversarial trigger added to any input can cause language models such as GPT-2 to output a predefined piece of text. 
A foundation model can also become a single point of failure for data privacy. 
If a foundation model is pretrained on a company's private data and the model memorizes part of this data, all downstream applications could run the risk of exposing this data [Carlini et al. 2021] . 
The provider of a foundation model may also be a single point of trust for the privacy of application data. 
For example, the current API for GPT-3 requires that all (potentially sensitive) data used for fine-tuning or inference be uploaded to OpenAI's servers. 
Designing a foundation model service that avoids this centralization of trust is an interesting problem. 
If the parameters of a foundation model are public, model stealing attacks on adapted applications could be facilitated, as the attacker only needs to reverse-engineer the "delta" with respect to the public foundation model [Krishna et al. 2019 ] (e.g., a linear model trained on features extracted from a public frozen model). 
Finally, denial-of-service attacks on the foundation model provider could also be a concern and might be exacerbated by querying the model with special high-cost inputs [Shumailov et al. 2020 ]. 
Data poisoning. 
Successful foundation models have so far been trained on large and often uncurated datasets scraped from the Web [Radford et al. 2021 [Radford et al. , 2019]] . 
This permissive data collectioncoupled with a lack of direct training supervision -facilitates poisoning attacks on a foundation model's training data (e.g., injecting hateful speech targeted at a specific individual or company into a few outbound pages from Reddit). 
Worse, the power of poisoning attacks may be exacerbated by the growing size and accuracy of today's models [Carlini 2021 ]. 
To illustrate, Schuster et al. [2021] show that a code auto-completion system trained with GPT-2 on Github data can be poisoned into suggesting insecure code snippets with the injection of only a few malicious files. 
 Carlini and Terzis [2021] further show that targeted attacks against CLIP-style [Radford et al. 2021 ] models require modifying as little as two out of 3 million training examples. 
Function creep & dual use. 
Foundation models learn general features that enable them to be easily adapted to a variety of tasks. 
This flexibility, however, raises concerns that foundation models could be used beyond their originally foreseen purposes -a risk commonly referred to as function creep or dual use. 
Examples of function creep in machine learning include overlearning [Song and Shmatikov 2019] and adversarial reprogramming [Elsayed et al. 2018] . 
To illustrate, CLIP was originally trained to solve the generic task of predicting image-text pairs, but in doing so also learned to capture rich facial features [Goh et al. 2021 ]. 
While CLIP's "model card" 79 explicitly places facial recognition and other surveillance technologies as out-of-scope, CLIP can certainly be re-purposed for such tasks [Radiya-Dixit and Tramèr 2021] . 
This example illustrates that it may be challenging to constrain (or even foresee) the possible nefarious uses of a foundation model when it is designed. 
§5.2: misuse provides further discussions on dual (mis)use of foundation models. 
Multimodal inconsistencies. 
Multimodality may increase the attack surface of foundation models, by enabling adversaries to exploit inconsistencies across modalities. 
The possibility of such attacks was demonstrated in an (in)famous example of CLIP classifying an apple with the word "iPod" stuck to it as an iPod [Goh et al. 2021 ]. 
More generally, whenever a concept can be expressed using different modalities, inconsistencies across these modalities may be exploitable. 
Such inconsistencies are particularly concerning when a foundation model is adapted to a task that primarily relies on only one of the learned modalities. 
For example, consider using features extracted from CLIP for facial recognition. 
This is a purely visual task, yet the adapted model's features will still be sensitive to textual signals (thus, an attacker might be able to evade facial recognition by wearing clothes with imprinted text). 
Alternatively, consider an autonomous driving system (an application that also relies primarily on vision) that sees a billboard with the word "green" on it, and mistakenly interprets this as a green light. 
4.7.2 Opportunities.
Security choke points. 
If adapted applications can inherit vulnerabilities from a foundation model, they can also inherit desirable security characteristics -such as robustness to adversarial examples or poisoning attacks. 
Foundation models could thus serve as security choke points. 
For example, a model robust to adversarial examples can retain its robustness when it is adapted to other tasks [Shafahi et al. 2019] . 
Similarly, a foundation model provider that can (somehow) defend against poisoning, model-stealing or resource-depletion attacks could then provide such security guarantees for its customers' applications. 
The tradeoff between a foundation model's role as a single point of failure or as a security choke point is reminiscent of similar security tradeoffs in other abstraction layers in the software stack (e.g., an operating system, database system, or a Web browser). 
By virtue of serving many different applications, an abstraction layer is a prime target for attack, but can typically also leverage far greater resources to enhance its security compared to any single application. 
Cheaper private learning. 
Current foundation models are often trained by amassing vast amounts of data from publicly available sources (e.g., from the open Web). 
This practice may raise concerns about privacy -in the broad sense of taking user data out of its intended context [Nissenbaum 2004; Carlini et al. 2021] . 
While some existing works aim to mitigate a model's propensity to memorize training data (e.g., by de-duplicating training data [Lee et al. 2021b] , or by pretraining under differential privacy [Anil et al. 2021] ), such solutions are unlikely to meet the broad privacy expectations that users could associate with text data [Brown et al. 2022 ]. 
On the other hand, public pretraining could also end up being a win for user privacy in applications that handle scarce and sensitive data (e.g., in healthcare). 
As an example, consider the problem of training a differentially private model [Dwork et al. 2006 ] for a healthcare task. 
Training such a model "end-to-end" (i.e., without leveraging any pretraining) to a decent privacy-utility tradeoff currently requires vast amounts of privacy-sensitive data [McMahan et al. 2018; Basu et al. 2021] . 
In contrast, a foundation model pretrained on public data in many cases could be adapted to perform specific tasks with significantly less confidential data [Bommasani et al. 2019; Tramèr and Boneh 2021; Li et al. 2022; Yu et al. 2022 ]. 
Robustness to adversarial examples at scale.
There is evidence suggesting that training a model that is robust to adversarial examples requires vastly more data compared to standard training [Schmidt et al. 2018 ], but that unlabeled data may suffice to bridge this gap [Carmon et al. 2019; Uesato et al. 2019] . 
Moreover, increasing model size and capacity (i.e., over-parameterization) has also been shown to be necessary for achieving adversarial robustness in some settings [Madry et al. 2018; Bubeck and Sellke 2021] . 
Understanding how best to leverage over-parameterization and unlabeled data to achieve adversarial robustness is an important direction for future research. 
Given their unprecedented scale (both in terms of model size and training set size), foundation models are uniquely positioned to benefit from this line of inquiry. 
Despite their unprecedented scale, current foundation models unfortunately see little gains in robustness to worst-case adversarial perturbations [Fort 2021; Wallace et al. 2019 ]. 
However, multimodal models such as CLIP are surprisingly robust to (non-adversarial) distributional shifts (see §4.8: robustness). 
Whether these gains in distributional robustness can translate to increased resilience against real-world attacks is another exciting open question. 
Particularly in settings where adversaries are subject to various constraints (e.g., limited query access or computational budget), there is reason to be optimistic that enhanced distributional robustness could lead to concomitant gains in overall security -even if the foundation model remains vulnerable to worstcase "white-box" attacks. 
4.8 Robustness to distribution shifts
Authors: Sang Michael Xie, Ananya Kumar, Rohan Taori, Tony Lee, Shiori Sagawa, Pang Wei Koh, Tatsunori Hashimoto 
Real-world ML systems need to be robust to distribution shifts -they should work well on test distributions which differ from the train distribution. 
High-stakes applications such as poverty mapping in under-resourced countries [Xie et al. 2016; Jean et al. 2016] , self-driving cars [Yu et al. 2020a; Sun et al. 2020a] , and medical diagnosis [AlBadawy et al. 2018; Dai and Gool 2018] all require models that generalize well to circumstances not seen in the training data, e.g., test examples from different countries, under different driving conditions, or from different hospitals. 
Prior work has shown that these types of distribution shifts can cause large drops in performance even in state-of-the-art models [Blitzer et al. 2006; Daumé III 2007; Sugiyama et al. 2007; Ganin and Lempitsky 2015; Peng et al. 2019; Kumar et al. 2020a; Arjovsky et al. 2019; Szegedy et al. 2014; Hendrycks and Dietterich 2019; Sagawa et al. 2020a; Recht et al. 2019; Abney 2007; Ruder and Plank 2018; Geirhos et al. 2018; Kumar et al. 2020b; Yu et al. 2020b; Geirhos et al. 2020; Xie et al. 2021a; Koh et al. 2021] . 
In this section, we consider the role of foundation models on robustness to distribution shifts. 
A foundation model is trained on a large and diverse unlabeled dataset sampled from a distribution pre and can be adapted to many downstream tasks. 
For each downstream task T , the foundation model is adapted to labeled training data sampled from an in-distribution (ID) training distribution T ID , and then evaluated on an out-of-distribution (OOD) test distribution T OOD . 
For example, a poverty prediction model [Xie et al. 2016; Jean et al. 2016 ] may be pretrained on unlabeled satellite data from across the world to learn useful features for all countries, then fine-tuned on labeled examples from Nigeria, and finally evaluated in Malawi where labeled examples are scarce. 
We argue that 1) foundation models are a particularly promising approach to robustness. 
Existing work shows that pretraining on unlabeled data is an effective, general-purpose way to improve accuracy on OOD test distributions, in contrast to many robustness interventions which are constrained to narrow types of distribution shifts. 
However, we also discuss why 2) foundation models may not always mitigate distribution shifts, such as shifts due to spurious correlations or changes over time. 
Finally, 3) we outline several research directions to leverage and improve foundation models for robustness. 
We note that one of the ways in which foundation models lead to improved extrapolation is by providing inductive biases (via model initialization) for the adapted model, which are learned on a diverse dataset that extends beyond the downstream training data. 
However, this same inductive bias can also encode harmful associations from the pretrained data and lead to representational and allocational harms in the presence of distribution shift. 
See §4.6: data and §5.1: fairness for further discussion of such harms and methods for mitigation. 
4.8.1 Advantages.
By learning representations on a large and diverse foundation model training distribution pre , foundation models can improve accuracy of the adapted derivative on the downstream test distribution T OOD . 
OpenAI's CLIP model, which is a foundation model trained on a diverse set of images and natural language documents, has been shown to be robust to some benchmark distribution shifts on ImageNet [Radford et al. 2021] : for example, both CLIP and a standard ResNet50 obtain 76% accuracy on ImageNet, but CLIP achieves 6% higher accuracy on ImageNetV2 [Recht et al. 2019 ] and 35% higher accuracy on ImageNet Sketch [Radford et al. 2021] , which are both related but different from the original ImageNet training distribution. 
In contrast, many other robustness interventions, such as adversarial training [Madry et al. 2018] , invariant risk minimization [Arjovsky et al. 2019] , Fig. 21 . 
In-distribution (ID) and out-of-distribution (OOD) inputs for a variety of distribution shifts. 
The prediction tasks depicted here are image classification for images and fact verification for text. 
Although representations learned by foundation models improve downstream robustness for many shifts (e.g., common corruptions) [Hendrycks and Dietterich 2019; Xie et al. 2021a; Radford et al. 2021] , some shifts such as spurious correlations (where grass is predictive of cow) [Beery et al. 2020 ] and extrapolation across time (with facts that change over time) [Lazaridou et al. 2021 ] are still likely unaddressed by foundation models. 
or using larger models have had little impact on effective robustness (defined as the gap between in-distribution and out-of-distribution performance) on these ImageNet tasks, especially without explicit knowledge of the distribution shift [Taori et al. 2020; Santurkar et al. 2020; Radford et al. 2021; Miller et al. 2021] . 
Many other works demonstrate that pretraining on large datasets can improve robustness to common image corruptions, label shift, and label corruptions [Hendrycks et al. 2019a,b] ; to realworld spatial shifts in satellite imagery tasks [Xie et al. 2021a; Kumar et al. 2022] ; and to shifts across topics in natural language understanding tasks [Hendrycks et al. 2020; Fisch et al. 2019; Yogatama et al. 2019] . 
As another example, diversifying the foundation model training data to include multiple languages (as in multilingual BERT [Liu et al. 2020b] ) significantly improves performance in unseen language pairs. 
4.8.2 Persistent challenges.
Despite promising signs that foundation models will result in substantial improvements to robustness, we anticipate that foundation models are not a panacea for distribution shifts. 
We discuss this in the context of two broad categories of distribution shifts below. 
Spurious correlations. 
Spurious correlations are statistical correlations between features and labels with predictive power on the training distribution but not on the test distribution [Heinze-Deml and Meinshausen 2017; Arjovsky et al. 2019; Sagawa et al. 2020a ]. 
Well-known examples include reliance on background color for object recognition [Xiao et al. 2020] , surgical markers for medical diagnostics [Winkler et al. 2019] , annotator biases in crowdsourced data [Tsuchiya 2018; Gururangan et al. 2018; Poliak et al. 2018; Geva et al. 2019] , and demographic biases [Abid et al. 2021; Nadeem et al. 2021; Gehman et al. 2020 ]. 
Models learn these spurious correlations largely because the foundation model training and adaptation data exhibit these biases [Nagarajan et al. 2020; Gehman et al. 2020] , and this issue cannot simply be addressed with larger models [Sagawa et al. 2020b] . 
Foundation models may exacerbate or mitigate the effects of spurious correlations, but this depends on the nature of the particular downstream task and its relation to the foundation model training data and algorithm. 
By training with a diverse dataset, foundation models may improve robustness to spurious correlations that are found only in a subset of the training data: e.g., existing studies find that pretrained language models can avoid spurious correlations by quickly learning from counterexamples to the spurious correlations [Tu et al. 2020 ]. 
However, foundation models can also exacerbate the issue by introducing biases present in the foundation model training data, as observed for demographic biases in GPT-3 and other NLP models [Abid et al. 2021; Nadeem et al. 2021; Gehman et al. 2020] . 
Moreover, training at scale alone need not fully address the root issue of identifying and not relying on the features that are predictive on the downstream training set but not on the downstream test set [Heinze-Deml and Meinshausen 2017] . 
Addressing these challenges will require us to understand and manage the inductive bias from foundation model training and develop adaptation algorithms that are resistant to learning spurious correlations. 
Extrapolation and temporal drift. 
Finally, the few-and zero-shot capabilities of foundation models will mean that these models will increasingly be used far beyond the training distribution. 
While large-scale foundation model training can help with certain forms of extrapolation to new distributions [Papadimitriou and Jurafsky 2020] , there may be limits to their extrapolation capabilities. 
For example, existing language models cannot handle changes to world knowledge or language change without re-training [Lazaridou et al. 2021; Dhingra et al. 2021] , zero-shot transfer in CLIP suffers greatly in satellite image domains [Radford et al. 2021] , and ImageNet pretraining does not substantially improve the performance of large models on medical images [Raghu et al. 2019; Ke et al. 2021 ]. 
We believe that foundation models cannot be assumed to automatically extrapolate within a given modality (e.g., all images), and it will become increasingly important to define and separate the forms of extrapolation that are newly enabled by foundation models from those that remain out of reach. 
Though existing taxonomies for distribution shifts have been proposed in generality [Quiñonero-Candela et al. 2009; Ye et al. 2021] , fully understanding and defining the types of distribution shifts for which foundation models are effective is a major open problem for robustness research. 
4.8.3 Opportunities.
Foundation models hold substantial promise as a general-purpose robustness intervention for distribution shifts and open new avenues for robustness research. 
We outline some opportunities and open questions below. 
Understanding foundation model representations. 
Existing studies of the robustness of foundation models have been largely empirical, and there is little understanding of the mechanism behind gains in robustness. 
 Sun et al. [2019b] hypothesize that pretrained representations bring disparate domains (such as ID and OOD distributions) closer together, which can in turn improve generalization from labeled ID data to OOD data [Ben- David et al. 2010] . 
Controlled experimentation on measuring the distance between domain representations with and without pretraining can elucidate this effect. 
There are initial promising directions in characterizing foundation model training (e.g., contrastive learning as a spectral graph decomposition [HaoChen et al. 2021a] ) and their inductive biases [Saunshi et al. 2020a; Lee et al. 2020a; Zhang and Hashimoto 2020; Xie et al. 2020] . 
However these theories are limited and fail to address other empirically effective foundation models such as fully generative language models (e.g., GPT-3 [Brown et al. 2020 ] and image-GPT [Chen et al. 2020d] ). 
Further understanding how these inductive biases are useful under distribution shift may lead to a more complete theory ( §4.10: theory) of how foundation models improve robustness. 
Data augmentation in foundation model training. 
While foundation models trained without knowledge of the downstream tasks can avoid some task-specific biases and often improve robustness, certain statistical biases stemming from how the foundation model was trained may persist. 
As a concrete example, many contemporary self-supervision algorithms are heavily dependent on choosing an appropriate set of data augmentations [Chen et al. 2020c] , which in turn confers different types of robustness in the adaptation phase. 
For instance, Xiao et al. [2021] show that a foundation model for vision trained with contrastive learning on rotation augmentations may improve OOD performance on adaptation tasks with rotation invariance, but may not improve robustness for tasks where OOD generalization requires other invariances. 
Further research into what types of data augmentations improve robustness for a wide range of downstream tasks -including data augmentations that are learned from data [Wong and Kolter 2020; Tamkin et al. 2021c ] or designed to be generally applicable across data modalities [Verma et al. 2021 ] -will inform better foundation model training algorithms ( §4.2: training). 
Encoding structure in foundation model training. 
In general, exploring new ways of encoding known structure and invariances in the data is an important path forward for foundation model training. 
Many real-world tasks have additional metadata (e.g., spatial location coordinates, climate information from auxiliary satellites in our poverty prediction example), which may provide additional structure for OOD generalization (e.g., across geographic areas) [Xie et al. 2021a; Koh et al. 2021] . 
For example, Xie et al. [2021a] show that metadata can be used as targets for pretraining to improve downstream OOD accuracy. 
In language, modeling the tags in HTML data provides additional downstream-task-adjacent supervision, allows for new forms of prompting (e.g., filling in <title> tags for title suggestion), and improves data efficiency [Aghajanyan et al. 2021] . 
While current data augmentation methods encode hand-crafted knowledge, other avenues such as exploiting metadata could provide more automated ways of determining which structures and invariances to incorporate for foundation model training. 
Specialization vs. diversity in foundation model training data. 
The choice of foundation model training data has downstream effects -training on a more diverse dataset is not always better for downstream performance than a more specialized foundation model [Cole et al. 2021; Chalkidis et al. 2020 ] (see §4.3: adaptation for a more detailed discussion). 
In some domains such as satellite images and specialized text topics, continued pretraining on the specialized domain improves the downstream performance significantly [Reed et al. 2021; Gururangan et al. 2020 ]. 
This is a potential source of tension: on one hand, we might want to train the foundation model on a large, diverse dataset in order to have more robust performance under distribution shifts, while on the other hand, we might need to specialize the foundation model to improve its in-distribution and out-of-distribution performance on downstream tasks. 
A better understanding of how specialization affects the in-distribution and out-of-distribution performance of foundation models will allow us to design and collect more effective foundation model training sets. 
Adaptation methods. 
Although foundation models provide a strong starting point, how the adaptation method uses the pretrained information can affect robustness. 
For instance, lightweight tuning methods for language models (e.g., adapter/prefix/prompt tuning [Houlsby et al. 2019; Li and Liang 2021; Lester et al. 2021] ), which adapt the model for a new task by optimizing a small set of parameters (such as a continuous prompt) while keeping the other foundation model parameters frozen, seem to give OOD performance benefits ( §4.3: adaptation). 
 Xie et al. [2021b] explain this in a special case, where composing a learned model with a frozen foundation model can reduce the complexity of the learned model, improving generalization both ID and OOD. 
On vision datasets, Wortsman et al. [2021] ; Kumar et al. [2022] find that freezing a foundation model and training only the head can lead to better OOD performance than fine-tuning the entire model. 
 Kumar et al. [2022] explain this theoretically by showing that full fine-tuning can distort pretrained features even in a simple setting (two layer linear networks). 
However, it is still poorly understood in general why freezing parameters seems to improve OOD performance. 
Finally, while current adaptation methods may suffice for good ID generalization, the methods do not explicitly account for distribution shift in their design. 
As a first step, we can investigate how methods for distribution shifts such as domain adaptation, domain generalization, and semi-supervised learning methods interact with foundation models when used for adaptation. 
Progress in these directions can lead to adaptation methods that can better leverage foundation models for robustness. 
4.9 AI safety and alignment
Authors: Alex Tamkin, Geoff Keeling, Jack Ryan, Sydney von Arx 
The field of Artificial Intelligence (AI) Safety concerns itself with potential accidents, hazards, and risks of advanced AI models, especially larger-scale risks to communities or societies. 
Current foundation models may be far from posing such risks; however, the breadth of their capabilities and potential applications is striking, and a clear shift from previous ML paradigms. 
While AI safety has historically occupied a more marginal position within AI research, the current transition towards foundation models and their corresponding generality offers an opportunity for AI safety researchers to revisit the core questions of the field in a new light and reassess their immediate or near-future relevance. 80 
4.9.1 Traditional problems in AI safety. 
A major branch of AI safety research concerns the implications of advanced AI systems, including those that might match or exceed human performance across a broad class of cognitive tasks [Everitt et al. 2018 ]. 81 
A central goal of safety research in this context is to mitigate large-scale risks posed by the development of advanced AI. 82 These risks may be significantly more speculative than those considered in §5.2: misuse, §4.8: robustness, and §4.7: security; however, they are of far greater magnitude, and could at least in principle result from future, highly-capable systems. 
Of particular concern are global catastrophic risks: roughly, risks that are global or trans-generational in scope-causing death or otherwise significantly reducing the welfare of those affected (e.g., a nuclear war or rapid ecological collapse) [Bostrom and Cirkovic 2011] . 
What AI safety research amounts to, then, is a family of projects which aim to characterize what (if any) catastrophic risks are posed by the development of advanced AI, and develop plausible technical solutions for mitigating the probability or the severity of these risks. 
The best-case scenario from the point of view of AI safety is a solution to the control problem: how to develop an advanced AI system that enables us to reap the computational benefits of that system while at the same time leaving us with sufficient control such that the deployment of the system does not result in a global catastrophe [Bostrom and Cirkovic 2011] . 
However technical solutions are not sufficient to ensure safety: ensuring that safe algorithms are actually those implemented into real-world systems and that unsafe systems are not deployed may require additional sociotechnical measures and institutions. 
Reinforcement Learning (RL), which studies decision-making agents optimized towards rewards, has been a dominant focus in AI safety for the past decade. 
What is at issue here is the difficulty of specifying and instantiating a reward function for the AI that aligns with human values, in the minimal sense of not posing a global catastrophic threat. 83 
While this problem, known as value alignment [Gabriel 2020; Yudkowsky 2016] , may seem trivial at first glance, human values are diverse, 84 amorphous, and challenging to capture quantitatively. 
Due to this, a salient concern is reward hacking, where the AI finds an unforeseen policy that maximizes a proxy reward for human wellbeing, but whose misspecification results in a significant harm. 85 
Many efforts to combat the 80 See Amodei et al. [2016] and Hendrycks et al. [2021d] for broader perspectives on open problems in AI Safety. 81 
This is referred to by some as AGI or artificial general intelligence, although terminology use varies [e.g., see Karnofsky 2016] . 
82 Note that this does not require a belief that building certain kinds of advanced AI is a desirable goal, nor even certainty that it is an achievable one. 83 
See Hubinger et al. [2019] for a discussion of some challenges that arise at the threshold between reward specification and reward instantiation. 84 
See Gabriel [2020] for an extended discussion of human diversity, ethics, and the value alignment problem 85 See this spreadsheet for a list of real-world examples of reward hacking, including an aircraft landing algorithm which achieved a perfect score by outputting large forces that exploited a flaw in the simulator. 
value alignment problem have focused on maximizing corrigibility, which is when errors in the design of a system can be corrected once the system is running [Soares et al. 2015 ]. 
This can be far from straightforward-in the RL context, an agent with a specified goal would be incentivized to prohibit attempts to alter that goal, as any attempt to alter that goal would likely be suboptimal for the goal's realization [Omohundro 2008 ]. 
However, pure RL is not the only theorized route to advanced AI. Foundation models can also be trained with simple (self-)supervised objectives like next-token prediction, yet can still be used in interactive and goal-directed ways, with or without additional RL training. 
Moreover, it appears that many of these methods may result in increased capabilities through straightforward scaling of compute, number of parameters, and dataset size [Hestness et al. 2017; Kaplan et al. 2020] . 
What concepts like value alignment and corrigibility amount to in the broader context of foundation models differ in several respects to the pure RL case, and must accordingly be carefully theorized. 
4.9.2 Current foundation models and AI safety.
Many of these risks in the RL setting result from models optimized to carry out goals. 
However, a key challenge for AI safety research on recent foundation models is that goal-directed behavior may emerge despite not being explicitly optimized for (see also §4.2: training). 
As an example, large language models may be trained on corpora where agents use language in goal-directed ways, such as in persuasive text. 
To predict the next token well, a model may acquire a general capability to reason and produce arguments, which could emerge with suitable contexts. 
Foundation models trained on other kinds of human data may capture other kinds of goal-directed behavior present in the data; e.g., robotic agents trained to mimic humans in videos may attempt to punch or knock-out their human operators if their training data includes videos of boxing matches. 
Recent work has also attempted to directly train agents to produce goal-directed behavior; for example, the Decision Transformer trains a sequence model on trajectories prepended with their returns [Srivastava et al. 2019; Schmidhuber 2019; Chen et al. 2021b ]. 
One can then generate high-return trajectories by "prompting" this model with a high return, which raises similar questions of reward hacking from the RL context. 
However, a major aim of safety research on goal-directed models is to gain more principled control and explainability over the actions being pursued by the agent, as opposed to relying on inscrutable decisions from a blackbox neural network. 86 
This makes current foundation models an exciting avenue of study for AI safety research, as aligning them may be a useful precursor for aligning more advanced models [Christiano 2016; Cotra 2021; Kenton et al. 2021 ]. 
One challenge is the misalignment between the foundation model's training objective and the desired behavior; for example, a language model may be trained to predict the next word of all documents in the training corpus regardless of veracity, but users may want the model to only output true or helpful text [Tamkin et al. 2021a ]. 
One potential way to steer goal-directed agents towards desired behavior may be to train them with natural language descriptions of actions -this may enable steering them with language as well as enabling them to output interpretable language describing the task they "believe" they are performing, similar to methods for controllable generation and source attribution [e.g., Keskar et al. 2019 , see also §2.3: robotics, §2.5: interaction, and §4.11: interpretability]. 
However, further advances would be necessary to ensure the reliability and self-consistency of such models in the wild ( §4.8: robustness), as well as gaining a more mechanistic understanding of how these models operate [Cammarata et al. 2020, also see §4.11: interpretability] . 
And even if natural language-based control of future foundation models enables better task specification 86 For more on the relationship between understanding and semantics see §2.6: philosophy and monitoring, models may acquire deceptive or otherwise undesirable behavior from human data -identifying and neutralizing this behavior is another important direction for future study. 
While the self-supervised objectives described in the previous paragraph train models to capture human behavior in the data, new training paradigms may produce goal-directed foundation models capable of carrying out a wide range of tasks in complex environments, and which exhibit capabilities superior to humans in different domains (see §4.2: training). 
For example, goal-directed foundation models may be trained in an open-ended self-play setting, similar to AlphaGo, or in vast multitask single-agent RL setups. 
This might lead to emergent capabilities that complicate efforts to get agents to carry out goals, especially if many agents are trained together in a rich world-simulator that encourages the development of skills like deception, misdirection, dissimulation, persuasion, and strategic planning. 
Aside from countering deceptive behavior, it also remains unclear how to effectively evaluate and control the behavior of very capable models, known as scalable oversight or alignment [Amodei et al. 2016; Leike et al. 2018 ]; e.g., scoring novel reactions proposed by a chemical foundation model (see §4.4: evaluation). 
New human-in-the-loop approaches for training, steering, monitoring, and understanding these models are thus exciting future directions. 
Finally, even before any of these more advanced capabilities emerge, an important research area for AI safety in the near term is characterizing and forecasting the capabilities of current self-supervised foundation models. 
There are three aspects which make this challenging. 
First, the generality of foundation models means that they can be applied to countless different kinds of applications in unexpected ways. 
Enumerating current and planned applications of foundation models is not sufficient to capture the full range of ways they could be used. 
Second, even within a particular application, model capabilities are emergent: they grow and change in unexpected ways as models scale. 
For example, the ability to control GPT-3 via "prompting" was an emergent phenomenon of which only the barest glimpses were evident in the smaller GPT-2 model [Radford et al. 2019; Brown et al. 2020] . 
What the emergent properties of future foundation models will look like is unknown. 
Third, even within a particular application and scale, a model's capabilities are not easy to characterize. 
For example, the ability of GPT-3 to perform addition improves dramatically once commas are added to the inputs [Branwen 2020; Brockman 2020] . 
Similarly, small rewordings of prompts can have large impacts on task performance. 
Since the space of prompts is intractable to enumerate, it is challenging to definitely assert that any task is outside the reach of current promptbased foundation models -this is a major challenge for reasoning about possible catastrophic risks from foundation models. 
4.9.3 Potential catastrophic risks from future foundation models. 
The broad and quickly-growing capabilities of current models suggest the benefit of attempting to characterize possible catastrophic risks from more advanced systems. 
We see at least two ways in which advanced foundation models might contribute to such outcomes. 
Catastrophic robustness failures. 
§4.8: robustness discusses how models may behave in unexpected or harmful ways when confronted with new kinds of data [Amodei et al. 2016; Yudkowsky et al. 2008 ]. 
These failures may be especially consequential if foundation models are integrated into important systems that leverage foundation models' ability to quickly adapt to many different tasks and situations. 
Failures could be catastrophic if they occur in warfare systems (resulting in unwanted discharge of weapons, possibly igniting a conflict), critical infrastructure (accidental destruction of critical energy or agricultural capabilities), or if they become essential to a large fraction of economic activity (whose unexpected failure could result in a sudden collapse in living standards and political instability; see also §5.5: economics). 
Indeed, the threat of catastrophic robustness failures is particularly pertinent for foundation models in contrast to other kinds of AI. 
This is because a foundation model consists of a single model that may be adapted for many different use cases, such that robustness failures derived from the statistical associations learned by the model could in principle manifest in a correlated way across several different domains. 
If the same foundation model is integrated into multiple critical functions, then lack of robustness in the model could lead to correlated failures that span multiple critical functions or failsafes. 
Misspecified goals. 
The use of foundation models might increase the risks of optimizing misaligned yet easy-to-specify goals, often referred to as Goodhart's Law [Kenton et al. 2021; Goodhart 1984] . 
A current-day example of these risks is the negative effects of some recommender systems (e.g., polarization, media addiction) which may optimize simple engagement metrics rather than a harder-to-measure combination of societal and consumer well-being [Burr et al. 2018; Milano et al. 2020] . 
Future institutions may leverage uninterpretable foundation models to maximize simple measures such as profit or GDP, due to these models' ability to adapt to the many different subproblems each of these metrics is dependent on. 
However, at larger scales optimizing for these proxy metrics instead of a more holistic goal designed for human welfare could inadvertently lead to environmental or geopolitical harms [Gabriel 2020; Creel and Hellman 2021 ]. 
4.9.4 Conclusion.
In sum, we argue that current and potential future emergent properties of foundation models make them ripe objects of study for the field of AI safety. 
We encourage future work on characterizing and forecasting the exact capabilities and risks of foundation models; developing new methods to align foundation models to human values and desired goals; and for states, research labs, and businesses to coordinate on proactive measures to mitigate salient risks. 
4.10 Theory
Authors: Aditi Raghunathan, Sang Michael Xie, Ananya Kumar, Niladri Chatterji, Rohan Taori, Tatsunori Hashimoto, Tengyu Ma Rigorous mathematical theory plays a foundational role in many engineering and science disciplines (e.g., information theory in electrical engineering). 
We believe that theory of foundation models can be particularly beneficial in guiding technical decisions and innovations because of the huge computational costs associated with experimenting on foundation models. 
In addition, theoretical insights help elucidate fundamental limitations and explain surprising empirical phenomena. 
However, the community currently has a limited theoretical understanding of foundation models, despite much recent progress [Arora et al. 2019b; HaoChen et al. 2021a; Wei et al. 2021 Wei et al. , 2020b;; Zhang and Hashimoto 2021; Saunshi et al. 2020b; Dao et al. 2019; Tosh et al. 2020 Tosh et al. , 2021;; Cai et al. 2021; Lee et al. 2020a; Zimmermann et al. 2021; Bansal et al. 2020; Wang and Isola 2020; Tsai et al. 2020; Tian et al. 2020a,b; Tripuraneni et al. 2020; Du et al. 2020] . 
Deep neural networks form the backbone of foundation models. 
Even in the well-studied supervised learning setting, where the train and test scenarios have the same distribution, there are numerous open questions around deep nets such as understanding non-convex optimization, the implicit regularization effect of optimizers, and expressivity. 
Foundation models raise questions that significantly go beyond the supervised deep learning setting. 
The core problem in theoretically analyzing foundation models is understanding why training on one distribution with a possibly unsupervised/self-supervised loss leads to good adaptation performance on different downstream distributions and tasks. 87 
e will discuss an intuitive modularization to analyze foundation models that lays bare the connections between supervised learning and foundation models, concrete and core technical questions, and some promising theoretical tools to address these questions. 
These new core questions can provide useful insight into foundation models and can be studied in parallel to supervised deep learning theory. 
While we focus on analyzing the downstream performance, the proposed modularization and tools could prove useful to analyze other metrics of interest such as robustness to distribution shifts ( §4.8: robustness) and security ( §4.7: security). 
Recall that foundation models are trained on a large amount of raw data ( §4.2: training) then adapted to specific tasks ( §4.3: adaptation) and therefore can be decomposed naturally into training and adaptation phases. 
We identify interfaces between them and disentangle parts specific to foundation models from parts that require standard deep learning theory, so that they can be independently worked on. 
We introduce a modularized analysis framework, which has also been implicitly or explicitly employed in recent works, e.g., Arora et al. [2019b] ; HaoChen et al. [2021a] ; Wei et al. [2020b] ; Tripuraneni et al. [2020] . 
The crucial component in this modularized analysis turns out to be the pretrain-adaptation interface. 
We first describe the modularization, and discuss why we find this modularization promising and finally some limitations. 
We will refer to the training phase explicitly as "pretraining" to distinguish it from the adaptation phase that could also involve training on a few samples from a particular task. 
Pretraining phase. 
The pretraining of foundation models often involves a data distribution pre (e.g., the distribution of natural text) and a pretraining loss function ℓ pre (; ) that measures the loss (e.g., language modeling loss in GPT-3) on an input for a model with parameters ∈ Θ. 
Let ppre denote the empirical distribution over a large number of independent samples from pre . 
Pretraining minimizes the loss ℓ pre on ppre , which we call the empirical pretraining loss, and produces a model θFM : 
We consider the corresponding loss on the population distribution pre , called the population pretraining loss, as a central concept: 
(2) 
Optimization-based adaptation phase. 
We frame adaptation as a general constrained optimization problem that depends on θFM , abstracting away those adaptation methods that are based on optimizing certain loss functions such as fine-tuning and prompt-tuning (see, e.g., [Houlsby et al. 2019; Li and Liang 2021; Lester et al. 2021 ], and §4.3: adaptation). 
Since different adaptation methods could modify different subsets of the model parameters, we denote the space of adapted model parameters by some Γ. 
Given a downstream task distribution task (e.g., question answering in a particular domain) and a few empirical samples ptask sampled from task , we model the adaptation phase as minimizing some adaptation loss ℓ adapt on ptask w.r.t adapted parameters ∈ Γ: 
where adapt (, θFM 
is the empirical adaptation loss, and (, θFM ) ≤ 0 is an optional constraint that controls the complexity of the adapted parameters, encompassing both explicit regularization (e.g., model dimensionality and norm) and the implicit regularization of the adaptation process. 
We list some common adaptation methods and discuss the corresponding adapted parameter and constraints (, θFM ) ≤ 0 . 
(1) Linear probing: training a linear classifier on top of the representations from a foundation model. 
Here Γ = R is the set of linear classifiers on the representations of dimensionality , and (, θFM ) could be the ℓ 2 or ℓ 1 norm of . 
(2) Fine-tuning: optimizing a randomly initialized linear head for a few steps, and all other parameters from the initialization of θFM . 
Here is the concatenation of and the linear head. 
Such a process could correspond to some implicit regularization of towards the initialization θFM captured by (, θFM ) ≤ 0 . 
The exact term (, θFM ) would depend on the optimization algorithm used, and such a characterization of the implicit regularization of optimization is an area of active research study [e.g., Gunasekar et al. 2017; Soudry et al. 2018; Gunasekar et al. 2018; Arora et al. 2019a; Blanc et al. 2019; Woodworth et al. 2020; Wei et al. 2020a; HaoChen et al. 2021b; Damian et al. 2021; Kumar et al. 2022, and references therein] . 88 
 (3) Prompt-tuning: optimizing a small set of continuous task-specific vectors that prepend the task inputs. 
Here is the continuous prompt vectors which often has small dimensionality, and we may optionally have a constraint on the norms of . 
One obvious limitation to note is that this formulation excludes adaptation methods such as in-context learning [Brown et al. 2020] where there is no "training" (i.e., the minimization of some empirical adaptation loss) during the adaptation phase. 
We discuss this and other limitations in §4.10.3: 
theory-incontext. 
Two central quantities for the adaptation phase are the population adaptation loss 
and the minimal adaptation loss 
Separate analysis for modularized phases. 
Existing generalization theory for standard supervised learning aims to show that pre ≈ pre and adapt ≈ adapt . 
Addressing these questions specifically for deep nets is an active research area. 
We can also leverage the standard learning theory decomposition to bound the final downstream task loss by the excess generalization error and the minimal adaptation loss as follows. 
 adapt ( task , θFM ) ≤ ★ adapt ( θFM ) minimal adaptation loss 
where the generalization error captures the closeness between adapt and adapt . 89 
The decomposition and relationship between these key quantities are shown in Figure 22 . 
The generalization and optimization arrows, as argued above, largely reduce to deep learning theory in the supervised setting. 
What we are left with is the main challenge with foundation models, which is to understand why the minimal adaptation loss * adapt ( θFM ) can be small as a result of a small pretraining population loss, which study in §4.10.2: 
theory-interface. 
The work of Arora et al. [2019b] pioneered the pursuit of this question by bounding from above ★ adapt ( θFM ) by pre ( θFM ) in the context of contrastive learning, and HaoChen et al. [2021a]; Tosh et al. [2020 Tosh et al. [ , 2021] ] relax the data assumptions. 
Other pretraining methods successfully analyzed 88 It may not always be feasible to characterize the inductive bias of adaptation via an explicit constraint (, θFM ) ≤ 0 . 
The modularization we propose is also applicable in these cases, but for notational simplicity, we focus on the case where implicit regularization can be approximated via an explicit constraint. 89 
More precisely, the generalization error term is the sum of adapt ( task , θFM ) - adapt ( task , θFM ) and adapt ( ★ task , θFM ) - adapt ( ★ , θFM ) = adapt ( ★ task , θFM ) - ★ adapt ( θFM ), where ★ task is the minimizer of ( 5 ). ( 6 ) follows easily be using adapt ( task , θFM ) ≤ adapt ( ★ task , θFM ). 
under this framework (implicitly or explicitly) include pretraining with language models [Wei et al. 2021] or self-supervision [Lee et al. 2020a ], with self-training algorithms [Wei et al. 2020b; Cai et al. 2021] , and with multiple supervised tasks [Tripuraneni et al. 2020; Du et al. 2020 ]. 
4.10.2 
Why is the pretraining-adaptation interface interesting?. 
As shown in Figure 22 , the main missing link beyond standard supervised theory is: 
Under what conditions does a small population pretraining loss pre ( θFM ) imply a small minimal adaptation loss ★ adapt ( θFM ) and why? 
The conditions that lead to a successful interface could depend on several quantities such as the pretraining and adaptation distributions, objectives and training methods, as well as the model architecture. 
This question is beyond the scope of standard generalization theory, but it does narrow us down to a few important factors specific to foundation models, and captures the essence of various important open questions on foundation models as we argue below. 
First, we note that this interface deals with population quantities that concern two different distributions. 
Hence, the conditions for a successful interface are likely to involve special properties of the distributions, for example, the diversity of the pretraining distribution and structural shifts between the pretraining and adaptation data. 
This makes the analysis of the interface challenging (as discussed below in §4.10.4: 
theory-tools) as we need to make careful modeling assumptions about how the two distributions relate to one another. 
However, this presents the possibility that tools and techniques developed to analyze such interfaces could be useful to understand the effect of distribution shifts and to predict when foundation models can improve robustness. 
Second, the population losses and possibly the conditions of a successful interface depend on the model architecture. 
This raises the challenge of opening up the black-box of the neural nets. 
What does a small pretraining loss on a particular distribution tell us about the properties of the intermediate layers? 
Such analyses would also guide us in designing new adaptation methods that more carefully exploit different intermediate representations. 
Third, few-shot learning or the sample efficiency of adaptation can be captured through the constraint on the complexity measure (, θFM ) < 0 in the minimal adaptation loss. 
We need to formally characterize these complexity measures (e.g., by understanding the implicit regularization effect of the adaptation process) and further understand why a small population pretraining loss would imply a low-complexity adaptation parameters task . 
A satisfactory answer to this question would likely allow us to improve the sample-efficiency of downstream adaptation. 
Finally, and importantly, critical components of the interface are the choice of the pretraining and adaptation losses. 
We want to understand how to best combine the pretraining and adaptation objectives for successful adaptation. 
It is possible that the pretraining objective that best guarantees successful adaptation differs from what is explicitly minimized during the pretraining process -the interface above allows one to use any surrogate population objective on the pretraining distribution. 
In addition, new surrogate objectives that provably lead to good adaptation across a broad set of tasks could shed light on the fundamental aspects that make foundation models successful. 
To summarize, the interface precludes the issue of generalization and allows us to formally reason about the interaction between several important quantities of the pretraining and adaptation phases that can guide practice in important ways. 
 4.10.3 
Challenge: analysis of in-context learning and other emergent behavior. 
GPT-3 [Brown et al. 2020] demonstrates the power of in-context learning, an adaptation method that does not need any parameter optimization. 
In the adaptation phase, the pretrained language foundation model takes in a prompt -a sequence of tokens that concatenates input-output examples from the task -followed by a test example and simply generates the label of the test example by conditioning on the sequence seen thus far (prompt plus test example). 
In other words, there is no explicit training or change to the model parameters. 
What is the mechanism by which the model "learns" from the different examples by simply executing with the examples as inputs? 
The previous modularization does not directly apply because we do not obtain new model parameters during adaptation, but rather we only use the generative capabilities of the foundation model by executing on structurally-designed inputs. 
However, the idea of separating pretraining with infinite data and pretraining with finite data can still be useful. 
For example, a recent work starts with the assumption of infinite pretraining data and sufficient model expressivity to study in-context learning [Xie et al. 2021c ]. 
These assumptions reduce the characterization of in-context learning to a matter of analyzing the pretraining distribution conditioned on in-context learning prompts, which are drawn from a different distribution than the pretraining data. 
In particular, Xie et al. [2021c] proposes that in-context learning emerges from long-term coherence structure in the pretraining distribution, which is described by a latent variable generative model with coherence structure. 
More broadly, while the modularization proposed in this section provides a nice framework to gain useful theoretical insights into foundation models, it is possible that some emergent behavior like in-context learning and other capabilities yet to be discovered would require going beyond the modularization, e.g., by opening the black box of the architecture. 
Understanding the interface between pretraining and adaptation phases requires a more careful study of data distributions than in traditional supervised learning. 
This is because the pretraining and task adaptation distributions are inherently different. 
By definition, foundation models are trained on raw data that is typically extremely diverse and task-agnostic, while the adaptation data depends heavily on the task. 
Similarly, in-context learning emerges as a result of learning to generate data that looks like the pretraining distribution, and thereby understanding in-context learning requires careful modeling of the pretraining data. 
Hence answering the central questions around foundation models requires realistic and interpretable assumptions that are also amenable to analysis. 
Recent works either assume certain properties of the population data, e.g., the expansion property in HaoChen et al. [2021a] ; Wei et al. [2020b] , or that the population data is generated from latent variable models with some structure [Saunshi et al. 2020a; Wei et al. 2021; Arora et al. 2016; Lee et al. 2020a; Zhang and Hashimoto 2020; Tosh et al. 2021] . 
We generally lack mathematical tools for relating properties of foundation models to the structure in the population data distribution. 
 HaoChen et al. [2021a] applies spectral graph theory to leverage the inner-class connectivity in the population distribution. 
More precise characterization of θFM via probabilistic and analytical derivations is possible for latent variable models, but so far restricted to relatively simple ones. 
The community will significantly benefit from more systematic and general mathematical tools to address this question. 
It is also highly desirable to define simple toy cases so that theoreticians can precisely compare the strengths of various tools and analyses. 
For example, HaoChen et al. [2021a] and Wei et al. [2020b] consider the mixture of manifolds problem which might potentially be a good simplified test bed for vision applications. 
We need more interesting test beds for discrete domains such as NLP. 
We believe that tractable theoretical models which capture relevant properties of real datasets are a crucial step towards placing foundation models on solid theoretical footing. 
4.11 Interpretability
Authors: John Hewitt*, Armin W. Thomas*, Pratyusha Kalluri, Rodrigo Castellon, Christopher D. Manning Compared to most other machine learning models, foundation models are characterized by a vast increase in training data and complexity and the emergence of unforeseen capabilities: foundation models are able to do unforeseen tasks and do these tasks in unforeseen ways. 
The increasing adoption of foundation models thereby creates growing desires, demands, and unprecedented challenges for understanding their behavior. 
In contrast to task-specific models, foundation models are trained across vast and usually highly disparate datasets, potentially spanning many domains and modalities (see §4.2: training) . 
Through this training, foundation models learn an exceptionally wide range of behaviors, which can vary profoundly between tasks and domains, as demonstrated by their ability to be adapted to different types of downstream tasks and to exhibit behaviors that are specific for each of these tasks (see §4.3: adaptation). 
Take GPT-3 as an example, which was trained as one huge model to simply predict the next word in a text. 
While this is a very specific and simple-to-define learning task, it has enabled GPT-3 to gain capabilities that far exceed those that one would associate with next word prediction, by combining it with a vast training dataset that comprises all kinds of internet text. 
As a result, GPT-3 can now adapt behaviors that are clearly outside of the scope of its original training task, such as simple arithmetic and computer programming, when provided with a few training samples. 
This demonstrates that it is challenging to answer even the seemingly simplest question about a foundation model: what capabilities does it have? 
Moreover, it is an open question to what extent these diverse capabilities rely on distinct or shared model mechanisms, akin to algorithmic building blocks within the model. 
On the one hand, foundation models can be interpreted as single models, which utilize some set of generalizable model mechanisms to perform well across tasks and domains. 
In this case, a full understanding of their behavior can be gained by identifying and characterising these mechanisms. 
On the other hand, the ability of foundation models to adapt profoundly distinct behaviors for different tasks suggests that they can also be understood as a large collection of independent expert models, each tailored to a specific task. 
For example, it seems unlikely that the model parameters that GPT-3 uses to do arithmetic could have much to do with the parameters used to translate from English to French. 
In this case, explanations of model behavior in one task are therefore not necessarily informative about behavior in other tasks. 
We refer to this as the one model-many model nature of foundation models (see Figure 23 ) and argue that understanding where foundation models lie on this spectrum between one and many models will be central to understanding their behavior. 
Toward systematizing this area of study, we present and discuss three levels of understanding foundation models [inspired by Marr 1982 ]: we first discuss the challenges and opportunities in understanding what a model is capable of doing, then why it outputs certain behaviors, and lastly how it does it. 
Specifically, questions of what aim to characterize the kinds of behaviors that a model can perform without peeking inside the model, while questions of why aim to provide explanations of the model's behaviors in terms of potential causes in the data, and questions of how aim to understand the internal model representations and mechanisms that produce these behaviors. 
After presenting all three levels, we conclude by discussing potential consequences resulting from the non-interpretability and interpretability of foundation models. 
The simplest understanding of a technology is widely taken to be knowing what the technology does. 
This seemingly straightforward question is significantly challenging for foundation models, due to the myriad unforeseen behaviors and tasks that these models are capable of performing. 
Task-specific neural network models are trained to perform a single task in a single domain, e.g., image classification. 
Their task and the input and output domains are therefore clear; yet even for these models it can be challenging to know exactly what the model will do, given a particular input. 
For instance, model behaviors can unexpectedly differ greatly for two perceptually similar inputs [Garg and Ramakrishnan 2020; Jin et al. 2020] or two subpopulations of the same data (stratified, for example, by race or gender [Hovy and Søgaard 2015; Blodgett et al. 2016; Tatman 2017; Buolamwini and Gebru 2018] ). 
This challenge of characterizing a model's behavior is amplified manyfold for foundation models. 
The space of tasks that the model is able to perform is generally large and unknown, the input and output domains are often high-dimensional and vast (e.g., language or vision), and the models are less restricted to domain-specific behaviors or failure modes. 
Consider, for example, the surprising ability of GPT-3 to be trained on large language corpora and to subsequently develop the ability to generate mostly-functional snippets of computer programs. 
A key challenge for characterizing the behavior of foundation models is therefore to identify the capabilities that it has. 
Even further, for each task that a foundation model can perform, and there may be many or infinitely many, all the challenges remain that one faces when trying to understand the behavior of much simpler, task-specific models. 
Characterizing each 'task' that a foundation model can perform is further complicated by their one model-many models nature (see Figure 23 ). 
Again taking GPT-3 as an example, it was shown that it can be tailored to many tasks through simple prompting (see §4.3: adaptation) . 
Yet, each task can be specified through many possible prompts and slight variations in prompts can result in meaningful changes of model behavior. 
For instance, the task of sentiment classification of a movie review can be specified by presenting the movie review followed by 'Her sentiment towards the film was...' or 'My overall feeling was that the movie was...'; despite these prompts appearing to pose closely related tasks, GPT-3 will exhibit different response accuracies for each prompt [Zhao et al. 2021] . 
Observations like these raise important questions regarding the relationship between the characteristics of prompts and the resulting model behaviors. 
Specifically, can meaningfully different responses to seemingly similar prompts actually be considered as resulting from the same model or do they result from highly distinct model mechanisms, and does characterizing the behaviors of the foundation model (or its adapted derivatives) in one task truly aid in characterizing the behaviors of other possible adaptations of the model? 
To identify the capabilities that a foundation model has and those it is missing, researchers can utilize controlled evaluations. 
Here, domain experts design prompts that are known to require a particular competence and then study the ability of a model to respond correctly to these prompts [Papadimitriou and Jurafsky 2020; Lu et al. 2021a; Kataoka et al. 2020; Wu et al. 2021c; Xie et al. 2021a; Koh et al. 2021] . 
For example, psycholinguists have designed prompts that require a language model to choose between a grammatically correct sentence and the same sentence with a specific grammatical inaccuracy; knowing whether the model consistently prefers the grammatically correct sentence over its grammatically incorrect counterpart tells us whether the model has the particular grammatical competence required to identify this inaccuracy [Linzen et al. 2016] . 
Given the huge range of possible capabilities of foundation models, and our current lack of any general method for determining a priori whether a foundation model will have a given capability, bespoke evaluations like these are crucial. 
They allow exploring the range of behaviors that foundation models are capable of, while requiring minimal model access: we only need to present inputs and receive model outputs, and we need not depend on access to the implementation or parameters of a model. 
Given the infinitely many desirable and undesirable tasks, subtasks, and behaviors that foundation models may be capable of (or incapable of), characterizing model behaviors and capabilities will be increasingly challenging and important. 
We believe that instead of relying on a few experts to formulate and test for possible behaviors, it will be critical to extend these types of analyses to test for many more behaviors, in part by opening up this line of exploration to diverse communities and experts in many disciplines, as well as by increasing access to and scale of these evaluations. 
4.11.2 Explaining behavior.
In addition to characterizing what a foundation model is doing, one can try to characterize why it performs certain behaviors by providing explanations of these behaviors in terms of potential causes in the data. 
While current explanation approaches, which provide such explanations of behavior, can reveal qualities of inputs that affect a model's responses, they often require full access to the model to do so and are generally limited in their ability to elucidate any general model mechanisms, which foundation models use to respond to many inputs, tasks, and domains. 
Current explanatory approaches can generally be understood as distinct models, which are designed to provide an explanation of particular behaviors of another black box model. 
Importantly, these approaches are separate from the model whose behavior is analyzed, which by itself is not interpretable. 
This separation can be problematic, as the provided explanations can lack faithfulness [Jacovi and Goldberg 2020] , by being unreliable and misleading about the causes of a behavior [cf. 
 Rudin 2019] . 
Even further, unsound explanations can entice humans into trusting unsound models more than they otherwise would (for a detailed discussion of trust in artificial intelligence, see Jacovi et al. [2021] ). 
These types of concerns grow as we transition from task-specific models towards the wide adoption of foundation models, as their behavior is vastly more complex. 
Current explanatory approaches can largely be divided into either providing local or global explanations of model behavior [Doshi-Velez and Kim 2017] . 
Local explanations seek to explain a model's response to a specific input, e.g., by attributing a relevance to each input feature for the behavior or by identifying the training samples most relevant for the behavior [Simonyan et al. 2013; Bach et al. 2015; Sundararajan et al. 2017; Shrikumar et al. 2017; Springenberg et al. 2014; Zeiler and Fergus 2014; Lundberg and Lee 2017; Zintgraf et al. 2017; Fong and Vedaldi 2017; Koh and Liang 2017] . 
Global explanations, in contrast, are not tied to a specific input and instead aim to uncover qualities of the data at large that affect model behaviors, e.g., by synthesizing the input that the model associates most strongly with a behavior [Simonyan et al. 2013; Nguyen et al. 2016] . 
Local and global explanations have provided useful insights into the behavior of task-specific models [e.g., Li et al. 2015; Wang et al. 2015b; Lapuschkin et al. 2019; Thomas et al. 2019; Poplin et al. 2018] . 
Here, the resulting explanations are often taken to be a heuristic of the model mechanisms that gave rise to a behavior; for example, seeing that an explanation attributes high importance to horizontal lines when the model reads a handwritten digit '7' easily creates the impression that horizontal lines are a generally important feature that the model uses to identify all sevens or perhaps to distinguish all digits. 
Given the one model-many models nature of foundation models, however, we should be careful not to jump from specific explanations of a behavior to general assumptions about the model's behavior. 
While current explanatory approaches may shed light on specific behaviors, for example, by identifying aspects of the data that strongly effected these behaviors, the resulting explanations do not necessarily provide insights into the model's behaviors for other (even seemingly similar) inputs, let alone other tasks and domains. 
Another approach could be to sidestep these types of post-hoc explanations altogether by leveraging the generative abilities of foundation models in the form of self-explanations [cf. Elton 2020; Chen et al. 2018] , that is, by training these models to generate not only the response to an input, but to jointly generate a human-understandable explanation of that response. 
While it is unclear whether this approach will be fruitful in the future, there are reasons to be skeptical: language models, and now foundation models, are exceptional at producing fluent, seemingly plausible content without any grounding in truth. 
Simple self-generated "explanations" could follow suit. 
It is thus important to be discerning of the difference between the ability of a model to create plausible-sounding explanations and providing true insights into its behavior. 
4.11.3 Characterizing model mechanisms.
Deep understanding of systems is generally taken to mean understanding how a system performs: which knowledge and mechanisms does it contain, and how are these assembled to form the whole? 
If this is indeed possible, characterizing the representations within foundation models and the mechanisms that operate on them will be central to satisfying the desire to thoroughly understand these proliferating models; and whether these mechanisms are many and specific or few and generalizable, they are at the core of the ability of foundation models to adopt a wide range of behaviors in varied tasks and domains. 
To make the notions of model representations and mechanisms concrete, consider a simple behavior exhibited by GPT-3: It was quickly observed what GPT-3 did when provided with examples of the addition of small numbers and then queried to perform addition of two new numbers: with high probability, it predicted the correct result of the addition [Branwen 2020; Brockman 2020] . 
When asking why GPT-3 performed as it did, one could find evidence in the input, like aspects of its prompt that highly affected its response (these might be the two numbers to be added, though not necessarily), or aspects of GPT-3's training data that affected its response (these might be examples of addition, though not necessarily). 
Delving into the model, we may envision a deeper understanding of the mechanisms that GPT-3 uses to add a specific pair of numbers and the mechanism that it uses to add other arbitrary pairs of numbers. 
We may also envision a deeper understanding of whether these mechanisms are similar to the mathematical notion of 'addition' or merely correlated with this notion. 
By understanding individual model mechanisms, we can build up a compositional understanding of complex behaviors of a foundation model. 
A task slightly more complex than the addition of numbers is solving mathematical word problems, in which numbers come with units and the problem is presented in natural language. 
Once we understand the mechanism (or mechanisms) by which a model performs addition, we can investigate whether this mechanism is used as an intermediate step in solving word problems. 
If the addition mechanism is used, we have built up our understanding of how the model solves word problems, we have increased confidence that the foundation model generalizes the notions of quantities and addition (not another correlation or heuristic), and, furthermore, we have increased confidence in our ability to predict the model's why (which parts of the inputs it is attending to) and the output's what (addition of two numbers). 
If the addition mechanism is not used, we may retain a healthy skepticism that this is truly addition, and we can investigate which representations and mechanisms are used instead. 
It is important to be aware that there are many potential cases of more complex and concerning model mechanisms, for instance, the estimation of race from the characters in a name, or the pixels in an image. 
Establishing evidence of such a mechanism in a foundation model and its use can support a moral or legal responsibility to ban the model from tasks like predictive policing, marketing, loan applications, and surveillance at large. 
A plethora of methods have emerged to investigate these internal aspects of neural network models. 
Typically, these approaches separate the model into nodes (e.g., neurons, layers, or parts of layers), then interrogate either the representations captured in nodes or the mechanisms by which nodes are assembled. 
Some approaches are hypothesis driven: by hypothesizing that nodes may capture certain information (e.g., a grammatical feature of a word, or the race of a person), one can probe all nodes to quantify how much of that information they make available [Alain and Bengio 2016; Veldhoen et al. 2016; Belinkov et al. 2017; Adi et al. 2017; Conneau et al. 2018; Hewitt and Liang 2019; Hewitt and Manning 2019; Voita and Titov 2020; Pimentel et al. 2020 ]. 
Other approaches build on explanatory methods, and, instead of identifying which data cause a certain behavior, they seek to identify which data cause a certain node to activate, or which nodes cause another node later in the model to activate, thereby uncovering collections of model representations and mechanisms [Olah et al. 2020; Mu and Andreas 2020; Carter et al. 2019; Goh et al. 2021] . 
Taken together, these approaches inspect the interior of models and provide a basis for the ongoing explorations of the behavior of foundation models. 
Yet, the number of potential representations and mechanisms within foundation models is vast, particularly given their one model-many models nature, and these types of approaches often only capture a small slice of a model's interiority. 
It is thus an open challenge to expand the discovery of representations and mechanisms and to elucidate those that are most relevant or general for model behavior. 
As with many approaches to interpreting foundation models, these types of explorations will benefit from including and supporting more diverse and interdisciplinary investigators and from more accessible, flexible, and scalable methods of discovery. 
In summary, we believe that the one model-many models nature of foundation models (recall Figure 23 ) provides novel opportunities and challenges for current interpretability research: there are many adaptations of a single foundation model, and we simply do not know the extent to which they share common mechanisms. 
To the extent that mechanisms are shared, understanding foundation models may be a tractable problem of characterizing these mechanisms and their relations. 
To the extent that mechanisms are independent, each adaptation of a foundation model must be analyzed independently, leading to profound uncertainty about the nature of any new adaptation of the foundation model. 
4.11.4 Impacts of non-interpretability and interpretability.
Lastly, we would like to highlight that the wide adoption of foundation models is at odds with a recent plea of many interdisciplinary researchers not to use complex black box models for high stakes decisions [e.g., Rudin 2019] , but instead to focus on the long-standing development and application of more intrinsically interpretable models. 
In the midst of these pleas, work aimed at interpreting foundation models is a double-edged sword. 
Large machine learning models, and now foundation models, are most often deployed by powerful corporations and institutions, and incremental advances in interpretability can be exaggerated to 'ethics-wash' and continue use of models as though they have achieved interpretability, belying the reality that they remain far below traditional standards of algorithmic interpretability. 
Moreover, when approaches to interpretability regularly presume easy access to models and their implementation and parameters, interpretability can serve not only as cover for powerful institutions but also centralize model knowledge in the same hands. 
For those working toward the interpretability of foundation models, it is a responsibility to consistently ask whether one is working toward making foundation models interpretable to researchers and model owners or interpretable to everyone. 
Simultaneously, to the extent that foundation models are already being deployed, work on interpretability presents unique opportunities to shift knowledge of foundation models, and thus power, back to datafied and evaluated peoples. 
Interpretation can facilitate the discovery of societally salient aspects of models. 
More radically, work creating accessible methods that allow anyone to interpret the behavior of foundation models shifts power to diverse peoples, creating opportunities to investigate models, opportunities to discover aspects of models important to individuals or their communities, and opportunities to meaningfully consent to, improve, or altogether contest the use of foundation models. 
Finally, it is important for researchers to view the interpretability of foundation models as not only a goal, but a question: research can explore and assess whether the lack of foundation model interpretability is intrinsic and should be deeply studied and widely known as a serious issue discouraging use (or increasing regulation) of these systems, or whether it is possible for future foundation models to uphold a high standard of interpretability for all. 
5 SOCIETY
The societal impact of foundation models, referring both to the construction of the models themselves and their role in developing applications, requires careful examination. 
Specifically, we anticipate that foundation models will have wide-ranging societal consequences that are challenging to understand: foundation models are intermediary assets that are not directly deployed, but rather serve as a foundation that is further adapted. 
As a result, traditional approaches to reasoning about the societal impact of technology are likely complicated; societal impact is easier (but still difficult) to grasp for systems with well-specified purposes. 
In this chapter, we discuss how we may grapple with and beginning to understand the complexity of the societal impact of models foundation models. 
Specifically, we discuss (i) the harms with respect to inequity ( §5.1: fairness) and misuse ( §5.2: misuse), (ii) the impact with respect to the economy ( §5.5: economics) and environment ( §5.3: environment), and (iii) the broader considerations with respect to the law ( §5.4: legality) and ethics ( §5.6: ethics). 
5.1 Inequity and fairness
Authors: Rishi Bommasani, Fereshte Khani, Esin Durmus, Faisal Ladhak, Dan Jurafsky Fig. 24 . 
The intrinsic bias present within foundation models is the byproduct of various training bias sources (left) which, alongside biases introduced during adaptation, determines the extrinsic harms (right) experienced by users in the context of specific downstream applications. 
We emphasize that the same foundation model is the shared foundation for many different applications; its biases propagate to these many applications as a result. 
Further, since the harms experienced by users are the result of specific adapted models, attributing these harms to the various processes and sources depicted in this diagram is both crucial and challenging. 
5.1.1 Introduction.
Foundation models have the potential to yield inequitable outcomes: the treatment of people that is unjust, especially due to unequal distribution along lines that compound historical discrimination [Hellman 2021 ]. 
Like any AI system, foundation models can compound existing inequities by producing unfair outcomes, entrenching systems of power, and disproportionately distributing negative consequences of technology to those already marginalized [Sweeney 2013; Kay et al. 2015; Buolamwini and Gebru 2018; Benjamin 2019; Ajunwa 2019; D'Ignazio and Klein 2020; Crawford 2021 ]. 
Here we ask what fairness-related harms relate to foundation models, what sources are responsible for these harms, and how we can intervene to address them. 
The issues we discuss here are related to broader questions of algorithmic fairness and AI ethics [Corbett-Davies and Goel 2018; Chouldechova and Roth 2020; Hellman 2020; Johnson 2020; Fazelpour and Danks 2021] , race and technology [Benjamin 2019; Hanna et al. 2020; Gebru 2021; Field et al. 2021] , and the coexistence of society and technology [Abebe et al. 2020] . 
5.1.2 Harms.
Foundation models are intermediary assets with no specified purpose before they are adapted; understanding their harms requires reasoning about both their properties and the role they play in building task-specific models. 
We delineate intrinsic biases, 90 i.e., properties of the foundation model that indirectly but pervasively affect downstream applications, and extrinsic harms, i.e., harms that arise in the context of specific downstream applications [Galliers and Spärck Jones 1993] . 
Intrinsic biases. 
Properties of the foundation model can lead to harm in downstream systems. 
As a result, these intrinsic biases can be measured directly within the foundation model, though the harm itself is only realized when the foundation model is adapted, and thereafter applied, i.e., these are latent biases or harms [DeCamp and Lindvall 2020] . 
We focus on the most widely studied form of intrinsic bias, representational bias, specifically considering misrepresentation, underrepresentation and overrepresentation. 
People can be misrepresented by pernicious stereotypes [Bolukbasi et al. 2016; Caliskan et al. 2017; Abid et al. 2021; Nadeem et al. 2021; Gehman et al. 2020] or negative attitudes [Hutchinson et al. 2020] , which can propagate through downstream models to reinforce this misrepresentation in society [Noble 2018; Benjamin 2019 ]. 
People can be underrepresented or entirely erased, e.g., when LGBTQ+ identity terms [Strengers et al. 2020; Oliva et al. 2021; Tomasev et al. 2021] or data describing African Americans [Buolamwini and Gebru 2018; Koenecke et al. 2020; Blodgett and O'Connor 2017] is excluded in training data, downstream models will struggle with similar data at test-time. 
People can be overrepresented, e.g., BERT appears to encode an Anglocentric perspective [Zhou et al. 2021a ] by default, which can amplify majority voices and contribute to homogenization of perspectives [Creel and Hellman 2021] or monoculture [Kleinberg and Raghavan 2021] ( §5.6: ethics) . 
These representational biases pertain to all AI systems, but their significance is greatly heightened in the foundation model paradigm. 
Since the same foundation model serves as the basis for myriad applications, biases in the representation of people propagate to many applications and settings. 
Further, since the foundation model does much of the heavy-lifting (compared to adaptation, which is generally intended to be lightweight), we anticipate that many of the experienced harms will be significantly determined by the internal properties of the foundation model. 
Extrinsic harms. 
Users can experience specific harms from the downstream applications that are created by adapting a foundation model. 
These harms can be representational [Barocas et al. 2017; Crawford 2017; Blodgett et al. 2020] , such as the sexualized depictions of black women produced by information retrieval systems [Noble 2018], the misgendering of persons by machine translation systems that default to male pronouns [Schiebinger 2013 [Schiebinger , 2014]] , or the generation of pernicious stereotypes [Nozza et al. 2021; Sheng et al. 2019; Abid et al. 2021] . 
They can consist of abuse, such as when dialogue agents based on foundation models attack users with toxic content [Dinan et al. 2021; Gehman et al. 2020] or microaggressions [Breitfeller et al. 2019; Jurgens et al. 2019 ]. 
All of these user-facing behaviors can lead to psychological harms or the reinforcement of pernicious stereotypes [Spencer et al. 2016; Williams 2020] . 
In addition to harms experienced by individuals, groups or sub-populations may also be subject to harms such as group-level performance disparities. 
For example, systems may perform poorly on text or speech in African American English [Blodgett and O'Connor 2017; Koenecke et al. 2020] , incorrectly detect medical conditions from clinical notes for racial, gender, and insurance-status minority groups [Zhang et al. 2020b] , or fail to detect the faces of people with darker skin tones [Wilson et al. 2019; Buolamwini and Gebru 2018] . 
As foundation models are more pervasively applied, including in high-stakes domains, these disparities can spiral into further, and more severe, harms. 
 Koenecke et al. [2020] discuss how if African American English speakers cannot reliably use speech recognition technologies (e.g., due to inequities in underlying foundation models), this may mean they cannot benefit from certain derivative products (e.g., voice assistants, assistive technologies) and will be disadvantaged if these technologies are used to conduct interviews for employment or transcribe courtroom proceedings. 
More generally, characterizing these group-level harms (and working towards justice for those harmed) also requires the AI community to improve its understanding of group-based prejudice [Allport 1954 ] and social groups: we point to relevant work in the social sciences and other communities on moving beyond binary treatments of gender [Lindsey 2015; Westbrook and Saperstein 2015; Richards et al. 2017; Darwin 2017; Keyes 2018; Hyde et al. 2019; Cao and Daumé III 2020; Dinan et al. 2020] , more nuanced treatments of race [e.g., Penner and Saperstein 2008; Freeman et al. 2011; Saperstein and Penner 2012; Saperstein et al. 2013; Penner and Saperstein 2015; Field et al. 2021] , better addressing intersectional identities [e.g., Crenshaw 1989; Nash 2008; Gines 2011; Penner and Saperstein 2013; Ghavami and Peplau 2013; Bright et al. 2016; Buolamwini and Gebru 2018; May et al. 2019; O'Connor et al. 2019; Guo and Caliskan 2021] , and more modern treatments of disability [e.g., Batterbury 2012; Spiel et al. 2019; Hutchinson et al. 2020] . 
Additional considerations. 
To more completely understand the harms of foundation models, further documentation is required of both the intrinsic biases and extrinsic harms; future work should articulate the relationship between intrinsic biases and extrinsic harms [Blodgett et al. 2020 [Blodgett et al. , 2021;; Goldfarb-Tarrant et al. 2021 ]. 
This documentation requires centering stakeholders beyond academics and industry practitioners: the inequitable impact of foundation models will be experienced largely by minority populations, which are underrepresented in both academia and industry. 
For foundation models specifically, their creation and study likely will be conducted by those with the access and resources required, further emphasizing the importance of venues that center marginalized voices [D' Ignazio and Klein 2020, §5.6: ethics] . 
In particular, user studies of specific adapted models, when aggregated across applications, can provide compelling and individualized documentation of the harms that derive from the intrinsic biases of foundation models, all while centering individual users. 
In this way, we imagine the methodologies in humancomputer interaction (HCI), with some adjustment to accommodate the abstraction involved in foundation models, will help center the voices of marginalized communities (further discussion in §2.5: interaction). 
5.1.3 Sources.
In order to fully characterize and properly intervene on the harms of foundation models, we must be able to trace their source to the properties of the foundation model and the adaptation process, and further decompose to the roles of individual sources of biases [Friedman and Nissenbaum 1996] . 
Source tracing is vital for attributing ethical and legal responsibility for experienced harm, though attribution will require novel technical research that foregrounds matters such as causality [Pearl 2000 ] and influence [Koh and Liang 2017] . 
Data. 
Data of several types shapes the behavior of applications, and the associated extrinsic harms, based on foundation models: the training data used to train the foundation model, the adaptation data used to adapt the foundation model, and test-time user data/interaction. 
For all of these data sources, the properties of the data (e.g., toxicity and hate speech [Henderson et al. 2017] , abusive language [Waseem et al. 2017] , microaggressions [Breitfeller et al. 2019] , stereotypes [Voigt et al. 2018] ) will manifest in the biases of the foundation model (and its adapted derivatives). 91 
Since the training data is the key data source that determines the foundation model and the associated intrinsic biases, we focus on the training data here. 
At present, the relationship between the training data, along with associated data practices (e.g., data curation, data selection, and data weighting [Paullada et al. 2020; Bender et al. 2021; Rogers 2021] ) and the intrinsic biases acquired by the foundation model remains unclear; future work is critically needed to clarify this relationship. 
Since foundation models generally require training data of immense scale, which poses clear challenges not only to its documentation [Bender et al. 2021] but also comprehensive scientific exploration to 91 In adaptation, which involves labelled task-specific data, biases in the choices of the label space [Crawford 2021 ] and biases in the annotators who label that data [Geva et al. 2019; Sap et al. 2019 ] can also contribute to extrinsic harms experienced by users. 
articulate the relationship of data biases and model biases, we anticipate new protocols are required to address this scale. 
Establishing scaling laws for bias, akin to those for accuracy metrics [Kaplan et al. 2020; Henighan et al. 2020] , may enable systematic study at smaller scales to inform data practices at larger scales. 
Modeling. 
Modeling decisions (e.g., training objective ( §4.2: training), model architecture ( §4.1: modeling), adaptation method ( §4.3: adaptation)) influence the biases in foundation models and their derivatives, thereby affecting the experienced extrinsic harms. 
Existing work demonstrates that foundation models amplify training data biases, extending trends seen for machine learning and deep learning models [Zhao et al. 2017; Wang et al. 2019d; Jia et al. 2020; Hashimoto et al. 2018] , though much still remains unclear about what and how model properties are responsible for this bias amplification. 
Further, given that applying foundation models directly may be infeasible (due to their scale), efforts to compress these models or make them more efficient also appear to amplify bias [Hooker et al. 2020; Renduchintala et al. 2021] . 
Amplification may also be exacerbated by feedback loops, in which foundation models modify societal behavior and induce sociological changes, which modifies subsequent training data; feedback effects of this form tend to exacerbate inequity in other ML applications [Lum and Isaac 2016; Ensign et al. 2018; Hashimoto et al. 2018] . 
Beyond the explicit decisions made in training and applying foundation models, community values [Birhane et al. 2020 ] and norms ( §5.6: ethics) both indirectly and implicitly [Liu et al. 2021b ] shape decision-making in building models. 
As a result, measuring biases in conjunction with work introducing foundation models [e.g., Brown et al. 2020 ] and in standard benchmarks [Friedman and Nissenbaum 1996, §4.4: evaluation] , as well as conducting user studies with diverse user groups to document experienced harm, are steps towards ensuring that best practices actively emphasize the consideration of bias and inequity. 
Modelers.
As with all algorithmic systems, poor representation and diversity of stakeholders and marginalized communities in decision-making bodies that develop or apply foundation models is inherently problematic, and may contribute to greater experienced harm for these communities. 92 
hile difficult to document, existing efforts to develop foundation models suggest this as a possibility: Caswell et al. [2021] demonstrate the flawed data handling of less-represented languages in the multilingual datasets used to train multilingual models and Hutchinson et al. [2020] show that models often contain undesirable biases towards disabled persons. 
In both instances, these biases and harms may have been noticed earlier by better representation of these parties in developer teams. 
Further, since end-users are likely more diverse than developers and may notice these concerns earlier, allowing for user feedback to contribute to foundation model design ( §2.5: interaction) is an important direction forward. 
5.1.4 Interventions and recourse.
Addressing, mitigating, and rectifying the inequities associated with technology requires integrating social and technical methodologies [Abebe et al. 2020] . 
For foundation models specifically, we consider both proactive methods, which change how models are developed and deployed to prophylactically reduce harm, as well as reactive methods, which respond to harm and make changes for the future. 
At its core, the abstraction of foundation models complicates both aspects: knowing if interventions at the level of the foundation level are successful in reducing harm requires downstream observations at the level of specific deployed applications and recourse in the event of harm requires upstream propagation of both feedback and accountability to foundation model providers. 
Intervention. 
General principles that govern intervention on technological systems apply to the foundation model setting: identifying which sources are most responsible for bias or harm provides the evidence required for targeted action. 
For example, the urgency of calls for improved diversity in the teams that design, produce, and control technology (e.g., foundation models) and their applications [Longino 1990; Harding 2015; Nielsen et al. 2017; O'Connor et al. 2019; Hofstra et al. 2020; Katell et al. 2020 ] is further intensified if the lack of diversity is shown to relate to harm [Caswell et al. 2021 ]. 
In addition, transparent documentation [e.g., Gebru et al. 2018; Bender and Friedman 2018; Mitchell et al. 2019] and auditing [e.g., Raji and Buolamwini 2019] are similarly critical in providing the impetus for intervention and change [Burrell 2016; Lipton 2018; Creel 2020; Raji et al. 2020; Wilson et al. 2021 ]. 
The scale of foundation models, as well as the specifics of their accessibility, introduce new challenges for existing protocols for documentation and auditing that we discuss further in §5.6: ethics. 
To date, many of the interventions considered for reducing the inequitable impact of technology, including in the foundation model regime, are methods for technical mitigation that center the data (to obviate reflecting inequities or biases) and modelling decisions (to avoid amplifying data biases) involved. 
Of specific importance in the foundation model regime is recognizing that these mitigation approaches may target different steps in the pipeline such as the training data [e.g., Lu et al. 2020] , modelling objectives [e.g., Zhao et al. 2018] ), and adaptation methods and test-time use [e.g., Park et al. 2018; Zhao et al. 2019 ]. 
As a result, different approaches may not only be more or less effective, but require action from different entities (e.g., foundation model providers vs. application developers) and more or less intensively affect the expensive training process for these models (e.g., changing the process of creating a foundation model vs. altering it post hoc). 
Technical intervention of this form may also target different goals: some interventions, such as changing the training data, aims to reduce intrinsic bias. 
On the other hand, most work on mitigation in algorithmic/ML fairness instead considers reducing outcome disparities in terms of model behavior, i.e., the outputs of downstream systems that more directly relate to extrinsic harm. 
Technical mitigation of all forms at present is severely limited: methods that measure or combat intrinsic bias are brittle or ineffectual [Gonen and Goldberg 2019; Ethayarajh et al. 2019; Bommasani et al. 2020; Zhou et al. 2021b; Antoniak and Mimno 2021] , methods that measure or combat extrinsic outcome disparities may not align with stakeholder goals [Saha et al. 2020] , and there is some evidence to suggest certain types of technical intervention may be simultaneously unsatisfiable [Corbett-Davies and Goel 2018; Kleinberg et al. 2017] , impossible [Lechner et al. 2021] , or may even exacerbate inequity [Xu et al. 2021] . 
In spite of this state of affairs, we continue to believe technical methods will still play an instrumental role in addressing the harms that arise in the foundation model regime; in general, we advocate for transparency, especially given that technical mitigation methods may not be able to achieve the intended goals. 
More broadly, claims of bias and bias mitigation must be made carefully to clearly communicate the status quo to various stakeholders with differing expertise (e.g., application developers building on top of foundation models and policymakers regulating the technology; [Nissim et al. 2020] ). 
Recourse. 
Unfortunately, proactive intervention is unlikely to fully resolve all potential harm or inequity that may arise due to foundation models. 
When harm arises, there is currently no widely-adopted (or legally required) framework for resolving the appropriate recourse for the harmed parties. 
While certain protocols may exist for specific applications, the abstraction of foundation models again introduces a disconnect: harms likely are partially attributable to both the foundation model providers and the downstream application developers, but allocating this responsibility to either party remains challenging. 
More simply, mechanisms are not in place to even communicate these harms to foundation model providers (even if feedback or complaints are raised to application developers). 
As a result, new norms and standards are needed on how feedback from application developers and end-users should reach upstream to the foundation model providers, how to determine the entities (e.g., foundation model providers, application developers) responsible for these harms, and the relationship to legal responsibility ( §5.4: legality). 
To make progress on this matter, we encourage future work to consult the practices used in other domains (especially those with similar abstractions and multi-entity structures), and we anticipate any standards introduced will likely need to be reasonably dynamic, so that they can be synchronized with the rapidly changing status quo for these models and their applications. 
5.1.5 Takeaways.
Machine learning has an established trackrecord of inequitable impact, with much of the burden of its harms borne by marginalized communities. 
Foundation models introduce new challenges to this calculus but, ultimately, for their societal impact to be equitable, significant research and change is required to understand the harms they cause and to meaningfully address and rectify these harms: 
(1) The one-to-many nature of foundation models, i.e., the same few foundation models being used across many applications, means the intrinsic properties of foundation models pervade to many downstream applications. 
Pernicious biases in these models therefore have out-sized effect on the experienced harms. 
(2) Biases and harms in the foundation model regime originate from many sources (e.g., training and adaptation data, modelling and adaptation decisions, modeler diversity and community values). 
Attributing the sources for bias and harm is fundamental for questions of intervention and responsibility; attribution requires new technical research to be done reliably. 
(3) The inequities of foundation models are not inevitable, but addressing them requires a multipronged approach comprised of both proactive intervention (e.g., data-centric and modelcentric changes) and reactive recourse (e.g., mechanisms for feedback and accountability). 
5.2 Misuse
Authors: Antoine Bosselut*, Shelby Grossman*, Ben Newman In this section, we consider misuse of foundation models -situations where people use foundation models as they are intended to be used (e.g., to generate language), but where their capabilities are intentionally leveraged to cause harm to populations or individuals. 
This definition positions misuse concerns between those of inequity (where models can cause harm without bad intentions; §5.1: fairness) and security (where bad actors exploit unintentional abilities or vulnerabilities in models to cause harm; §4.7: security). 
Below, we outline how foundation models both enable new forms of misuse and support new tools for misuse detection and mitigation. 
5.2.1 Foundation models will be misused for harmful purposes.
Advances in the scale ( §4.2: training), multimodality ( §4.1: modeling), and adaptivity ( §4.3: adaptation) of generative foundation models will allow them to be misused to generate high-quality, cheap, and personalized content for harmful purposes. 
In this section, we discuss these three dimensions within the context of two examples of malicious activity: manipulative content creation and harassment. 
Content quality. 
Foundation models are capable of automatically generating much higher-quality, human-looking content than prior AI methods. 
They may empower disinformation actors, where states, for example, create content to deceive foreign populations without being transparent that the content is linked to a state. 
Currently, creating this content often requires hiring people who speak the language of the population being targeted. 
Governments may outsource content production to native speakers in the country they are targeting, 93,94 but this decision causes real risks for operational security. 
Foundation models will allow for the creation of content that is often indistinguishable from content created by humans [Kreps et al. 2020; Clark et al. 2021 ] -and indeed it will be able to do this for a wide variety of languages -enabling both goals of creating content that resonates and maintaining operational security. 
In addition to deceiving foreign populations, foundation models' ability to generate high quality synthetic images (deepfakes) or text may be abused to harass individuals. 
Deepfakes have already been used for the purpose of harassment. 
For example, Rana Ayyub, an Indian investigative journalist, was targeted by a high-quality deepfake that superimposed her face onto a pornographic video, leading her to leave public life for months. 95 
Because foundation models are often multimodal ( §4.1: modeling), they could similarly impersonate speech, motions, or writing, and potentially be misused to embarrass, intimidate, and extort victims. 96 
 Cost of content creation. 
Foundation models will substantially decrease the costs of content creation, further lowering the barrier to entry for malicious actors to carry out harmful attacks [Brundage et al. 2018] . 
The budget for one 2017 influence operation that originated in Russia and targeted Americans was $12.2 million [DiResta et al. 2018 ]. 
More recently, individuals in Russia paid $75-$200 per article to American freelancers as part of a disinformation campaign. 97 
Foundation models will lower these marginal costs. 
While foundation models, such as GPT-3, may make mistakes when generating content [Buchanan et al. 2021 ], it will be more feasible to hire a small number of editors to fix them than to hire content creators directly. 
Initial costs to train foundation models are more significant ( §4.5: systems), but these expenses should be manageable for most state actors [Buchanan et al. 2021 ]. 
In addition to monetary cost, foundation models require fewer technical skills to achieve highquality results. 
Current tools, such as video editing software, can enable credible photo or video deepfakes, but require several hours of a skilled user's time to yield quality content. 
Foundation models lower this barrier to use: their few-shot adaptation capabilities ( §4.3: adaptation) enable new modes of interaction for application users ( §2.5: interaction) that will allow users to rapidly iterate for content creation. 
Personalization. 
Foundation models will reduce obstacles to creating personalized content. 
For example, disinformation from Russian individuals that targeted the US in 2016 included highly customized content. 
Social media posts were crafted to push narratives about Syria (e.g., the U.S. should get out of Syria) that resonated with Black Lives Matter activists [DiResta et al. 2018 ] (e.g., by suggesting that the U.S. should focus on issues facing the Black community in America, and not on issues in Syria). 
The same narratives were repackaged to resonate with Texas secessionists [DiResta et al. 2021] . 
Such a content creation endeavor is costly and time consuming. 
Foundation models will allow for similar activity, but at scale due to the low cost of adaptation ( §4.3: adaptation). 
In addition to foundation models allowing an actor to personalize content for niche audiences, they also allow an actor to personalize content to target a single individual -a capability that can be abused by harassers. 
Foundation models that condition their generations on personal attributes or information can create realistic personalized content, which could be more embarrassing, place victims in more danger, 98 and lead to more successful extortion attempts. 
93 https://www.lawfareblog.com/outsourcing-disinformation 94 
https://fsi.stanford.edu/content/ira-takedown-20201215 95 
https://www.huffingtonpost.co.uk/entry/deepfake-porn_uk_5bf2c126e4b0f32bd58ba316 96 https://www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402 97 
https://www.nytimes.com/2020/09/02/technology/peacedata-writer-russian-misinformation.html 98 
https://www.dw.com/en/social-media-uptick-in-honor-crime-in-middle-east/a-56370773 5.2.2 Foundation models will be powerful detectors of harmful content. 
While the generative capabilities of foundation models will provide ample misuse opportunities, these same abilities may make them strong detectors of harmful content. 
While these capabilities are equally relevant for detecting human-and model-generated content, we focus on the detection of model-generated content in this section. 
First, we outline the challenges that current manual detection approaches will face in discovering harmful misuses of foundation model. 
Then, we propose how the interactive and multimodal representation capabilities of foundation models may make them powerful tools for automatic detection of harmful content. 
Finally, we discuss the risks associated with deploying automatic detection models in online settings to combat potential foundation model misuse. 
Rethinking human interventions. 
Currently, malicious practices are frequently uncovered (and on social media, sometimes removed) by humans searching the internet to uncover content origination. 99 
For example, fake social media profiles commonly steal profile photos from dating sites, which are discoverable through reverse image searches. 
Similarly, disinformation websites frequently use plagiarized content to mask deceptive content [DiResta and Grossman 2019], which is easily identified by conducting internet phrase searches. 
Foundation models will limit the efficacy of these detection strategies. 
Already, relatively unsophisticated disinformation campaigns have leveraged AI-generated photos 100 to remove the possibility of discovery through reverse image search. 
Tools for assessing whether these photos are AI-generated are available, but foundation models will complicate this work -for text and video as well -challenging manual human discovery techniques [Ippolito et al. 2020; Clark et al. 2021] . 
Foundation models as detectors. 
The same abilities of foundation models that make them strong generators of creative content may make them strong detectors of model-generated content. 
Existing works demonstrate that foundation models can be adapted to detect disinformation from text generators [Zellers et al. 2019b ] -which generate statistical textual artifacts [Holtzman et al. 2020 ] -and that they can be used to evaluate the toxicity levels of their own generations using prompt questions [Schick et al. 2021] . 
Below, we describe how future foundation models will enable more powerful detection systems of machine-generated, harmful content. 
Improvements in the interactive and multimodal interfaces of foundation models will provide new opportunities to improve detection of foundation model misuse for harmful content generation. 
Current statistical detectors must be retrained and re-deployed to integrate new knowledge about the textual content of misuse strategies [Dinan et al. 2019 ]. 
The rapid learning capabilities of foundation models ( §4.3: adaptation) may allow them to adapt from human feedback to new misuse strategies that the foundation model was not initially trained to recognize [Lee et al. 2021a] . 
Simultaneously, the multimodal abilities of foundation models will enable more expressive representation of misuse ecosystems. 
Prior work has explored how misinformation spreads more rapidly across social networks than authentic content [Starbird et al. 2018; Vosoughi et al. 2018] , yielding recognizable signatures when analyzed retrospectively. 
The multimodal capabilities of foundation models could allow them to jointly learn representations of harmful content and its typical dissemination signature on social networks. 
These joint representations could provide powerful tools for predicting whether certain types of automatically-generated content are indicative of misuse behavior. 
Risks of foundation models as automatic detectors. 
Improvements in automatic detection systems for both model-generated and human-generated harmful content will make these systems more prevalent online, yielding potential negative consequences. 
Any detection system will have false positive cases where human-generated fair content will be flagged as harmful [Sap et al. 2019; Xu et al. 2021] . 
The rate at which algorithmic false positives affect users (or groups of users) may cause downstream harm ( §5.1: fairness). 
The adaptive capabilities of foundation models should make systemic false positives easier to address as the model can be locally edited to re-classify those examples ( §4.3: adaptation). 
However, corner cases will likely not be prioritized and recourse will be challenging in these situations. 
More broadly, wide-scale deployment of misuse detection systems may engender an "arms race" between harmful content generators and detectors. 
Most content generators that use foundation models will lack the resources to develop them individually, and will use systems deployed by larger entities. 
While terms of use policies should outline acceptable uses of these systems ( §5.6: ethics), deployers of foundation models will also need internal detection systems to identify misuse of their products 101 and mitigate them ( §5.4: legality). 
However, there will be fewer controls for misuse actors with the resources to develop their own foundation model-based content generators, putting pressure on platforms to curate the content shared through their distribution channels. 
Optimistically, content platforms encompass some of the most well-capitalized firms in the world. 
Their resources may enable the development of detectors beyond the capabilities of most individual misuse agents. 
This resource advantage could disincentivize individual foundation model development due to the high costs of repeatedly training these systems at scale. 
However, many instances of foundation model misuse could still be successful even without the largest foundation models to power them, particularly as attackers may leverage the interactive capabilities of foundation models to rapidly generate content that can evade detection. 
5.3 Environment
Authors: Peter Henderson, Lauren Gillespie, Dan Jurafsky Foundation models can potentially lead to many social and environmental benefits, for example in legal domains ( §3.2: law), healthcare ( §3.1: healthcare), or even tackling climate change [Rolnick et al. 2019] . 
But because of their scale, they themselves can negatively impact the environment through increased carbon emissions if model creators are not careful [Strubell et al. 2019; Lottick et al. 2019; Schwartz et al. 2019; Lacoste et al. 2019; Cao et al. 2020; Henderson et al. 2020; Bender et al. 2021 To understand where such emissions can occur in foundation models, we consider their lifecycle. 
First, they are trained on vast amounts of data, possibly for up to months of time and often distributed across hundreds to thousands of GPUs. 
Afterwards, they may be adapted to new domains or perhaps distilled into smaller models. 
All of this can be considered part of the training regime. 
Models used purely for research may not move beyond these steps. 
After models have been adapted and/or distilled, they might move on to be deployed into production. 
At this point many rounds of inference will run through the model until a new model is trained and the cycle repeats. 
Each one of these steps has the potential to utilize large amounts of energy and can contribute to carbon emissions. 
Foundation models can generate large, one-time energy costs and carbon emissions during the initial training phase. 
For example, the amount of emissions from training one BERT-base model, under some conditions, would only be offset by 40 trees grown for 10 years. 102 
nd if deployed at scale, foundation models can require substantial energy to service millions of requests 103 -translating to large carbon emissions if nonrenewable resources are used. 
Therefore, the environmental impacts of certain design decisions for both training and deploying foundation models can be substantial. 
Even seemingly minuscule decisions, like reducing the number of layers a model has, may lead to significant environmental cost reductions at scale. 
For example, based on calculations from Henderson et al. [2020] , a slightly more energy efficient translation model deployed at the scale of a commercial translation service could save between 78 kgCO2eq and 12,768 kgCO2eq of carbon emissions per day depending on the energy grid used. 
This is roughly equivalent to the carbon sequestered by 1 to 211 trees grown for 10 years, or the carbon sequestered by .35 to 57.4 acres of forest in one year. 104 
Thus the design, deployment, and post-deployment monitoring of foundation models should adequately reflect these risks. 
There are of course uncertainties in calculating the amount of energy used or carbon emitted by any given model [Henderson et al. 2020; Cao et al. 2020; Patterson et al. 2021] , and other sources of emissions may currently be much greater than those generated by foundation models [Mora et al. 2018] . 
But if foundation models continue to scale and gain in popularity, they may very well become a significant contributor to carbon emissions. 
Our goal is to provide a framework for foundation model developers and large-scale deployers 105 to consider how they can mitigate any unnecessary carbon emissions and keep the net social impact of these models positive. 
We recommend that: 
(1) Carbon impacts can and should be mitigated in many cases. 
This can be accomplished by training models in low-carbon intensity regions, or by using more efficient models and hardware ( §5.3.1: 
environment-mitigation). 
(2) When all mechanisms for mitigation have been exhausted and mitigation is no longer possible, the costs and benefits to society should be assessed to determine if and when a larger foundation model should be deployed over a smaller, more efficient, model -with the understanding that the up-front costs of a large foundation model may be amortized over the lifetime of the model ( §5.3.2: 
environment-costs). 
(3) Energy, computational, and carbon costs -as well as any efforts taken to mitigate negative impacts -should be clearly reported to inform policymaking and research ( §5.3.3: 
environmentreporting). 
5.3.1
Carbon impacts can and should be mitigated in many cases. 
The carbon impacts of training foundation models differ from the impacts of deploying them for inference. 
Model training has no latency requirements, so training can be moved across energy grids with relative ease in cloud environments. 
Every energy grid has its own carbon intensitythe amount of carbon emitted per kilowatt-hour of energy used. 
For example, Québec has an extremely low carbon intensity due to its reliance on hydroelectricity, while Estonia's energy grid has an extremely high carbon intensity due to its reliance on shale oil (though that is changing quickly) [Henderson et al. 2020] . 
Recent research has even suggested that the top 5% of polluting power plants contributed 73% of all electricity-based emissions [Grant et al. 2021 ]. 
Thus, while training foundation models can be quite energy intensive, researchers have demonstrated that the carbon impacts of these models can be partly mitigated by selecting energy grids with minimal carbon emissions [Henderson et al. 2020; Lacoste et al. 2019; Patterson et al. 2021 ]. 
Carbon offsets have also been proposed as a stopgap until carbon-free renewable electricity is available at all data centers. 
This strategy involves reducing carbon emissions in one activity to offset the emissions from another. 
But most -if not all -carbon offsets are a strictly worse solution than not emitting CO 2 in the first place [Holl and Brancalion 2020] . 
Some carbon offset programs can even have a negative impact. 
For example, studies of forest planting campaigns (often a source of carbon offsetting) show that they can do more harm than good. 
They can yield monocultures (the use of one particular species of tree) that diminish the biodiversity of the region and reduce carbon storage in the forest soil [Heilmayr et al. 2020; Hong et al. 2020b ]. 
This could result in more carbon emissions when using carbon offsets than if the original carbon had never been emitted in the first place. 
Therefore, when training or deploying a foundation model, we recommend designing for as little carbon emission as possible up-front, rather than simply relying on carbon offsets to cancel emissions. 
When it is not possible to run in low-carbon regions, other mitigation strategies should be leveraged, reducing unnecessary energy usage. 
This includes: 
• using more efficient hardware, 106 
• using mixed-precision training [Micikevicius et al. 2017] or quantization [Gholami et al. 2021 ], 
• using more efficient architectures (e.g., using an evolved transformer over a vanilla transformer architecture; or using sparse models) [So et al. 2019; Patterson et al. 2021; Mostafa and Wang 2019 ], 
• distilling models and using distilled models (e.g., [Sanh et al. 2019] ), 
• and utilizing other optimization strategies that will reduce energy costs (see more discussion in §4.5: systems). 
Maintainers of open source projects and cloud compute should strive to set their default settings to the most efficient possible, since "green defaults" are known to be the most effective mitigation strategies (see discussion in [Henderson et al. 2020] ). 
Other mitigation strategies can be found in recent literature [Strubell et al. 2019; Lacoste et al. 2019; Schwartz et al. 2019; Henderson et al. 2020] . 
We also note that reducing and mitigating energy usage also has the added benefit of making models more accessible to those with limited compute access (see §5.6: ethics for more discussion). 
However, when a model is mainly used for inference, e.g., deployed in a production application, it often cannot be moved to a less carbon-intensive energy grid for low-latency applications. 
In addition to using the mitigation strategies specified above, in this case it is important to weigh the benefits of the proposed foundation model versus a more energy efficient alternative. 
We discuss this further in the subsequent section. 
5.3.2
Costs and benefits should be assessed before using foundation models. 
After taking as many steps as possible towards mitigation (or where mitigation is not possible), it is vital to assess the required size of a foundation model -or whether a foundation model should be used at all. 
This cost-benefit analysis should consider: 
(1) Is the social cost and environmental cost from deploying the foundation model greater than the social benefit of the model? 
(2) Would another, computationally simpler and cheaper approach achieve comparable social benefit (e.g., a much more efficient foundation model, or perhaps simple baseline)? 
A simplified scheme for assessing this trade-off considers the overall impact of a model as: () = () - () - () - (). 
(7) Figure 26 represents this equation and the costs and benefits that may enter each variable. 
Here, is the model and is the net social benefit, as well as environmental benefit, in dollars. 
 can be increased by improving healthcare, access to justice, decreasing poverty, improving environmental monitoring, aiding ecosystem conservation efforts, and so on. 
 is the social cost of carbon from energy use. 
This represents the future harm to society from the carbon released as a present-day monetary value. 
The upper bound U.S. Environmental Protection Agency (EPA) estimate from 2017 for the social cost of carbon was $105 (in 2007 U.S. dollars) per metric ton of CO 2 emitted. 107 
 is the energy cost of the model. 
For example, in April 2021, the average U.S. residential energy cost was about $0.1376 per kWh. 108 
Added to this variable could be the costs from increased strain on the energy grid. 
For example, a recent study suggested that the cost per energy grid interruption event, normalized by average demand, could be as high as $15.9 per average kW [Sullivan et al. 2015 ]. 109 
 is the social cost of other second order environmental effects. 
This could include: 
• The compounding carbon impacts from increased chip demand and chip production [Gupta et al. 2021a ]. 
• Other environmental impacts of chip manufacturing, like the creation of toxic waste sites in Silicon Valley, whose health effects are unequally distributed to socially vulnerable populations [Stewart et al. 2014] , or pollution from manufacturing in Taiwan that has been linked to chronic health problems [Tu and Lee 2009; Lin et al. 2016 ]. 
• The compounding effects of climate change that are not already included in the SCC model. 
For example, these effects could include accelerated desertification [Huang et al. 2016] , rapid ecosystem changes that put many species at risk of extinction [Urban 2015 ], and increased carbon emissions due to melting permafrost [Schuur et al. 2015 ]. 
• Unnecessary strain on chip production capacities. 
Recent chip shortages have led to work stoppages in automobile manufacturing. 110 
There is no evidence to suggest that increasing demand for ML-optimized chips led to this shortage. 111 
But such considerations fall into second order effects, where researchers might weigh whether the risks, however slight, of contributing to such negative impacts are worth using or deploying a large model. 112 
It is important to consider in this analysis that the economic benefits and social costs of carbon could be distributed unequally across communities, with poorer communities being impacted more heavily by climate change and wealthier communities being benefited by a model [Bender et al. 2021] . 113 
As such, when conducting the Equation 7 analysis, one should consider the benefits and harms to society more broadly rather than for a given organization or country. 
In this case () can be viewed as a distribution and should ideally be evenly distributed across the population. 
In cases where the distribution is highly uneven -for example where all the benefits fall to the model designer while all the harms fall to populations that will never benefit from the model -the designer should spend substantially more effort on mitigation before deploying the model. 
There is, of course, some uncertainty in which methodology to use when valuing each component of Equation 7 . Empirical estimates for many of these terms can range in multiple magnitudes depending on the data source and modeling choice for the phenomena, such as the different mechanisms for evaluating the social cost of carbon. 
And of course additional externalities, that may be difficult to quantify monetarily, will continue to need to be considered. 
The key takeaway of this cost-benefit analysis, however, is not the dollar valuation of each term in the equation, but rather the existence of and relative importance of each of these effects. 
Our goal is to provide a high-level framework for beginning to consider these trade-offs. 
Future research may give more guidance on how to quantify each of these values. 
Finally, we note that these factors should also be evaluated over the lifetime of the model, not on a per-run basis. 
Consider an alternative baseline model that must be trained from scratch for every new task. 
The baseline may well require an expensive hyperparameter search to achieve equivalent performance on downstream tasks. 
In contrast, the foundation model places the brunt of the costs on the initial pretraining procedure, with fine-tuning perhaps being much simpler and more energy efficient. 
Over the lifetime of the foundation model, it could be more carbon efficient than the baseline (Figure 27 ). 
Even more efficient adaptation mechanisms could improve this amortization further (see §4.3: adaptation). 
The efficiency of adaptation, however, is not guaranteed. 
It may be true that some foundation models will never be more efficient than a particular baseline, even when amortized over many tasks. 
For example, it cannot be assumed that a smaller model with fewer parameters will translate to energy efficiency improvements. 
Due to increased hyperparameter tuning costs or other optimizations, the number of parameters has been shown not to correlate with energy efficiency in some cases [Zhou et al. 2020; Henderson et al. 2020] . 
Therefore, foundation model developers should rigorously assess the efficiency of their models and adaptation mechanisms before beginning large-scale training efforts. 
The framework in this section is meant to guide the reader in thinking about the environmental and societal trade-offs in training and deploying their model, but there are other substantial social justice considerations involved in deploying a foundation model, discussed in §5.6: ethics. 
§5.5: economics also discusses in more detail the dynamics of social welfare from algorithm deployment. 
5.3.3 Carbon/energy impacts should be systematically reported.
A cost-benefit analysis cannot be conducted unless researchers and engineers working on foundation models report the computational, energy, and carbon costs of their models. 
We encourage foundation model developers, providers, and curators to report these metrics, as well as what carbon reduction strategies were used in the making of the foundation model. 
See [Henderson et al. 2020; Lottick et al. 2019; Lacoste et al. 2019; Schmidt et al. 2021; Anthony et al. 2020] for examples of a Carbon Impact Statement and for tools that can facilitate this reporting. 
For researchers, such reporting can occur at publication time, but we also encourage industry actors to adopt transparency mechanisms to report these metrics for their deployed models. 114 
This will help set policy recommendations within industry and academia, as well as help downstream users identify carbon-friendly usage patterns. 
Standardized reporting will also aid in determining which models are accessible to those with limited compute access (see §5.6: ethics for more discussion on accessibility). 
To encourage more reporting of energy and carbon impacts, we suggest, among other strategies: giving green badges at conferences, requiring reporting of relevant metrics for submission to conference venues, lobbying large-scale deployers of foundation models to provide more transparency, and generally shifting professional norms in academia and industry towards standard reporting of these metrics (see more discussion on professional norms in §5.6: ethics and more discussion on reporting mechanisms by Henderson et al. [2020] ). 
5.4 Legality
Authors: Neel Guha, Peter Henderson, Lucia Zheng, Mark Krass, Daniel E. Ho In this section, we describe how US law may influence, constrain, or foster the creation and use of foundation models. 115 
We note that the legal landscape surrounding algorithmic tools remains uncertain. 
We highlight issues pertaining to (1) model training, (2) liability for model predictions, and (3) protections for model outputs. 
Though understanding how the law affects foundation models is crucial, it is important to recognize that the law cannot be the only lens through which we evaluate the construction, maintenance, and use of foundation models. 
Ethical frameworks are necessary to understand where legally permissible applications of foundation models may still be ill-advised for the harms they inflict and are discussed in more depth in §5.6: ethics and §5.1: fairness. 
Studying the potential for misuse and possible security concerns (see §5.2: misuse and §4.7: security) is critical for preventing harmful outcomes ex ante, as opposed to the ex post treatment that legal mechanisms often provide. 
5.4.1 Training.
Training foundation models will require accumulating vast amounts of multi-modal data, raising questions around data collection and data use. 
First, the ability for model creators to grow datasets via web scraping will be governed by the manner in which courts will interpret terms of service provisions and, notably, the U.S. Computer Fraud and Abuse Act (CFAA), which criminalizes accessing a server "without authorization" [Wajert and Rottman 2019] . 
Courts are in conflict on these questions, and recent cases have sought to clarify the circumstances under which web scraping may be barred. 116 
The restrictiveness of data access would fundamentally affect the diversity of data practitioners can use to train foundation models [Levendowski 2018 ]. 
Second, much of the data contained in training sets will be copyrighted and potentially protected by intellectual property law. 
However, copyright law recognizes exceptions when individuals may be permitted to use copyrighted material. 117 
Some scholars believe that the legal permissibility of training datasets will largely rest on whether courts interpret the process of model training as "transformative" under fair use doctrine [Lemley and Casey 2020] . 
Though the question of what qualifies as transformative is highly context dependent, the general rule is that transformative uses are those "that add something new, with a further purpose or different character, and do not substitute for the original use of the work" [Office 2021]. 
Already, the recently released Github Copilot tool is bringing these arguments to the fore [Gershgorn 2021 ]. 
Finally, some training datasets may run afoul of privacy laws. 
Illinois, for instance, enables individuals to sue for improper collection or use of biometric data (e.g., retina or iris scans, fingerprints, voiceprints, or scans of hand or face geometry). 118 
Foreign privacy laws like the E.U. 's General Data Protection Regulation (GDPR) -which will affect American model creators if datasets contain information from E.U. citizens -would require data subjects to be informed about the purpose of data collection. 
Further issues could arise for laws like the California Consumer Protection Privacy Act (CCPA), which provide individuals with a "right to be forgotten, " raising questions as to whether model creators will need to "remove" training data from models [Villaronga et al. 2018; Ginart et al. 2019] . 
5.4.2 Output liability.
Though foundation models themselves are task agnostic, fine-tuned models -or the representations learned by foundation models themselves -may be used for traditional prediction tasks. 
Where these tasks form components of larger decision-making systems, foundation models will thus influence actions, decisions, or policies. 
When these result in harm, model creators -and the individuals operating them -may be legally responsible. 
Embedding foundation models in physical systems (e.g., self-driving cars, electric grid management, medical diagnostics, etc.) may result in physical harm to individuals. 
Here, courts will likely resolve questions of liability under tort doctrine [Lemley and Casey 2019; Selbst 2020] . 
Key open questions include the interplay between the liability of users, foundation model providers, and application developers, as well as the standards courts will use to assess the risk profile of foundation models. 
Deployments in particularly sensitive domains (e.g., medicine) will require regulatory approval, and the development of standardized processes to assess safety [Wu et al. 2021g] . 
Fine-tuned foundation models that classify individuals in ways that correlate with protected attributes (e.g., race, gender) may face challenges under civil rights laws. 
Scholars have noted that claims for disparate treatment resulting from foundation models may be brought in the context of hiring, housing, or credit lending [Gillis and Spiess 2019; Scherer et al. 2019] . 
Exactly how courts will adjudicate these issues is far from clear. 
Scholars have noted for instance, that the courts' traditional views on "discrimination" would actually prevent machine learning practitioners from implementing many algorithmic fairness techniques [Xiang 2021; Ho and Xiang 2020 ]. 119 
.S. law recognizes special privileges and limits on governmental entities. 
Thus, the use of foundation models by governmental entities -at a local, state or federal level -will implicate special considerations, in addition to equal protection claims. 
The use of models for risk assessmentor in other settings which result in a deprivation of life, liberty, or property -will invite procedural due process claims. 120 
When models are used by administrative agencies (e.g., the Environmental Protection Agency) for instance, plaintiffs may allege that such use violates basic standards of due process, reasonableness / non-arbitrariness, and transparency. 
5.4.3 Legal protections for outputs.
Model outputs -and by extension the model creators responsible for the models -may also be afforded certain legal protections. 
First, content produced by generative models may implicate free speech issues. 
The extent to which courts will find First Amendment protections for machine generated content is unclear. 
Scholars have discussed a number of open questions, including whether "AI speech" is protected [Massaro et al. 2016] or if model outputs are in effect the human programmer's speech [Kajbaf 2019 ]. 
Others have noted the possibility of disclosure requirements (akin to safety disclosures for pharmaceutical drugs or other substances), also implicating speech doctrine, under which models would be forced to share with listeners that their content is machine generated [Lamo and Calo 2019] . 
These issues could have wide ranging consequences, affecting whether individuals can use foundation models to mass produce speech, or whether model creators could be held liable for content generated by foundation models. 
Second, there is uncertainty regarding who may assert ownership over model outputs. 
Existing copyright law does not recognize computer programs as authors, and hence, does not afford copyright protection to "work" created by computer programs [Grimmelmann 2015 ]. 
As a result, scholars have advocated for a variety of approaches. 
Some have argued that, depending on the circumstances, both the human creator of a program and its human user may have viable claims to being the "author" of the program's output [Ginsburg and Budiardjo 2019] . 
As models are increasingly used in the process of "creation" -from artistic endeavors to more mundane settings like news filings -disputes over the ownership of machine generated content will become more commonplace. 
While our analysis above only skims the surface of the legal issues implicated by foundation models, the resolution of these questions will be critical to the construction, use, and deployment of foundation models, or, to borrow Larry Lessig's phrase, how "code is law" [Lessig 2000 ]. 
5.5 Economics
Authors: Zanele Munyikwa, Mina Lee, Erik Brynjolfsson Foundation models have the potential to substantially improve overall living standards by increasing productivity and innovation. 
These models can be deployed to substitute for human labor, augment humans, or help in the discovery of new tasks and opportunities, which can lead to increased concentration of ownership and power, or more decentralization. 
On a broader level, the result can be either increased inequality due to potential centralization ( §5.1: fairness, §5.6: ethics), or more broadly shared prosperity due to the easier adaptation of foundation models for a wide range of applications ( §1: introduction). 
The ultimate outcomes on all these dimensions are not dictated solely by technology or economics, but by the choices and actions of technologists, policymakers, managers, workers, and other members of society. 
Foundation models can be thought of as what economists refer to as a general-purpose technology [Bresnahan and Trajtenberg 1995] . 
General-purpose technologies refer to technologies like the steam engine and electricity, which drive waves of transformation and productivity growth due to their pervasiveness, improvement over time, and ability to spawn complementary innovations (a host of products and services that revolve around one core product). 
While foundation models may not be pervasive at the moment, they seem poised to be the basis of widespread technological innovations, and have the key hallmarks of a general-purpose technology. 
As a result, these models are likely to be economically important. 
In considering the impact of foundation models on the economy, we will focus on three broad areas of impact: productivity, wage inequality, and ownership. 
5.5.1 Productivity and Innovation.
Foundation models are likely to substantially increase both productivity and innovation. 
Productivity growth is one of the main contributing factors to boosting living standards, as it increases the wealth of nations and addresses a host of challenges from poverty and healthcare to the environment and education. 
Productivity is defined as output per unit input. 121 
One way to boost productivity is to reduce the denominator; for instance, enabling a company's advertisements to be written with fewer copywriters or fewer labor hours per copywriter lowers the number of units of input. 
Productivity can also be boosted by increasing the numerator, for instance by enabling a software developer to write more code in a given time. 
If the growth in the numerator is great enough, this can lead to more people developing software, not fewer [Autor 2015]. 
In many tasks, we have already observed machine learning systems increasing productivity. 
For instance, an autocomplete system for clinical documentation reduces keystroke burden of clinical concepts by 67% [Gopinath et al. 2020 ]. 
Likewise, the potential for foundation models to affect productivity spans almost every industry and many occupations. 
Considering language alone, an analysis of U.S. occupations using the US Department of Labor's O*NET database shows that many occupations involve the types of language-related work that could be affected by foundation models. 
Approximately 13% of occupations have a primary task that is related to writing, and the total wage bill of these occupations (annual salary multiplied by the number of individuals employed in the occupation) is over 675 billion dollars. 
However, the potential impact of foundation models extends beyond language. 
They will also have effects on diagnostic imaging in medicine, graphic design, music 122 , and many other tasks where people are creating something that is similar to something else that already exists [Winkler et al. 2019; Ramesh et al. 2021] . 
Perhaps the most profound, if still speculative, effect of foundation models is their potential to enhance creativity and boost the rate of innovation itself. 
For instance, DALL•E [Ramesh et al. 2021 ] could transform the market for illustrations much as inexpensive cameras revolutionized photography. 
If these models enable humans to develop new ways to write new songs and novels ( §2.5: interaction), discover variants of drug molecules ( §3.1: healthcare), extend patents ( §3.2: law), build innovative software applications, or develop new business processes, then not only the level of productivity, but the rate of growth of productivity would be increased. 
In this way, foundation models have some of the characteristics of the ideas or blueprints in Paul Romer's growth models [Romer 1990 ], or even meta-ideas (ideas about ideas) which, unlike most other goods, are non-rival, thus speeding growth. 
It is worth noting that changes in productivity are not always visible in the official statistics, because many aspects of input and output are difficult to measure [Brynjolfsson and Collis 2019] . 
As a result, the benefits and costs of foundation models will not be fully captured by traditional productivity metrics, nor by related metrics like gross domestic product (GDP) or price levels (the average of current prices across the entire spectrum of goods and services). 
This is especially true for general purpose technologies historically, since they are catalysts for a cascade of secondary innovations that often transform the set of goods and services in the economy, and even the nature of production and innovation over a period of years or even decades. 
5.5.2 Wage inequality.
Even if foundation models increase average productivity or income, there is no economic law that guarantees everyone will benefit. 
In part, this is because not all tasks will be affected to the same extent. 
More importantly, the effects of foundation models on the demand for labor (and thus employment and wages) can be either positive or negative, regardless of productivity growth [Brynjolfsson and McAfee 2011; Brynjolfsson and Mitchell 2017] . 
When a technology substitutes for human labor in completing tasks, it tends to reduce demand for the workers doing those tasks. 
This depresses employment and wages. 
However, when a technology complements labor, or facilitates the creation of new opportunities or tasks, it tends to increase labor demand [Acemoglu and Restrepo 2019] . 
Employment can (and often does) go up, even as productivity increases. 
For instance, the invention of the airplane created the demand for an entirely new occupation, the airline pilot. 
In turn, the development of jet engines was complementary to human pilots, further increasing demand for them. 
Similarly, the effects of foundation models on employment, wages, and income inequality will differ depending on how they are used. 
While the industrial revolution mainly transformed physical work, foundation models are likely to transform tasks involving cognitive work, like content creation and communication. 
In general, since foundation models are intermediary assets that often possess strong generative capabilities, we envision that they will be able to augment humans in many creative settings, rather than replace humans as there are still significant limitations in using these models stand-alone for open-ended generative tasks [See et al. 2019 ]. 
As we describe in §2.5: interaction, foundation models may also power systems that users can leverage to co-construct novel forms of art or more efficiently prototype new applications. 
Fluid human-machine and human-in-the-loop interaction will require advances in interface design ( §2.5: interaction) as well as fundamental improvements in the interpretability ( §4.11: interpretability) and robustness ( §4.8: robustness) of these models, so that humans can understand model behavior and expect models to perform well in diverse contexts. 
5.5.3 Centralization.
Another key determinant of foundation models' economic impact is who owns data and models. 
In particular, pushing the frontier of foundation models has thus far primarily been the purview of large corporate entities. 
As a result, the ownership of data and models are often highly centralized, leading to market concentration ( §5.6: ethics). 
In turn, this can lead to significant centralization of decision rights and power, reducing income and opportunities for those who don't have ownership. 
This centralization of power can lead to an equilibrium where fewer people have social and economic mobility and opportunity, a situation that Brynjolfsson [2022] calls "The Turing Trap". 
To counterbalance this centralization, there have been grassroots efforts to open source AI research such as Masakhane, EleutherAI, and HuggingFace, or build foundation models through distributed training. 
However, it likely that the gap between the private models that industry can train and the ones that are open to the community will remain large due to foundation models' dependence on massive amount of data and computational resources ( §5.3: environment). 123 
5.5.4 Other considerations.
This short chapter is not meant to be comprehensive of all the economic effects of foundation models. 
In addition to affecting productivity, wage inequality, and ownership, foundation models may also have significant effects on job quality and job satisfaction. 
For instance, they may increase job satisfaction by automating repetitive, uninteresting parts of work, or decrease satisfaction by increasing the pace of work, thereby inducing more frequent burnout. 
As discussed in §5.1: fairness and §5.6: ethics, they can also amplify and perpetuate bias, often in unexpected ways, or be used as a tool for reducing it. 
Foundation models can facilitate global trade and remote work, just as earlier uses of machine translation systems had significant effects in these areas [e.g., Brynjolfsson et al. 2019] . 
There may also be significant environmental effects ( §5.3: environment), as well as unexpected and unanticipated effects on the rate and direction of occupational change and business transformation in an economy. 
More broadly, given the emergent capabilities of foundation models, we should expect new unknown unknowns to arise that are difficult to predict, and which may have substantial follow-on effects. 124 
n summary, foundation models are poised to be an important general-purpose technology of our era. 
They have potential to increase living standards substantially, but also pose risks of increasing inequality and concentrating power. 
The economic implications of these technologies are not predetermined, but rather depend on how technologists, policymakers, managers, workers, and other stakeholders answer challenges such as: 
• How can we harness the potential of foundation models to boost productivity? 
• Can we develop models that enhance creativity and boost the rate of innovation? 
• Will the benefits and control rights be limited to a few or widely shared? 
Understanding the economic potential of these systems is the first step to guiding them in directions that match our values. 
5.6 Ethics of scale
Authors: Kathleen Creel, Dallas Card, Rose E. Wang, Isabelle Levent, Alex Tamkin, Armin W. Thomas, Lauren Gillespie, Rishi Bommasani, Rob Reich The widespread adoption of foundation models poses ethical, social, and political challenges in addition to concerns about increased inequity, the subject of §5.1: fairness. 
In this section, we discuss social, political, and ethical risks related to the scale of application of foundation models, such as homogenization and the concentration of power, the norms and release strategies appropriate to address them, and concerns about the broader political economy in which foundation models are developed and deployed. 
5.6.1 Homogenization and scale.
If the same model is used across a variety of domains with minimal adaptation, the strengths, weaknesses, biases, and idiosyncrasies of the original model will be amplified ( §5.1: fairness). 
This is true of the widespread adoption and reliance on any standardized technology. 
Similar to how a failure in the manufacturing of a part used in many cars or airplanes could have widespread and severe consequences across sectors, a bias or failure of service intrinsic to a foundation model could ripple outwards. 
However, the current uninterpretability ( §4.11: interpretability) of foundation models and their task-agnostic training makes predicting, understanding, and addressing these weaknesses challenging. 
If, as seems likely, foundation models become widely adopted, foundation model developers bear greater responsibilities of care than standard model developers, as their choices in design and deployment have widespread implications [Arendt 1987 ]. 
The defining feature of foundation models -their capacity to be usefully adapted for a multiplicity of tasks -is what makes them likely to be widely adopted for a vast range of socially consequential tasks. 
In contrast to the current distributed and varied model of decision making, employing many adaptations of the same foundation model for multiple automated decision-making tasks means that decision subjects may face a more homogeneous set of judgments rooted in the underlying foundation model. 
This algorithmic monoculture [Kleinberg and Raghavan 2021] could lead to consistent and arbitrary rejection, mis-classification, or ill-treatment of individual decision subjects [Gandy 2021 ]. 
We will call this homogenization [Creel and Hellman 2021] . 
For example, §4.6.2: 
data-solutions discusses data quality issues that lead to undesirable behavior on subpopulations of data, where subpopulations can be produced by any filter that stratifies the data, including by social group (see related discussions in §4.11.1: interpretability-behavior and §4.8.1: robustness-advantages). 
Until improvements are made in data quality tooling ( §4.6.2: 
data-solutions) and the ability to identify slices of data on which the model under-performs [Chung et al. 2019; Goel et al. 2021 ], a foundation model might consistently fail to provide accurate information or services to a subgroup of people (see also §4.8: robustness. 
Homogenization has the potential to amplify bias; to standardize bias, compounding injustices rather than distributing them; and to amplify arbitrary exclusion [Creel and Hellman 2021; Gandy 2021] . 
For example, Zhou et al. [2021a] have argued that BERT encodes an Anglocentric similarity metric by default, one that could be harmful if applied across contexts where foundation models are applied. 
The application of foundation models across domains has the potential to act as an epistemically and culturally homogenizing force, spreading one implicit perspective, often a socially dominant one, across multiple domains of application. 
Existing trends in standardization of training corpora are likely to be exacerbated in foundation models due to the massive scale of both unlabeled and labeled data needed. 
To the extent that models train on similar data, they are likely to acquire similar patterns of behavior, biases ( §5.1.3: 
fairnesssources), and errors. 
Previous high-effort data curation and labeling efforts such as ImageNet have standardized training corpora. 
In doing so, they have also standardized errors: models trained on ImageNet often rely on the same "spurious cues" and "shortcuts", for example using background textures like green grass to predict foreground object classes such as cows [Geirhos et al. 2020; Hendrycks et al. 2021e] . 
Despite their increased robustness to many types of distribution shifts ( §4.8.1: robustness-advantages), foundation models and other large models have been no less likely to learn spurious correlations ( §4.8.2: robustness-challenges), and are therefore likely to learn similar errors if trained on the same datasets. 
Similar effects may arise due to the choice of publicly available unlabeled data. 
Many foundation models are trained on unlabeled corpora that are chosen for their convenience and accessibility, for example public internet data [Caswell et al. 2021] , rather than their quality. 
However, publicly accessible data, whether labeled or unlabeled, is often outweighed by proprietary data in the training corpora of many proprietary foundation models, as discussed in [Marr 2017 ] and §4.6.1: 
data-desiderata. 
Therefore more research is needed on the extent to which training on similar data homogenizes correlations within foundation models and the extent to which this homogenization might cause uniform failures in adapted derivatives of the model (unless constraints are applied to eliminate the behavior during each adaptation, as discussed in §4.3.2: 
adaptation-usecases). 
Homogenization is not inevitable. 
As model developers intentionally broaden the range of perspectives represented in their datasets ( §5.1.3: 
fairness-sources), more research is needed on the capacity of foundation models to deliver a diversity of perspectives when used for generative tasks. 
For example, Sheng et al. [2021] have demonstrated that dialogue systems that adopt "personas" of specific demographic groups behave differently on measures of social bias. 
In addition to choosing between "personas" with the goal of avoiding bias, "personas" that are diverse along a variety of cognitive and demographic axes could also be used to generate a broader range of coherent outputs for generative tasks. 
There remain many open questions about how to balance diversity of outputs with relevance and utility to an individual user. 125 5.6.2 
Surveillance, exclusion, and power. 
A key premise of foundation models is that massive unlabeled datasets can be combined with vast computational resources to create a basis from which numerous products can be derived for a variety of applications. 
This paradigm shift has the potential to alter social structures and shift power, establishing or entrenching the influence of model creators [Zimmerman 2020 ]. 
We discuss three potential implications below. 
Mass data collection and surveillance. 
Whereas collecting a labeled dataset typically requires working with domain experts and understanding the problems with and limitations of such data, the need for exceptionally large amounts of data in training foundation models has encouraged some researchers to emphasize quantity rather than quality. 126 
Though preprocessing can help improve the quality of this data [e.g., Brown et al. 2020] , the scale involved necessitates automated approaches, which may be blunt or poorly documented [Dodge et al. 2021] . 
Although there is an evolving landscape of data protection legislation (e.g., GDPR in Europe), a variety of questionable practices continue to be used in acquiring data, from opaque policies [Obar and Oeldorf-Hirsch 2020] and the use of "dark patterns" (i.e., manipulative interfaces [Narayanan et al. 2020] ) to outright violation of terms of service. 
Indeed, this was essentially the strategy taken by Clearview AI -a company which scraped photos from social media, without user consent, and in violation of platforms' terms of service, for the purpose of developing facial classification software. 
The company was nevertheless able to sell this technology to police departments and other organizations, in many cases without the knowledge of state lawmakers or department heads [Mac et al. 2021 ]. 
To the extent that the paradigm of foundation models increases the value of being first to have the largest possible dataset for any particular domain, this may further encourage actors to pursue aggressive data collection, even when that pursuit is legally questionable or contrary to user expectations [Nissenbaum 2009; Zuboff 2018] . 
The importance of data to foundation models also means that organizations already engaged in widespread data collection will be in a strong position to develop such models, and will likely have incentive to maintain this advantage. 
To the extent that derivative products could themselves be used to collect additional data (e.g., in surveillance or health diagnostic applications), developers of foundation models may seek to ensure that they obtain ownership of such data. 
Thus, even though a key advantage of the foundation model paradigm is the ability to generate adapted derivatives, the developers of foundation models might seek to license their work in a way that ensures that data flows back to them from all adapted derivatives. 127 
Concentration of power. 
Although the absolute cost of computation has become dramatically cheaper over time, the training of the largest foundation models currently requires computational resources that put their development beyond the reach of all but a few institutions and organizations ( §5.3: environment). 
Thus, the question of who has access to the relevant computational resources and data will likely determine who is able to produce cutting-edge foundation models in the coming years (see also §5.5.3: economics-centralization). 
GPT-3 was at least partly an experiment in scale, showing that major gains could be achieved by scaling up the model size, amount of data, and training time, without major modeling innovations. 
Although there is extensive ongoing research into reducing the amount of resources required in training such models (see §4.2: training), OpenAI's work suggests that there are still gains to be had from even larger scale efforts [Kaplan et al. 2020] , and it seems plausible that other organizations may seek to follow this path in other domains (for example, see [Lieber et al. 2021] ). 
If scale does turn out to be critical to success, the organizations most capable of producing competitive foundation models will be the most well-resourced: venture-funded start-ups, already-dominant tech giants, and state governments. 
This raises potential concerns about market concentration, and might indicate the kind of incumbent monopoly or oligopoly that currently exists in extreme capital-intensive industries such as defense and semi-conductor manufacturing [Carril and Duggan 2020] . 
Moreover, this centralization of power raises concerns about the ability of currently-marginalized individuals and communities to participate in the foundation model development process [Kalluri 2020 ]. 
Especially within the realm of government services, the adoption of foundation models could further transfer decision making power from governments to corporate service providers, and introduce additional barriers to due process and accountability [Citron 2008 ]. 
Nevertheless, more grassroots efforts (e.g., Masakhane, EleutherAI, HuggingFace) provide encouraging alternatives, and there is extensive work on ways to incorporate participatory or value-sensitive design [Friedman and Hendry 2019; Prabhakaran and Donald Martin 2020] . 
Fueling widespread automated decision-making. 
Recent years have seen a dramatic expansion in the use of automated decision-making systems in industry and government [O'Neil 2016; Engstrom et al. 2020] . 
Although many of the concerns over such automation are not specific to foundation models, the generative abilities of models such as GPT-3, as well as the impressive performance on benchmark tasks (e.g., Devlin et al. [2019] ), have the potential to prompt a less-thancareful adoption of this technology by, for example, administrative agencies, many of which lack the expertise necessary to understand sophisticated ML systems [Calo and Citron 2021] . 
As such, it is especially important to communicate clearly about the realistic capabilities and limitations of foundation models. 
Most automated decision-making systems will exist as parts of broader sociotechnical systems in which humans play key roles [Selbst et al. 2018] . 128 
As such, there is no guarantee that even large improvements in performance on standardized evaluations will translate into the desired outcomes in the real world (especially if systems are deployed without careful consideration or ongoing evaluation). 
For example, research has shown that judges may re-impose racial prejudice in interpreting the outputs of a risk assessment system [Albright 2019 ], or otherwise impose their own biases [Stevenson and Doleac 2021] . 
Ongoing evaluation with proper ecological validity [de Vries et al. 2020] will be critical in this regard, but may not stop potentially dangerous or costly systems from being adopted without adequate evidence [Ferguson 2017 ]. 
Research is ongoing on methods of refusal: ways for individuals to opt out of participation in foundation models and their adapted derivatives, either as data or decision subjects, without repercussions [Benjamin 2016 ]. 
In short, the existing problems with algorithmic decision making will be seen in the functioning of foundation models once they are deployed. 
And to the extent that adopting foundation models accelerates a shift from human to machine decision making, foundation models accentuate the concerns with automation. 
Although there are not obvious solutions to these challenges, it is important to make questions about how foundation models will impact power part of the conversation about their creation; to communicate with civil society organizations, policy makers, and citizens about the capabilities and limitations of such systems; and to strive for broader dialogue among diverse segments of society about the adoption of such models. 
5.6.3 Norms.
Public policy and formal regulation by law ( §5.4: legality) play an essential role in creating the infrastructure for technological innovation as well as mitigating the potentially harmful effects of widely disseminated technologies. 
As illustrated by the decades-long gap between the Tuskegee Syphilis experiments and the development of research protocols and institutions like the IRB, public policy to protect human subjects and stakeholders tends to lag behind public awareness and evidence of harms to them [Grady 2015; Stark 2012; Department of Health and Welfare 1979] . 
As a result, society relies upon professional norms for responsible development and deployment and the establishment of best practices. 
Norms exist on a continuum between recommendation and requirement. 
As a nascent technology, the norms for responsible foundation model development and deployment are not yet well established at either strength of recommendation [Crootof 2019] . 
In what follows we will discuss norms for deployed models, as models for research have a wider latitude. 
Those who wish developers of foundation models to adopt certain norms might lead by example, allowing their own conduct and statements to recommend the norm. 
As discussed in §1.2: ecosystem, we believe that universities and other nonprofit institutions have an important role in modeling norms for foundation models. 
As educational institutions, universities are in the unique position to encourage the next generation of theorists and practitioners to consider the issues raised in this report and also to foster interdisciplinary conversation between researchers and students [Rogers 2021 ]. 
Universities and colleges may also contribute to the establishment of norms by auditing existing foundation models and publishing their findings, instituting ethics review boards [Bernstein et al. 2021] , and developing their own foundation models. 
To create and adopt norms will require institutionalization in funding structures, model repository, release practices, conference submission, and grant proposal requirements. 129 
For example, HuggingFace's interface currently encourages the posting of data and model cards, including discussions of bias and social impact. 130 
Since it is not required, and perhaps since data quality work is undervalued relative to its importance [Sambasivan et al. 2021] , few are filled out. 
Bias and social impact are included in ethics statements for conferences and some forms of standard evaluation (as discussed in §4.4: evaluation), but otherwise treated as optional considerations by some researchers. 
This must change. 
For some socially consequential use cases, we recommend legal standards be established that require adapted derivatives to provably exhibit certain properties ( §5.4: legality). 
Domains of special concern should be democratically decided but are likely to include allocating and distributing government services, medical diagnosis and monitoring, hiring, and lending: all contexts in which opportunities or even lives of people rest on the proper functioning of an adapted derivative. 
What norms should we promote, institutionalize, or require? 
We recommend a few here, but aim primarily to encourage dialogue about appropriate norms for the development and use of foundation models. 
Prior work has often focused on norms that advocate documentation [Gebru et al. 2018; Bender and Friedman 2018; Mitchell et al. 2019; Dodge et al. 2019] . 
Because many of the negative social consequences that appear in a downstream context may initially appear to be extrinsic or particular to a use case ( §5.1: fairness), documentation and transparency are especially important for foundation models. 
Currently, those who adapt foundation models that document the biases or other negative features of their adapted derivatives have no automatic mechanism to report their findings to the developers of the foundation model. 
Compiling multiple reports of related problems in adapted derivatives may allow the model development team to discover an intrinsic property of the model that spans multiple use cases. 
Because creators of adapted derivatives often represent different entities than from foundation model developers or providers, additional reporting structures and norms or regulation would be needed for this type of feedback to reach foundation model developers. 
Such feedback could also be made available to the general audience of model auditors, thereby making auditing and pursuing recourse more accessible. 
Public commitment to norms, standards, and creation of reporting mechanisms could also allow downstream users to submit feedback to foundation model providers. 
In order to enable this, adapted derivatives should be consistently labeled in a way that allows impacted parties to trace problems to their source. 
Significant technical and social barriers may impede this tracing in practice, such as privacy considerations and the proprietary nature of many foundation models, but without labeling it would be impossible. 
It is important that model developers and providers create mechanisms for such reporting. 
Reporting mechanisms could be informed by similar structures on current platforms, such as issue tracking on open source projects on GitHub. 
In particular, the submitted issues should be public so that other users can identify trends even if changes have not yet been made and so that developers and providers can be held accountable for unaddressed issues. 
Additional mechanisms are needed to escalate trends upwards to foundation model providers. 
Similar suggestions regarding tracking issues in training data are discussed in Dinan et al. [2021] and §4.6: data. Holland et al. [2018] suggest the nutrition label as a helpful model, drawing from labeling discussions in consumer privacy [Kelley et al. 2009] . 
A nutrition label includes both a list of the "raw" ingredients and the full nutritional information of the processed food. 
So too a model card [Mitchell et al. 2019] or nutrition label for an adapted derivative could include both a list of the "raw materials" such as training data and foundation models used, and the full "nutritional content" of the adapted derivative such as its known capacities, weaknesses, and biases. 
Reporting of the full pipeline is necessary in order for data subjects and impacted parties to trace harms to their sources. 
However, without the ability to attribute responsibility for the harm to either the adapted derivative, the foundation model, or both, and without a framework for recourse once harm has been attributed, even a successful tracing of a harm will be unlikely to lead to changes in the model (see also §5.1.4: 
fairness-recourse). 
Thus, significant technical, policy, and legal work is needed in order to develop frameworks for communicating data, model, and derivative contents to other experts and eventually to the public; to attribute responsibility for harms; and to create avenues for recourse. 
5.6.4 Release and Auditing.
In February 2019, OpenAI embarked on an experiment. 
By releasing a reduced 124M parameter GPT-2, sans datasets, they hoped to buy time: time to test for bias, time to prepare for misuse, and time for society to adapt to the presence of large language models [Solaiman et al. 2019 ]. 
Eight months later, when OpenAI released the full ∼1.5 billion parameter version, testing had exposed some but by no means all of the model's capabilities and limitations. 
When considering similar questions today, the possible harms of release, centering primarily on misuse ( §5.2: misuse), 131 must be weighed against the benefit of transparency that no closed-door testing can replicate, namely broader and independent auditing and access. 
Auditing Auditors probe the limitations of current models and suggest paths to fixing them, as well as testing the model's adapted derivatives in a wide variety of natural settings. 
A policy of open access for auditing allows more numerous and diverse researchers to investigate any model's biases, limitations, and security vulnerabilities, better informing acceptable uses of the models and calibrating appropriate trust in them [Danks 2019; Baier 1986 ]. 132 
In order to support independent audits of foundation models, model developers or third-party intermediaries could host open API access for auditors, including gradient access, and allow access to training data [Raji and Buolamwini 2019; Raji et al. 2020] . 
Foundation models trained on proprietary data in industry are unlikely to be released, and those trained on private data (as in a medical context) should not be. 
In order for proprietary models to benefit from independent audits, and for model subjects to benefit from improvements prompted by an auditing process, we recommend that audits occur during a staged release. 
While staged release may not illuminate all possible model use cases, one way to broaden the range of uncovered use cases is to enlist a neutral third party to decide which individuals or organizations should receive early access in the staged-release program. 
When model developers decide who should receive staged access, they open themselves up to charges of favoritism, selective distribution, and manipulating public perception of their product. 
A neutral "staged release board", or federal auditors, could provide a backstop against these failure modes and ensure that a wide range of auditors and users are provided access in order to capture a range of disciplinary expertise and sectors of society. 
A staged release board could also mitigate any perception that auditors would be at risk of losing their early access to the model if they share unflattering outputs, as they might be in a standard staged release process. 
Access and adaptation. 
To the extent that there are social benefits to foundation models, release of models holds the potential to further distribute them. 
Large language models such as BERT and M-BERT are capable of cross-lingual transfer, which -when the models are open-sourced -may allow for adaptation to languages which otherwise would have too few texts available [Wu and Dredze 2019; Wang et al. 2020a] . 
Given the number of languages not currently well served by commercial providers, such a benefit alone could be substantial. 
Release is not sufficient to democratize access to foundation models, as the barrier of compute power still precludes many from modifying or even loading foundation models, let alone developing their own. 
However, on each of these points we have seen significant recent technical improvement. 
Memory techniques such as the zero redundant optimizer (ZeRO) allow researchers to run and train very large models on a simple setup [Rasley et al. 2020; Rajbhandari et al. 2021 ]. 
Techniques such as distillation could allow the release of smaller, more tractable models that recoup much of the performance of their parent model while being much easier to train [Li et al. 2020d ]. 
Development of less energy-intensive training methods, as discussed in §5.3: environment, could further spread the ability to work with released models. 
Increases in efficiency such as the co-design of hardware and software are needed to train yet larger models, as discussed in §4.5: systems, but could also be used to lower the price of access to current models. 
The most powerful of the harms, by contrast, are not obviously fueled by release. 
Sophisticated or institutional actors with the capacity to embark on large-scale disinformation, cyberwarfare, or targeted phishing also are likely to have the capacity to create a similar model if none were released. 
Although potentially significant, these harms should not therefore weight heavily on a release calculus [Solaiman et al. 2019; Shevlane and Dafoe 2020] . 
The harms to be weighed against the benefits are those from less well-resourced actors who would not be able to create their own foundation model but may be motivated to generate spam or abuse, fake reviews, or cheat on tests. 
Does the benefit of release outweigh the potential for harm from actors sophisticated enough to use a released model or API but not sophisticated enough to create their own? 
We believe that the answer is yes. 
Research teams with the resources and connections necessary to develop foundation models are few in number. 
Even collectively, we are unlikely to be numerous or diverse enough to imagine all possible beneficial use cases or all possible probes that could illuminate the capability surface of a foundation model. 
5.6.5 When not to build.
The development and deployment of powerful technologies is not like gravity, an external force that acts upon us. 
Technologies reflect a set of choices made by humans; human agency shapes the technological frontier. 
It follows that technologists can choose when not to build, design, or deploy foundation models [Zimmermann 2021 ]. 
This decision need not be binary; instead, one can refuse to engage in the default way by subverting embedded values, challenging assumptions, and shaping research agendas [Simpson 2007 ]. 
Technical artifacts, foundation models included, are inherently political, so the research about them has a socio-political context, not solely a technical one. 
Developers and researchers should be cognizant of which problems they seek to address, e.g., how to scale up a foundation model versus how to make it more computationally accessible; how those problems are formulated; and who their solutions ultimately empower [Rogaway 2016; Winner 1980; Passi and Barocas 2019] . 
We should value research that seeks to make foundation models more interpretable, accessible, sustainable, and fair (see §4.11: interpretability, §5.3: environment, §5.1: fairness) . 
By asking when not to build a foundation model or adapted derivative, we are implicitly asking not only "What should we build or not build?" 
but also, "Under what conditions should a model be built?" 
and "What criteria and principles govern building?" 
The first question stems from the model view; the following questions from the ecosystem view ( §1: introduction). 
An invitation to consider refusing to build is not tantamount to saying, "Do nothing." 
It is an invitation to make deliberate and judicious choices about what is worth the time, financial resources, expertise, and energy use to build, design, and deploy. 
Ultimately, this is a difficult, moral question rooted in context and values. 
There are cases in which the application of adaptive derivatives (and algorithms and machine learning more generally) is inappropriate, because the community impacted protests or because the adaptive derivative naively exacerbates systemic issues that are better addressed with public policy, additional funding, or interdisciplinary collaborations [Angwin et al. 2016] . 
The Belmont Report, as applied to machine learning in Floridi et al. [2018] , provides one possible framework for this question. 
Drawing from the principle of "beneficence" [Department of Health and Welfare 1979], we can identify cases to reconsider building when an adaptive derivative or a research avenue might cause more harm than good or even provide no benefit at all. 
Alternatively, there may be cases in which an adaptive derivative is better at a task on a metric of efficiency, performance, and generalization, values prioritized in the machine learning community [Birhane et al. 2020 ], but an individual, community, or organization might choose to prioritize an existing solution that highlights other values such as human connection and interpretability [Benjamin 2016 ]. 133 
In doing so, they exercise their autonomy -as explained in the Belmont Report's "respect for persons" -in deciding that this is not an appropriate context in which to build [Department of Health and Welfare 1979] . 
Answering the question of when not to build is a matter of individual responsibility as well as a broader professional responsibility. 
The decision not to build something by one person, or one team, or one company, invites the reply, "But if we don't build this, someone else will, and they may likely do it worse. 
" A simple utilitarian weighing of comparative harms of the outcomes of the two models misses the importance of integrity. 
It matters very much whether we are the ones building the bad model or whether someone else is [Williams 1973 ]. 
Individuals have reasons not to build something that goes against their values or that they cannot endorse as right to build [Korsgaard 2009 ]. 
However, the structural environment so created is different. 
If even one company decides to build the most effective version of an ethically-dubious model, they open the door for other companies to consider similar avenues of research; they make it competitively disadvantageous not to pursue the research [Askell et al. 2019] . 
When not to build is then a collective question as much as it is an individual one, requiring the community to adhere to codes of professional ethics and responsibility. 
In the AI/ML community this infrastructure is underdeveloped compared to other fields such as the medical field. 
Although professional bodies like the Association for Computing Machinery (ACM) have ethics statements, both industry and academia lack widely used and accepted professional oaths (e.g., the Hippocratic Oath or the the Obligation of the Engineer), regulatory bodies involved in deployment and research (e.g., the FDA for drugs), and official protocols for ethics review (e.g., the IRB for research involving human subjects; [Bernstein et al. 2021] ). 
The ability to opt-out can be incorporated into the foundation model ecosystem at many stages, including during data production, adaptation, and deployment. 
As the norm veers towards collecting larger and larger swaths of training data ( §4.6: data), we should endeavor to maintain a "respect for persons, " [Department of Health and Welfare 1979] emphasizing privacy and consent as part of the data life cycle. 
This would require innovation in data management and a more concrete understanding -technically and philosophically -of informed consent online, ways of documenting and ensuring that consent is respected, and privacy (see §4.6: data for a specific data management proposal; [Ohm 2014] ). 
Although data and foundation models are diverse in their applications, data participants should be able to indicate how they do not want to have their data used. 
An opt-out consent model favors developers, as it does not require them to to get consent for each new, unexpected use case. 
Important then is the right to revoke consent given vacuously for applications that are now being pursued, but were not when consent was originally given. 
5.6.6 Conclusion.
In this section, we have surveyed some of the risks to society that accompany the widespread adoption of foundation models, such as the homogenization of outcomes and centralization of power. 
Developers of foundation models should adopt norms regarding development, auditing, and release of foundation models in order to address these risks, aided by legislative requirements, and individuals should be able refuse to be data or decision subjects of foundations models without repercussion. 
Many implications of foundation models' generative and interactive capacities remain unsurveyed here. 
For example, §5.5: economics discusses the potential gains to economic productivity from the automation of creative and design work. 
However, in virtue of their generative nature, foundation models may replace work that many people find meaningful and fulfilling, such as graphic design and writing. 
We hope that the scope of this report will aid others in their pursuit of the questions of ethics and society unaddressed here. 
6 CONCLUSION
In this report, we have endeavored to comprehensively discuss many of the most critical aspects of foundation models, ranging from their technical underpinnings to their societal consequences. 
In this way, we acknowledge the unusual approach taken: we have attempted to clarify the nature of a paradigm that may only have just begun, rather than waiting for more to unfold or the dust to settle. 
Therefore, much still remains unclear in spite of our efforts and we reiterate that this is just the beginning of a paradigm shift: foundation models have only just begun to transform the way AI systems are built and deployed in the world. 
Moving forward, we view this document as serving an important role in orienting and framing dialogue on these models and this new paradigm in AI. 
That said, to ensure the responsible development and deployment of these models on durable foundations, we envision collaboration between different sectors, institutions, and disciplines from the onset to be especially critical. 
Fig.2. 
A foundation model can centralize the information from all the data from various modalities. 
This one model can then be adapted to a wide range of downstream tasks. 
Fig.3. 
Before reasoning about the social impact of foundation models, it is important to understand that they are part of a broader ecosystem that stretches from data creation to deployment. 
At both ends, we highlight the role of people as the ultimate source of data into training of a foundation model, but also as the downstream recipients of any benefits and harms. 
Thoughtful data curation and adaptation should be part of the responsible development of any AI system. 
Finally, note that the deployment of adapted foundation models is a decision separate from their construction, which could be for research. 
Fig. 5. 
Only a tiny percentage of the world's languages are currently represented in foundation models.There are over 6,000 languages in the world, with estimates varying due to the inherent uncertainty of what constitutes a separate language [Nordhoff and Hammarström 2011] . 
This map shows the languages of the world, with each dot representing one language and its color indicating the top-level language family. 
Data is from Glottolog [Hammarström et al. 2021 ]. 
We label a few of the languages on the map as examples. 
Fig. 10. 
Left: A reaction route for 1,6-Heptadiene-3,5-dione predicted by machine learning-based drug retrosynthesis planner AiZynthFinder [Genheden et al. 2020; Yoshikawa et al. 2021]. 
Right: A sample proof tree in propositional logic where the formulas outlined in green represent axioms. 
Although they arise from different domains, both trees are structurally the same. 
Fig. 13. 
An example of various steps of a civil case in the United States and where foundation models might help. 
At each stage different modalities might need to be processed and adaptation is needed to a new court or legal perspective. 
Fig. 14. 
An extract from a fictional brief written by one of the authors of this work. 
The prototypical form that law students are instructed to write a brief involves: (1) introducing the argument; (2) stating the legal rule in a persuasive manner; (3) applying the legal rule to the facts of the case; (4) persuasively concluding the argument. 
This often involves information retrieval and paraphrasing from both prior cases and the facts of the current case. 
Fig. 17. 
The five key properties of a foundation model: expressivity -to flexibly capture and represent rich information; scalability -to efficiently consume large quantities of data; multimodality -to connect together various modalities and domains; memory capacity -to store the vast amount of accumulated knowledge; and compositionality -to generalize to new contexts, tasks and environments. 
Fig. 17. 
The five key properties of a foundation model: expressivity -to flexibly capture and represent rich information; scalability -to efficiently consume large quantities of data; multimodality -to connect together various modalities and domains; memory capacity -to store the vast amount of accumulated knowledge; and compositionality -to generalize to new contexts, tasks and environments. 
Fig. 18. 
During adaptation, a foundation model is converted into an adapted model (bottom row) in order to reflect updated information, desired behaviors, or deployment constraints. 
Fig. 19. 
Plot showing the growth of number of parameters and number of training operations (FLOPs) of transformer-based language models (shown in blue), and memory capacity and peak device throughput of NVIDIA P100, V100, and A100 GPUs (shown in red) with time. 
The rate of growth (slope of each line) of state-of-the-art language models (roughly 10× a year) far exceeds the rate of increase in computational capacity of hardware (roughly 10× in four years), motivating the need for parallelism across a large number of accelerators and co-design of algorithms, models, software, and hardware to drive further progress. 
Number of parameters and number of training operations are obtained from relevant papers [Brown et al. 2020], and memory capacities and peak throughputs are obtained from GPU specification sheets. 
Fig. 20. 
Risks and opportunities raised by foundation models for security and privacy of ML systems. 
Fig. 22. 
The analysis of foundation models from pretraining on diverse data to downstream performance on adapted tasks involves capturing the relation between different loss terms as shown above. 
The main challenge is to analyze the highlighted pretraining-adaptation interface which requires reasoning carefully about the population losses in addition to the model architecture, losses and data distributions of the pretraining and adaptation stages ( §4.10.2: 
theory-interface). 
Analysis of generalization and optimization largely reduces to their analysis in standard supervised learning. 
Theoretical formulations and modularizations. 
Challenge: appropriate data assumptions and mathematical tools. 
Fig.23. 
The one model-many model nature of foundation models: A central interpretability question is to understand where a foundation model lies on the spectrum between one model and many models. 
As one model, behavior can be made interpretable by identifying and characterising the finite number of generalizable model mechanisms used to produce behaviors across tasks (e.g., mechanisms that assign meaning to words, compare quantities, and perform arithmetic). 
As many models, explanations of model behavior in one task are not necessarily informative about behavior in other tasks, thus requiring the independent study of behavior in each task. 
Fig. 25. 
This figure shows the effect foundation models will have on manipulative and harmful content generation, and the implications for detection. 
Fig. 26. 
A visualization of a cost-benefit analysis for deploying a foundation model. 
The total value of a model can be approximated by first considering the net positive social benefits of the model, as well as any environmental benefits. 
Then, we subtract the negative energy costs to train and deploy the model, the social cost of the carbon emitted to train the model, and the secondary environmental effects. 
If the net costs outweigh the benefits, then foundation model developers and large-scale deployers should consider harm reduction strategies. 
This could include deploying a more efficient model or not deploying the model at all. 
; Patterson et al. 2021; Lannelongue et al. 2021; Parcollet and Ravanelli 2021]. 
Addressing such emissions is an imperative: current forecasts show that climate change is occurring more rapidly than previously thought [Masson-Delmotte et al. 2021]. 
Fig. 27. 
A hypothetical example of amortized fine-tuning showing the point at which a foundation model (in this case BERT Base) will have lower energy costs than a transformer model trained from scratch. 
We estimate the up-front energy cost for training BERT from Strubell et al. [2019], and cost for fine-tuning a downstream task from Chaudhary et al. [2020]. 
We compare against the linearly increasing cost of training a transformer from scratch, from Strubell et al. [2019]. 
If BERT is used for less than ∼80 tasks, the up-front energy costs are not recovered. 
After that point, BERT is more energy efficient than the model trained from scratch. 
[Ramsundar et al. 2015;Camacho et al. 2018;Duran-Frigola et al. 2020;al. 
2020molecule requires years of expensive wet lab experiments[Hughes et al. 2011;Schenone et al. 2013;Schneider 2018].Foundation models' generativity can improve the search space and efficiency (see §2.4: reasoning), which not only reduces the amount of experiments but also helps to discover new and better drugs[Jin et al. 2018;You et al. 2018;Walters and Barzilay 2020;Stokes et al. 2020].Moreover, the simultaneous solution of related drug discovery problems (i.e., target identification, efficacy prediction, side effect prediction, and others) by a single foundation model may improve the solutions to each of them[Ramsundar et al. 2015;Camacho et al. 2018;Duran-Frigola et al. 2020; Huang et al. 2021a]. 
As an example, one area where foundation models have shown significant potential for impacting therapeutic design is the modeling of proteins using language models. 
Ashley 2016]. 
For instance, given a set of drugs and a patient genome, foundation models may help predict which drug is likeliest to treat the patient with minimal side effects 
[Harrer et al. 2019als study efficacy and safety of treatment or drug candidates.Conventional clinical trials are inefficient and costly: 80% of trials fail due to inability to show efficacy/safety or problems with patient matching[Ali et al. 2020; Liu et al. 2021c].Foundation models can help in the following: predicting potential failures and design promising clinical trial protocols (e.g., patient eligibility criteria) based on existing studies; and automating matching of eligible patients based on patient individual profiles, which are multimodal data including EHRs, gene sequence, etc.[Harrer et al. 2019]. 
[Precup et al. 2000;Lange et al. 2012;Ajay et al. 2021;Yang and Nachum 2021;Schwarzer et al. 2021;Pathak et al. 2017;Haber et al. 2018] the data influence the training signal, 56 but the training algorithm itself could adaptively seek out or construct richer training examples as the model improves to accelerate learning[Tamkin et al. 2021c].Stiennon et al. [2020]).Instead, one might imagine more sophisticated versions of current methods which acquire a diverse range of real-world capabilities from raw online[Klyubin et al. 2005;Singh et al. 2005;Salge et al. 2013;Shakir Mohamed 2015;Florensa et al. 2017;Pathak et al. 2017;Haber et al. 2018]or offline[Precup et al. 2000;Lange et al. 2012;Ajay et al. 2021;Yang and Nachum 2021;Schwarzer et al. 2021] interactions, without the need for human annotations or task construction. 
Such methods might use techniques quite similar to existing SSL algorithms: 
