Visual Saliency Transformer
Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. 
Alternatively, we rethink this task from a convolution-free sequence-tosequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. 
Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). 
It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. 
Unlike conventional architectures used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. 
We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism. 
Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. 
Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models. 
Code is available at https://github.com/nnizhang/VST. 
1. Introduction
SOD aims to detect objects that attract peoples' eyes and can help many vision tasks, e.g., [58, 19] . 
Recently, RGB-D SOD has also gained growing interest with the extra spatial structure information from the depth data. 
Current stateof-the-art SOD methods are dominated by convolutional architectures [28] , on both RGB and RGB-D data. 
They often adopt an encoder-decoder CNN architecture [47, 57] , where the encoder encodes the input image to multi-level features and the decoder integrates the extracted features to predict the final saliency map. 
Based on this simple architecture, most efforts have been made to build a powerful decoder for predicting better saliency results. 
To this end, they introduced various attention models [37, 80, 7] , multi-scale feature integration methods [24, 49, 16, 43] , and multi-task learning frameworks [67, 77, 82, 69, 25 ]. 
An additional demand for RGB-D SOD is to effectively fuse cross-modal information, i.e., the appearance information and the depth cues. 
Existing works propose various modality fusion methods, such as feature fusion [22, 4, 16, 18, 89] , knowledge distillation [53] , dynamic convolution [48] , attention models [31, 78] , and graph neural networks [43] . 
Hence, CNNbased methods have achieved impressive results [66, 88] . 
However, all previous methods are limited in learning global long-range dependencies. 
Global contexts [21, 83, 56, 44, 37] and global contrast [75, 2, 8] have been proved crucial for saliency detection for a long time. 
Nevertheless, due to the intrinsic limitation of CNNs that they extract features in local sliding windows, previous methods can hardly exploit the crucial global cues. 
Although some methods utilized fully connected layers [36, 22] , global pooling layers [44, 37, 65] , and non-local modules [38, 7] to incorporate the global context, they only did such in certain layers and the standard CNN-based architecture remains unchanged. 
Recently, Transformer [61] was proposed to model global long-range dependencies among word sequences for machine translation. 
The core idea is the self-attention mechanism, which leverages the query-key correlation to relate different positions in a sequence. 
Transformer stacks the self-attention layers multiple times in both encoder and decoder, thus can model long-range dependencies in every layer. 
Hence, it is natural to introduce the Transformer to SOD, leveraging the global cues in the model all the way. 
In this paper, for the first time, we rethink SOD from a new sequence-to-sequence perspective and develop a novel unified model for both RGB and RGB-D SOD based on a pure transformer, which is named Visual Saliency Transformer. 
We follow the recently proposed ViT models [12, 74] to divide each image into patches and adopt the Transformer model on the patch sequence. 
Then, the Transformer propagates long-range dependencies between image patches, without any need of using convolution. 
However, arXiv:2104.12099v2 
[cs.CV] 23 Aug 2021 it is not straightforward to apply ViT for SOD. 
On the one hand, how to perform dense prediction tasks based on pure transformer still remains an open question. 
On the other hand, ViT usually tokenizes the image to a very coarse scale. 
How to adapt ViT to the high-resolution prediction demand of SOD is also unclear. 
To solve the first problem, we design a token-based transformer decoder by introducing task-related tokens to learn decision embeddings. 
Then, we propose a novel patch-task-attention mechanism to generate denseprediction results, which provides a new paradigm for using transformer in dense prediction tasks. 
Motivated by previous SOD models [82, 87, 79, 25] that leveraged boundary detection to boost the SOD performance, we build a multi-task decoder to simultaneously conduct saliency and boundary detection by introducing a saliency token and a boundary token. 
This strategy simplifies the multitask prediction workflow by simply learning task-related tokens, thus largely reduces the computational costs while obtaining better results. 
To solve the second problem, inspired by the Tokens-to-Token (T2T) transformation [74] , which reduces the length of tokens, we propose a new reverse T2T transformation to upsample tokens by expanding each token into multiple sub-tokens. 
Then, we upsample patch tokens progressively and fuse them with low-level tokens to obtain the final full-resolution saliency map. 
In addition, we also use a cross modality transformer to deeply explore the interaction between multi-modal information for RGB-D SOD. 
Finally, our VST outperforms existing state-of-the-art SOD methods with a comparable number of parameters and computational costs, on both RGB and RGB-D data. 
Our main contributions can be summarized as follows: 
• For the first time, we design a novel unified model based on the pure transformer architecture for both RGB and RGB-D SOD, from a new perspective of sequence-to-sequence modeling. 
• We design a multi-task transformer decoder to jointly conduct saliency and boundary detection by introducing task-related tokens and patch-task-attention. • We propose a new token upsampling method for transformer-based framework. 
• Our proposed VST model achieves state-of-the-art results on both RGB and RGB-D SOD benchmark datasets, which demonstrates its effectiveness and the potential of transformer-based models for SOD. 
2. Related Work
2.1. Deep Learning Based SOD
CNN-based approaches have become a mainstream trend in both RGB and RGB-D SOD and achieved promising performance. 
Most methods [24, 65, 49, 84, 16] leveraged a multi-level feature fusion strategy by using UNet 
[57] or HED-style [71] network structures. 
Some works introduced the attention mechanism to learn more discriminative features, including spatial and channel attention [52, 80, 16, 7] or pixel-wise contextual attention [37] . 
Other works [36, 64, 11, 42, 6] tried to design recurrent networks to refine the saliency map step-by-step. 
In addition, some works introduced multi-task learning, e.g., fixation prediction [67] , image caption [77] , and edge detection [54, 82, 69, 79, 25] to boost the SOD performance. 
As for RGB-D SOD, many methods have designed various models to fuse RGB and depth features and obtained significant results. 
Some models [4, 5, 18] adopted simple feature fusion methods, i.e., concatenation, summation, or multiplication. 
Some others [81, 30, 52, 31] leveraged the depth cues to generate spatial or channel attention to enhance the RGB features. 
Besides, dynamic convolution [48] , graph neural networks [43] , and knowledge distillation [53] were also adopted to implement multi-modal feature fusion. 
In addition, [38, 39, 7] adopted the cross-attention mechanism to propagate long-range cross-modal interactions between RGB and depth cues. 
Different from previous CNN-based methods, we are the first to rethink SOD from a sequence-to-sequence perspective and propose a unified model based on pure transformer for both RGB and RGB-D SOD. 
In our model, we follow [54, 82, 69, 79, 25] to leverage boundary detection to boost the SOD performance. 
However, different from these CNNbased models, we design a novel token-based multitask decoder to achieve this goal under the transformer framework. 
2.2. Transformers in Computer Vision
Vaswani et al. [61] first proposed a transformer encoderdecoder architecture for machine translation, where multihead self-attention and point-wise feed-forward layers are stacked multiple times. 
Recently, more and more works have introduced the Transformer model to various computer vision tasks and achieved excellent results. 
Some works combined CNNs and transformers into hybrid architectures for object detection [3, 91] , panoptic segmentation [62] , lane shape prediction [40] , and so on. 
Typically, they first use CNNs to extract image features and then leverage the Transformer to incorporate long-range dependencies. 
Other works design pure transformer models to process images from the sequence-to-sequence perspective. 
ViT [12] divided each image into a sequence of flattened 2D patches and then adopted the Transformer for image classification. 
Touvron et al. 
[60] introduced a teacher-student strategy to improve the data-efficiency of ViT and Wang et al. [68] proposed a pyramid architecture to adapt ViT for dense prediction tasks. 
T2T-ViT [74] adopted the T2T module to model local structures, thus generating multiscale token features. 
In this work, we adopt T2T-ViT as the backbone and propose a novel multitask decoder and a reverse Then, a convertor is adopted to convert the patch tokens to the decoder space, and also performs cross-modal information fusion for RGB-D data. 
Finally, a decoder simultaneously predicts the saliency map and the boundary map via the proposed task-related tokens and the patch-task-attention mechanism. 
An RT2T transformation is also proposed to progressively upsample patch tokens. 
The dotted line represents exclusive components for RGB-D SOD. 
T2T token upsampling method. 
It is noteworthy that our usage of task-related tokens is different from previous models. 
In [12, 60] , the class token is directly used for image classification via adopting a multilayer perceptron on the token embedding. 
However, we can not obtain dense prediction results directly from a single task token. 
Thus, we propose to perform patch-task-attention between patch tokens and the task tokens to predict saliency and boundary maps. 
We believe our strategy will also inspire future transformer models for other dense prediction tasks. 
Another related work to ours is [86] , which introduces transformer into the semantic segmentation task. 
The authors adopted a vision transformer as a backbone and then reshaped the token sequences to 2D image features. 
Then, they predicted full-resolution segmentation maps using convolution and bilinear upsampling. 
Their model still falls into the hybrid architecture category. 
In contrast, our model is a pure transformer architecture and does not rely on any convolution operation and bilinear upsampling. 
3. Visual Saliency Transformer
Figure 1 shows the overall architecture of our proposed VST model. 
The main components include a transformer encoder based on T2T-ViT, a transformer convertor to convert patch tokens from the encoder space to the decoder space, and a multi-task transformer decoder. 
3.1. Transformer Encoder
Similar to other CNN-based SOD methods, which often utilize pretrained image classification models such as VGG [59] and ResNet [23] as the backbone of their encoders to extract image features, we adopt the pretrained T2T-ViT [74] model as our backbone, as detailed below. 
3.1.1 Tokens to Token
Given a sequence of patch tokens T with length l from the previous layer, T2T-ViT iteratively applies the T2T module, which is composed of a re-structurization step and a soft split step, to model the local structure information in T and obtain a new sequence of tokens. 
Re-structurization. 
As shown in Figure 2 (a), the tokens T is first transformed using a transformer layer to obtain new tokens T ∈ R l×c : 
where MSA and MLP denote the multi-head self-attention and multilayer perceptron in the original Transformer [61] , respectively. 
Note that layer normalization [1] is applied before each block. 
Then, T is reshaped to a 2D image I ∈ R h×w×c , where l = h × w, to recover spatial structures, as shown in Figure 2 (a). 
Soft split. 
After the re-structurization step, I is first split into k×k patches with s overlapping. 
p zero-padding is also utilized to pad image boundaries. 
Then, the image patches are unfolded to a sequence of tokens 
, where the sequence length l o is computed as: 
(2) Different from ViT [12] , the overlapped patch splitting adopted in T2T-ViT introduces local correspondence within neighbouring patches, thus bringing spatial priors. 
The T2T transformation can be conducted iteratively multiple times. 
In each time, the re-structurization step first transforms previous token embeddings to new embeddings and also integrates long-range dependencies within all tokens. 
Then, the soft split operation aggregates the tokens in each k × k neighbour into a new token, which is ready to use for the next layer. 
Furthermore, when setting s < k -1, the length of tokens can be reduced progressively. 
We follow [74] to first soft split the input image into patches and then adopt the T2T module twice. 
Among the three soft split steps, the patch sizes are set to k = [7, 3, 3], the overlappings are set to s = [3, 1, 1], and the padding sizes are set to p = [2, 1, 1]. 
As such, we can obtain multilevel tokens 
Given the width and height of the input image as H and W , respectively, then , and
We follow [74] to set c = 64 and use a linear projection layer on T 3 to transform its embedding dimension from c to d = 384. 
3.1.2 Encoder with T2T-ViT Backbone
The final token sequence T 3 is added with the sinusoidal position embedding [61] to encode 2D position information. 
Then, L E transformer layers are used to model longrange dependencies among T 3 to extract powerful patch token embeddings T E ∈ R l3×d . 
For RGB SOD, we adopt a single transformer encoder to obtain RGB encoder patch tokens T E r ∈ R l3×d from each input RGB image. 
For RGB-D SOD, we follow two-stream architectures to further use another transformer encoder to extract the depth encoder patch tokens T E d from the input depth map in a similar way, as shown in Figure 1 . 
3.2. Transformer Convertor
We insert a convertor module between the transformer encoder and decoder to convert the encoder patch tokens T E * from the encoder space to the decoder space, thus obtaining the converted patch tokens T C ∈ R l3×d . 
3.2.1 RGB-D Convertor
We fuse T E r and T E d in the RGB-D converter to integrate the complementary information between the RGB and depth data. 
To this end, we design a Cross Modality Transformer (CMT), which consists of L C alternating cross-modalityattention layers and self-attention layers. 
Cross-modality-attention. 
Under the pure transformer architecture, we modify the standard self-attention layer to propagate long-range cross-modal dependencies between the image and depth data, thus obtaining the crossmodality-attention, which is detailed as follows. 
First, similar with the self-attention in [61] , 
, and values V r ∈ R l3×d through three linear projections. 
Similarly, we can obtain the depth queries Q d , keys K d , and values V d from T E d . 
Next, we compute the "Scaled Dot-Product Attention" [61] between the queries from one modality with the keys from the other modality. 
Then, the output is computed as a weighted sum of the values, formulated as: 
(3) 
We follow the standard Transformer architecture in [61] and adopt the multi-head attention mechanism in the crossmodality-attention. 
The same positionwise feed-forward network, residual connections, and layer normalization [1] are also used, forming our CMT layer. 
After each adoption of the proposed CMT layer, we use one standard transformer layer on each RGB and depth patch token sequence, further enhancing their token embeddings. 
After alternately using CMT and transformer for L C times, we fuse the obtained RGB tokens and depth tokens by concatenation and then project them to the final converted tokens T C , as shown in Figure 1 . 
3.2.2 RGB Convertor
To align with our RGB-D SOD model, for RGB SOD, we simply use L C standard transformer layers on T E r to obtain the converted patch token sequence T C . 
3.3. Multi-task Transformer Decoder
Our decoder aims to decode the patch tokens T C to saliency maps. 
Hence, we propose a novel token upsam-pling method with multi-level token fusion and a tokenbased multi-task decoder. 
3.3.1 Token Upsampling and Multi-level Token Fusion
We argue that directly predicting saliency maps from T C can not obtain high-quality results since the length of T C is relatively small, i.e., l 3 = H 16 × W 16 , which is limited for dense prediction. 
Thus, we propose to upsample patch tokens first and then conduct dense prediction. 
Most CNNbased methods [84, 82, 38, 18] adopt bilinear upsampling to recover large scale feature maps. 
Alternatively, we propose a new token upsampling method under the transformer framework. 
Inspired by the T2T module [74] that aggregates neighbour tokens to reduce the length of tokens progressively, we propose a reverse T2T (RT2T) transformation to upsample tokens by expanding each token into multiple sub-tokens, as shown in Figure 2(b) . 
Specifically, we first project the input patch tokens to reduce their embedding dimension from d = 384 to c = 64. 
Then, we use another linear projection to expand the embedding dimension from c to ck 2 . 
Next, similar to the soft split step in T2T, each token is seen as a k × k image patch and neighbouring patches have s overlapping. 
Then, we can fold the tokens as an image using p zero-padding. 
The output image size can be computed using (2) reversely, i.e., given the length of the input patch tokens as h o × w o , the spatial size of the out image is h × w. 
Finally, we reshape the image back to the upsampled tokens with size l o × c, where l o = h × w. 
By setting s < k -1, the RT2T transformation can increase the length of the tokens. 
Motivated by T2T-ViT, we use RT2T three times and set k = [3, 3, 7], s = [1, 1, 3], and p = [1, 1, 3]. 
Thus, the length of the patch tokens can be gradually upsampled to H × W , equaling to the original size of the input image. 
Furthermore, motivated by the widely proved successes of multi-level feature fusion in existing SOD methods [24, 49, 84, 16, 43] , we leverage low-level tokens with larger lengths from the T2T-ViT encoder, i.e., T 1 and T 2 , to provide accurate local structural information. 
For both RGB and RGB-D SOD, we only use the low-level tokens from the RGB transformer encoder. 
Concretely, we progressively fuse T 2 and T 1 with the upsampled patch tokens via concatenation and linear projection. 
Then, we adopt one transformer layer to obtain the decoder tokens T D i at each level i, where i = 2, 1. 
The whole process is formulated as: 
where [, ] means concatenation along the token embedding dimension. 
"Linear" means linear projection to reduce the embedding dimension after the concatenation to c. Finally, we use another linear projection to recover the embedding dimension of T D i back to d. 
3.3.2 Token Based Multi-task Prediction
Inspired by existing pure transformer methods [74, 12] , which add a class token on the patch token sequence for image classification, we also leverage task-related tokens to predict results. 
However, we can not obtain dense prediction results by directly using MLP on the task token embedding, as done in [74, 12] . 
Hence, we propose to perform patch-task-attention between the patch tokens and the taskrelated token to perform SOD. 
In addition, motivated by the widely used boundary detection in SOD models [82, 69, 79, 25] , we also adopt the multi-task learning strategy to jointly perform saliency and boundary detection, thus using the latter to help boost the performance of the former. 
To this end, we design two task-related tokens, i.e., a saliency token t s ∈ R 1×d and a boundary token t b ∈ R 1×d . 
At each decoder level i, we add the saliency and boundary tokens t s and t b on the patch token sequence T D i , and then process them using L D i transformer layers. 
As such, the two task tokens can learn image-dependent task-related embeddings from the interaction with the patch tokens. 
After this, we take the updated patch tokens as input and perform the token upsampling and multi-level fusion process in (4) to obtain upsampled patch tokens T D i-1 . 
Next, we reuse the updated t s and t b in the next level i -1 to further update them and T D i-1 . 
We repeat this process until we reach the last decoder level with the 1 4 scale. 
For saliency and boundary prediction, we perform patchtask-attention between the final decoder patch tokens T D 1 and the saliency and boundary tokens t s and t b . 
For saliency prediction, we first embed T D 1 to queries Q D s ∈ R l1×d and embed t s to a key K s ∈ R 1×d and a value V s ∈ R 1×d . 
Similarly, for boundary prediction, we embed T D 1 to Q D b and embed t b to K b and V b . 
Then, we adopt the patch-task-attention to obtain the task-related patch tokens: 
Here we use the sigmoid activation for the attention computation since in each equation we only have one key. 
Since T D s and T D b are at the 1 4 scale, we adopt the third RT2T transformation to upsample them to the full resolution. 
Finally, we apply two linear transformations with the sigmoid activation to project them to scalars in [0, 1], and then reshape them to a 2D saliency map and a 2D boundary map, respectively. 
The whole process is given in Figure 1 . 
4. Experiments
4.1. Datasets and Evaluation Metrics
For RGB SOD, we evaluate our VST model on six widely used benchmark datasets, including ECSSD [72] Table 1 . 
Ablation studies of our proposed model. 
"Bili" denotes bilinear upsampling. 
"F" means multi-level token fusion. 
"TMD" denotes our proposed token-based multi-task decoder, while "C2D" means using conventional two-stream decoder to perform saliency and boundary detection without using task-related tokens. 
The best results are labeled in blue. 
We adopt four widely used evaluation metrics to evaluate our model performance comprehensively. 
Specifically, Structure-measure S m [13] evaluates region-aware and object-aware structural similarity. 
Maximum F-measure (maxF) jointly considers precision and recall under the optimal threshold. 
Maximum enhanced-alignment measure E max ξ [14] simultaneously considers pixel-level errors and image-level errors. 
Mean Absolute Error (MAE) computes pixel-wise average absolute error. 
To evaluate the model complexity, we also report the multiply accumulate operations (MACs) and the number of parameters (Params). 
Settings NJUD [26] DUTLF-Depth [52] STERE [46] LFSD [33]
4.2. Implementation Details
For fair comparisons, we follow most previous methods to use the training set of DUTS to train our VST for RGB SOD and use 1,485 images from NJUD, 700 images from NLPR, and 800 images from DUTLF-Depth to train our VST for RGB-D SOD. 
We follow [82] to use a sober operator to generate the boundary ground truth from GT saliency maps. 
For depth data preprocessing, we normalize the depth maps to [0,1] and duplicate them to three channels. 
Finally, we resize each image or depth map to 256 × 256 pixels and then randomly crop 224 × 224 image regions as the model input and use random flipping as data augmentation. 
We use the pre-trained T2T-ViT t -14 [74] model as our backbone since it has similar computational complexity as ResNet50 [23] does. 
This model uses the efficient Performer [10] and c = 64 in T2T modules, and sets L E = 14. 
In our convertor and decoder, we set L C = L D 3 = 4 and L D 2 = L D 1 = 2 according to experimental results. 
We set the batchsizes as 11 and 8, and the total training steps as 40,000 and 60,000, for RGB and RGB-D SOD, respectively. 
For both of them, Adam [27] is adopted as the op-timizer and the binary cross entropy loss is used for both saliency and boundary prediction. 
The initial learning rate is set to 0.0001 and reduced by a factor of 10 at half and three-quarters of the total step, respectively. 
Deep supervision is also used to facilitate the model training, where we use the patch-task attention to predict saliency and boundary at each decoder level. 
We implemented our model using Pytorch [50] and trained it on a GTX 1080 Ti GPU. 
4.3. Ablation Study
Since our RGB-D VST is built by adding one more transformer encoder and additional CMT based on our RGB VST, while the other parts of the two models are the same, we conduct ablation studies based on our RGB-D VST to verify all of our proposed model components. 
The experimental results on four RGB-D SOD datasets, i.e., NJUD, DUTLF-Depth, STERE, and LFSD, are given in Table 1 . 
We remove the transformer convertor and the decoder from our RGB-D VST as the baseline model. 
Specifically, it uses the two-stream transformer encoder to extract RGB encoder patch tokens T E r and the depth encoder patch tokens T E d , and then directly concatenate them and predict the saliency map with 1/16 scale by using MLP on each patch token. 
Effectiveness of CMT.
For cross-modal information fusion, we deploy our proposed CMT right after the transformer encoder to substitute the concatenation fusion method in the baseline model, shown as "+CMT" in Table 1 . 
Compared to the baseline, CMT brings performance gain especially on the NJUD and LFSD datasets, hence demonstrating its effectiveness. 
Effectiveness of RT2T. 
Based on "+CMT" model, we further simply use bilinear upsampling ("+CMT+Bili") to progressively upsample tokens to the full resolution and then predict the saliency map. 
The results show using bilinear upsampling to increase the resolution of the saliency map can largely improve the model performance. 
Then, we replace bilinear upsampling with our proposed RT2T token upsampling method ("+CMT+RT2T"). 
We find that RT2T leads to obvious performance improvement compared with using bilinear upsampling, which verifies its effectiveness. 
Effectiveness of multi-level token fusion. 
We progressively fuse T 1 and T 2 in our decoder ("+CMT+RT2T+F") to supply low-level fine-grained information. 
We find that this strategy further improves the model performance. 
Hence, leveraging low-level tokens in transformer is as important as fusing low-level features in CNN-based models. 
Effectiveness of the multi-task transformer decoder. 
Based on "+CMT+RT2T+F", we further use our tokenbased multi-task decoder (TMD) to jointly perform saliency and boundary detection ("+CMT+RT2T+F+TMD"). 
It shows that using boundary detection can bring further performance gain for SOD on three out of four datasets. 
To very the effectiveness of our token-based prediction scheme, we try to directly use a conventional two-stream decoder (C2D) by using the "+RT2T+F" architecture twice to predict the saliency map and boundary map via MLP, without using task-related tokens. 
This model is denoted as "+CMT+RT2T+F+C2D" in Table 1 . 
The parameters and MACs of TMD vs. C2D are 17.22 M vs. 20.35 
M and 17.70 G vs. 28.27 
G, respectively. 
The results show that using our TMD can achieve better results than using C2D on three out of four datasets, and also with much less computational costs. 
This clearly demonstrates the superiority of our proposed token-based transformer decoder. 
4.4. Comparison with State-of-the-Art Methods
For RGB-D SOD, we compare our VST with 14 stateof-the-art RGB-D SOD methods, i.e., A2dele [53] , JL-DCF [18] , SSF-RGBD [79] , UC-Net [76], S 2 MA [38] , PGAR [6], DANet [85] , cmMS [29], ATSA [78] , CMW [31] , Cas-Gnn [43] , HDFNet [48] , CoNet [25] , and BBS-Net [16] . 
For RGB SOD, we compare our VST with 12 state-of-theart RGB SOD models, including GateNet [84] , CSF [20] , LDF [69] , MINet [49] , ITSD [87] , EGNet [82] , TSPOANet [41], AFNet [17] , PoolNet [35] , CPD [70] , BASNet [55], and PiCANet [37] . 
Table 2 and Table 3 show the quantitative comparison results for RGB-D and RGB SOD, respec- tively. 
The results show that our VST outperforms all previous state-of-the-art CNN-based SOD models on both RGB and RGB-D benchmark datasets, with comparable number of parameters and relatively small MACs, hence demonstrating the great effectiveness of our VST. 
We also show visual comparison results among best-performed models in Figure 3 . 
It shows our proposed VST can accurately detect salient objects in very challenging scenarios, e.g., big salient objects, cluttered backgrounds, foreground and background having similar appearances, etc. 
5. Conclusion
In this paper, we are the first to rethink SOD from a sequence-to-sequence perspective and develop a novel unified model based on a pure transformer, for both RGB and RGB-D SOD. 
To handle the difficulty of applying trans-formers in dense prediction tasks, we propose a new token upsampling method under the transformer framework and fuse multi-level patch tokens. 
We also design a multitask decoder by introducing task-related tokens and a novel patch-task-attention mechanism to jointly perform saliency and boundary detection. 
Our VST model achieves state-ofthe-art results for both RGB and RGB-D SOD without relying on heavy computational costs, thus showing its great effectiveness. 
We also set a new paradigm for the open question of how to use transformer in dense prediction tasks. 
Table 4 . Ablation studies of our proposed model on RGB SOD datasets. 
"RC" means RGB Convertor. 
"Bili" denotes bilinear upsampling and "F" means multi-level token fusion. 
"TMD" denotes our proposed token-based multi-task decoder, while "C2D" means using conventional two-stream decoder to perform saliency and boundary detection without using task-related tokens. 
The best results are labeled in blue. 
Settings
Figure 1. 
Overall architecture of our proposed VST model for both RGB and RGB-D SOD.It first uses an encoder to generate multilevel tokens from the input image patch sequence. 
Then, a convertor is adopted to convert the patch tokens to the decoder space, and also performs cross-modal information fusion for RGB-D data. 
Finally, a decoder simultaneously predicts the saliency map and the boundary map via the proposed task-related tokens and the patch-task-attention mechanism. 
An RT2T transformation is also proposed to progressively upsample patch tokens. 
The dotted line represents exclusive components for RGB-D SOD. 
Figure 2. (a) T2T module merges neighbouring tokens into a new token, thus reducing the length of tokens. 
(b) Our proposed reverse T2T module upsamples tokens by expanding each token into multiple sub-tokens. 
Figure 3. Qualitative comparison against state-of-the-art RGB-D (left) and RGB (right) SOD methods. 
(GT: ground truth) 
Figure 4. Qualitative comparison against state-of-the-art RGB SOD methods. 
(GT: ground truth) 
Quantitative comparison of our proposed VST with other 14 SOTA RGB-D SOD methods on 9 benchmark datasets. 
Red and blue denote the best and the second-best results, respectively. 
'-' indicates the code or result is not available. 
Quantitative comparison of our proposed VST with other 12 SOTA RGB SOD methods on 6 benchmark datasets. 
"-R" and "-R2" means the ResNet50 and Res2Net backbone, respectively. 
.874 0.939 0.039 0.925 0.932 0.966 0.032 0.871 0.845 0.897 0.068 0.851 0.861 0.899 0.068 +RC+RT2T+F+TMD 0.896 0.877 0.939 0.037 0.928 0.937 0.968 0.030 0.873 0.850 0.900 0.067 0.854 0.866 0.902 0.065 +RC+RT2T+F+C2D 0.891 0.870 0.937 0.040 0.924 0.931 0.966 0.033 0.869 0.844 0.896 0.069 0.852 0.860 0.898 0.067 Comparison of using different numbers of transformer layers in our VST model. 
The final model setting is labeled in blue. 
