视觉显著性检测的简要综述

显著性物体检测模型模仿人类的行为，并从图像或场景中捕获最显着的区域/物体。
该领域在计算机视觉和模式识别任务中都有许多重要的应用。
尽管该领域提出了数百种模型，但它仍然有很大的研究空间。
本文从启发式技术和深度学习技术的角度详细概述了显著性检测模型的最新进展。
本文讨论并回顾了其共相关领域，如眼力预测、RGBD显著性目标检测、共显著性目标检测和视频显著性检测模型。
我们回顾了当前显著性模型的关键问题，并讨论了未来的趋势和建议。
此外，本文还对广泛使用的数据集和评估策略进行了研究。
介绍：

人类视觉系统 （HVS） 具有令人难以置信的能力，可以快速识别和聚焦令人印象深刻的物体或区域，这些物体或区域在视觉上更加独特和突出 在图像/场景中 这个过程已经在计算机视觉中进行了探索 [1] [2] [3] [4] 以检测那些在图像或视频中具有更重要和更有价值的信息的突出物体， 如物体识别任务、场景感知、水下视觉等。

这是一个新兴的话题，最近引起了来自各个学科的研究人员的广泛考虑。
从图像中检测显著性物体的机制称为显著性检测或显著性物体检测。
显著性目标检测的基本概念如图 1 所示。
第一行表示原始图像，第二行显示每个图像的相应真实值。
显著性检测过程首先定位并识别对象的正确位置/区域，然后将其从其背景中分割出来。
为此，已经提出了许多模型，这些模型在具有单个对象的简单图像/场景中取得了良好的性能。
然而，在背景更加复杂、杂乱的复杂场景中，仍然很难找到一个突出的物体[5]。
其中，自下而上的显著性检测是一种机制，可以在没有任何先验知识的情况下自动捕获人类视觉注意力的更聚焦和刺激对象的区域 [6] 。
通常，显著性被称为像素与其周围位置之间的方差和对比度 [7] 。
此外，saliency-map用于描述图像显著度。
在显著性图中，每个显著性值表示图像中其相应区域的像素值。
它有着悠久的历史，至今仍被认为是计算机视觉研究中一个活跃的研究领域。
一般来说，良好的显著性检测方法必须确保精确的目标检测、高分辨率和计算效率[8] 。
目前，不同的研究人员已被归类为基于不同原理的最先进方法。
在这项工作中，我们全面讨论了显著性目标检测模型。
我们还讨论了用于显著性检测方法的常见数据集和评估措施。
本文对相关工作进行了总结，并对今后的研究工作提出了一些建议。
剩下的论文组织如下：在第 2 节中，我们简要回顾了各种显著性目标检测模型，例如 RGBD 显著性检测模型、协同显著性检测模型和视频显著性检测模型。
在第 3 节中，我们将简要讨论用于显著性检测的相关数据库。
在第 4 节中，我们列出了显著目标检测的数据库和应用，最后，我们提供了结论和未来的建议。
2. 视觉显著性检测模型综述：

视觉注意力已经在计算机视觉的多个学科中进行了探索 [9] [10] [11] [12] 。
在早期认知理论的基础上，1980年，Treisman和Gelade[13]提出了特征整合理论，并提出了视觉注意的特征整合模型和特征注册模型。
Wolfe等[14]提出了一种生物结构（Guided-Search-Model），Koch和Ullman[15]提出了一种计算注意力框架。
这些理论建立在自下而上的中心环绕机制之上。
1998 年，Itti 等人提出了一种视觉注意力模型 [12， 1] 来描述人类的视觉注意，该模型通过结合基于中心-环绕机制的不同尺度的三种不同特征图（即颜色、方向和强度）来生成用于显著性检测的地图。
最近已经提出了数百种视觉注意力模型，包括注视点预测模型。
随后，Liu等[16]将显著性检测定义为二元分割工作。
Zhang和Sclaroff[17]使用布尔映射拓扑分析了显著性映射。
为了得到显著性图，Li等[9]通过密集和稀疏表示引入了重建误差方案。
Zhu等[18]添加了一个简单的边界来计算背景测量值，并找到图像区域的空间格式及其对应的边界。
因此，采用一种优化方法，引入不同的低级线索，如背景测量，得到均匀的显著性图。
Scharfenberger等[19]提出了一种统计模式方案，该方案稳健地利用了图像的基本异构纹理特征，并有效地计算了图像中每个区域的相关显著性。
此外，还有其他几种技术依赖于数学计算。
Hou和Zhang[2]利用傅里叶变换相位谱生成显著性图，提出了残差光谱框架。
Achanta等[20]通过整合低级特征，得到了基于局部对比度的显著性图。
这些经典模型已经取得了令人钦佩的性能，但是，由于缺乏高级语义信息，这些低级模型仍然很难达到预期的结果。
如今，基于深度学习的卷积神经网络[21]，尤其是全卷积神经网络[22]的复兴，为显著性检测提供了一种可行的技术。
与主要使用基于对比先验的低级视觉信息的传统方法不同[23]，基于CNN的方法使用高级语义信息，并消除了对手工制作特征的需求。
CNN通常有数百甚至数万个参数和具有不同感受野大小的神经元。
感受野大小较大的神经元用于识别图像最显著区域的全局信息，而小感受野用于识别图像小区域之间的局部信息。
研究人员对 CNNs 模型的兴趣正在上升，因为它与经典的手工制作的基于特征的模型相比具有出色的性能和更理想的特性。
从信息处理机制的角度来看，显著性检测方法一般可分为自下而上的方法和自上而下的模型。
自下而上的方法基于低级视觉特征，没有高级语义信息。
另一方面，自上而下的方法假设显著性检测的外在线索具有更多的语义信息。
自上而下的方法[2,24]通常是任务驱动的，需要大量的训练数据和人工标记的地面事实。
因此，这些模型可以从图像中提取高级语义特征来描述特定对象（例如
汽车、行人）。
然而，由于日常任务和行为的复杂性和变化性，高级方法并没有得到太多探索。
在过去的二十年中，该领域的研究工作朝着两个方向发展：视觉注意力预测（即眼睛注视预测）和计算机视觉中的显著性检测。
前一类强调在人类观察者第一眼就找到注视点[25,26]，而后一类则试图从原始图像中识别或/和分割最突出和最突出的物体[27]。
在以下小节中，我们将简要回顾注视预测模型，同时提供有关显著物体检测模型的全面详细信息。
2.1 注视预测模型：

为了模拟视觉注意力，眼睛注视预测模型通常已经与人类注意力的眼球行为进行了对比。
眼球运动表达了有关认知过程的重要信息，例如分析、场景感知和视觉搜索。
因此，它们经常被保存为注意力变化的代理 [7] 。
灵长类动物具有很强的才能，可以实时分析复杂的场景。
视觉系统将首先在收集的信息中进行选择，然后再对视觉信息进行额外处理。
它可以大大减少所获得信息的复杂性。
这种选择方法是在有限的视野中完成的，称为 visualattention-prediction。
HVS在感应外部环境时施加了坚实的动态选择性过程;在这种情况下，动态选择充当视觉-注意力-点转移的过程。

此外，HVS可以快速捕获大量的图像信息。
开销情绪阐明了注意力点预测的生物学基础。图 2 显示了人眼注视预测的一些样本，其中红光斑点表示更显着的区域。

注意力预测模型的初始类别主要集中在人类视觉注意力和眼睛凝视预测中。
Itti 等人的基本模型使用了三个简单的特征通道（即颜色、方向和强度）。
该模型成为该领域未来模型的基础和评估的标准基准。
它已被提出与自由观看任务中的人眼通量相关联 [28， 29] 。
Le Meur等[30]提出了一种基于对比敏感函数、知觉分解、中心-周围交互和视觉掩蔽的自下而上的显著性检测方法。
后来，Le Meur等[31]通过结合色差、消色差和基于时空的信息，将该模型扩展到时空领域。
在这个修改后的模型中，他们从视觉输入中提取了早期的视觉特征到一些单一的平行通道中。
为每个通道实现一个特征图，然后从这些通道的并集构建一个独特的显著性图。
Kootstra等[32]提出了3个对称性基本算子，并将其与人类眼动追踪数据进行了比较。
该技术建立在Reisfeld等人[33]的径向对称算子和各向同性对称性以及Heidemann[34]的颜色对称性之上。
2.2 显著性检测模型

在本文中，显著性检测的文献分为基于启发式的方法和基于学习的方法。
在显著性检测中，对比度是显著性区域识别的一个非常重要的因素[35] [36]。
大脑对图像中的高对比度物体/区域非常敏感。
传统的启发式显著性检测方法大多基于低级视觉特征，大多数计算框架都是无监督的[37]。
在显著性检测过程中，这些传统的自下而上方法遵循启发式特征方法（即，例如对比度、位置和纹理）。
启发式特征通常被称为视觉先验或显著性检测的线索[38] [39]。
对比先验是一个非常关键的特征，也是最常用的先验之一。
具体而言，对比先验包括局部对比先验和全局对比先验，对比先验假设显著区域总是与其邻域或场景不同[40]。
除了对比先验之外，位置先验还包括中心先验和背景先验。
中心先验表示显著物体出现在图像的中间，而背景先验表示图像的边框更有可能成为背景的一部分。
在本小节中，我们将讨论基于启发式的显著性检测模型中的一些重要线索或先验。
2.2.1 基于启发式的显著性检测模型

A. 基于局部对比度的显著性检测

对比度表示图像中两个或多个像素/区域之间的明显差异。
两个要素之间的距离称为基于对比度的显著性值。
在局部对比显著性方法中，显著性目标的边缘产生高显著性得分[39]，从而突出了整个显著性目标。
提出了基于局部对比度的显著性检测[41] [42] [43] [44] 9， 45， 46， 24， 47]，该方法通过考虑不同区域之间的局部特征（即颜色、照明、方向和其他运动信息）来计算显著性值图。
Itti等[41]提出了一种中心-环绕方法，通过使用线性和非线性组合的多尺度显著性映射来提取低级元素（即颜色、强度、纹理和方向）。
马和张[42]使用颜色对比度作为局部社区的显著性度量。
在[43]中，江等人引入了一种主要基于局部对比度、背景和其他众所周知的特征的区域水平显著性描述符。
江等[44]提出了一种基于多尺度局部对比区域的策略，该策略计算不同区域分割的显著性值以创建鲁棒性，并结合这些区域的每个值以获得像素级显著性图。
在[9]中，作者采用了类似的框架，通过使用多层次分割来估计区域显著性。
Li等[45]借助创建超图（由超像素的非参数多尺度非参数聚集而成）来延长成对局部对比度，以获得区域的内部一致性和外部分离。
然后，通过在超图中查找显著的顶点和超边来实现显著性对象检测。
Liu等[24]通过线性合并高斯图像金字塔中的局部特征，提出了一种基于多尺度对比度的显著性检测算法。
Goferman等[47]连续设计了一种基于局部低级对比、全局对比、视觉组织政策和其他高级元素的模型，以捕捉显眼的突出项目及其背景。
Jian等[48]设计了一种基于主要局部颜色对比度的显著性检测模型。
B. 基于全局对比度的显著性检测

与基于局部对比的方法不同，基于全局对比的方法[23， [49] [50] [51] [52] [53] [54] [55]通常将物体与其周围环境分开。
与基于局部对比度的技术相比，基于全局对比度的方法具有优势，因为它们在其对象边界处生成过多的显著性值。
在全局特征考虑中，相似的显著性值分布在相似的区域，导致产生高显著性成本。
Cheng 等人提出了一种颜色直方图作为全局对比度，并计算了每个区域与同一图像所有其他区域的色差加权和 [23] 。
Harel等[49]提出了一种基于图论的全局显著性检测方法。
Zhai和Shah[50]通过计算每个像素与所有其他像素的色差之和来计算显著性得分。
Achanta等[51]提出了一种频率调谐模型，该模型通过直接计算与其平均图像颜色的色差来估计像素级显著性得分。
Perazzi等[48]通过应用元素的唯一性和图像的空间分布来测量全局对比度。
Goferman等[52]提出了一种通过考虑与其他斑块的全局对比度来进行显著性估计的斑块唯一性方法。
Yan等[53]提出了一种层次显著性检测方法，以解决高对比度结构的小尺度变化。
Shen等[54]引入了一种低秩恢复技术，将低级视觉结构与高级先验相辅相成，用于显著性检测。
Imamoglu等[55]利用小波变换产生多尺度结构，这些结构抑制了局部对比与全局显著性。
Perazzi等[48]应用高斯滤波器计算了显著性目标检测的全局唯一性和空间分布。
尽管已经对全球先验进行了充分的研究，但是，它在捕获语义信息方面仍然存在弱点。
C. 基于中心先验的显著性检测

原始中心先验实际上基于这样一种观点，即突出的物体经常位于图像的中间附近[53,56,57,44,43]。
在显著性检测过程中，中心先验尝试高亮显示中心区域或与其他线索相结合，将显著区域/对象作为空间特征高亮显示。
但是，我们知道突出的物体并非每次都出现在图像中心。
为了克服这一缺点，Xie等[57]利用兴趣点的凸包来预测显著物体的粗中心。
Jian等[35]使用基于离散小波帧的感知定向补丁，该小波帧变换到显著物体的固定位置。
D. 基于背景先验的显著性检测

背景性先于 [58， 9， [59] [60] [61] 将窄边框视为图像的背景区域。
通过将背景种子作为参考，可以将显著性分数计算为与背景的对比度。
江等[58]提出了一种利用吸收马尔可夫链的显著性检测方法，其中超像素是围绕图像中心和边界的传输和吸收节点。
Li等[9]以图像边界为背景模板，提出了一种基于密集和稀疏重建误差的显著性检测方案。
Wei等[60]构建了一个无向加权图，并将显著性值估计为到背景的最短距离。
Yang等[61]提出了一种双方案显著性计算模型，该模型基于无向加权图，采用流形排序方法，考虑背景查询中每一方的相关性得分。
基于伪背景，显著性检测可能会失败，特别是当项目附加边界时。
使用先于[23,18]的边界连通性来解决这个问题。
自然，背景与边界的联系比任何突出的物体都更紧密。
Zhu等[18]利用这一思想，通过估计图像边界相对于显著区域的跨越区域的长度来找到边界连通性分数。
最近，提出了一种基于背景种子的显著性检测模型，通过目标建议和扩展随机游走[38]。
E. 基于先验客观性的显著性检测

除了这些技术之外，对象性先验还可用于通过使用对象建议来辅助显著目标检测，这是由Alexe等[62]引入的，用于通过评估图像的每个随机窗口的对象性分数来测量存在整个对象的概率值。
Chang等[63]通过将区域显著性和客观性结合到图形显著性中，提出了一种计算方案。
江等[64]根据其所有区域像素的平均对象性值计算了区域对象性。
根据先验的客观性，Jia和Han[65]计算了每个区域的显著性得分，然后将其与柔和的前景和背景进行比较。
为了将对象性与显著性得分联系起来，通过随机抽取大量抽样窗口来计算局部显著性[66]。
针对复杂场景的图像，Li等[67]提出了一种三中心偏置的客观性度量。
他们提出了一种共转导方法，将边界超像素和对象性标签相互融合。
此外，江等[64]通过非线性融合独特性、客观性和焦点性分数来计算显著性得分。
F. 基于贝叶斯框架的显著性检测

在显著性计算方面，提出了贝叶斯模型[57]，通过近似像素x后验概率作为图像的前景来寻找显著性物体。
对于显著性先验计算，通过凸壳函数估计感兴趣像素，该函数将图像分为内部和外部区域，然后得到前景和背景的粗略估计分数。
Liu等[68]使用基于贝叶斯框架的优化模型进行显著性检测，通过粗略估计凸包，将输入图像分类为潜在的前景区域和纯背景区域。
为了生成显著性图，使用这些线索提出了一种具有狄利克雷边界的通用线性椭圆机制，以模拟种子向其他区域的扩散。
表 1 显示了使用不同线索/先验的基于视觉启发式模型的一些代表性方法。
G. 讨论

上述先验是基于启发式的显著性检测模型中最常用的先验。
此外，还引入了一些其他传统技术进行显著性检测，如频域分析[51]、元胞自动机[76]、稀疏表示[9]、随机游走[59]、低秩恢复[77]、紧致性先验[78]和定向先验[12]。
这些基于传统的显著性对象检测方法由内在线索组成，这些线索旨在通过以下方式从给定的输入图像中提取不同的线索 在本概述中，基于常见的先验/线索，我们的分类仅指定了先验的优先性，因为模型可以由单一的先验或不同先验的组合组成。
局部和全局是显著性检测中最常用的唯一性显著性先验[235]。
传统上基于启发式的显著性检测方法在计算机视觉领域取得了巨大的成就，但在某些空间情况下仍然失败，特别是当图像包含非常复杂的场景、低对比度（例如
水下图像）和交错物体。
为了克服这些问题，我们采用了基于学习的方法（监督学习、半监督学习或基于无监督学习的方法），我们将在下一节中介绍。
2.2.2 基于学习的显著性检测：

我们在传统方法中研究的所有上述方法都使用固有的低级线索并基于无监督技术，这些技术有时不足以准确检测突出目标，尤其是当图像复杂且具有共同的视觉特征时。
为了解决这些问题，利用基于学习的训练数据方法在复杂的背景图像中找到一个显著的物体。
A. 基于经典学习的显著性检测方法

这些是基于监督或半监督学习的显著性检测方法，也称为数据驱动方法，其中集成了高级特征和监督信息，以提高显著性图的准确度。
Judd等[79]提出了一种通过支持向量机（SVM）分类器预测眼睛注视预测的模型，该模型基于包括15个观众注视位置的训练数据集。
在[24]中，Liu等人提出了一种基于条件随机场（CRF）的二元显著性估计方案。
Yang等[80]提出了一种训练条件随机场（CRF）和判别字典进行显著性检测的方法。
该设计方法包括从自上而下的方式开始的分层结构，在结构化监督下进行训练，然后遵循最大边际机制进行高效学习。
在[81]中，Borji等人将低级特征（如方向、颜色和强度）与高级视觉特征（如人、脸和汽车等）集成在一起，通过AdaBoost分类器训练直接映射方法，用于眼睛注视。
Wang等[82]提出了一种基于多实例学习的方法，该方法将低级、中级和高级特征集成在一起，用于显著目标检测。
江等[43]提出了一种显著性检测模型作为回归结构，然后训练回归森林分类器生成显著性值。
在[91]中，Lu等人提出了一个模型，并训练它学习最优种子，然后通过扩散过程将这些种子繁殖。
Tong等[91][35]提出了一种通过Bootstrap学习技术的显著目标检测模型，而不是仅在大数据集中训练分类器。
他们还训练一组弱支持向量机，以便通过多核提升方法合并弱分类器来获得强分类器。
由于经典的基于学习的方法利用了先验知识，并且偶尔会优于传统的手工制作的基于特征的显著性检测技术，因此这些方法提高了显著性检测的性能。
由于传统的基于学习的方法仍然是手工制作的特征，如果不仔细收集模型，这可能会降低模型的性能。
但是，最近基于 CNN 的方法的发展将研究人员转向深度学习方法的趋势，而不是经典的机器学习算法，因为它们具有巨大的性能。
B. 基于深度学习的显著性检测模型

在本节中，我们将介绍基于深度学习的显著性检测模型，尤其是基于 CNN 和基于 FCN 的方法。
卷积神经网络（CNNs）[21]因其在表示高级语义方面的功能而受到研究者的高度关注，并已成功应用于许多计算机视觉问题[22,83]。
最近，CNNs[84,85]也展示了其在显著性检测领域的有效性，并且能够在没有先验知识的情况下捕获最显着的区域。
通常，基于CNN建立的显著性检测方法可以根据其对输入图像的处理情况分为两大类：（1）基于区域的模型和（2）基于FCN（即基于像素的）模型。
基于区域的方法将输入图像划分为多尺度或更小的区域。
然后，利用CNN提取这些小区域的高级特征，然后输入到多层感知器（MLPs）中，得到每个小区域的显著性值。
与传统的现有模型相比，基于区域的模型取得了较好的表现，但由于小区域的分割，这些模型无法持久地处理空间信息。
为了克服这个缺点，一个完全卷积神经网络（即
基于FCN的方法）通过直接使用端到端网络预测显著性图来设计，也称为端到端模型。
Wang等[86]通过结合来自输入图像局部区域的形状、纹理和对比度信息，开发了用于显著性计算的深度网络。
在全局搜索阶段，通过对象提议方法[87]创建候选对象区域的列表。
在[88]中，Lee等人提出了一种统一的深度学习框架，利用图像的高级和低级特征进行显著性检测。
对VGGNet[89]进行训练以提取高级特征，然后对低级特征进行集成以识别显著区域。
He等[84]提出了一种基于区域的模型来学习超像素的特征表示。
与像素级 CNN 相比，它可以降低计算成本。
Zou等[90]提出了一种用于显著性检测的层次相关特征（HARF）框架，该框架利用多层次深度学习网络整合了来自区域的基本特征。
Kim等[91]通过考虑粗表示和精细表示，提出了一种基于双麸皮CNN的显著性检测模型。
通过选择性搜索[92]方法生成了一些候选区域，然后将其作为CNN的输入。
Wang等[93]提出了一种基于R-CNN的快速多尺度掩模框架，用于显著性检测，该框架将输入图像分割为多尺度区域，并采用基于边缘的传播方法来细化显著性图。
在[94]中，Kim等人提出了一个CNN模型来估计每个图像斑块/区域的显著性值。
Li等[95]利用手工制作方法捕获的低级特征和利用CNNs方法捕获的高级特征来提高显著性精度。
在该模型中，使用选择性搜索方法[92]生成了带有内部区域掩码的候选边界框。
Li等[85]从多尺度区域捕获深度特征进行显著性检测。
并利用超像素细化方案得到增强的空间相干结果。
Zhao等[96]引入了一种多上下文深度学习模型，该模型从给定的超像素中捕获局部和全局尺度特征，以预测每个区域对应的显著性值。
在[97]中，Hariharan等人提出了一种用于显著目标分割的超列方法，并将不同类型层的特征融合在一起以进行进一步的分类。
Liu等[97]提出了一种层次细化方案，该方案通过利用VGG网络逐步生成显著性图，从而产生全局粗略预测。
在[98]中，设计了一种细化子网络循环卷积层（RCL），用于将粗级预测图微调为精细级显著性图。
最近先进的CNNs显著性检测框架比早期的手工制作特征方法获得了更好的结果。
此外，CNN 提取的特征包含更多高级特征，因为这些 CNN 通常针对非常大的数据集上的视觉识别活动进行了预训练。
然而，基于区域的 CNN 在基于片段或补丁级别运行，而不是利用像素级别，其中每个像素基本上都分配了其封闭段的显著性分数。
因此，它给出了一个模糊的显著性图，缺少突出对象及其边界的精细细节。
此外，图像的所有分割斑块或区域都作为独立样本进行处理，以便进行分类;甚至它们也可能相互重叠。

这种冗余会导致计算量显著增加，并且在训练和测试期间需要更多空间。
此外，基于区域的CNNs模型不能很好地保留上下文信息。
因此，为了克服基于区域的CNN的缺点，该文采用了著名的基于端到端的全卷积网络（Fully Convolution Network），该算法可以预测像素级显著性图。
众所周知，基于区域的 CNN 技术不能很好地保留显著对象的上下文信息，因为 CNN 是针对每个图像、块或区域独立操作的。
为了解决上述问题，全卷积网络（FCN）[22]在像素级别而不是区域或块级别上运行。
基于FCNs的显著性检测技术可以消除诸如对显著性对象的模糊边界的模糊预测等问题。
基于FCNs的显著目标检测模型也因其出色的性能而引起了研究人员的注意。
Long等[22]提出了一种基于FCNs的显著性检测模型，该模型通过呈现深层和粗层获得的有意义信息进行像素到像素的训练。
Li等[99]提出了一个具有空间池化流（SPS）和像素级全卷积流（FCS）的模型，用于生成显著性图。
Tang等[100]利用深度监督网络[101]，设计了一种整体嵌套边缘检测器（HED）[83]进行显著性检测。
在[102]中，Tang等人提出了一种融合像素级CNN和区域级CNN显著性预测的显著性检测方案。
Kruthiventi等[103]提出了一种结合的深度架构，用于通过全连接CRF进行注视预测和显著目标检测[104]。
在[105]中，作者设计了一种用于显著性检测的递归注意卷积-反卷积（RACDNN）方法。
在RACDNN中，输入图像的一段由空间变换器[106]在每个时间步长中选择。
Zhang等[107]提出了一种基于CNNs和多级汞合金框架的显著性检测方法。
采用Deeplab[108]方案获取高级特征，并采用多尺度二进制像素标注框架恢复空间相干性。
Li等[109]
讨论：

与基于区域的 CNNs 模型相比，基于 FCN 的模型是基于端到端的 CNNs 模型，利用像素级值来预测显著性图，因此也称为像素到像素 CNNs 模型。
基于FCN的方法非常有效，并克服了基于区域的CNNs模型的局限性。
它还可以以非常好的方式保留上下文信息，因此提供更强大的结果。
由于基于区域的 CNN 模型使用单独的网络来利用局部和全局特征，因此基于 FCN 的模型在一个网络中学习局部和全局特征。
较浅的层提供有关对象边缘的全局信息和更多详细信息，而较深的层则提供高语义、局部和更有意义的信息。
这些基于FCN的网络大多在ImageNet数据集[128]上进行预训练/学习，用于图像分类目的，然后可以对这些学习的模型进行微调以用于多种目的（例如，目标检测[129]、对象定位[130]和显著性检测[96,122]。
与从头开始训练相比，预训练模型可最大限度地降低训练成本，并提供更复杂的结果。
此外，FCN模型包含一系列不同类型的层，这些层可以执行不同类型的功能，因此，与以前的基于区域的CNNs模型相比，在结构上提供了灵活性和多样性。
表2显示了基于深度学习的模型的简要摘要，图7显示了一些传统的基于启发式的方法和新的基于学习的方法的视觉比较。
尽管深度学习技术，尤其是基于FCN的方法，已经取得了非常出色的性能，但在许多情况下，它都失败了，需要在未来进行改进。
例如，它需要改进低对比度图像，这些图像具有更常见的前景和背景相似性、透明对象以及包含复杂背景的图像。
同样，FCN 中重复池化和跨步操作会降低图像分辨率并降低模型的性能。
对于这些深度模型来说，更多的时间和大内存也是一个具有挑战性的问题。
此外，这些方法需要大量的训练数据。
为了解决这些问题，近年来提出了几种不同类型的基于 CNN 的架构。
一些方法已经显示出巨大的反响，今后需要进一步探索。
例如，多尺度和多层次的深度网络可以通过在不同层次之间使用融合、跳过连接和短连接来利用不同层次的特征。
同样，编码器-解码器架构是最有前途的方法，并且在不同的分类和分割任务中表现出了出色的性能。
在这些类型的方法中，高级特征被反向传播到较低层，并使多级特征形成更强的联合。
另一个很好的方法是使用ResNet [131]，它是一个深度网络，可以很好地执行复杂的任务。
ResNet比VGGNet [132]更强大。
不同交叉模型的融合也可以提高性能。
标准的训练损失函数也可以提高性能，并且在未来需要更多关注。
同样，手机、机器人、自动驾驶等嵌入式应用也需要在显著目标检测领域进行大量研究，以减少时间、内存空间和能耗
2.3 RGBD显著性检测

RGBD显著性检测是一个新兴的课题，在研究方面仍有很大的改进空间。
与 2D 图像显著性检测方法不同，深度线索必须包含在 3D 图像的显著性检测中。
RGBD显著性检测方法同时利用颜色信息和深度线索来识别显著性物体。
通常有两种方法可以将深度线索与二维图像相结合[133]：（1）基于深度特征的方法[134] [135] [136] [137] [138] [139]，旨在将深度事实作为附加材料与颜色测量一起合并。
（2）基于深度测量的方法[140] [141] [142] [143]，利用设计的深度测量从深度线索中捕获全面的信息，如形状和结构。
这些是手工制作的基于特征的方法，具有深度提示，用于检测图像中的突出物体。
各种研究都致力于 3D 多媒体内容的显著性检测。
Lang等[134]通过将全局上下文深度先验纳入2D模型来感知显著物体。
Ju等[135]提出了在各向异性中心周围方差上创建的RGBD显著性过程，其中显著性被估计为物体与其周围环境的差异程度。
在[139]中，Fang等人从基于RGBD的图像中提取颜色、纹理、亮度和深度特征来估计对比度特征图。
然后，利用组合和改进方法得到得到的三维显著性图。
Song等[136]利用深度信息作为区域特征来计算基于对比度的低水平显著性，也被用作测量中级显著性的加权特征。
然后，应用高级位置先验来构建高级显著性地图。
在最后阶段，应用多尺度判别显著性融合技术将多个显著性图合并，得到结论显著性输出。
此外，由于评估认为显著区域在深度特征图中与其局部和全球环境肯定不同，“深度对比”是需要计算的一般深度属性。
为此，Niu等[138]用领域知识计算全局对比度来估计立体显著性。
在[137]中，Peng等人提出了一种多语境对比度框架，通过考虑对比先验、全局唯一性和背景先于深度图来计算深度显著性。
然后，利用多级RGBD显著性方法融合低对比度特征、中级局部联盟和高级先验技术;
Ju等[140]通过应用各向异性中心环绕差（ACSD）测量，提出了一种用于显著性检测的深度感知框架，此外，他们还建立了一个庞大的立体显著性检测数据集，其中包含1985张立体图像和估计的深度图。
将ACSD测量方法与颜色显著性图相结合，在[141]中，Guo等人提出了一种基于进化策略建立的RGB-D图像显著性目标检测模型。
这是一个重复的生成过程，以增强早期显著性图并产生最终输出。
由于背景包括深度图中极其可变的区域，因此一些高对比度的背景区域可能会引发假阳性。
为了克服这种干扰，Feng等[142]使用局部-背景-封闭度量（LBE）框架从深度图中直接提取一个显著区域，该深度图计算位于背景正面的物体边缘的比率。
Wang等[143]通过连接最小阻垒距离变换显著性图和基于多层元胞自动机的显著性图，提出了一种用于RGBD图像的多阶段显著目标检测方案。
最近，37]也被应用于RGBD显著性检测中，以学习更具判别性的RGBD特征。
在[144]中，Qu等人提出了一种用于RGBD显著性检测的CNN模型。
它们结合了局部对比度、全局对比度、空间先验和背景先验等低级显著性特征，并生成了粗略的显著性向量。
然后，这些向量与深度模态相结合，并输入到CNN中，从头开始训练它以产生RGBD超特征。
Han等[145]提出了一种双流时末融合结构来结合RGBD深度特征。
采用分阶段的方法对网络进行训练，并获得了乐观的性能。
类似地，在[37]中，Wang等人提出了RexNet，该方法可以生成带有锐利对象的端到端显著性图。
在这种方法中，首先将图像分为两个独立的片段：边缘区域和超像素区域。
然后，该网络为这些区域生成端到端显著性分数，并将多个层中的上下文与区域显著性分数相结合。
然后，通过应用深度细化，将所提出的模型扩展到RGBD显著性检测。
Chen等[146]提出了一种端到端的RGBD显著对象检测网络，该网络具有对应感知的跨模态和跨层次特征的结合能力。
所提出的跨模态连接和层次监督显然激励了从对应方捕获互补事实，从而通过降低融合不确定性来提高融合能力。
在
讨论：

目前，捕获3D图像深度图的方法有三种：（1）结构光技术[149]用于通过相机产生的光信号的变化来提取深度信息。
这是一个很好的技术，但对照明最敏感。
双目成像）[151]，利用两个不同位置的相机拍摄两张照片，通过三角法则找到物体的距离。
这种方法成本低，但需要后处理步骤。
因此，确实需要进一步研究如何获得高质量的深度信息，然后如何以适当的方式利用它，因为深度信息的不当使用会导致性能下降。
图 3 显示了深度图的一些不同条件，以及
2.4 协同显著性检测

协同显著性检测是试图从给定的图像组中发现最常见和最突出的物体的过程。
为此，图像间对应特征被用作简单的属性检查，以区分共享对象（属性方面）与所有其他显著对象。
首先计算序列中每个图像的低级或高级特征，以获得协同显著性图。
低级特征是图像的启发式特征，代表颜色、纹理和亮度等，高级特征代表通过深度学习技术获得的语义信息，同时使用两种类型的模型来提取图像内和图像间特征以进行协同性检测。
图像内显著性模型用于从单个图像中提取特征，图像间显著性模型用于从一组图像中提取特征。
对于图像内显著性，可以采用常用的显著性检测方法，但图像间模型使用不同类型的技术，如基于相似度的匹配、基于低秩的分析、聚类和传播方法。
在计算了这两类模型后，利用融合方案对这些模型进行融合，得到了最终的余异度图。
协同显著性检测通常与共分割方案的概念几乎相关，该方案计划从多个图像中分割大多数相同的对象或区域[152]。
如[153]所示，共显著性过程和共分割过程之间存在三个主要变化。
首先，协同显著性检测方法只关注遇到常见的显著性对象，而另一方面，在共同分割方法中也可以考虑背景中相似的非显著性部分[154,155]。
其次，一些共分割方法，例如[156]，希望用户反应在模糊的情况下引导分割过程。
第三，显著性对象检测经常作为预处理步骤执行，因此比协同分割方法更真实、更高效的方法更受青睐，尤其是在大量图像上。
传统的方法基本上是最早和简单的协同显著性检测方法，通过使用手抓的共同显著性特征对图像组中的每个像素/区域进行评分。
通常，这些是低级方法，由四个基本组件组成，包括预处理、特征提取、应用低级线索和加权组合。
Chang等[157]提出了一种完全无监督的方法来求解共分割问题。
他们通过在关于可想象的前景位置的线索之前建立协同显著性来替代用户输入数据，并通过建立唯一的全球能量项来有效地获得共同分割过程，从而产生了优化的 CRF 模型。
Tan等[158]提出了一种起源于相似性矩阵的自主协同显著性检测方案，该方案通过使用二分超像素级的图匹配机制在图像对集合中测量共同显著性过程。
Fu等[153]提出了一种基于聚类的协同显著性检测方法，该方法利用全局对比度和空间分布线索对单幅图像进行研究，并在一组图像上使用相应的线索来找到显著性共现。
Li等[159]利用低秩矩阵恢复方案计算显著性内检测和区域级融合方案，提出了一种协同显著性模型。
区域级融合方案利用了不同区域之间存在的相似性以及图像集上的全局均匀性度量。
利用像素级细化方案来度量像素与区域之间的相似性以及它们的目标先验性。
Ye等[160]提出了一种基于目标发现和恢复的显著性检测框架，该框架使用粗相似性匹配。
他们首先通过发现共同显著性对象的一致样本来生成样本显著性图。
然后，利用共同显著性目标区域、关注区域焦点和区域边界连通性的局部和全局恢复，为所有对应的图像集创建最终的共同显著性图。
Li等[161]提出了一种显著性引导的协同显著性检测方案，其中第一步通过使用高效的流形排序方案恢复在单一显著性图中丢失的共同显著性块，第二步通过具有不同类型查询的排序方案提取相关关系。
Ge等[162]提出了一种用于协同性检测的两阶段传播方法，其中利用显著性间传播阶段识别共享特征并构建成对共享的前景线索图，利用显著性内传播阶段抑制背景位置并精细化第一阶段的处理。
Song等[163]利用基于袋装的聚类方法提出了一种RGBD Co-saliency-detection模型。
候选对象区域是利用区域预分割和 RGBD 单一显著性映射创建的。
然后，通过特征袋技术重复执行聚类，以计算基于聚类水平的各种弱协同显著性度量。
最后，利用自适应融合多重（WCS）映射对聚类质量进行评估。
在[164]中，Huang等人利用颜色特征强化方法设计了一种协同显著性检测方案，并利用特征编码系数和显著前景字典得到共同显著性图。
在[165]中，Cong等人提出了一种用于RGBD共显著性检测的能量函数细化和层次稀疏性重构框架。
利用层次稀疏性重建方案，借助内显著性图来构建图像间对应关系。
将全局稀疏性重建框架与排序方案结合，通过通用字典捕捉整个图像的全局特征，利用成对稀疏性重建模型通过一组成对字典找到图像之间的相互关系。
最后，采用能量函数来提高图像间一致性和图像内平滑度。
在[166]中，Li等人通过两阶段EMR规划了一种低秩加权协同显著性检测框架。
采用两阶段排序方法为每个输入图像创建多个协同显著性图，然后针对每个图像，提取一组可变大小的显著性区域，并将这些协同显著性图与其对应的超像素融合。
然后，通过稀疏误差矩阵设计了每个协同性图的自适应权重。
最后，将协同显著性图及其对应的权重相乘得到融合结果，并利用Graph Cuts进一步优化。
近年来，基于学习的协同显著性检测方法在深度学习、自定进度学习和度量学习等方面引起了广泛的研究关注，并取得了较好的性能。
这些方法直接从给定的图像组中学习协同对象的特征，而是依赖于手工制作的线索。
在[167]中，Zhang等人通过引入贝叶斯框架下的深度观察和宽感知，提出了一种协同显著性目标检测框架。
术语“看深”旨在通过使用具有多层的 CNN 提取高级特征，以发现更好的表示性，而术语“看宽”试图检测一些视觉上相同的邻居，以有效地抑制相互背景区域。
Zhang等[168]通过集成MIL和SPL模型，提出了一种自定进度的多实例学习（SP-MIL）框架，其中多实例学习（MIL）模型指定通过增加类间差异和减少类内差异来训练每个实例的预测因子。
自定进度学习 （SPL） 旨在逐步从简单/忠实的例子学习到更综合/易混淆的例子。
在[169]中，Wei等人提出了一种基于像素到像素的群深度协同显著性检测框架。
引入一个由13个卷积层组成的块来捕获基本特征，然后提取分组属性和单个属性来指定分组属性和单个图像属性。
最后，设计了一种结合卷积-反卷积过程的学习方案，得到协同显著性图。
为了应对图像场景中的广泛变化，Han等[170]通过一种新的目标函数提出了一种度量学习协同显著性模型，其中度量学习旨在学习距离度量，使同一类样本更接近，并使不同类样本彼此远离。
.图 4 .

使用 iCoseg 数据集进行协同显著性检测的示例。
第 1 行显示输入图像，第 2 行表示相应的地面实况图像 讨论：
协同显著性检测是研究界的一个新兴课题，在过去几年中取得了相当大的进展，但该领域仍有非常大的未来改进空间。
在此，我们列举了该领域需要发展的一些主要问题：（1）图像复杂度，对于复杂和杂乱的图像，协同显著性模型需要相当大的改进。
（ 2 ） 如果前景由具有多种颜色的不同类型的物体组成，那么很难只找到突出的物体。
（ 3 ） 协同显著性在大规模数据上表现不佳，因为它包含的异常值较多，噪声和变异 （ 4 ） 协同显著性模型效率低下且消耗更多时间。
（ 5 ） 对应约束需要大量改进，以有效地垄断多个图像之间的公共属性。
表4和图4总结了协同效应技术，显示了几幅图像中常见的突出物体。
2.5 视频显著性

视频序列利用顺序特征、运动和颜色外观信息来感知和识别场景。
在视频显著性中，如果一个物体在视频序列中具有一些重复性、运动相关性和其他一些独特的目标，那么它就是显著的。
这些是利用低级线索的无监督方法，例如颜色外观、运动线索和其他一些先验约束。
传统的基于视频显著性的方法进一步分为基于融合的模型和基于直接流水线的模型[133]。
基于融合的模型首先计算空间显著性（即空间线索，描述每一帧的帧内信息）及其相应的时间显著性（时间线索，表示不同帧之间的帧间关联）。
然后，将这两个显著性图的结果结合起来，得到视频显著性检测。
空间显著性检测利用中心环绕、对比先验、背景先验、稀疏重建和低秩分析来获得每个单独帧的显著性表示，而时间显著性检测则利用运动线索来描述视频中的运动物体。
Fang等[172]利用压缩域中的亮度、颜色和纹理特征获得静态显著性，利用运动线索获得运动显著性，然后利用融合方法实现每帧视频的最终显著性图。
任等[173]通过使用稀疏重建方法检测中心-周围对比度高的区域，获得了空间显著性。
对于时间显著性，使用目标斑块及其相邻重叠斑块的重建过程来重建目标斑块。
最后，应用融合机制来研究视频显著性。
在[174]中，Liu等人提取了超像素级的低级特征和帧级的全局特征来评估空间显著性。
在时间显著性方面，对超像素的运动唯一性进行整合，最后采用自适应融合方法融合时空显著性图。
习等[175]利用背景先验法来表示空间显著性，利用SIFT流动和双向一致传播法来获得时间显著性，并通过简单的加法将两者融合得到最终显著性。
在[176]中，Chen等人分别使用颜色对比和梯度引导对比法对空间和时间显著性图进行处理，并应用融合方法得到最终的显著性。
此类中的模型使用时空特征来直接发现显著对象。
Xue等[177]在视频切片上使用低秩和稀疏分解方案作为时间特征，并将前景与背景分离。
空间信息用于保持所发现运动对象的完整性。
Wang等[178]提出了一种基于梯度流动和能量改进方案的时空显著性方法，适用于复杂场景、不同运动安排和不同外观的处理。
梯度流场通过对帧内和帧间进行积分来描述突出部分。
Liu等[179]利用基于超像素级、空间传播和时间传播的基于图的运动显著性，提出了一种用于视频显著性检测的动态流水线方案。
Guo等[180]通过计算空间显著性和运动显著性，提出了视频显著性方法，然后应用对象提案方案进行排序和投票，对非显著性区域进行过滤，估计初始显著性。
最后，通过考虑时间一致性和外观多样性来提炼初始显著性。
在[181]中，Kim等人利用随机游走和重启随机游走来识别显著性对象，其中利用时间一致性和运动独特性来提取时间一致性，利用快速变化作为随机走者的重启分布。
同样，[182]和[183]提出了一种基于测地线距离的方法，通过使用由时空边缘、外观和运动构建的无向帧间和帧内图来计算超像素显著性。
总而言之，融合技术比直接管道技术更自然。
此外，空间显著性方法是一种图像显著性方法，可以为时空显著性提供基础，并可直接用于视频显著性。
事实上，与现有的传统方法（基于手工制作特征）相比，基于深度学习的视频显著性方法已经表现出了出色的性能。
这些基于学习的方法独立地从每个单独的帧中提取特征，然后利用逐帧处理来计算显著性。
Le等[184]提出了一种深度学习模型来提取时空深度特征（STF）。
采用基于区域的CNN技术提取局部特征，利用基于块的CNN从时间段中提取全局特征。
利用STF特征，提出了随机森林（RF）和时空CRF（CRF）算法，以实现最终显著性。
在[185]中，Wang等人提出了一种深度学习模型来检测视频中的显著物体。
静态网络使用 FCN 为每一帧生成一个固定的显著性图，然后将帧对映射和静态显著性输入到动态网络中以生成动态显著性图。
Le等[186]提出了一种用于视频流显著性区域检测的端到端3D循环全卷积网络（DSRFCN3D），该网络分别包含编码器、解码器和细化网络。
编码器网络从馈送视频块中捕获 3D 特征（空间和时间信息）。
解码器网络通过在每个隐藏的3D反卷积层上通过监督学习逐渐细化中间显著体素，从3D深度特征中估计出精确的显著性体素[101]。
另一方面，利用跳跃连接层和3D递归卷积层（RCL）进行细化方法的设计，用于学习相关的上下文证据。
在[187]中，Li等人通过使用自动编码器的显著性引导堆叠方案引入了无监督视频显著性。
首先，从三个不同阶段（像素级、超像素级和对象级）捕获的时空熟人捕获的显著性线索被收集为高维属性的特征向量。
第二步，通过无监督方式学习堆叠自编码器，得到初始显著性图。
最后，应用一些后处理动作来进一步增强显著性物体并消除虚假线索，同样，Cong等[188]提出了一种稀疏重建和传播方法来检测视频中的显著性目标。
讨论：

综上所述，视频显著性检测也是一个未来研究的新兴领域，因为它在很大程度上尚未得到探索，并且仍有许多挑战需要解决。
视频显著性检测的关键问题是如何消除背景和固定物体，从而在视频中找到更相关的显著点。
为此，主要使用光流，但它不是一种有效的技术，也不能提供更高的精度。
最近，深度学习技术的表现优于传统技术，但深度学习的主要问题是无法使用大型注释数据集进行视频显著性检测。
视频显著性检测的下一个关键问题是找到鲁棒的技术来捕获帧间属性，为所有帧提供一致的外观显著性图，为此，需要调整一些能量函数以提高一致性，但仍然需要进一步改进。
视频显著性也需要改进，其中大多数帧由复杂的背景和多个对象组成。
视频显著性检测摘要如表5所示，一些示例视频帧如图5所示错误！
未找到参考源“，它在同一视频的不同帧中显示相同的突出对象。
3. 数据集和应用程序

3.1 显著性检测数据集：

在本节中，我们介绍了用于显著性检测技术的最常见数据集，例如 RGB-D 显著性检测、协同显著性检测和视频显著性检测。
随着显著性检测技术的进步，引入了更具挑战性的数据集，以进一步挑战最先进的模型。
早期的数据集包含非常简单的背景和前景中的单个图像，并使用边界框方法（例如MSRA-A和MSRA-B）[192]对地面实况进行注释。
最近的数据集非常复杂和杂乱，背景有多个对象，通过像素级的地面实况标注进行标注，基于像素的标注数据集比边界框标注的结果更准确。
对于简单的RGB图像显著性检测，我们共收集了10个数据集，如表6所示，如5] ， UCSB [194] ， OSIE [195] ， ECSSD [25] ， DUT-OMRON [61] ， MSRA10K [196] ， ACSD [51] 和 XPIE [198] 。
有一些数据集还包含注视数据，这些数据是在免费观看过程中为每张图像收集的，例如 Judd-A、UCSB 和 OSIE。
RGBD数据集列表由RGBD1000 [137] ， NJUD [140] ， DES [199]组成，如表7所示。图 6 显示了来自 PASCAL 挑战数据集的一些示例图像。

对于协同显著性检测，我们总共列出了8个常用的数据集，如表8 VOS [187]和DAVIS [208]所示，如表9所示。
DAVIS 数据集是常用且更具挑战性的数据集之一，包含 50 个视频系列以及每个视频帧的像素级地面实况。
UVSD 数据集是一种新的数据集，专为视频显著性检测而设计，其中包含 18 个无限制的视频，这些视频具有复杂的运动模式和更分散的场景，每个视频帧都有像素注释的地面实况。
创建了一个称为 VOS 的扩展视频显著性检测数据集，该数据集包含 116103 个总帧，分布在 200 个（即
200） 视频序列。
该数据集包含 7467 个二进制地面实况注释帧，这足以训练和学习深度学习模型来捕获视频中的显著对象。
数据集是特定应用程序域的数据集合。
不幸的是，每个数据集都可能遭受不同类型的偏差，这可能会影响模型的性能。
例如，Torralba和Efros承认了计算机视觉领域的三种偏见，称为选择偏见、捕获偏见（即中心偏见）和负集偏见[209]。
当某人在数据组装过程中更喜欢特定类型的图像时，就会发生选择偏差，并且可能会产生错误，因为个人更喜欢自己的选择，而违反了选择的标准规则。
选择偏差在数据集中收集更多相似的图像，因此在数据集中缺乏可变性。
为了避免选择偏差，有必要有一个独立的选择。
捕获偏差将图像结构的影响传递到数据集中（即人们倾向于以相似的方式捕获相似物体的图像），而数据集中也缺乏可变性。
例如，中心偏置意味着大多数捕获的物体位于图像的中心。
这种类型的偏差使数据集难以进行定量比较，有时甚至会产生模棱两可的比较。
例如，在图像中心包含高斯斑点的微不足道的显著性方法总是比许多注视预测方法产生最好的分数 [79 ] 。
负集偏差表示个人不喜欢将特定对象包含在数据集中，而数据集必须代表所有可能的事物。
负集偏差可以通过对某些特定对象使用注释者的特定偏好来扰乱基本事实。
因此，鼓励在一个好的数据集中拥有更多种类的图像。
3.2 显著性检测的应用：

显著性检测技术通常用于图像检索[210,211][43,227]，图像编辑和操作[228,229]，人机交互[230,231]和视觉跟踪[36,232,233]等领域。
3.3 评价措施

定性和定量评估技术是评估显著性目标检测模型性能的两种常用技术。
定性技术直观地将预测的显著性图与其相应的地面实况掩码进行比较。
这是更简单的技术，但它没有固定的值，因此因人而异。
另一方面，定量评估给出了一个固定的值，每个观察者都可以接受。
文献中提供了不同类型的评估技术，用于将预测的显著性图与其相应的地面实况进行比较。
在这里，我们只讨论被视为显著目标检测标准的前五种标准技术。
所有这些技术都考虑了预测地图与其相应的地面实况掩码之间的重叠区域。
对于数学表示法，我们使用 G 表示真值掩码，使用 S 表示预测的显著性图。
我们使用 |• |对于两个二进制掩码，以指示掩码中的条目数。

图 6 .
来自PASCAL挑战数据集的一些示例图像。
这个过程的关键阶段是 S 到 B 的二值化。三种最常见的方法，如固定阈值、自适应阈值 [51] 和 GrabCut 方法 [235]，掩码用于二值化过程。

2. F-测量。

精确度和召回率无法全面估计显著性图的优劣性。
为此，F-measure 方法定性为非负权重的 Precision 和 Recall 方法的谐波加权均值
而 P 是精度，R 代表召回率。
2 值通常设置为 0.3 以比召回率 [51] 更能提高精度的权重。
3. 接收机工作特性

（ROC） 曲线。
同样，真阳性率 （TPR） 和假阳性率 （FPR） 可以通过在显著性映射二值化期间应用固定阈值来计算。
其中 和 分别表示二进制掩码和真值 G 的补码集。
ROC 曲线是通过尝试所有可能的阈值来绘制 TPR 值与 FPR 值的对比。
4. 中华民国曲线下 （AUC）。

顾名思义，它被计算为 ROC 曲线下的面积。
在完美显著性方法下，AUC 的表现将恰好得到 1 分，而 AUC 在随机猜测下的表现将获得大约 0.5 分。
5. 均值绝对误差 （MAE）。

上述基于重叠的评估措施实际上并不关注真正负显著性值的分配（即，正确标记为非显著性的像素）。
他们更喜欢那些能够有效地为显著像素分配高显著性值的方法，但大多数情况下，他们忽略了对非显著性区域的检测。
此外，对于某些应用[223]，显著性映射有时比其二进制掩码需要更多的考虑。
因此，平均绝对误差（MAE）是一种简单可靠的显著性图评估度量。
它的计算方式是显著性映射 S 与相应的真值 G 之间的像素级绝对误差的平均值，归一化为 [0， 1]，其定义如下：
其中 H 和 W 分别表示图像的高度和宽度。
[9] ， （d） FES [69] ， （e） GR [72] ， （f） MC [58] ， （g） ELD [88] ， （h） PiCANet [236]， （i） NLDF [70] ， （j） RAS [118] .
3.1. 讨论和未来建议：

在这篇综述中，我们全面介绍了关于显著目标检测的调查，并讨论了传统的基于启发式的方法和新的基于学习的方法。
我们还讨论了注视预测、RGBD显著性检测、协同显著性检测和视频偏离性检测等相关领域。
图 7 显示了一些示例启发式模型和基于学习的模型的视觉比较，这清楚地表明，基于深度学习的模型在最先进的模型中表现更好。
这篇综述为显著性检测领域即将取得的进展提供了深入的见解和指南。
基于启发式的方法遵循内在线索，因此这些方法在特定环境中运行良好，在其他场景中不能很好地推广。
最近，基于深度学习的模型比传统的基于启发式的方法表现出了出色的性能。
基于深度学习的方法遵循外在线索，可以从大型数据集中收集高级语义知识，因此具有在不同场景中很好地泛化的能力。
这些方法也称为任务驱动方法，因为它们可以从特定数据集中学习特征，并且可以有效地将学到的知识应用于其他环境。
尽管深度学习方法的性能优于所有传统的基于启发式的方法，但它们仍有许多问题需要在未来解决。
以下是未来需要解决的一些重要问题：
需要大数据进行训练：基于学习的技术在训练过程中需要大量数据来提取特征，在不同环境中拥有大量数据是非常困难的。
为了解决这个问题，已经提出了不同的增强技术来创建虚假数据。
然而，性能仍然不如原始数据集，需要进一步努力。
另一种选择是设计这样一个可以在很少数据上训练的模型。
Encoderdecoder模型相对需要较少的数据，需要进一步探索。
数据集偏差：如果收集器违反了标准规则，数据集偏差也会降低性能。
为此，需要适当的知识来收集数据集。
由于池化和步幅导致的特征损失：在基于学习的方法中，由于池化和步幅操作的不同，图像的分辨率变得越来越小，并导致在训练过程中丢失重要特征。
为此，鼓励采用不同的多尺度、多级、跳连、短接网络来恢复损失特征。
手动注释：

基于学习的方法需要对数据集中的每个相应实例进行手动注释。
使用相应的像素级标注生成大数据是非常困难的。
为此，未来鼓励无监督学习。
无监督学习方法比监督学习方法最省时。
复杂的背景：CNNs技术在简单的背景图像中取得了巨大的成功。
然而，复杂杂乱的背景图像在显著性目标检测方面仍有很大要求。
众所周知，显著性检测具有广泛的应用前景，引起了研究者的广泛关注。
对于这个 1.
实例级显著性对象检测：最近的显著性对象检测方法是客观无关的（即，显著性区域不会分裂成对象），但是，人类具有在实例级别分割显著性或刺激性对象的才能。
实例级显著性方法可用于多种应用，例如视频压缩和照片编辑。
2. 灵活

3. 不同模块之间的协作：

在计算机视觉中，对象分割、对象检测、对象跟踪和对象分类等常见任务之间的协作和信息共享相互促进，极大地促进了彼此之间的合作和共享。
同样，来自其他模块的上下文和先验信息也可以提高显著目标检测。
特别是，探索显著性目标检测、注视预测和语义感知模型之间的关联可以相互受益。
4.

将显著性目标检测行为延伸到其他领域：除了图像和视频，视觉显著性概念还可以扩展到语音识别、听觉感知、触摸行为和场景字幕。
5. 3D

目标检测：RGB-D图像可以提高显著性目标检测的性能，然而，该领域的工作范围非常狭窄。
6. 协同显著性和视频显著性需要更先进的技术：

在协同显著性检测中，使用图像间对应技术来查找一组图像中的常见显著性对象。
为此，采用了不同的技术。
但是，将来需要多加考虑。
同样，在视频显著性方面，帧间对应技术也需要进一步探索，以找到多帧之间的鲁棒关联，以便进行显著性目标检测。
7. 可解释的深度学习模型：

惰性技术可以帮助理解特定模型在特定场景中的预测。
通过使用这些方法，我们可以了解哪种类型的数据集、模型和超参数可以在显著性对象检测中表现出色。
8. 基于情绪的显著性检测：

将基于视觉的显著性模型与基于情感的模型相结合，可以用于扩展显著性检测的性能。
这些模型发现了显著性与情感的关系，即图像如何唤起人类情感。
图 1.
显著性物体及其相应的地面实况的示例。
图2.人类眼睛注视预测的例子。

[147]，Wang等人利用融合策略提出了一种双流CNN。
同样，在[148]中，Liu等人提出了一种基于融合的RGBD显著性检测双流网络。
深度结构信息有助于前景和背景识别。
然后，使用基于传播的模块来识别目标边界。
图3.
RGBD 图像中的 3D 显著性条件。
（a） 颜色-深度显著性：RGB图像和深度图像均显著。
（二）颜色显著性：只有 RGB 图像是显着的。

（c） 深度显著性：只有深度图像才具有显著性。
图5.
DAVIS数据集上的视频显著性检测示例。
第一行表示输入数据的原始视频帧，第二行表示相应的地面实况。
图7.
显著性图的视觉比较。
（a） 原始图像，（b） 地面实况，（c） DSR
图8.
非学习模型和基于学习的模型的 MAE 分数图。
* 表示基于启发式的显著目标检测模型。
跟踪研究趋势可能在未来发挥重要作用。
提出了一个多任务CNN
基于深度学习的显著性检测模型的简要总结。
Zeng 等人合并了三个独立的 CNN，分别用于全局特征、局部特征和空间一致性。
Feng等[120]设计了Attentive Feedback和Boundary-Enhanced Loss用于提取结构和边界特征。
同样，在[121]中，Qi等人提出了一种带有编码器-解码器模块的预测-优化架构，以获得具有更精细边界的显著性图。
RGBD 显著性检测的简要总结。
协同显著性检测的简要总结。
视频显著性检测模型的简要总结。
RGB 的显著性目标检测数据集列表。
RGBD 图像的显著性目标检测数据集列表。
协同显著性检测数据集的列表。
网络架构：验证了更深层次的CNNs模型能够基于其高层次语义知识捕获更准确的显著对象。
为此，像 ResNet 这样的更深层次的网络可能是未来更可取的选择。
同样，为了避免特征丢失，编码器-解码器和多级网络在模型选择中也能表现出色。
