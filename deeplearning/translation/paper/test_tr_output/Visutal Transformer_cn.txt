一张图片胜过 16X16 个字：用于大规模图像识别的变压器

虽然 Transformer 架构已成为自然语言处理任务的事实标准，但它在计算机视觉中的应用仍然有限。
在视觉中，注意力要么与卷积网络结合使用，要么用于替换卷积网络的某些组件，同时保持其整体结构不变。
我们发现，这种对CNN的依赖是不必要的，直接应用于图像补丁序列的纯转换器可以在图像分类任务中表现出色。
当对大量数据进行预训练并转移到多个中小型图像识别基准测试（ImageNet、CIFAR-100、VTAB 等）时，与最先进的卷积网络相比，Vision Transformer （ViT） 获得了出色的结果，同时需要更少的计算资源来训练。
1 引言

基于自我注意力的架构，特别是 Transformers（Vaswani et al.， 2017），已成为自然语言处理 （NLP） 的首选模型。
主要方法是在大型文本语料库上进行预训练，然后在较小的特定任务数据集上进行微调（Devlin et al.， 2019）。
由于 Transformer 的计算效率和可扩展性，可以训练具有超过 100B 参数的空前规模的模型（Brown 等人，2020 年;Lepikhin 等人，2020 年）。

随着模型和数据集的增长，仍然没有出现性能饱和的迹象。
然而，在计算机视觉中，卷积架构仍然占主导地位（LeCun et al.， 1989;Krizhevsky 等人，2012 年;He et al.， 2016） 。

受到 NLP 成功的启发，多部作品尝试将类似 CNN 的架构与自我关注相结合（Wang et al.， 2018;Carion 等人，2020 年），其中一些完全替换了卷积（Ramachandran 等人，2019 年;Wang et al.， 2020a） .

后一种模型虽然在理论上是有效的，但由于使用了专门的注意力模式，因此尚未在现代硬件加速器上有效地扩展。
因此，在大规模图像识别中，经典的ResNetlike架构仍然是最先进的（Mahajan等人，2018;Xie等，2020;Kolesnikov 等人，2020 年）。

受到 NLP 中 Transformer 缩放成功的启发，我们尝试将标准 Transformer 直接应用于图像，尽可能少地修改。
为此，我们将图像分割成多个补丁，并提供这些补丁的线性嵌入序列作为 Transformer 的输入。
在 NLP 应用程序中，图像补丁的处理方式与标记（单词）相同。
我们以监督方式训练模型进行图像分类。
在没有强正则化的情况下，在中等规模的数据集（如ImageNet）上进行训练时，这些模型的准确率比同等大小的ResNet低几个百分点。
这种看似令人沮丧的结果可能是意料之中的：Transformer 缺乏 CNN 固有的一些归纳偏差，例如翻译等价性和局部性，因此在训练数据量不足时不能很好地泛化。
但是，如果模型在更大的数据集（14M-300M 图像）上训练，情况会发生变化。
我们发现，大规模的训练胜过归纳偏见。
我们的视觉转换器 （ViT） 在进行足够规模的预训练并转移到数据点较少的任务时，可以获得出色的结果。
当在公共 ImageNet-21k 数据集或内部 JFT-300M 数据集上进行预训练时，ViT 在多个图像识别基准测试上接近或击败了最先进的技术。
其中，最佳模型在ImageNet上达到88.55%的准确率，在ImageNet-ReaL上达到90.72%，在CIFAR-100上达到94.55%，在VTAB套件的19个任务上达到77.63%。
2 相关工作

Transformer 由 Vaswani 等人 （2017） 提出用于机器翻译，此后已成为许多 NLP 任务中最先进的方法。
基于 Transformer 的大型模型通常在大型语料库上进行预训练，然后针对手头的任务进行微调：BERT （Devlin et al.， 2019） 使用去噪自监督预训练任务，而 GPT 工作线使用语言建模作为其预训练任务 （Radford et al.， 2018; 2019;Brown 等人，2020 年）。

对图像的自我关注的幼稚应用将要求每个像素都关注其他每个像素。
由于像素数是二次成本，因此这不会缩放到实际的输入大小。
因此，为了在图像处理环境中应用Transformers，过去已经尝试了几种近似方法。
Parmar et al. （2018） 仅在每个查询像素的局部社区中应用了自我关注，而不是全局应用。
这种局部多头点积自注意力块可以完全替代卷积（胡等，2019;Ramachandran 等人，2019 年;Zhao et al.， 2020） .

在不同的工作领域，稀疏变压器（Child et al.， 2019）采用可扩展的近似值来适应全局自注意力，以便适用于图像。
调整注意力的另一种方法是将其应用于不同大小的块（Weissenborn 等人，2019 年），在极端情况下仅沿单个轴（Ho 等人，2019 年;Wang et al.， 2020a） .

这些专业的注意力架构中有许多在计算机视觉任务上显示出有希望的结果，但需要复杂的工程设计才能在硬件加速器上有效地实施。
与我们最相关的是 Cordonnier 等人 （2020） 的模型，该模型从输入图像中提取大小为 2 × 2 的斑块，并在顶部应用完全的自注意力。
这个模型与ViT非常相似，但我们的工作进一步证明，大规模预训练使普通转换器与最先进的CNN竞争（甚至更好）。
此外，Cordonnier 等人 （2020） 使用 2 × 2 像素的小块尺寸，这使得该模型仅适用于小分辨率图像，而我们也处理中等分辨率图像。
人们也对将卷积神经网络 （CNN） 与自我注意力形式相结合产生了浓厚的兴趣，例如通过增强特征图进行图像分类（Bello et al.， 2019） 或通过使用自注意力进一步处理 CNN 的输出，例如用于物体检测 （胡 et al.， 2018;Carion et al.， 2020） ， 视频处理 （Wang et al.， 2018;Sun et al.， 2019） 、图像分类 （Wu et al.， 2020） 、无监督对象发现 （Locatello et al.， 2020） 或统一文本视觉任务 （Chen et al.， 2020c;Lu et al.， 2019;Li 等人，2019 年）。

另一个最近的相关模型是图像 GPT （iGPT） （Chen et al.， 2020a） ，它在降低图像分辨率和色彩空间后将 Transformer 应用于图像像素。
该模型以无监督方式作为生成模型进行训练，然后可以对生成的表示进行微调或线性探测以获得分类性能，在 ImageNet 上实现 72% 的最大准确率。
我们的工作增加了越来越多的论文集，这些论文在比标准ImageNet数据集更大规模上探索图像识别。
使用额外的数据源可以在标准基准上获得最先进的结果（Mahajan et al.，2018;Touvron 等人，2019 年;Xie等人，2020）。

此外，Sun et al. （2017） 研究了 CNN 性能如何与数据集大小成比例，Kolesnikov et al. （2020） ;Djolonga et al. （2020） 对来自 ImageNet-21k 和 JFT-300M 等大规模数据集的 CNN 迁移学习进行了实证探索。

我们也关注后两个数据集，但训练的是 Transformers，而不是之前工作中使用的基于 ResNet 的模型。
3 方法

在模型设计中，我们尽可能紧密地遵循原始变压器（Vaswani et al.， 2017）。
这种有意简化的设置的一个优点是，可扩展的 NLP Transformer 架构及其高效的实现几乎可以开箱即用。
3.1 视觉转换器（VIT）

该模型的概述如图 1 所示。
标准 Transformer 接收一个 1D 令牌嵌入序列作为输入。
为了处理 2D 图像，我们将图像 x ∈ R H×W ×C 重塑为一系列扁平化的 2D 斑块 x p ∈ R N ×（P 2 •C） ，其中 （H， W ） 是原始图像的分辨率，C 是通道数，（P， P ） 是每个图像斑块的分辨率，N = HW/P 2 是生成的斑块数， 这也用作 Transformer 的有效输入序列长度。

Transformer 在其所有层中使用恒定的潜在向量大小 D，因此我们展平了面片并使用可训练的线性投影 （Eq.
1).
我们将此投影的输出称为面片嵌入。
与 BERT 的 [class] 标记类似，我们在嵌入补丁序列 （z 0 0 = x class） 中预置了一个可学习的嵌入，其在 Transformer 编码器 （z 0 L ） 输出端的状态用作图像表示 y （Eq.
4).
在预训练和微调期间，分类头都连接到 z 0 L 。
分类头由MLP实现，在预训练时有一个隐藏层，在微调时由一个线性层实现。
位置嵌入被添加到补丁嵌入中，以保留位置信息。
我们使用标准的可学习 1D 位置嵌入，因为我们没有观察到使用更高级的 2D 感知位置嵌入（附录 D.4）显着的性能提升。
生成的嵌入向量序列用作编码器的输入。
Transformer 编码器（Vaswani et al.， 2017）由交替的多头自注意力层（MSA，参见附录 A）和 MLP 块（方程。
2, 3).
Layernorm （LN） 在每个块之前应用，残差连接在每个块之后应用（Wang et al.， 2019;Baevski&Auli，2019）。

MLP包含两个具有GELU非线性的层。
感应偏置。
我们注意到，与CNN相比，Vision Transformer具有更少的图像特异性感应偏差。
在 CNN 中，局部性、二维邻域结构和平移等方差被融入整个模型的每一层。
在ViT中，只有MLP层是局部的和平移等变的，而自注意力层是全局的。
二维邻域结构的使用非常少：在模型开始时，通过将图像切割成块，并在微调时间调整不同分辨率图像的位置嵌入（如下所述）。
除此之外，初始化时的位置嵌入不携带有关补丁的 2D 位置的信息，并且补丁之间的所有空间关系都必须从头开始学习。
混合架构。
作为原始图像补丁的替代方法，输入序列可以由CNN的特征图形成（LeCun等人，1989）。
在这个混合模型中，补丁嵌入投影E（方程1）应用于从CNN特征图中提取的补丁。
作为一种特殊情况，补丁的空间大小可以是 1x1，这意味着输入序列是通过简单地展平特征图的空间维度并投影到 Transformer 维度来获得的。
如上所述，添加了分类输入嵌入和位置嵌入。
3.2 微调和更高分辨率

通常，我们会在大型数据集上预训练 ViT，并针对（较小的）下游任务进行微调。
为此，我们删除了预训练的预测头，并在 K 个前馈层附加一个零初始化× D，其中 K 是下游类的数量。
与预训练相比，以更高的分辨率进行微调通常是有益的（Touvron 等人，2019 年;Kolesnikov 等人，2020 年）。

当提供更高分辨率的图像时，我们保持补丁大小不变，这会导致更大的有效序列长度。
Vision Transformer 可以处理任意序列长度（直到内存约束），但是，预训练的位置嵌入可能不再有意义。
因此，我们根据预训练位置嵌入在原始图像中的位置对它们进行 2D 插值。
请注意，这种分辨率调整和补丁提取是将有关图像 2D 结构的感应偏置手动注入 Vision Transformer 的唯一点。
4 实验

我们评估了 ResNet、Vision Transformer （ViT） 和混合体的表示学习能力。
为了理解每个模型的数据需求，我们在不同大小的数据集上进行预训练，并评估许多基准任务。
当考虑到预训练模型的计算成本时，ViT的表现非常有利，在大多数识别基准上以较低的预训练成本达到了最先进的水平。
最后，我们使用自我监督进行了一个小实验，并表明自我监督的ViT对未来充满希望。
4.1 设置

数据。
为了探索模型的可扩展性，我们使用了具有1k类和1.3M图像的ILSVRC-2012 ImageNet数据集（我们在下面将其称为ImageNet），其超集ImageNet-21k具有21k类和14M图像（邓等人，2009），以及具有18k类和303M高分辨率图像的JFT（Sun等人，2017）。
我们根据 Kolesnikov 等人 （2020） 对下游任务的测试集对预训练数据集进行了重复数据删除。
我们将在这些数据集上训练的模型转移到几个基准任务中：原始验证标签上的ImageNet和清理后的ReaL标签（Beyer等人，2020），CIFAR-10/100（Krizhevsky，2009），Oxford-IIIT Pets（Parkhi等人，2012）和Oxford Flowers-102（Nilsback和Zisserman，2008）。
对于这些数据集，预处理遵循 Kolesnikov 等人 （2020） 。
我们还评估了 19 个任务的 VTAB 分类套件（Zhai et al.， 2019b）。
VTAB评估到不同任务的低数据传输，每个任务使用1000个训练样本。
任务分为三组：自然 - 如上述任务、宠物、CIFAR 等。专业 - 医疗和卫星图像，以及需要几何理解（如定位）的结构化任务。

模型变体。
我们将ViT配置基于用于BERT的配置（Devlin等人，2019），如表1所示。
“Base”和“Large”模型直接从BERT采用，我们添加了更大的“Huge”模型。
在下文中，我们使用简短的表示法来表示模型大小和输入补丁大小：例如，ViT-L/16 表示输入补丁大小为 16 × 16 的“大”变体。
请注意，Transformer 的序列长度与补丁大小的平方成反比，因此补丁大小较小的模型在计算上成本更高。
对于基线CNN，我们使用ResNet（He et al.， 2016），但用组归一化（Wu&He， 2018）替换批量归一化层（Ioffe&Szegedy，2015），并使用标准化卷积（Qiao et al.， 2019）。
这些修改改善了迁移（Kolesnikov et al.， 2020），我们表示修改后的模型“ResNet （BiT）”。
对于混合体，我们将中间特征图输入到ViT中，补丁大小为一个“像素”。
为了试验不同的序列长度，我们要么 （i） 获取常规 ResNet50 的第 4 阶段的输出，要么 （ii） 删除第 4 阶段，在第 3 阶段放置相同数量的层（保持层总数），并获取此扩展阶段 3 的输出。选项（ii）的序列长度延长了4倍，ViT模型成本更高。

培训和微调。
我们使用Adam（Kingma&Ba，2015）训练所有模型，包括ResNets，β 1 = 0.9，β 2 = 0.999，批量大小为4096，并应用0.1的高权重衰减，我们发现这对于所有模型的传输都很有用（附录D.1显示，与通常的做法相比，在我们的设置中，Adam的ResNets效果略好于SGD）。
我们使用线性学习率预热和衰减，详见附录 B.1。
对于微调，我们使用带动量的 SGD，批量大小为 512，对于所有模型，请参阅附录 B.1.1。
对于表 2 中的 ImageNet 结果，我们在更高的分辨率下进行了微调：ViT-L/16 为 512，ViT-H/14 为 518，并且还使用 Polyak & Juditsky （1992） 平均因子为 0.9999 （Ramachandran et al.， 2019;Wang et al.， 2020b） .

指标。
我们通过少样本或微调精度来报告下游数据集的结果。
微调精度是在各自的数据集上微调每个模型后捕获每个模型的性能。
通过求解正则化最小二乘回归问题获得少样本精度，该问题将训练图像子集的（冻结）表示映射到 {-1， 1} K 个目标向量。
该配方使我们能够以封闭形式回收确切的溶液。
虽然我们主要关注微调性能，但我们有时会使用线性小脉冲精度进行快速即时评估，而微调的成本太高。
4.2 与最新技术的比较

我们首先将我们最大的模型 ViT-H/14 和 ViT-L/16 与文献中最先进的 CNN 进行比较。
第一个比较点是大迁移（BiT）（Kolesnikov等人，2020），它使用大型ResNets执行监督迁移学习。
第二个是 Noisy Student （Xie et al.， 2020） ，这是一个大型 EfficientNet，在 ImageNet 和 JFT-300M 上使用半监督学习进行训练，去除了标签。
目前，Noisy Student 是 ImageNet 和 BiT-L 上最新技术，适用于此处报告的其他数据集。
所有模型都在 TPUv3 硬件上训练，我们报告了预训练每个模型所需的 TPUv3 核心天数，即用于训练的 TPU v3 核心数（每个芯片 2 个）乘以天的训练时间。
与先前的技术状态相比，模型预训练所需的计算量仍然大大减少。
然而，我们注意到，预训练效率不仅可能受到架构选择的影响，还可能受到其他参数的影响，如训练计划、优化器、权重衰减等。
我们在第 4.4 节中提供了不同架构的性能与计算的对照研究。
最后，在公共 ImageNet-21k 数据集上预训练的 ViT-L/16 模型在大多数数据集上也表现良好，同时需要更少的资源来预训练：它可以在大约 30 天内使用具有 8 个核心的标准云 TPUv3 进行训练。
图 2 将 VTAB 任务分解为各自的组，并在此基准上与以前的 SOTA 方法进行了比较：在 ImageNet 和 Youtube 上共同训练的 BiT、VIVI -a ResNet （Tschannen et al.， 2020） 和 ImageNet 上的 S4L -supervised plus semi-supervised learning （Zhai et al.， 2019a） 。
ViT-H/14 在自然和结构化任务上优于 BiT-R152x4 和其他方法。
在 Specialized 上，前两个模型的性能相似。
4.3 训练前数据要求

Vision Transformer 在大型 JFT-300M 数据集上进行预训练时表现良好。
与 ResNet 相比，视觉的归纳偏差更少，数据集大小有多重要？
我们进行了两个系列的实验。
首先，我们在越来越大的数据集上预训练 ViT 模型：ImageNet、ImageNet-21k 和 JFT-300M。
为了提高较小数据集的性能，我们优化了三个基本的正则化参数——权重衰减、丢弃和标签平滑。
图 3 显示了对 ImageNet 进行微调后的结果（其他数据集的结果显示在 其次，我们在 9M、30M 和 90M 的随机子集以及完整的 JFT-300M 数据集上训练我们的模型。
我们不会对较小的子集执行额外的正则化，而是对所有设置使用相同的超参数。
通过这种方式，我们评估了内在模型的属性，而不是正则化的影响。
但是，我们确实使用早期停止，并报告在训练期间实现的最佳验证准确性。
为了节省计算量，我们报告了少量线性精度，而不是完全微调精度。
图 4 包含结果。
在较小的数据集上，Vision Transformer 比 ResNet 过拟合更多，计算成本相当。
例如，ViT-B/32 比 ResNet50 略快;它在 9M 子集上的表现要差得多，但在 90M+ 子集上表现更好。

ResNet152x2 和 ViT-L/16 也是如此。
这一结果强化了一种直觉，即卷积归纳偏差对于较小的数据集很有用，但对于较大的数据集，直接从数据中学习相关模式就足够了，甚至是有益的。
总体而言，ImageNet上的少样本结果（图4）以及VTAB上的低数据结果（表2）对于非常低的数据传输来说似乎很有希望。
进一步分析ViT的少样本特性是未来工作的一个令人兴奋的方向。
在ICLR 2021上作为会议论文发表
4.4 比例研究

我们通过评估JFT-300M的传输性能，对不同模型进行了受控缩放研究。
在此设置中，数据大小不会影响模型的性能，并且我们评估每个模型的性能与训练前成本。
模型集包括：7 个 ResNets、R50x1、R50x2、R101x1、R152x1、R152x2，预训练 7 个时期，加上 R152x2 和 R200x3，预训练 14 个时期;6 个视觉转换器，ViT-B/32、B/16、L/32、L/16，预训练了 7 个时期，加上 L/16 和 H/14 预训练了 14 个时期;以及 5 个杂交种，R50+ViT-B/32、B/16、L/32、L/16 预训练 7 个 epoch，加上 R50+ViT-L/16 预训练 14 个 epoch（对于杂交种，模型名称末尾的数字不代表补丁大小，而是代表 ResNet 骨干网中的总 dowsampling 比率）。

图 5 包含传输性能与总预训练计算的关系（有关计算成本的详细信息，请参阅附录 D.5）。
每个模型的详细结果在附录的表 6 中提供。
可以观察到一些模式。
首先，Vision Transformer 在性能/计算权衡方面主导了 ResNet。
ViT 使用大约 2 -4× 的计算来获得相同的性能（平均超过 5 个数据集）。
其次，混合模型在较小的计算预算下略优于ViT，但对于较大的模型，这种差异消失了。
这个结果有些令人惊讶，因为人们可能会期望卷积局部特征处理能够帮助任何规模的ViT。
第三，Vision Transformer 似乎没有在尝试的范围内饱和，这激发了未来的扩展努力。
为了开始了解Vision Transformer如何处理图像数据，我们分析了其内部表示。
Vision Transformer 的第一层将扁平化的块线性投射到低维空间中（方程。
图7（左）显示了学习到的嵌入滤波器的顶级主成分。
这些分量类似于每个斑块内精细结构的低维表示的合理基函数。
4.5 检查视觉变压器

输入注意

在投影之后，学习到的位置嵌入被添加到面片表示中。
图 7（中）显示，该模型根据位置嵌入的相似性学习编码图像内的距离，即更近的补丁往往具有更相似的位置嵌入。
此外，出现行列结构;同一行/同一列中的补丁具有相似的嵌入。

最后，对于较大的网格，正弦结构有时是显而易见的（附录 D）。
位置嵌入学习表示 2D 图像拓扑解释了为什么手工制作的 2D 感知嵌入变体不会产生改进（附录 D.4）。
自注意力使ViT能够将信息整合到整个图像中，即使在最低层也是如此。
我们调查了网络在多大程度上利用了此功能。
具体来说，我们根据注意力权重计算信息集成在图像空间中的平均距离（图7，右）。
这种“注意力距离”类似于CNN中的感受野大小。
我们发现，一些头部已经关注了最低层的大部分图像，这表明模型确实使用了全局整合信息的能力。
其他注意力头在低层的注意力距离始终很小。
在将 ResNet 应用于 Transformer 之前的混合模型中，这种高度局部的注意力不太明显（图 7，右），这表明它可能具有与 CNN 中早期卷积层类似的功能。
此外，注意力距离随着网络深度的增加而增加。
在全球范围内，我们发现该模型关注的是语义上与分类相关的图像区域（图 6）。
4.6 自我监督

Transformer 在 NLP 任务上表现出令人印象深刻的性能。
然而，他们的成功很大程度上不仅源于其出色的可扩展性，还源于大规模的自我监督预训练（Devlin 等人，2019 年;Radford 等人，2018 年）。

我们还对用于自我监督的掩码补丁预测进行了初步探索，模仿了BERT中使用的掩码语言建模任务。
通过自监督预训练，我们较小的 ViT-B/16 模型在 ImageNet 上实现了 79.9% 的准确率，与从头开始训练相比显着提高了 2%，但仍落后于监督预训练 4%。
附录 B.1.2
包含更多详细信息。
我们留下了对比预训练的探索（Chen et al.， 2020b;He 等人，2020 年;Bachman 等人，2019 年;Hénaff 等人，2020 年）到未来的工作。

5 结论

我们已经探索了Transformers在图像识别中的直接应用。
与之前在计算机视觉中使用自注意力的工作不同，除了最初的补丁提取步骤外，我们不会在架构中引入特定于图像的感应偏差。
取而代之的是，我们将图像解释为一系列补丁，并通过 NLP 中使用的标准 Transformer 编码器对其进行处理。
这种简单但可扩展的策略在与大型数据集上的预训练相结合时效果惊人。
因此，Vision Transformer 在许多图像分类数据集上匹配或超过最先进的技术，同时预训练成本相对较低。
虽然这些初步结果令人鼓舞，但仍然存在许多挑战。
一种是将ViT应用于其他计算机视觉任务，例如检测和分割。
我们的研究结果与Carion等人（2020）的研究结果相结合，表明了这种方法的前景。
另一个挑战是继续探索自我监督的预训练方法。
我们的初步实验表明，自监督预训练有所改善，但自监督预训练和大规模监督预训练之间仍然存在很大差距。
最后，ViT的进一步扩展可能会导致性能的提高。
对于 ResNets，我们还运行 Kolesnikov 等人 （2020） 的设置，并选择此运行和我们的扫描中的最佳结果。
最后，如果没有特别提及，所有微调实验都以 384 分辨率运行（以与训练不同的分辨率运行微调是常见的做法（Kolesnikov 等人，2020 年））。
当将 ViT 模型转移到另一个数据集时，我们删除了整个头部（两个线性层），并将其替换为一个零初始化的线性层，该线性层输出目标数据集所需的类数。
我们发现这比简单地重新初始化最后一层要健壮一些。
对于 VTAB，我们遵循 Kolesnikov et al. （2020） 中的协议，并对所有任务使用相同的超参数设置。
我们使用 0.01 的学习率并训练 2500 步 （Tab.
4).
我们通过对两个学习率和两个计划进行小幅扫描来选择此设置，并在 200 个样本验证集上选择具有最高 VTAB 分数的设置。
我们遵循 Kolesnikov et al. （2020） 中使用的预处理，只是我们不使用特定于任务的输入分辨率。
相反，我们发现 Vision Transformer 从所有任务的高分辨率 （384 × 384） 中受益最大。
B.1.2 自我监督

我们采用掩蔽斑块预测物镜进行初步的自我监督实验。
为此，我们通过用可学习的 [mask] 嵌入 （80%）、随机的其他 patch 嵌入 （10%） 替换它们的嵌入或仅保持原样 （10%） 来破坏 50% 的补丁嵌入。
这种设置与Devlin等人（2019）用于语言的设置非常相似。
最后，我们使用各自的补丁表示来预测每个损坏的补丁的 3 位平均颜色（即总共 512 种颜色）。
我们训练了 1M 步长 （ca.
14 个时期），JFT 上的批量大小为 4096。
我们使用 Adam，基础学习率为 2 • 10 -4，预热为 10k 步，余弦学习率衰减。
作为预训练的预测目标，我们尝试了以下设置：1） 仅预测均值 3 位颜色（即 1 种预测 512 种颜色），2） ×预测 16 × 16 个并行颜色的 4  4 个缩小版本（即，512 种颜色的 16 个预测），3） 使用 L2 对完整补丁进行回归（即，3 个 RGB 通道上的 256 个回归）。
令人惊讶的是，我们发现一切都运行良好，尽管 L2 稍微差一些。
我们仅报告选项 1） 的最终结果，因为它显示了最佳的少样本性能。
我们还试验了 Devlin 等人（2019 年）使用的 15% 的腐败率，但在我们的少数指标上结果也略差。
最后，我们想指出的是，我们对掩码补丁预测的实例化不需要如此大量的预训练，也不需要像 JFT 这样的大型数据集，从而在 ImageNet 分类上实现类似的性能提升。
也就是说，我们观察到在 100k 预训练步骤后下游性能的回报递减，并且在 ImageNet 上进行预训练时也看到了类似的收益。
C 其他结果

我们报告了与论文中提出的数字相对应的详细结果。
表 8 总结了该消融研究在 ViT-B/16 模型上的结果。
正如我们所看到的，虽然没有位置嵌入的模型和有位置嵌入的模型的性能之间存在很大差距，但编码位置信息的不同方式之间几乎没有区别。
我们推测，由于我们的 Transformer 编码器在补丁级输入上运行，而不是像素级输入，因此在如何编码空间信息方面的差异就不那么重要了。
更准确地说，在补丁级输入中，空间维度比原始像素级输入小得多，例如，14 × 14 而不是 224 × 224，对于这些不同的位置编码策略，学习在此分辨率下表示空间关系同样容易。
即便如此，网络学习到的位置嵌入相似性的具体模式取决于训练超参数（图 10）。
D.5 经验计算成本

我们还对硬件上架构的实际速度感兴趣，由于通道宽度和缓存大小等细节，理论上的 FLOP 并不总是能很好地预测到这一点。
为此，
图 1：模型概述。
我们将图像分割成固定大小的块，线性嵌入每个块，添加位置嵌入，并将生成的向量序列馈送到标准 Transformer 编码器。
为了执行分类，我们使用标准方法，即向序列添加一个额外的可学习的“分类令牌”。
Transformer 编码器的插图受到 Vaswani 等人 （2017） 的启发。
图 2：自然、专业和结构化任务组中 VTAB 绩效的细分。
图 5：不同架构的性能与预训练计算的对比：视觉转换器、ResNet 和混合架构。
在相同的计算预算下，Vision Transformer 的性能通常优于 ResNets。
混合动力车在纯变压器的基础上进行了改进，适用于较小的模型尺寸，但对于较大的模型，差距消失了。
图 6：从输出令牌到输入空间的注意力的代表性示例。
详见附录D.7。
图 7：左图：ViT-L/32 的 RGB 值初始线性嵌入滤波器。
中心：ViT-L/32位置嵌入的相似性。
瓦片显示面片的位置嵌入与指示的行和列之间的余弦相似性，以及所有其他面片的位置嵌入之间的余弦相似性。
右图：按头部和网络深度划分的有人值守区域的大小。
每个点表示一层 16 个头部之一在图像上的平均注意力距离。
详见附录D.7。
图5来自论文，显示了ViT、ResNet和不同大小的混合模型的传输性能，以及它们预训练的估计计算成本。
图 11：按头部和网络深度划分的有人值守区域的大小。
通过对查询像素与所有其他像素之间的距离进行平均，并按注意力权重加权，计算 128 个示例图像的注意力距离。
每个点表示一层 16 个头部之一在图像上的平均注意力距离。
图像宽度为 224 像素。
图 14：图 6 中的其他注意力图示例（随机选择）。
Vision Transformer 模型变体的详细信息。
ImageNet 88.55 ± 0.04 87.76 ± 0.03 85.30 ± 0.02 87.54 ± 0.02 88.4/88.5 * ImageNet ReaL 90.72 ± 0.05 90.54 ± 0.03 88.62 ± 0.05
Touvron等人（2020）对流行的图像分类基准进行了本领域的研究。我们报告了精度的平均值和标准偏差，在三次微调运行中取平均值。在 JFT-300M 数据集上预训练的 Vision Transformer 模型在所有数据集上都优于基于 ResNet 的基线，同时预训练所需的计算资源要少得多。在较小的公共 ImageNet-21k 数据集上预训练的 ViT 也表现良好。

Touvron 等人（2020 年）报告的结果提高了 88.5%。
用于微调的超参数。
所有模型都经过微调，具有余弦学习率衰减、批量大小为 512、无权重衰减和全局范数 1 的梯度剪裁。
如果没有特别提及，微调分辨率为 384。
表 5 对应于论文中的图 3，显示了在越来越大的数据集上预训练的不同 ViT 模型的传输性能：ImageNet、ImageNet-21k 和 JFT-300M。表6对应于在Im-ageNet、ImageNet-21k或JFT300M上预训练时，Vision Transformer在各种数据集上的Top1准确率（以%）为单位。这些值对应于正文中的图 3。模型以 384 分辨率进行微调。请注意，ImageNet 结果是在没有使用其他技术（Polyak 平均和 512 分辨率图像）的情况下计算的，这些技术用于获得 Table2 中的结果。

模型缩放实验的详细结果。这些对应于主论文中的图5。

我们展示了几个数据集的传输准确性，以及训练前计算（在前 aFLOP 中）。
