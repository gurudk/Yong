关注就是你所需要的
显性序列转导模型基于复杂的递归或卷积神经网络，包括编码器和解码器。
性能最好的模型还通过注意力机制连接编码器和解码器。
我们提出了一种新的简单网络架构，即Transformer，它完全基于注意力机制，完全省去了递归和卷积。
对两个机器翻译任务的实验表明，这些模型在质量上更出色，同时具有更高的可并行化性，并且需要更少的训练时间。
我们的模型在 WMT 2014 英语到德语翻译任务中实现了 28.4 BLEU，比现有的最佳结果（包括集成）提高了 2 BLEU 以上。
在 WMT 2014 英法翻译任务中，我们的模型在 8 个 GPU 上训练 3.5 天后，建立了新的单模型最先进的 BLEU 分数 41.8，这只是文献中最佳模型训练成本的一小部分。
我们表明，Transformer 通过成功地将其应用于英语选区解析，既有大的训练数据，也有有限的训练数据，从而很好地推广到其他任务。
* 等额出资。
上市顺序是随机的。
Jakob 提议用自我关注替换 RNN，并开始努力评估这个想法。
Ashish 和 Illia 一起设计并实施了第一个 Transformer 模型，并积极参与了这项工作的各个方面。
Noam 提出了缩放的点积注意力、多头注意力和无参数位置表示，并成为几乎每个细节都参与其中的另一个人。
Niki 在我们的原始代码库和 tensor2tensor 中设计、实现、调整和评估了无数的模型变体。
Llion 还尝试了新的模型变体，负责我们的初始代码库，以及高效的推理和可视化。
Lukasz 和 Aidan 花了无数天来设计和实现 tensor2tensor 的各个部分，替换了我们早期的代码库，极大地提高了结果并大大加速了我们的研究。
1 引言
递归神经网络，特别是长短期记忆[13]和门控递归[7]神经网络，已被牢固地确立为序列建模和转导问题（如语言建模和机器翻译）的最新方法[35,2,5]。
此后，许多努力继续推动递归语言模型和编码器-解码器架构的界限 [38， 24， 15] 。
递归模型通常沿输入和输出序列的符号位置进行因子计算。
将位置与计算时间的步骤对齐，它们生成一系列隐藏状态 h t ，作为前一个隐藏状态 h t-1 和位置 t 的输入的函数。
这种固有的顺序特性排除了训练样本中的并行化，这在较长的序列长度下变得至关重要，因为内存约束限制了跨样本的批处理。
最近的工作通过分解技巧[21]和条件计算[32]在计算效率上取得了显着的提高，同时也提高了后者的模型性能。
然而，顺序计算的基本约束仍然存在。
在各种任务中，注意力机制已成为引人注目的序列建模和转导模型的一个组成部分，允许在不考虑它们在输入或输出序列中的距离的情况下对依赖关系进行建模 [2， 19] 。
然而，除了少数情况[27]外，这种注意力机制都与循环网络结合使用。
在这项工作中，我们提出了 Transformer，这是一种避免重复出现的模型架构，而是完全依赖于注意力机制来在输入和输出之间绘制全局依赖关系。
Transformer 支持显著提高并行化度，在 8 个 P100 GPU 上训练仅 12 小时后，即可达到翻译质量的新水平。
2 背景
减少顺序计算的目标也构成了扩展神经GPU [16]、ByteNet [18]和ConvS2S [9]的基础，它们都使用卷积神经网络作为基本构建块，并行计算所有输入和输出位置的隐藏表示。
在这些模型中，关联来自两个任意输入或输出位置的信号所需的操作数量随着位置之间的距离而增加，对于 ConvS2S 呈线性增长，对于 ByteNet，则呈对数增长。
这使得学习远处位置之间的依赖关系变得更加困难 [12] 。
在 Transformer 中，这被减少到恒定的操作数，尽管由于平均注意力加权位置而降低了有效分辨率，我们用多头注意力抵消了这种效果，如第 3.2 节所述。
自我注意，有时称为内部注意，是一种注意机制，将单个序列的不同位置联系起来，以计算序列的表示。
自我注意已成功用于各种任务，包括阅读理解、抽象总结、文本蕴涵和学习与任务无关的句子表征 [4， 27， 28， 22] 。
端到端记忆网络基于递归注意力机制，而不是序列对齐的递归，并且已被证明在简单语言问答和语言建模任务上表现良好[34]。
然而，据我们所知，Transformer 是第一个完全依赖自注意力来计算其输入和输出表示的转导模型，而不使用序列对齐的 RNN 或卷积。
在接下来的章节中，我们将介绍 Transformer，激发自我关注，并讨论其相对于 [17， 18] 和 [9] 等模型的优势。
3 模型架构
大多数竞争性神经序列转导模型都具有编码器-解码器结构 [5， 2， 35] 。
在这里，编码器将符号表示的输入序列 （x 1 ， ...， x n ） 映射到连续表示序列 z = （z 1 ， ...， z n ）。
给定 z，解码器然后一次生成一个元素的符号输出序列 （y 1 ， ...， y m ）。
在每一步中，模型都是自回归的[10]，在生成下一个符号时，会消耗先前生成的符号作为额外的输入。
Transformer 遵循这一整体架构，使用堆叠自注意力和逐点全连接层用于编码器和解码器，分别如图 1 的左半部分和右半部分所示。
3.1 编码器和解码器堆栈
编码器：编码器由 N = 6 个相同层的堆栈组成。
每个层都有两个子层。
第一种是多头自注意力机制，第二种是简单的、按位置完全连接的前馈网络。
我们在两个子层中的每一个层周围都采用残差连接 [11]，然后进行层归一化 [1]。
也就是说，每个子层的输出为 LayerNorm（x + Sublayer（x）），其中 Sublayer（x） 是子层本身实现的函数。
为了促进这些残差连接，模型中的所有子层以及嵌入层都会产生维度 d 模型 = 512 的输出。
解码器：解码器也由 N = 6 个相同层的堆栈组成。
除了每个编码器层中的两个子层外，解码器还插入第三个子层，该子层对编码器堆栈的输出执行多头注意。
与编码器类似，我们在每个子层周围采用残差连接，然后进行层归一化。
我们还修改了解码器堆栈中的自注意力子层，以防止位置参与后续位置。
这种掩蔽，再加上输出嵌入偏移一个位置的事实，确保了对位置 i 的预测只能依赖于小于 i 位置的已知输出。
3.2 注意
注意力函数可以描述为将查询和一组键值对映射到输出，其中查询、键、值和输出都是向量。
输出计算为值的加权和 Scaled Dot-Product Attention Multi-Head Attention，其中分配给每个值的权重由具有相应键的查询的兼容性函数计算。
3.2.1 缩放点积注意力
我们将我们的特别关注称为“缩放的点积注意力”（图 2）。
输入由维度 d k 的查询和键以及维度 d v 的值组成。
我们计算所有键的查询的点积，将每个键除以 √ d k ，并应用 softmax 函数来获得值的权重。
在实践中，我们同时计算一组查询的注意力函数，这些查询被打包到一个矩阵 Q 中。
键和值也被打包到矩阵 K 和 V 中。
我们将输出矩阵计算为：
两个最常用的注意力函数是加法注意力 [2] 和点乘积（乘法）注意力。
点积注意力与我们的算法相同，只是比例因子为 1
.加性注意力使用具有单个隐藏层的前馈网络计算兼容性函数。
虽然两者在理论复杂度上相似，但点积注意力在实践中要快得多，并且更节省空间，因为它可以使用高度优化的矩阵乘法代码来实现。
虽然对于较小的 d k 值，这两种机制的性能相似，但对于较大的 d k 值，加性注意力优于没有缩放的点积注意力 [3] 。
我们怀疑，对于较大的 d k 值，点积的大小会变大，从而将 softmax 函数推入梯度极小的区域 4 。
为了抵消这种效应，我们将点积按 1 缩放
3.2.2 多头注意力
我们发现，与使用 d 模型 - 维度键、值和查询执行单一注意力函数相比，使用不同的学习线性投影将查询、键和值线性投影 h 次分别到 d k、d k 和 d v 维度是有益的。
然后，在查询、键和值的这些投影版本中的每一个上，我们并行执行注意力函数，产生 d v 维输出值。
这些值被连接起来并再次投影，从而产生最终值，如图 2 所示。
多头注意力使模型能够共同关注来自不同位置的不同表示子空间的信息。
对于单一的注意力头，平均会抑制这一点。
哪里
其中，投影是参数矩阵
在这项工作中，我们使用 h = 8 个平行的注意力层或头部。
对于其中的每一个，我们使用
由于每个头部的维度减小，因此总计算成本与全维的单头部注意力相似。
3.2.3 注意力在我们的模型中的应用
Transformer 以三种不同的方式使用多头注意力：
• 在“编码器-解码器注意”层中，查询来自前一个解码器层，内存键和值来自编码器的输出。
这允许解码器中的每个位置都参与输入序列中的所有位置。
这模仿了序列到序列模型中的典型编码器-解码器注意力机制，例如 [38， 2， 9] 。
• 编码器包含自注意力层。
在自注意力层中，所有键、值和查询都来自同一个位置，在本例中是编码器中前一层的输出。
编码器中的每个位置都可以处理编码器前一层的所有位置。
• 同样，解码器中的自注意力层允许解码器中的每个位置关注解码器中的所有位置，直到并包括该位置。
我们需要防止信息在解码器中向左流动，以保持自回归特性。
我们通过屏蔽（设置为 -∞）softmax 输入中的所有值来实现这一点，这些值对应于非法连接。
请参阅 图 2 。
3.3 位置前馈网络
除了注意力子层外，我们的编码器和解码器中的每一层都包含一个完全连接的前馈网络，该网络分别以相同的方式应用于每个位置。
它由两个线性变换组成，中间有一个 ReLU 激活。
虽然线性变换在不同位置上是相同的，但它们在层与层之间使用不同的参数。
另一种描述方式是两个核大小为 1 的卷积。
输入输出的维数为 d model = 512，内层的维数为 d f f = 2048。
3.4 嵌入和 Softmax
与其他序列转导模型类似，我们使用学习嵌入将输入标记和输出标记转换为维度 d 模型的向量。
我们还使用通常学习的线性变换和softmax函数将解码器输出转换为预测的下一个令牌概率。
在我们的模型中，我们在两个嵌入层和 pre-softmax 线性变换之间共享相同的权重矩阵，类似于 [30] 。
在嵌入层中，我们将这些权重乘以 √ d 模型。
3.5 位置编码
由于我们的模型不包含递归和卷积，为了让模型利用序列的顺序，我们必须注入一些关于序列中标记的相对或绝对位置的信息。
为此，我们在编码器和解码器堆栈底部的输入嵌入中添加了“位置编码”。
位置编码与嵌入具有相同的维度 d 模型，因此可以将两者相加。
位置编码有很多选择，有学习的和固定的 [9] 。
在这项工作中，我们使用了不同频率的正弦和余弦函数：
其中 pos 是位置，i 是维度。
也就是说，位置编码的每个维度都对应于一个正弦曲线。
波长形成从 2π 到 10000 • 2π 的几何级数。
我们之所以选择这个函数，是因为我们假设它允许模型轻松学习按相对位置参与，因为对于任何固定的偏移量 k，P E pos+k 可以表示为 P E pos 的线性函数。
我们还尝试使用学习位置嵌入[9]，发现两个版本产生的结果几乎相同（见表3行（E））。
我们之所以选择正弦版本，是因为它可能允许模型推断出比训练期间遇到的序列长度更长的序列长度。
4 为什么要自我关注
在本节中，我们将自注意力层的各个方面与通常用于将一个可变长度的符号表示序列 （x 1 ， ...， x n ） 映射到另一个长度相等的序列 （z 1 ， ...， z n ） 的循环层和卷积层进行比较，其中 x i ， z i ∈ R d ，例如典型序列转导编码器或解码器中的隐藏层。
为了激励我们使用自我关注，我们认为有三个原因。
一个是每层的总计算复杂度。
另一个是可以并行化的计算量，通过所需的最小顺序操作数来衡量。
第三个是网络中远程依赖项之间的路径长度。
在许多序列转导任务中，学习长程依赖性是一个关键挑战。
影响学习这种依赖关系能力的一个关键因素是前进和后退信号在网络中必须遍历的路径长度。
输入和输出序列中任意位置组合之间的这些路径越短，就越容易学习长程依赖性 [12] 。
因此，我们还比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。
如表 1 所示，自注意力层通过恒定数量的顺序执行操作连接所有位置，而循环层需要 O（n） 顺序操作。
就计算复杂度而言，当序列长度 n 小于表示维数 d 时，自注意力层比递归层更快，这在机器翻译中最先进的模型使用的句子表示中最常见的情况，例如词片 [38] 和字节对 [31] 表示。
为了提高涉及非常长序列的任务的计算性能，可以将自我注意力限制为仅考虑以各自输出位置为中心的输入序列中大小为 r 的邻域。
这将使最大路径长度增加到 O（n/r）。
我们计划在未来的工作中进一步研究这种方法。
核宽 k < n 的单个卷积层不会连接所有对输入和输出位置。
在连续核的情况下，这样做需要一堆 O（n/k） 卷积层，在膨胀卷积的情况下需要 O（log k （n）） [18]，从而增加网络中任何两个位置之间的最长路径的长度。
卷积层通常比循环层更昂贵，是 k 的倍数。
然而，可分离卷积 [6] 大大降低了复杂度，达到 O（k
然而，即使 k = n，可分离卷积的复杂度也等于自注意力层和逐点前馈层的组合，这是我们在模型中采用的方法。
作为附带好处，自我关注可以产生更多可解释的模型。
我们检查了模型中的注意力分布，并在附录中展示和讨论了示例。
不仅个体的注意力头清楚地学会了执行不同的任务，而且许多人似乎表现出与句子的句法和语义结构相关的行为。
5 培训
本节介绍模型的训练制度。
5.1 训练数据和批处理
我们在标准的 WMT 2014 英语-德语数据集上进行训练，该数据集由大约 450 万个句子对组成。
句子使用字节对编码 [3] 进行编码，该编码具有大约 37000 个标记的共享 sourcetarget 词汇表。
对于英语-法语，我们使用了明显更大的 WMT 2014 英语-法语数据集，该数据集由 36M 个句子组成，并将标记拆分为 32000 个单词-片段词汇表 [38] 。
句子对按近似序列长度分批在一起。
每个训练批次都包含一组句子对，其中包含大约 25000 个源标记和 25000 个目标标记。
5.2 硬件和时间表
我们在一台配备 8 个 NVIDIA P100 GPU 的机器上训练了我们的模型。
对于使用本文中描述的超参数的基础模型，每个训练步骤大约需要 0.4 秒。
我们总共训练了 100,000 个步骤或 12 小时。
对于我们的大型模型，（如表 3 的底线所述），步进时间为 1.0 秒。
大型模型接受了 300,000 步（3.5 天）的训练。
5.3 优化器
我们使用了 Adam 优化器 [20]，β 1 = 0.9，β 2 = 0.98，ε = 10 -9。
在训练过程中，我们根据以下公式改变了学习率：
这对应于在前 warmup steps 个训练步骤中线性增加学习率，然后与步骤数的平方反比按比例减少学习率。
我们使用 warmup steps = 4000。
5.4 正则化
我们在训练过程中采用三种类型的正则化： 残差辍学 我们将 dropout [33] 应用于每个子层的输出，然后再将其添加到子层输入并归一化。
此外，我们将 dropout 应用于编码器和解码器堆栈中的嵌入和位置编码。
对于基本模型，我们使用 P 下降 = 0.1 的比率。
标签平滑
在训练期间，我们采用了值 ε ls = 0.1 [36] 的标签平滑。
这伤害了困惑，因为模型学会了更加不确定，但提高了准确性和 BLEU 分数。
6 结果
6.1 机器翻译
在 WMT 2014 英德翻译任务中，大转换器模型（表 2 中的 Transformer （big）） 比之前报道的最佳模型（包括集成）高出 2.0 BLEU 以上，建立了 28.4 的新最新 BLEU 分数。
此模型的配置列在表 3 的底线中。在 8 个 P100 GPU 上训练了 3.5 天。
甚至我们的基础模型也超过了所有以前发布的模型和集成，而训练成本只是任何竞争模型的一小部分。
在 WMT 2014 英语到法语的翻译任务中，我们的大模型取得了 BLEU 41.0 的分数，优于所有先前发布的单一模型，而训练成本不到之前最先进模型的 1/4。
为英语到法语训练的 Transformer （big） 模型使用辍学率 P drop = 0.1，而不是 0.3。
对于基础模型，我们使用了一个通过平均最后 5 个检查点获得的单一模型，这些检查点以 10 分钟的间隔写入。
对于大型模型，我们对最后 20 个检查点求平均值。
我们使用波束大小为 4 且长度损失 α = 0.6 的波束搜索 [38] 。
这些超参数是在开发集上进行实验后选择的。
我们将推理期间的最大输出长度设置为输入长度 + 50，但在可能的情况下提前终止 [38] 。
表 2 总结了我们的结果，并将我们的翻译质量和培训成本与文献中的其他模型架构进行了比较。
我们通过将训练时间、使用的 GPU 数量以及每个 GPU 的持续单精度浮点容量的估计值相乘来估计用于训练模型的浮点运算数 5 。
6.2 模型变化
为了评估 Transformer 不同组件的重要性，我们以不同的方式改变了我们的基本模型，测量了 上 的英语到德语翻译的性能变化。
我们使用了上一节所述的波束搜索，但没有检查点平均。
我们在表 3 中列出了这些结果。
在表 3 的第 3 行 （A） 中，我们改变了注意力头的数量以及注意力键和值维度，保持计算量不变，如第 3.2.2 节所述。
虽然单头注意力比最佳设置差 0.9 BLEU，但头部过多，质量也会下降。
在表 3 行 （B） 中，我们观察到减小注意力键大小 d k 会损害模型质量。
这表明确定兼容性并不容易，并且比点积更复杂的兼容性函数可能是有益的。
我们在（C）和（D）行中进一步观察到，正如预期的那样，更大的模型更好，并且丢弃对于避免过度拟合非常有帮助。
在第（E）行中，我们用学习到的位置嵌入[9]替换了我们的正弦位置编码，并观察到与基础模型几乎相同的结果。
6.3 英语选区解析
为了评估 Transformer 是否可以推广到其他任务，我们在英语选区解析上进行了实验。
这项任务提出了特定的挑战：产出受到强大的结构性约束，并且比输入长得多。
此外，RNN序列到序列模型在小数据体系中无法获得最先进的结果[37]。
我们在 Penn Treebank 的华尔街日报 （WSJ） 部分 [25] 上训练了一个 d 模型 = 1024 的 4 层变压器，大约 40K 个训练句子。
我们还在半监督环境中对其进行了训练，使用更大的高置信度和 BerkleyParser 语料库，其中包含大约 17M 个句子 [37] 。
我们仅对 WSJ 设置使用了 16K 个标记的词汇表，对半监督设置使用了 32K 个标记的词汇表。
我们只进行了少量实验来选择第 22 节开发集上的辍学、注意力和残差（第 5.4 节）、学习率和波束大小，所有其他参数与英语到德语的基础翻译模型保持不变。
在推理过程中，我们将最大输出长度增加到输入长度 + 300。
我们使用 21 的光束大小和 α = 0.3 仅适用于 WSJ 和半监督设置。
我们在表 4 中的结果表明，尽管缺乏特定于任务的调优，但我们的模型表现得出奇地好，比之前报告的所有模型都产生了更好的结果，但递归神经网络语法 [8] 除外。
与RNN序列到序列模型[37]相比，即使仅在40K句子的WSJ训练集上进行训练，Transformer的性能也优于Berkeley-Parser[29]。
7 结论
在这项工作中，我们提出了 Transformer，这是第一个完全基于注意力的序列转导模型，用多头自注意力取代了编码器-解码器架构中最常用的循环层。
对于转换任务，Transformer 的训练速度明显快于基于递归层或卷积层的架构。
在 WMT 2014 英语到德语和 WMT 2014 英语到法语的翻译任务中，我们都达到了新的技术水平。
在前一个任务中，我们的最佳模型甚至优于所有先前报告的集合。
我们对基于注意力的模型的未来感到兴奋，并计划将其应用于其他任务。
我们计划将 Transformer 扩展到涉及文本以外的输入和输出模式的问题，并研究本地的、受限的注意力机制，以有效地处理大量的输入和输出，如图像、音频和视频。
减少生成顺序是我们的另一个研究目标。
我们用于训练和评估模型的代码可在 https://github.com/
tensorflow/tensor2tensor。
