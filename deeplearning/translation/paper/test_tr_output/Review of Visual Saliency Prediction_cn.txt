视觉显著性预测研究综述：从神经生物学基础到深度模型的发展过程

通过将显著性预测任务与神经科学和心理学紧密关联，可以理解和模拟人类的注意力机制。
此外，显著性预测在计算机视觉和跨学科学科中得到了广泛的应用。
近年来，随着深度学习的快速发展，深度模型在显著性预测方面取得了令人瞩目的成就。
深度学习模型可以自动学习特征，从而解决了经典模型的许多缺点，例如手工制作的特征和任务设置等。
然而，深度模型仍然存在一些局限性，例如在涉及多模态和语义理解的任务中。
本文重点总结了显著性预测领域的相关成果，包括经典模型的早期神经和心理机制以及指导作用，其次是经典显著性预测模型和深度显著性预测模型的发展过程和数据对比。
本文还探讨了模型与人类视觉的关系，以及引起语义差距的因素、注意力在认知研究中的影响、显著性模型的局限性以及新兴应用，以期为后续工作提供新的显著性预测以及必要的帮助和建议。
1. 引言

人类每天接收的信息中大约有80%来自视觉。
然而，人类的视觉神经资源是有限的[1]。
人类视觉通路中存在信息瓶颈。
例如，视觉系统每秒接收数百兆字节的视觉媒体数据，但信息处理速度仅为每秒 40 比特 [2] 。
在这个过程中，视觉注意机制起着重要的作用[3]。
在我们日常生活中接收到的信息中，只有少量的刺激可以随时进入视觉系统进行进一步处理，从而避免了计算浪费，降低了分析的难度。
互联网的发展和智能设备的普及，提高了信息收集和传播的速度，甚至达到了前所未有的水平。
但是，如果所有信息都不加区别地分配相同的计算资源，那么就会导致计算资源的浪费和过度的时间消耗。
因此，知道如何从海量场景中选择有趣的内容，以与人类相同的方式进行分析和处理，是一项非常重要的工作。
视觉显著性预测是一种模仿人类视觉注意力的机制，包括神经生物学、心理学和计算机视觉等相关知识。
早期的注意力模型经常使用认知心理学知识来查找有关行为、任务或目标的信息。
例如，Itti等[4]提出了一种基于自下而上模型的显著性预测模型，深度学习模型由此逐渐蓬勃发展。
与经典模型相比，这些新开发的模型的性能有了很大的提高，性能逐渐接近人类的观察者。
视觉显著性检测研究的意义在于两个方面：首先，作为一种可验证的预测，它可以作为一种基于模型的假设检验，从行为和神经层面理解人类的注意力机制。
其次，基于注意力机制的显著性预测模型已通过多种方式得到广泛应用，如目标预测[4]、目标跟踪[5]、图像分割[6]、图像分类[7]、图像拼接[8]、视频监控[9]、图像或视频压缩[10]、图像或视频检索[11]、显著性目标检测[12]、视频分割[13]、图像裁剪[14]、 视觉SLAM（同步定位和映射）[15]、端到端驾驶[16]、视频问答[17]、医疗诊断[18]、健康监测[19]等。

目前显著性检测的研究主要涉及两类任务，即显著性预测（或注视眼预测）和显著性目标检测（SOD）。
这两种类型的任务都旨在检测图片或视频中最重要的区域。
然而，这两种模型及其应用场景之间存在差异。
显著性预测是由人类视觉注意机制提供信息，并预测人眼在场景中停留在某个位置的可能性。
相比之下，显著性目标检测作为另一个分支，侧重于对象层面的感知和描述，这是一项纯粹的计算机视觉任务。
图 1 中显示了两种类型的任务。
许多研究人员最近研究了SOD任务。
据推测，作为一项纯粹的视觉任务，SOD可以更容易、更直接地应用于某些视觉场景，这更多是由不同领域的应用驱动的。
得益于大规模基准测试和深度学习，SOD发展迅速，并取得了令人瞩目的成就[20]。
近年来，许多研究人员做出了突出的贡献。
Wang等[21]提出了一个使用迭代自上而下和自下而上的显著性推断的通用框架。
此外，该框架还使用参数共享和权重共享来减少参数数量。
此外，Wang等[22]提出了PAGE-Net，主要包括金字塔注意力和显著边缘检测两个模块。
在感受野扩大的情况下，PAGE-Net通过监督学习预测重要物体的边缘来获取边缘信息。
Zhang等[23]提出的模型将联合训练应用于SOD和COD（伪装目标检测）两个几乎相反的任务。
Zhang等[24]提出的双重精细化网络（Dual Refinement Network，DRFNet）可直接应用于高分辨率图像。
DRFNet由一个共享的特征提取器和两个有效的细化头组成，可以从高分辨率图像中获取更具判别力的特征。
但是，显著对象不一定是图中唯一可能的突出目标。
还应考虑其他复杂因素。
显著性预测任务除了应用范围广泛外，还与人类视觉本身有关，与神经科学和心理学密切相关。
因此，显著性预测在跨学科和新兴学科中得到了广泛的应用。
本研究的主要贡献如下：
•

本研究聚焦显著性预测任务，分析了显著性预测相关的心理和生理机制，介绍了已经受到显著性预测影响的经典模型，并确定了这些理论对深度学习模型的影响。
•

详细分析了基于深度学习的视觉显著性模型，分别对具有代表性的实验数据集和模型在静态和动态条件下的性能评价措施进行了讨论和总结。
•

分析了当前深度学习模型的局限性，提出了可能的改进方向，基于深度学习的最新进展，探讨了新的应用领域，并提出了显著性预测对未来发展趋势的贡献和意义。
2. 视觉显著性的心理学和神经生物学基础

注意机制一直是神经科学和心理学的重要学科。
在1950年代中期，认知心理学逐渐出现。
注意力被认为是人脑信息处理的重要机制，产生了几种有影响力的注意力模型，如过滤模型（1958）、衰减模型（1960）和反应选择模型（1963）等。
Treisman[25]提出了一个重要的模型，称为特征整合理论（FIT），生动地说明了视觉注意的选择性作用。
该模型中的视觉过程分为前注意阶段和焦点注意阶段。
在提取与位置相关的要素后，实现了要素集成。
Koch和Ullman[26]通过整合返回抑制机制来实现焦点偏移，从而增强了FIT。
此外，在对早期FIT模型的批评基础上，Wolfe[27]提出了引导搜索模型来解释和预测搜索结果。
这些神经学和心理学研究为计算视觉显著性提供了重要的基础和标准，例如中心环绕拮抗、全局稀有性或信息最大化。
视觉显著性预测主要利用数学模型模拟人类视觉注意函数，进而计算视觉信息的重要性。
人类视觉注意系统的模拟主要利用了上述视觉生理学和心理学方面的一些重要成果。
值得注意的是，视觉显著性预测并未研究视觉注意力中的眼动策略，而是计算与眼动决策相关的场景不同重要程度的信息。
这些研究对显著性检测模型的后续发展起到了指导和规范作用。
3. 经典视觉显著性模型

经典的视觉显著性模型考虑了心理学和神经生物学基础，其中大部分是手工制作的特征模型。
作为心理学的研究基础，经典的视觉显著性模型根据信息处理的水平通常可以分为两种模型：自下而上的显著性模型（数据驱动、任务无关模型）和自上而下的显著性模型（任务驱动、任务特定模型）。
3.1. 自下而上的视觉显著性模型

自下而上的视觉显著性模型通常提取低级特征，例如对比度、颜色和纹理。
低级特征和背景特征的区别强烈地吸引了注意力资源。
这种注意力预测机制是非自愿的，需要快速处理。
例如，图像中行人、车辆、个别花朵和野兽的存在将显示出强烈的视觉显著性。
其中，局部对比模型基于FIT和中心环绕对抗的生理和心理学原理，在选择图像中的显著区域时定义了一定的机制，以实现视觉注意机制的模拟。
例如，最早的Itti模型[4]可以在没有任何先验信息的情况下模拟人类视觉注意力转移的过程。
根据从图像中捕获的特征，模型分析视觉刺激，分配计算资源，根据不同位置的显著性强度选择场景中的显著区域，模拟人类视觉注意力转移的过程。
虽然该模型的性能一般，但这是神经生物学模型的首次成功尝试，具有重要意义。
从那时起，其他研究人员也做出了贡献。
Harel[28]在合成阶段将基于图的视觉显著性（GBVS）模型改为具有非线性组合的马尔可夫随机场。
该模型在特定特征通道上形成激活映射，然后以突出显著性并允许与其他映射结合的方式对其进行归一化。
马和Zhang[29]利用局部对比分析提取了图像的显著性图，在此基础上，Tie Liu等[30]使用了9个×9个邻域，并采用了条件随机场（CRF）学习模型。
Borji[31]基于稀疏编码分析了局部稀有度。
Sclaroff等[32]提出了一种基于布尔映射的显著性预测模型。
此外，研究人员还使用其他模型通过使用局部或全局对比来预测显著性。
一些值得注意的例子包括Zhai和Shah[33]提出的像素级对比显著性模型，Wei[34]提出的基于滑动窗口的全局对比度计算模型，Margolin[35]提出的颜色对比线性融合模型，Achanta[36]提出的频率调谐模型，以及Cheng [37]提出的色彩空间量化模型。
其他研究者利用超像素[38] [39] [40]作为处理单元，计算色彩空间分布的方差，作为提高计算效率的手段。
一些模型基于信息理论和图像变换。
这些基于信息论的模型的本质是从视觉环境中计算出最大的信息采样，从场景中选择最丰富的部分，并丢弃剩余的部分。
其中，Bruce和Tsotsos[41]的Attention-based on Information maximization（AIM）模型具有一定影响力。
AIM 模型使用 Shannon 的自我信息度量来计算图像的显著性。
首先，随机选取一定数量的自然图像块进行训练，得到基本函数;
然后，将图像分割成相同大小的块，通过独立分量分析（ICA）提取相应块的基系数作为块的特征，通过概率密度估计得到每个特征的分布，最后得到特征的概率密度。
其他值得注意的模型包括Hou[42]提出的增量编码长度模型、Mancas[43]提出的稀有线性组合模型、Seo[44]提出的自相似性预测模型和Rosenholtz[45]提出的Mahalanobis距离计算模型。
关于利用图像变换模型进行显著性预测，Hou[46]提出的光谱残差模型并未对前景特征进行研究，而是利用了研究背景。
与这些特征不匹配的区域就是感兴趣的区域。
计算残差谱后，通过逆傅里叶变换将残差谱映射回空间域，得到显著性图。
在此基础上，Guo[47]提出了一种利用相位谱得到显著性图的模型，Holtzman-Gazit[48]提取了图片的各种分辨率。
Sclaroff[49]通过布尔图发现周边区域，提出了一种基于布尔图的显著性模型（BMS）。
该模型通过分析布尔图的拓扑结构，得到了显著性图。
尽管 BMS 易于实现且运行高效，但它在经典模型中表现良好。
3.2. 自上而下的视觉显著性模型

自上而下的视觉显著性模型通常基于某些特定任务。
由于任务的多样性和复杂性，建模也更加困难[50]。
自上而下的视觉显著性模型主要基于贝叶斯模型。
此外，贝叶斯模型可以看作是决策理论模型的一个特例，因为它都模拟了人类视觉显著性的生物学计算过程。
显著性预测中的贝叶斯模型是根据贝叶斯规则将场景信息和先验信息相结合的概率组合模型。
Torrallba等[51]提出的模型将自下而上和自上而下的显著性图相乘，得到最终的显著性图。
在此基础上，Ehinger等[52]将目标的特征先验信息融入到上述框架中。
Xie等[53]提出了一种基于后验概率的显著性预测模型。
Zhang等[54]提出的SUN模型以视觉特征和空间位置作为先验知识。
显著性预测中基于决策理论的模型是一种基于信息和评价准则要求，即如何对周围环境的感知信息做出最优决策，从而决定最优计划的策略模型。
Gao和Vasconcelos[55,56]认为识别过程中的显著特征源自其他感兴趣的类别，他们将自上而下的注意力定义为预期误差最小的分类问题。
Kim等[57]推荐了一种基于运动感知分组的时间和空间显著性模型。
Gu等[58]提出了一种基于决策理论机制的模型来预测感兴趣区域。
早期的机器学习模型通常使用多种机器学习方法，例如卷积神经网络（CNN）、支持向量机（SVM）或概率核密度估计，它们大多结合了自下而上和自上而下的方法。
值得注意的例子包括Kienzle等[59]提出的非线性映射模型、Peters等[60]提出的回归分类器模型和Judd等[61]提出的线性SVM模型。
这些早期的机器学习模型对后续的深度学习模型具有一定的探索性，对后续发展的深度学习模型起到了重要的指导作用。
尽管这些经典模型的设计方式多种多样，但由于手工制作的功能，它们的性能逐渐达到了瓶颈。
神经生物学模型和经典模型的开发过程如图2所示。
4. 深度视觉显著性模型

2014年，Vig等[62]提出了一种名为eDN的深度卷积网络，可以在全自动数据驱动模式下实现提取特征。
与经典模型相比，eDN能够通过融合不同层的特征图，自动学习图像表达并得到最终的显著性图。
然而，由于数据集的数量有限，并且数据集中可训练的图数量有限，由于结构可扩展性有限，网络的深度是不够的。
此后，越来越多的研究人员使用深度模型来研究显著性预测，深度模型在静态和动态显著性预测中的应用取得了更好的效果。
4.1. 静态模型

在eDN之后，Kümmerer等[63]利用AlexNet [64]网络提出了一种基于图像分类的CNN模型Deep Gaze I。
Deep Gaze I的主要创新是应用迁移学习进行显著性预测，通过在ImageNet上使用预训练的权重[65]，将它们连接到AlexNet的输出层。
该网络包含一个中心偏差，该中心偏差通过使用 softmax 函数转换为概率分布。
典型的显著性数据集相对较小，训练效果有限。
ImageNet作为百万级数据库，训练效果很好，但训练资源庞大，训练时间过长。
使用基于ImageNet的迁移学习，可以更容易地学习深度CNNs（DCNN）的特征，并获得更好的泛化效果。
Kruthiventi等[66]于同年提出了DeepFix模型，通过使用VGG-16[67]网络作为主要特征提取网络，允许网络使用与位置相关的信息。
与AlexNet相比，VGG-16网络更简单，更有效。
使用更好的目标预测网络成为更好的选择。
然后，DeepGaze II [68]切换到VGG-19 [67]，在SALICON [69]数据集上重新训练图像特征，然后在MIT1003[70]数据集上进行微调。
因此，与Deep Gaze I相比，更新后的模型的性能有了显著的提高。
这种发展趋势表明，重新训练深层特征和微调任务有助于性能的提高。
许多研究人员已经采用了小规模的再训练和微调，并成功地使用了迁移学习。
同样，许多研究人员已经采用了可以通过调整不同分辨率的输入来捕获相对精细或粗糙的特征的模型，作为获得更好结果的一种手段。
其中，Pan等[71]提出了两种显著性模型：浅层ConvNet（JuntingNet）和深度ConvNet（SalNet）来训练端到端架构。
利用SALICON技术，利用具有双分支多尺度特征的VGG-16网络训练卷积网络。
双分支可以有效提高模型性能，但在训练和测试中计算成本和内存较高。
然后，通过结合不同图像尺度的迁移整合信息，该模型可以大大超过当时的进步水平。
Jetley等[72]提出的概率分布预测模型将显著性定义为广义伯努利分布，它包括一个将经典的softmax损失与不同概率分布的计算相结合的全端到端训练深度CNN。
他们的结果表明，在显著性预测中，新的损失函数比经典的损失函数（例如欧几里得）更有效。
Liu和Han[73]提出了一种深度空间上下文长期循环卷积网络（DSCLRCN）模型。
首先，利用CNN学习小图像区域的局部显著性，然后利用长短期记忆网络（LSTM）扫描水平和垂直方向的图像，捕捉全局背景。
这两项操作使DSCLRCN能够同时有效地合并局部和全局上下文，以推断图像的显著性图。
Cornia等[74]提出的ML-Net模型综合了上述模型的优点。
他们的模型由特征提取DCNN、特征编码网络和先验学习网络组成。
同时，对网络的损失函数进行加权，分为NSS、CC和SIM三个部分。
SALICON 模型还使用可微分指标（如 NSS、CC、SIM 和 KL 散度）来训练网络。
Cornia等[75]随后提出的SAM-ResNet模型和SAM-VGG模型将全卷积网络和循环卷积网络相结合，得到了空间注意力机制。
SalGAN[76]使用对抗网络进行训练，它由两部分组成，一个生成器和一个判别器。
该网络通过下采样二元交叉熵损失计算的反向传播来学习参数。
该模型的成功表明，选择合适的损失函数可以作为提高预测效果的方法。
近年来，已经提出了一些优秀的显著性预测模型。
Jia等[77]基于图像之间的相似性和极限学习机（ELMs）的集成，提出了一种显著性模型EML-Net。
Wang等[78]提出了深度视觉注意力（DVA）模型，该模型基于跳跃层网络对架构进行多尺度训练，以预测像素显著性。
Gorji[79]提出的模型利用共享注意力来增强显著性预测。
Dodge等[80]提出了一种名为MxSalNet的模型，该模型由专家混合制定。
Mahdi等[81]提出了一种基于深度特征的显著性（DeepFeat）模型，通过结合自下而上和自上而下的显著性图来利用特征。
AKa等[82]提出了基于编码器-解码器结构的MSI-Net，它包括一个具有不同膨胀率的多个卷积层的模块，用于捕获多尺度特征。
4.2. 动态模型

与静态模型的设置不同，动态模型中的观察时间从大约 4 秒减少到 0.05 秒。
此外，由于视频中的运动信息明显，预测动态视频的显著性较为困难。
因此，存在的动态模型要少得多。
然而，随着应用需求的不断增长，对动态模型的研究也一直在不断发展。
动态模型通常将时间信息添加到 CNN 或使用 LSTM 进行建模。
早期的动态模型主要基于自下而上模型，将静态显著性特征与时序信息相结合。
Gao等[83]整合了额外的运动信息，Seo等[84]使用局部回归核来计算视频中像素与其周围区域之间的相似性。
然而，这些模型的性能受到其手工制作的功能的限制。
深度学习框架的出现改善了这种情况。
Bak等[85]提出了基于双流网络的动力学模型，并增加了运动特征。
由于两股流信息的最终融合，网络在学习时空特征方面受到限制。
Chaabouni等[86]基于迁移学习向CNN添加了连续两帧的残差运动和RGB色平面。
Leifman等[87]的模型将RGB色平面、密集光流图和显著性图合并为一个七层CNN网络。
Wang等[88]提出了一种时空残差注意力网络（STRA-Net），该网络学习了一堆局部注意力以及全局注意力先验，以过滤掉不相关的信息。
该模型在精确定位动态人类注视点以及捕捉时间注意力转换方面具有优势。
LSTM 也广泛用于动态模型。
Bazzani等[89]使用3D CNN与LSTMs连接，并将LSTMs的输出投影到高斯混合模型中。
江等[90]提出的物体到运动（OM）-CNN模型分析了基于显著物体网络和运动信息网络的帧内显著性。
在此基础上，Gorji[91]提出了一种基于静态模型的多流卷积LSTM（ConvLSTM）网络，该网络具有3个网络（注视跟踪、快速场景变化和注意力反馈）。
Wang等[92]提出的ACLNet使用增强的CNN-LSTMs对静态显著性信息进行编码。
然而，网络捕获运动信息的能力是有限的。
通过这种方式，LSTM 可以专注于学习连续帧的时间显著性表示，并避免过拟合。
Hang等[93]设计了一种注意力感知的ConvLSTM，用于从静态网络获取空间特征，从动态网络获取时间特征，然后对其进行集成。
从连续帧中提取的特征用于预测显著区域，并为每个视频帧生成最终的显著图。
近两年来，动态显著性领域逐渐向全向图像（ODIs）和3D ODIs方向发展。
Xu等[94]利用对抗网络通过模仿物体的头部轨迹来预测ODIs的显著性，并应用生成对抗仿真模型来训练深度模型。
显著性预测模型的开发过程如图3所示。
5. 视觉显著性预测数据集

许多用于目标检测和图像分割的数据库都可以作为实验数据;其中许多是通过眼动追踪设备和手动注释获得的。

不同显著性模型生成的显著性图谱的性能需要进行定量评估。
目前，视觉显著性预测的应用主要针对图像和视频进行。
相应的数据库也分为静态和动态两种类型。
在这些数据集中，SALICON 拥有最多的静态模型数据量。
大多数模型可以使用迁移学习来对SALICON进行微调。
Mit300和cat2000作为包含模型比较最多的数据库，通常用于模型性能测试。
5.1. 静态数据集

5.2. 动态数据集

第4.2节的讨论确立了动态信息和人类注意力的特殊性以及眼动设备的局限性，这导致了观察动态数据的困难。
顺便说一句，由于应用需求的增长，近年来出现了一些大型数据集。
目前，动态数据集主要由以下几个数据集组成： 对于早期的动态模型，DIEM、Hollywood-2和UCF-sports数据集是视频显著性研究中应用最广泛的三个数据集。
近年来，随着数据集的不断更新，越来越多的模型也在使用DHF1K数据库进行训练和测试。
DHF1K数据库数据量巨大，应用范围广泛。
6. 视觉显著性预测的评价措施

视觉显著性预测的指标主要利用估计预测值与Ground Truth（GT）之间的相似性和差异性，然后输出评估分数来判断它们之间的相似度或差异度。
给定一组用于定义评分函数的真值，可以将显著性预测图用作输入，并返回评估预测准确性的结果。
具体评估措施如下：
AUC 变体：曲线下面积 （AUC） 用作两类模式识别问题的测量标准。
与目标检测、图像分割等任务中的AUC不同，鉴于显著性预测任务的特殊性，显著性预测任务中经常使用以下AUC变体：
• AUC-Judd：Judd 等人。
[102] 提出了一种称为 AUC-Judd 的 AUC 变体。
对于给定阈值，真阳性概率是在所有真值显著点上预测为显著的像素的比率，而假阳性概率是在非显著点上预测为显著的像素比率。
该变体使用非焦点的均匀随机抽样来计算误报率，并将高于这些像素阈值的显著性映射值定义为误报。
AUC-Borji 中的假阳性计算是 AUC-Judd 中计算的离散近似值。
由于使用了随机抽样，因此对同一模型的评估可能会得出不同的结果。
•

随机 AUC：随机 AUC （sAUC） [97] 也是一种常用的 AUC 变体。
它通过对其他图像的突出点分布进行采样，降低了 AUC 对中心偏移的敏感性。
AUC-Judd、AUC-Borji 和 sAUC 作为 AUC 的变体，广泛应用于显著性模型的评估。
它们的值与模型性能呈正相关。
虽然AUC是一个重要的评价指标，但它无法区分不同地区的相对重要性。
因此，还需要其他基于分布的相似性评价措施：
•

归一化扫描路径显著性 （NSS）：NSS 是显著性预测的独特评估度量。
它用于计算感兴趣点的平均归一化显著性值[104]。
NSS的计算公式为：
其中 P 是人眼注视点 Q 处的平均值，N 是人眼注视的总数，i 表示第 i 个像素，N 是注视点处的像素总数。
正 NSS 表示映射之间的一致性，而负 NSS 则相反。
NSS 值与模型性能呈负相关。
• 线性相关系数 （CC）：

CC 是用于衡量两个随机变量之间的线性相关性的统计量。
对于显著性预测评估，预测显著性图（P）和真值视图（G）可以看作是两个随机变量。
CC的计算公式为：
其中 CoV 是协方差，σ 是标准差。
CC 可以同样区分值范围为 （-1,1） 的假阳性和假阴性。
接近两端的值表示模型性能越好。
•

地球移动器距离（EMD）：EMD [105]表示用G和S表示的两个二维地图之间的距离，它计算将显著性图S的估计概率分布转换为G表示的GT图的概率分布的最小成本。因此，低 EMD 对应于高质量的显著性图。

在显著性预测中，EMD表示将显著性图的概率分布转换为人眼注意力图（称为注视图）的最小成本。
•

Kullback-Leibler （KL） 背离：KL 背离是对应于两个概率分布之间的差值的一般信息论度量。
KL的计算公式为：
与其他基于分布的度量类似，KL 散度将预测的显著性映射 （P） 和真实值视图 （G） 作为输入，并评估信息损失，其中 P 用于近似 G，ε是正则化常数。
此外，KL背离是一种不对称的相异性度量。
分数越低，表示显著性图接近真实值。
•

（6）相似度量（SIM）：SIM衡量两个分布之间的相似性。
对输入映射进行归一化后，SIM 卡将计算为每个像素处的最小值之和。
SIM卡的计算公式为：
给定预测的显著性图 （P） 和真实值视图 （G），SIM 卡为 1 表示分布相同，而 SIM 卡为 0 表示没有重叠。
SIM 可以惩罚未考虑所有真实密度的预测。
总的来说，这些评价措施是互补的。
一个好的模型应该在各种评估措施下是好的，因为这些措施反映了显著性图的不同方面。
通常，在评估模型时，会选择多种评估度量。
作为一种广泛使用的基于位置的度量，AUC 是必不可少的。
同时，应选择CC、SIM等多种其他度量，以反映相对显著性区域或相似性等其他显著性地图因子。
到目前为止，我们总结了上述六种常见的评估措施，这些指标基于它们是否适合作为统计和分类的概率分布、相似性和连续 GT 工具。
具体情况如表1所示。
7. 视觉显著性预测模型的性能

麻省理工学院基准测试具有最全面的显著性模型和评估基准。
本章基于MIT基准测试，选取了MIT300和CAT2000数据集中模型的静态图像性能评估结果。
然后，在DHF1K数据集上选择动态模型的性能。
数据来自麻省理工学院基准测试的运行结果、作者的研究以及作者在 GitHub 上的程序。
麻省理工学院基准共有八个评估指标（包括三个 AUC 变体）。
总共评估了 93 个静态模型。
以下 16 个性能更好的模型进行比较：eDN、Deep Gaze I、Deep Gaze II、DeepFix、SALICON、SalNet、ML-Net、SalGAN、EML-Net、SAM-VGG、SAM-ResNet、AIM、Judd Model、GBVS、ITTI 和 SUN。
此外，麻省理工学院还考虑了五个基线。
这些基线之一，即无限的人类，被用作参考度量。
infinitehuman基线可以模拟无限人观察下的注视点，与最高分相似。
所得结果如表2所示。
最佳指标以粗体标记。
到目前为止，CAT2000数据集共包含31个评估模型，其中10个是基于神经网络的模型。
所得结果如表3所示。表 2 显示了 MIT300 数据集的结果。

AUC-Judd 指数按降序排列。
排名靠前的模型都是基于深度学习的。
EML-NET表现最好，在各种措施下都获得了最高分。
根据 AUC-Judd 指标，DeepGaze II 和 EML-NET 以 0.88 的分数位居前两位。
DeepGaze II 在 AUC-Borji 中以 0.86 分排名第一。
根据 sAUC 指标，SALICON 表现最好，得分为 0.74。
不同评估方法得出的排名差异很大。
DeepGaze II 和 DeepFix 在 AUC 中表现良好，但其他分数一般。
尽管SAM-ResNet、SAM-VGG、EML-NET和SalGAN在AUC方面没有获得最高分，但这些模型都非常出色。
CAT2000数据集的结果如表3所示。
AUC-Judd 按降序排列。
根据 AUC-Judd 度量，SAM-ResNet、MSI-Net 和 SAM-VGG 以 0.88（无限人类得分为 0.90）并列第一。
在经典模型中，BMS的性能是更优越的。
它的 AUC-Borji 分数是最高的，其他分数几乎高于 eDN。
一般来说，在 MIT300 数据集上表现良好的模型在 CAT2000 数据集上也表现良好。
模型在 CAT2000 数据库上的显著性图如图 4 所示。采用AUC-Judd、sAUC、NSS、CC和SIM作为5个评价度量来判断模型在DHF1K数据集上的性能。

平均值是在计算每帧的分数后得出的。
评估结果主要基于DHF1K数据集的公开结果。
模型性能如表4所示。STRA -Net 在所有评级中排名第一，其次是 ACLNet。

在动态模型中，OM-CNN的性能优于其他类型模型。
在静态模型中，SALICON的性能更胜一筹。
结果表明，深度模型的性能优于在经典模型中添加时间信息的性能。
8. 深度显著性模型的共性与局限性

尽管各种深度显著性模型的结构各不相同，但它们有许多共性。
与经典模型相比，深度显著性模型能够自动捕获特征。
虽然经典模型可以手动编码特征，但具有多层结构的深度网络可以自动捕获更多特征。
基于CNN的显著性模型以端到端的方式进行训练，结合特征提取和显著性预测，与经典模型相比，可以大大提高性能。
这些显著性预测模型的成功表明了基于深度学习框架自动捕获特征的重要性。
为了提高模型性能，显著性模型通常会进行类似的优化。
首先，为了减少一系列卷积层和池化层中图像特征的损失，一些模型使用多尺度网络或跳跃层来保留损失信息。
其次，使用迁移学习方法或向模型中添加一些预训练的分类网络或LSTM，可以在添加先验知识方面发挥作用，并且该方案对模型结果有显著影响。
最后，由于评估度量对模型性能有很大的影响，一些模型往往会选择多个评估度量来训练模型（即ML-Net）。
动态模型还包括多流、多模态和 3D CNN 等形式。
然而，整体框架类型在多任务处理、动作识别等框架方面不如静态模型，因此需要开发。
尽管深度显著性模型可以充分捕获特征，但结果与GT之间存在很大的差距。
这个问题可以通过研究如何模仿人类的分析场景和理解人类凝视的机制来解决。
为了在模型上实现这些方面，需要更高水平的视觉理解。
具体而言，除了利用常规的优化模型并找到更好的损失函数外，还可以在以下基础上探索和改进显著性预测：
1.
新数据集：数据集对于模型性能极为重要[120]。
从数据中获得的GT和测量预测误差对模型性能有显著影响。
在早些年，显著性数据集的收集依赖于眼动追踪数据，数据集的图像较少。
尽管SALICON的出现改善了结果，但与相关领域（例如ImageNet）的数据集相比，差距仍然是一个数量级。
Sun等人最近收集的JFT-300M数据集。
[121] 包含 3 亿张图像，它很好地执行了在该数据集上训练的目标识别模型。
使用眼动追踪数据与通过鼠标点击收集的类似SALICON数据之间的性能差异显然是有争议的。
2.
多模态方法：随着显著性预测在动态领域的发展，越来越多的不同模式的特征，如视觉、听觉、字幕等，都可以用于训练模型。
这种多模态特征输入模式已被证明是提高模型性能的有效方法。
理解高级语义：深度显著性模型擅长提取常见特征，例如人类和纹理等。
显著性预测器也可用于处理这些特征。
但是，如图 5 所示，图像中最有趣或最重要的部分并不一定是所有这些特征。
人类视觉模型通常需要一个基于感官刺激的推理过程。
为了确定图像区域在显著性模型上相对重要性背后的原因，研究人员可以使用更高级别的特征，例如情绪、注视方向和身体姿势。
此外，为了接近人类水平的显著性预测，研究人员需要开展认知注意力研究，以帮助克服上述局限性。
已经提供了一些有用的探索。
例如，Zhao[98]通过他的实验结果表明，情绪具有优先效应。
尽管如此，现有的显著性模型仍然不能完全解释场景中的高级语义。
“语义鸿沟”的概念和确定对象相对重要性的过程仍然无法解决;此外，自然场景中的显著性是由物体引导还是由低级特征引导也是一个有争议的问题[124]。

显著性预测任务的研究与认知学科密切相关，其研究结果有助于完善后续的各种视觉研究。
随着深度模型在显著性预测方面的巨大成功，深度学习的新发展也为显著性模型的新应用和新任务提供了可能。
例如，Aksoy等[16]提出了一种基于注意力的新模型，用于做出制动决策和其他驾驶决策，如转向和加速。
Jia等[19]提出了一种用于睡眠分期的多模态显著波检测网络SalientSleepNet，该网络将时间序列分类问题转化为显著性检测问题，并将其应用于睡眠分期分类。
Wei等[125]使用显著性模型对自闭症谱系障碍（ASD）进行研究。
他们发现，患有自闭症的儿童，尤其是自闭症儿童，受到特殊物体的影响，而较少受到社会物体（例如，脸部）的影响，并且应用明显性验证模型有助于监测和评估他们的状况。
O'Shea等[126]提出了一种从原始脑电图（EEG）信号中检测癫痫发作事件的模型，对精确临床标签的依赖性较小。
这项工作为深度学习在新生儿脑电图中的应用开辟了新的途径。
有神论等。
[127] 使用全连接网络和 Fisher 剪枝将显著性计算速度提高了 10 倍，作为为具有高实时性要求的应用程序提供想法的一种手段。
Fan等[128]提出了一种检测视频中共享注意力的模型，以推断第三人称社交场景视频中的共享注意力，这对研究人类社交互动具有重要意义。
他们提出了一种新的视频数据集VACATION[129]和一种时空图推理模型，以明确表示社交场景中多样化的注视交互，并通过消息传递推断原子级的注视通信。
9. 结论

视觉显著性预测任务的发展产生了许多方法，这些方法都在各个研究方向上发挥了重要作用。
深度网络可以自动捕获特征，并有效地将特征提取和显著性预测相结合。
此外，与使用手工制作功能的经典模型相比，性能可以显着提高。
然而，深度显著性模型提取的特征可能无法完全表示图像中的显著对象和区域，尤其是在包含情感、文本或符号信息等高级信息的复杂场景中。
为了进一步提高模型的性能，必须模仿HVS的推理过程，实现对场景中相对重要区域的判别。
本文总结了显著性预测的文献，包括早期的心理和生理机制、受该任务影响的经典模型、基于深度学习的视觉显著性模型的引入、静态和动态领域的数据对比和总结。
分析了显著性模型具有优越性和局限性的原因，并指出了改进途径和可能的发展方向。
尽管基于深度学习的视觉显著性模型取得了长足的进步，但在可视化和多模态以及对高层次语义的理解方面仍有探索空间，特别是在注意力机制的研究以及与认知科学相关的应用方面仍有探索空间。
图 1.
两个显著性检测任务：（a）原始图像，（b）显著性预测任务，（c）显著性目标检测任务。
图2.神经生物学模型和经典模型的开发过程。

图3.深度显著性预测模型的开发过程.

图4.CAT2000 数据集上的显著性映射。

图5.
图片中的动物比人类更受关注。
[41]NTO数据集：2006年，Bruce等[41]建立了多伦多数据集。

它是最早和使用最广泛的计算机视觉数据集之一。
它包括 120 张彩色图像，分辨率为 511 × 681。
这些图像包含室内和室外场景以及总共 20 名记录的观察者的眼球运动数据。
• MIT300数据集：2012年，麻省理工学院的Judd等[70]建立了MIT300数据集。
它包含 Flickr 创建和分享的 300 张自然图像以及 39 位观察者的眼球运动数据。
当时，MIT300数据集是数据集显著性领域中最具影响力和应用最广泛的数据集。
数据集通常不用作训练集。
但是，可以评估包含 MIT300 数据集的模型。
• MIT1003数据集：MIT1003数据集也由Judd等[61]建立。
它包含来自 Flickr 图像集和 LabelMe 网站的总共 1003 张图像，以及 15 位观察者的眼球运动数据。
MIT1003数据集可以看作是对MIT300数据集的补充。
MIT1003 和 MIT300 数据集可以分别用作训练集和测试集进行性能评估。
• DUT-OMRON数据集：2013年，Yang等[95]建立了DUT-OMRON数据集。
它包含 5168 张图像，每张图像提供 5 个观察者的眼球运动数据。
该数据集使用眼动数据进行标注，但主要侧重于显著性目标检测，具有一个或多个显著性目标和相对复杂的背景。
MS-COCO 中的图像。
就规模和上下文可变性而言，它是目前最大的注意力数据集。
与上述数据库的区别在于，SALICON数据集不使用眼动仪来记录眼动数据，而是使用Amazon Mechanical Turk平台;然而，小鼠记录的眼球运动数据被用来评估模型的性能。

Tavakoli等[97]强调，当小鼠记录眼动数据时，评估模型性能可能会出现问题。
尽管如此，SALICON数据集是当前领域最大的数据集，并且继续被当前主流的基于深度学习技术的显著性预测模型广泛使用。
SALICON 数据集提供训练集（10,000 张图片）和验证集（5000 张图片）的眼动数据，并且可以保留测试集（5000 张图片）的眼动数据。
• EMOd数据集：EMOd数据集是Fan等[98]提出的新数据集。
它包含 1019 个带有目标级和图像级注释的情感图像。
它专为研究视觉显著性和图像情感而设计。
在EMOd数据集的图像标注过程中，对每张图像中的主要目标对象进行标签，如目标轮廓、目标名称、情感类别（负面、中性或正面）、语义类别等。
这四个语义类别如下：与人类直接相关的目标、与人类非视觉感知相关的目标、旨在吸引注意力或与人类互动的目标以及带有隐含符号的目标。
每个目标都被编码为具有一个或多个类别。
此外，EMOd 数据集共有 4302 个目标，具有精细的轮廓、情感标签和语义标签。
正面、中性和负面目标的数量分别为 839、2429 和 1034。
持续时间为 19,420 秒。
DHF1K数据集还提供了运动模式和物体数量的标定等功能，为研究动态注意力机制的高级信息提供了便利。
• LEDOV数据集：LEDOV数据集[101]由Wang等人于2018年建立。
它包括日常活动、体育运动、社交活动、艺术表演和其他内容。
共有 538 个视频，分辨率为 720px，总共包含 179,336 帧视频和 5,058,178 个注视位置。
视觉显著性预测的评估措施摘要。
静态模型在 MIT300 数据集上的性能。
静态模型在CAT2000数据集上的性能。
动态模型在 DHF1K 数据集上的性能。
[79]ROT等[122]使用音频数据来帮助视频预测。3.可视化：深度学习的黑盒模型难以以人类可以理解的方式呈现。

然而，显著性预测本身就是视觉概念的表示。
可视化 CNN 对于理解模型有很多好处，包括
包括过滤器、视觉模式或视觉概念的含义。
Bylinskii等[123]设计了一个视觉数据集，发现特定类型的数据库可能更适合训练。
可视化可以帮助我们更好地理解模型，同时也带来了提出更好的模型和数据库的可能性。
4.
