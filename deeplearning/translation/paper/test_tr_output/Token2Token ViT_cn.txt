Tokens-to-Token ViT：在 ImageNet 上从头开始训练视觉转换器

Transformer 在语言建模中很受欢迎，最近人们一直在探索用于解决视觉任务，例如，用于图像分类的 Vision Transformer （ViT）。
ViT 模型将每个图像分割成具有固定长度的标记序列，然后应用多个 Transformer 层来建模它们的全局关系以进行分类。
然而，当在像ImageNet这样的中型数据集上从头开始训练时，ViT的性能不如CNN。
我们发现这是因为：1）输入图像的简单标记化无法对相邻像素之间的边缘和线条等重要的局部结构进行建模，导致训练样本效率低下;2）ViT的冗余注意力骨干设计导致固定计算预算的特征丰富度有限，训练样本有限。

为了克服这些限制，我们提出了一种新的Tokens-to-Token视觉转换器（T2T-ViT），它结合了1）分层Tokens-to-Token（T2T）变换，通过递归地将相邻的Tokens聚合为一个Token（Tokens-to-Token）来逐步构建图像到Tokens，从而可以对周围的Token表示的局部结构进行建模，并减少Token的长度;2）基于CNN架构设计的实证研究，为视觉转换器提供了一种具有深窄结构的高效骨干网。

值得注意的是，T2T-ViT 将原版 ViT 的参数数量和 MAC 减少了一半，而在 ImageNet 上从头开始训练时，实现了 3.0% 以上的改进。
它还优于 ResNets，并通过直接在 ImageNet 上进行训练来实现与 MobileNets 相当的性能。
例如，与ResNet50大小相当的T2T-ViT（参数为21.5M），在ImageNet上的图像分辨率为384×384，可以达到83.3%的top1准确率。1

1. 引言

用于语言建模的自注意力模型（如Transformers[37]）最近已应用于视觉任务，包括图像分类[5,12,43]，对象识别[3,61]和图像处理，如去噪，超分辨率和去雨[4]。
其中，Vision Transformer（ViT）[12]是第一个可以直接应用于图像分类的全Transformer模型。
具体来说，ViT 将每个图像分割成 14×14 或 16×16 个具有固定长度的补丁（也称为令牌）;然后，遵循 transformer 进行语言建模的实践，ViT 应用 transformer 层来建模这些令牌之间的全局关系以进行分类。

尽管 ViT 证明了全 transformer 架构在视觉任务方面很有前途，但其性能仍然不如类似大小的 CNN 对应物（例如
ResNets），在中型数据集（例如，Im-ageNet）上从头开始训练时。
我们假设这种性能差距源于 ViT 的两个主要局限性：1） 通过硬分割直接标记输入图像使得 ViT 无法对边缘和线条等图像局部结构进行建模，因此它需要比 CNN 多得多的训练样本（如用于预训练的 JFT-300M）才能实现相似的性能;2）ViT的注意力骨干在视觉任务中不如CNN设计得更好，存在冗余性，导致特征丰富度有限，模型训练困难。

为了验证我们的假设，我们进行了一项初步研究，通过图 2 中的可视化来调查 ViT-L/16 [12] 和 ResNet50 [15] 学习特征的差异。我们观察了ResNet捕获所需的本地特性，如图2所示。ResNet50、ViT-L/16 [12] 和我们提出的在 ImageNet 上训练的 T2T-ViT-24 的特征可视化。

绿色框突出显示学习过的低级结构特征，例如边和线条;红色框突出显示值为零或过大的无效特征图。

请注意，此处可视化的 ViT 和 T2T-ViT 特征图不是注意力图，而是从令牌重塑的图像特征。
为了获得更好的可视化效果，我们将输入图像缩放为 1024 × 1024 或 2048 × 2048。
结构（边缘、线条、纹理等）逐渐从底层 （conv1） 到中间层 （conv25）。
然而，ViT的特点是完全不同的：结构信息建模得很差，而全局关系（例如，整只狗）被所有注意力块捕获。
这些观察结果表明，vanilla ViT 在直接将图像分割为具有固定长度的标记时忽略了局部结构。
此外，我们发现 ViT 中的许多通道的值为零（图 2 中以红色突出显示），这意味着 ViT 的骨干网不如 ResNet 高效，并且在训练样本不足时提供的特征丰富度有限。
然后，我们有动力设计一种新的全变压器视觉模型来克服上述限制。
1）摒弃了ViT[12]中朴素的tokenization，我们提出了一种渐进式tokenization模块，将相邻的Token聚合为一个Token（称为Tokens-to-Token模块），该模块可以对周围Token的局部结构信息进行建模，并迭代减少Token的长度。
具体来说，在每个 Token-to-Token （T2T） 步骤中，转换器层输出的代币被重建为图像（重组），然后将其分割成具有重叠（软分割）的代币，最后通过展平分裂的补丁将周围的代币聚合在一起。
因此，来自周围补丁的局部结构被嵌入到令牌中，以输入到下一个变压器层。
通过迭代进行 T2T，将本地结构聚合成代币，并且可以通过聚合过程减少代币的长度。
2）为了找到视觉Transformers的高效骨干网，我们探索借鉴CNN的一些架构设计来构建Transformer层，以提高特征丰富度，我们发现ViT中通道更少但层数更多的“deepnarrow”架构设计在可比的模型大小和MACs（Multi-Adds）下带来了更好的性能。
具体来说，我们研究了Wide-ResNets（浅宽结构与深窄结构）[52]、DenseNet（密集连接）[21]、ResneXt结构[44]、幽灵操作[14,59]和通道注意力[20]。
我们发现其中，深窄结构[52]对ViT最有效，可以显着减少参数数量和MACs，而性能几乎没有下降。
这也表明，CNN的架构工程可以使视觉转换器的骨干设计受益。
基于T2T模块和深窄骨干架构，我们开发了Tokens-to-Token Vision Transformer（T2T-ViT），在ImageNet上从零开始训练时，性能显著提升（图1），比原版ViT更轻量级。
如图1所示，我们的T2T-ViT参数为21.5M，MACs为4.8G，在ImageNet上的top-1准确率为81.5%，远高于参数为48.6M，MACs为10.1G MACs的ViT[12]（78.1%）。
这一结果也高于流行的类似尺寸的CNN，如ResNet50，参数为25.5M（76%-79%）。
此外，我们还通过简单地采用较少的层数来设计T2T-ViT的精简版，从而实现了与MobileNets相当的结果[17,32]（图1）。
总而言之，我们的贡献有三个方面：
• 我们首次通过精心设计 transformers 架构（T2T 模块和高效骨干网）来证明，视觉 transformer 可以在 ImageNet 上不同复杂度下优于 CNN，而无需在 JFT-300M 上进行预训练。
• 我们开发了一种新型的ViT渐进式标记化方法，并展示了其相对于ViT的简单标记化方法的优势，并提出了一种T2T模块，可以编码每个令牌的重要本地结构。
• 研究表明，CNN的架构工程可以使ViT的骨干设计受益，从而提高特征丰富度并减少冗余。
通过大量的实验，我们发现深窄架构设计最适合ViT。
二、相关工作

视觉转换器中的Transformers[37]是完全依靠自注意力机制在输入和输出之间绘制全局依赖关系的模型，目前它们在自然语言建模中占主导地位[10,30,2,46,29,23]。
变压器层通常由多头自注意力层（MSA）和MLP模块组成。
Layernorm （LN） 应用于自注意力层和 MLP 块中的每一层和残差连接之前。
最近的工作探索了将转换器应用于各种视觉任务：图像分类 [5， 12] 、目标检测 [3， 61， 58， 8， 34] 、分割 [4， 40] 、图像增强 [4， 45] 、图像生成 [27] 、视频处理 [60， 53] 和 3D 点云处理 [56] 。
其中，Vision Transformer（ViT）证明了纯Transformer架构在图像分类上也可以达到最先进的性能。
然而，ViT严重依赖ImageNet-21k和JFT-300M（非公开可用）等大规模数据集进行模型预训练，需要巨大的计算资源。
相比之下，我们提出的 T2T-ViT 效率更高，可以在 ImageNet 上进行训练，而无需使用那些大规模数据集。
近期的并发工作DeiT [36]应用知识蒸馏[16， 49]对原有的ViT进行改进，通过在类令牌中加入一个KD token，这与我们的工作是正交的，因为我们的T2T-ViT侧重于架构设计，而我们的T2T-ViT可以在没有CNN作为教师模型的情况下实现比DeiT更高的性能。
CNN中的自我注意机制 自我注意机制已被广泛应用于视觉任务中的CNNs[38,57,19,47,20,39,1,6,18,31,42,13,50,48]。
在这些工作中，SE块[20]将注意力应用于信道维度，非局域网络[39]被设计用于通过全局注意力捕获长程依赖关系。
与大多数在图像上探索全局注意力的工作[1,42,13,39]相比，一些工作[18,31]也探索了局部斑块中的自我注意力，以减少内存和计算成本。
最近，SAN [55]研究了图像识别的成对和逐块自注意力，其中按斑块自注意力是卷积的推广。
在这项工作中，我们还在实验中用多个卷积层替换了T2T模块，发现卷积层的性能并不比我们设计的T2T模块好。
然后 Ii 被分裂并再次重叠到标记 Ti+1。
具体来说，如粉红色面板所示，输入 Ii 的四个令牌 （1,2,4,5） 连接在一起，在 Ti+1 中形成一个令牌。
T2T 转换器可以是普通的 Transformer 层 [37]，也可以是 GPU 内存有限的其他高效 Transformer 层 [34]。
3. 代币到代币的 ViT

为了克服ViT标记化简单、骨干网效率低下的局限性，我们提出了Tokens-to-Token视觉转换器（T2T-ViT），该转换器可以将图像逐步标记化为标记，并具有高效的骨干。
因此，T2T-ViT由两个主要组成部分（图4）组成：1）逐层的“Tokens-to-Token模块”（T2T模块），用于对图像的局部结构信息进行建模并逐步减少Token的长度;2）高效的“T2T-ViT骨干网”，从T2T模块中吸引全球关注关系到代币。

在探索了几种基于CNN的架构设计后，我们采用深窄结构作为骨干网，以减少冗余并提高特征丰富度。
现在，我们将逐一解释这些组件。
3.1. Tokens-to-Token：渐进式代币化

Token-to-Token（T2T）模块旨在克服ViT中简单令牌化的局限性。
它逐步构建图像以标记和建模本地结构信息，通过这种方式可以迭代减少标记的长度。
每个 T2T 过程都有两个步骤：重组和软分裂 （SS）（图 3）。
重组

如图 3 所示，给定来自前一个 transformer 层的一系列代币 T，它将被自注意力块（图 3 中的 T2T transformer）变换：
其中MSA表示具有层归一化的多头自注意力操作，“MLP”是标准Transformer中具有层归一化的多层每感知器[12]。
然后，标记 T 将在空间维度上被重塑为图像，
这里的 “Reshape” 将标记 T ∈ R l×c 重新组织为 I ∈ R h×w×c，其中 l 是 T 的长度，h、w、c 分别是高度、宽度和通道，l = h × w。
软分割 如图 3 所示，在得到重组后的图像 I 后，我们对其应用软分割来建模局部结构信息并减少标记的长度。
具体来说，为了避免在从重构图像生成令牌时发生信息丢失，我们将其分割成重叠的补丁。
因此，每个补丁都与周围的补丁相关联，以建立一个先验，即周围的标记之间应该有更强的相关性。
每个分割补丁中的令牌连接为一个令牌（Tokens-to-Token，图 3），因此可以从周围的像素和补丁中聚合本地信息。
在进行软分割时，每个补丁的大小为 k ×k，图像上有 s 重叠和 p 填充，其中 k -s 类似于卷积运算中的步幅。
所以对于重建的图像 I ∈ R h×w×c，软分割后输出标记 T o 的长度为
每个分割贴片的大小为 k × k × c。
我们将空间维度上的所有斑块展平为标记 T o ∈ R lo×ck 2 。
软拆分后，输出令牌将用于下一个 T2T 进程。
T2T模块通过迭代进行上述重构化和软拆分，可以逐步减少标记的长度，并改变图像的空间结构。
T2T模块中的迭代过程可以表述为
对于输入图像 I 0 ，我们首先应用软拆分以将其拆分为标记：T 1 = SS（I 0 ）。
经过最终迭代后，T2T 模块的输出标记 T f 具有固定长度，因此 T2T-ViT 的骨干可以在 T f 上建模全局关系。
此外，由于 T2T 模块中的令牌长度大于 ViT 中的正常情况 （16 × 16），因此 MAC 和内存使用量巨大。
为了解决这些限制，在我们的 T2T 模块中，我们将 T2T 层的通道尺寸设置为较小（32 或 64）以减少 MAC，并可选择采用高效的 Transformer 如 Performer [7] 层，以减少有限 GPU 内存下的内存使用。
我们在实验中对采用标准变压器层和执行器层之间的差异进行了烧蚀研究。
3.2. T2T-ViT骨干网

由于vanilla ViT骨干网中的许多通道都是无效的（图2），我们计划为我们的T2T-ViT找到一个高效的骨干网，以减少冗余并提高特征丰富度。
因此，我们探索了ViT的不同架构设计，并借鉴了CNN的一些设计，以提高骨干网效率，增强学习特征的丰富度。
由于每个Transformer层都有ResNets的跳跃连接，一个简单的思路是应用密集连接作为DenseNet [21]来增加连通性和特征丰富度，或者应用Wide-ResNets或ResNeXt结构来改变ViT主干网中的通道尺寸和头数。
我们探讨了从 CNN 到 ViT 的五种架构设计：
1. 密集连接，如 DenseNet [21] ;2. 深-窄与浅-宽结构，如Wide-ResNets [52];

3. 将注意力引导为挤压激励（SE）网络[20];
4. 多头注意力层中的分头更多，如ResNeXt [44] ;
5. 作为 GhostNet 的 Ghost 操作 [14] 。
ViT中这些结构设计的详细信息在附录中给出。
我们对在Sec中转移的结构进行了广泛的实验。
4.2.
实证研究发现：1）采用深窄结构，简单地减小信道维度以减少信道冗余度，增加层深度以提高ViT中的特征丰富度，模型大小和MACs均减小，但性能有所提高;2）作为SE块的通道注意力也提高了ViT，但效果不如使用深窄结构。

基于这些发现，我们为我们的 T2T-ViT 骨干网设计了一种深窄架构。
具体来说，它具有较小的通道号和隐藏的维度 d，但层 b 较多。
对于 T2T 模块最后一层长度为 T f 固定的令牌，我们为其连接一个类令牌，然后向其添加正弦位置嵌入（PE），与 ViT 一样做分类：
其中 E 是正弦位置嵌入，LN 是层归一化，fc 是用于分类的全连接层，y 是输出预测。
在 T2T 模块中，输入图像首先被软分割为斑块，然后展开为一系列标记 T0。
在 T2T 模块中，令牌的长度逐渐减少（我们在这里使用两次迭代并输出 T f ）。
然后，T2T-ViT骨干网将固定的令牌作为输入，并输出预测。
两个 T2T 模块如图 3 所示，PE 是位置嵌入。
3.3. T2T-ViT架构

T2T-ViT 由两部分组成：Tokens-to-Token （T2T） 模块和 T2T-ViT 骨干网（图 4）。
T2T 模块有多种可能的设计选择。
在这里，我们设置 n = 2，如图 4 所示，这意味着 T2T 模块中有 n+1 = 3 软分裂和 n = 2 重构化。
三个软分割的补丁大小为 P = [7， 3， 3]，重叠为 S = [3， 1， 1]，根据方程 （3），输入图像的大小从 224 × 224 减小到 14 × 14。
T2T-ViT骨干网从T2T模块获取固定长度的令牌作为输入，与ViT相同;但具有深窄架构设计，与 ViT 相比，具有更小的隐藏尺寸 （256-512） 和 MLP 尺寸 （512-1536）。

例如，T2T-ViT-14在T2T-ViT骨干中有14个transformer层，有384个隐性维度，而ViT-B/16有12个transformer层和768个隐性维度，在参数和MAC上是T2T-ViT-14的3倍。
为了与常见的手工设计的CNN进行公平比较，我们使T2T-ViT模型具有与ResNets和MobileNets相当的大小。
具体来说，我们设计了三个模型：T2T-ViT-14、T2T-ViT-19和T2T-ViT-24，分别与ResNet50、ResNet101和ResNet152参数相当。
为了与MobileNets等小型模型进行比较，我们设计了两个精简模型：T2T-ViT-7，T2T-ViT-12，其模型大小与MibileNetV1和MibileNetV2相当。
两个 lite TiT-ViT 没有像高效卷积 [26] 这样的特殊设计或技巧，只是减少了层深度、隐藏维度和 MLP 比率。
表 1 总结了网络详细信息。
4. 实验

我们在ImageNet上使用T2T-ViT进行以下实验进行图像分类。
a） 我们通过在ImageNet上从头开始训练来验证T2T-ViT，并将其与一些常见的卷积神经网络（如大小相当的ResNets和MobileNets）进行比较;我们还将预训练的 T2T-ViT 转移到下游数据集，例如 CIFAR10 和 CIFAR100 （Sec.

4.1. ImageNet 上的 T2T-ViT

所有实验都在ImageNet数据集[9]上进行，训练集约有130万张图像，验证集有50k张图像。
我们使用批量大小为 512 或 1024 和 8 个 NVIDIA GPU 进行训练。
我们采用Pytorch [28]库和Pytorch图像模型库（timm）[41]来实现我们的模型并进行所有实验。
为了公平比较，我们对 CNN 模型、ViT 和我们的 T2T-ViT 实施了相同的训练方案。
在ImageNet的整个实验中，我们将默认图像大小设置为224×224，除了384×384上的一些特定情况外，并采用一些常见的数据增强方法，如mixup [54]和cutmix [11， 51]进行CNN和ViT&T2T-ViT模型训练，因为ViT模型需要更多的训练数据才能达到合理的性能。
我们使用 AdamW [25] 作为优化器和余弦学习率衰减 [24] 来训练这些模型 310 个 epoch。
实验设置的细节在附录中给出。
我们还在 T2T 模块中使用了 Transformer 层和 Performer 层作为我们的模型，从而产生了 T2T-ViT -14/19/24（变压器）和 T2T-ViT-14/19/24（表演者）。
T2T-ViT 与 ViT 我们首先比较了 T2T-ViT 和 ViT 在 ImageNet 上的性能。
结果如表2所示。我们的 T2T-ViT 在参数数量和 MAC 数量上比 ViT 小得多，但性能更高。

例如，在ImageNet上从头开始训练的小型ViT模型ViT-S/16具有48.6M和10.1G MACs的top-1精度为78.1%，而我们的T2T-ViT t -14参数为44.2%，MACs为51.5%，提高了3.0%以上（81.5%）。
如果我们将 T2T-ViT t -24 与 ViT-L/16 进行比较，前者将参数和 MAC 降低约 500%，但在 ImageNet 上实现了 1.0% 以上的改善。
将 T2T-ViT-14 与 DeiT-small 和 DeiT-small-Distilled 进行比较，我们的 T2T-ViT 可以在没有大型 CNN 模型作为增强 ViT 的教师的情况下实现更高的准确度。
我们还采用更高的图像分辨率，如384×384，并通过我们的T2T-ViT-14↑384获得83.3%的准确度。
T2T-ViT 与 ResNet 为了公平比较，我们设置了三个 T2T-ViT 模型，它们与 ResNet50、ResNet101 和 ResNet152 具有相似的模型大小和 MAC。
实验结果见表3。拟议的T2T-ViT达到1.4%-2.7%

与具有相似模型大小和 MAC 的 ResNet 相比，性能有所提高。
例如，与25.5M参数和4.3G MAC的ResNet50相比，我们的T2T-ViT-14在ImageNet上的准确率为21.5M，4.8G MACs的准确率为81.5%。
T2T-ViT 与 MobileNet 的比较 T2T-ViT-7 和 T2T-ViT-12 与 MobileNetV1 [17] 和 Mo- bileNetV2 [32] 具有相似的模型大小，但实现了与 MobileNets 相当或更高的性能（Tab.
4).
例如，我们的 T2T-ViT-12 参数为 6.9M，可实现 76.5% 的 top1 准确率，比 MobileNetsV2 高 1.4 倍 0.9%。
但我们也注意到，由于变压器中的密集操作，我们的 T2T-ViT 的 MAC 仍然比 MobileNet 大。
然而，目前的T2T-ViT-7和T2T-ViT-12中没有像高效卷积[26,32]这样的特殊操作或技巧，我们只是通过减小隐藏维度、MLP比例和层深度来减小模型尺寸，表明T2T-ViT作为轻量级模型也非常有前景。
我们还将知识蒸馏应用于我们的T2T-ViT作为并行工作DeiT [36]，发现我们的T2T-ViT-7和T2T-ViT-12可以通过蒸馏进一步改进。
总体而言，实验结果表明，我们的T2T-ViT在ResNets中等尺寸时能获得优越的性能，在MobileNets模型尺寸较小时能获得合理的性能。
迁移学习 我们将预训练的 T2T-ViT 转移到下游数据集，例如 CIFAR10 和 CIFAR100。
我们通过使用 SGD 优化器和余弦学习率衰减对预训练的 T2T-ViT-14/19 进行 60 个 epoch 的微调。结果如表5所示。我们发现，在下游数据集上，我们的 T2T-ViT 可以比原始 ViT 在模型大小较小的情况下实现更高的性能。

4.2. 从CNN到ViT

为了找到视觉Transformer的高效骨干网，我们实验性地应用了DenseNet结构、Wide-ResNet结构（宽或窄通道维度）、SE块（通道注意力）、ResNeXt结构（多头注意力中的更多头）和从CNN到ViT的Ghost运算。
附录中详细介绍了这些架构设计。
根据 Tab 中“CNN 到 ViT”的实验结果。
6、我们发现SE（ViT-SE）和深窄结构（ViT-DN）均对ViT有利，但最有效的结构是深窄结构，使模型大小和MACs减少了近2倍，比基线模型ViT-S/16提高了0.9%。
我们进一步将CNN的这些结构应用到我们的T2T-ViT中，并在相同的训练方案下在ImageNet上进行实验。
我们以 ResNet50 为基线进行 CNN，ViT-S/16 作为 ViT，T2T-ViT-14 作为 T2T-ViT 的基线。
所有实验结果均在表中给出。
6、CNN和ViT&T2T-ViT上的标注颜色相同。
我们在下面总结了每个基于 CNN 的结构的影响。
深窄结构有利于 ViT：Tab 中的模型 ViT-DN（深窄）和 ViT-SW（浅宽）。
6 是通道尺寸和层深度上的两种相反设计，其中 ViT-DN 有 384 个隐藏维度和 16 层，ViT-SW 有 1,024 个隐藏维度和 4 层。
与具有768个隐性维度和8层的基线模型ViT-S/16相比，浅宽模型ViT-SW的性能下降了8.2%，而只有一半的模型尺寸和MAC的ViT-DN性能提高了0.9%。
这些结果验证了我们的假设，即具有浅宽结构的香草ViT在通道维度上是多余的，而浅层的特征丰富度有限。
密集连接对ViT和T2T-ViT的性能都有所影响：与ResNet50相比，DenseNet201的参数更小，MAC数量相当，但性能更高。
但是，密集连接可能会损害 ViT-Dense 和 T2T-ViT-Dense 的性能（表 6 中的深蓝色行）。
SE 块改善了两者

ViT 和 T2T-ViT：从选项卡中的红色行开始。
6、我们可以发现SENets、ViT-SE和T2T-ViT-SE都高于对应的基线。
SE 模块可以提高 CNN 和 ViT 的性能，这意味着将注意力应用于频道对 CNN 和 ViT 模型都有好处。
ResNeXt结构对ViT和T2T-ViT影响不大：ResNeXts在ResNets上采用多头结构，而Transformers也是多头注意力结构。
当我们采用更多像 32 这样的头部时，我们会发现它对性能几乎没有影响（表 6 中的红色行）。
然而，采用大量的磁头会使GPU内存变大，因此在ViT和T2T-ViT中是不必要的。
Ghost 可以进一步压缩模型并减少 T2T-ViT 的 MAC：比较 Ghost 操作的实验结果（Tab.
6），在ResNet50上准确率下降2.9%，在T2T-ViT上下降2.0%，在ViT上准确率下降4.4%。
因此，与ResNet相比，Ghost操作可以进一步降低T2T-ViT的参数和MAC，且性能下降较小。
但对于原始的 ViT，它会导致比 ResNet 更多的下降。
此外，在5种结构中，T2T-ViT的性能均优于ViT，进一步验证了我们提出的T2T-ViT的优越性。
我们也希望这项将CNN结构转移到ViT的研究能够激发Transformers在视觉任务中的网络设计。
4.3. 消融研究

为了进一步确定T2T模块和深窄结构的影响，我们对T2T-ViT进行了烧蚀研究。
为了验证所提出的T2T模块的效果，我们实验比较了三种不同的模型：T2T-ViT-14、T2T-ViT-14 wo T 2T和T2T-ViT t -14，其中T2T-ViT-14 wo T 2T具有相同的T2T-ViT骨干，但没有T2T模块。
我们可以发现，在模型大小和MAC相似的情况下，T2T模块在ImageNet上可以提高2.0%-2.2%的模型性能。
由于T2T模块中的软分裂类似于没有卷积滤波器的卷积操作，因此我们还用3个卷积层替换T2T模块，分别为核大小（7,3,3），步幅大小（4,2,2）。
这种用卷积层构建T2T模块的模型表示为T2T-ViT c -14。
从表7可以看出，T2T-ViT c -14比T2T-ViT-14和T2T-ViT t -14差0.5%-1.0%
在ImageNet上。
我们还注意到，T2T-ViT c -14 仍然高于 T2T-ViT-14 wo T 2T，因为早期的卷积层也可以对结构信息进行建模。
但是我们设计的T2T模块比卷积层更好，因为它既可以对图像的全局关系建模，也可以对图像的结构信息进行建模。
深窄结构 我们使用深窄结构，隐藏维度较少，但层数更多，而不是原始 ViT 中的浅宽结构。
我们比较了 T2T-ViT-14 和 T2T-ViT-d768-4 以验证其效果。
T2T-ViT-d768-4 是一种浅宽结构，隐蔽尺寸为 768 和 4 层，具有与 T2T-ViT-14 相似的模型尺寸和 MAC。
从表7中可以发现，将深窄结构改为浅宽结构后，T2T-ViT-d768-4的top-1精度下降了2.7%，验证深窄结构对T2T-ViT至关重要。
5. 结论

在这项工作中，我们提出了一种新的T2T-ViT模型，该模型可以在ImageNet上从头开始训练，并达到与CNN相当甚至更好的性能。
T2T-ViT有效地模拟了图像的结构信息，增强了特征丰富度，克服了ViT的局限性。
它引入了新颖的令牌到令牌 （T2T） 过程，以逐步将图像标记化为令牌并在结构上聚合令牌。
我们还探索了CNN的各种架构设计选择，以提高T2T-ViT性能，并实证发现深窄架构比浅宽架构性能更好。
在ImageNet上从头开始训练时，我们的T2T-ViT实现了优于ResNets的性能，并与具有相似模型大小的MobileNets相当的性能。
它为进一步开发基于Transformer的视觉任务模型铺平了道路。
图 1.
在 ImageNet 上从头开始训练时，T2T-ViT 与 ViT、ResNets 和 MobileNets 之间的比较。
左图：MAC 的性能曲线与 top-1 精度的关系图。
右图：模型大小与前 1 精度的性能曲线。
图3.T2T 过程图示。

标记Ti在变换和重塑后被重构为图像Ii;然后 Ii 被分裂并再次重叠到标记 Ti+1。

具体来说，如粉红色面板所示，输入 Ii 的四个令牌 （1,2,4,5） 连接在一起，在 Ti+1 中形成一个令牌。
T2T 转换器可以是普通的 Transformer 层 [37]，也可以是 GPU 内存有限的其他高效 Transformer 层 [34]。
图4.
T2T-ViT的整体网络架构。
在 T2T 模块中，输入图像首先被软分割为斑块，然后展开为一系列标记 T0。
在 T2T 模块中，令牌的长度逐渐减少（我们在这里使用两次迭代并输出 T f ）。
然后，T2T-ViT骨干网将固定的令牌作为输入，并输出预测。
两个 T2T 块如图 3 所示，PE 是位置嵌入。
4.1).
（b） 我们比较了五种受CNNs启发的T2T-ViT骨干架构设计。
4.2).
（c） 我们进行烧蚀研究，以证明 T2T 模块和 T2T-ViT 的深窄架构设计的效果（Sec.
4.3).
[12]T2T-ViT-14/19/24与ResNet50/101/152具有相当的模型尺寸，T2T-ViT-7/12与MobileNetV1/V2具有相当的模型尺寸。对于T2T变压器层，我们在有限的GPU内存下，将Transformer层用于T2T-ViTt-14，将Performer层用于T2T-ViT-14。对于ViT，“S”表示小，“B”是基础，“L”是大。“ViT-S/16”是原始ViT-B/16[12]的变体，具有更小的MLP尺寸和层深度。

通过在ImageNet上从头开始训练，T2T-ViT和ViT之间的比较。
我们的 T2T-ViT 与 Im-ageNet 上的 ResNets 之间的比较。
T2T-ViTt-14：在T2T模块中使用Transformer。
T2T-ViT-14：在 T2T 模块中使用 Performer。
* 表示我们使用我们的训练方案来训练模型，以便进行公平的比较。
[36]我们的lite T2T-ViT与MobileNets.带有“-Distilled”的模型由教师模型教授，方法为DeiT[36]。
将预训练的 T2T-ViT 微调到下游数据集的结果：CIFAR10 和 CIFAR100。
将CNN中的一些常见设计转移到ViT&T2T-ViT，包括DenseNet、Wide-ResNet、SE模块、ResNeXt、Ghost操作。
相同的颜色表示相应的传输。
所有模型都是在ImageNet上从头开始训练的。
* 表示我们使用我们的训练计划复制模型，以便进行公平的比较。
T2T模块，深窄（DN）结构的消融研究结果。
