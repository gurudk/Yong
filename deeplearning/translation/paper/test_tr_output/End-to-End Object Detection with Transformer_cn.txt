使用变压器进行端到端目标检测

我们提出了一种新方法，将目标检测视为直接集合预测问题。
我们的方法简化了检测流程，有效地消除了对许多手工设计的组件的需求，如非最大抑制程序或锚点生成，这些组件明确编码了我们对任务的先验知识。
新框架（称为 DEtection TRansformer 或 DETR）的主要组成部分是基于集合的全局损失，通过二分匹配强制进行唯一预测，以及转换器编码器-解码器架构。
给定一小组固定的学习对象查询，DETR 对对象的关系和全局图像上下文进行推理，以直接并行输出最终的预测集。
新模型在概念上很简单，与许多其他现代探测器不同，不需要专门的库。
在具有挑战性的 COCO 对象检测数据集上，DETR 的准确性和运行时性能与完善且高度优化的 Faster R-CNN 基线相当。
此外，DETR可以很容易地泛化，以统一的方式产生全景分割。
我们发现，它的表现明显优于竞争基线。
1 引言

对象检测的目标是为每个感兴趣的对象预测一组边界框和类别标签。
现代检测器通过在大量提案 [37， 5] 、锚点 [23] 或窗口中心 [53， 46] 上定义代理回归和分类问题，以间接的方式处理这组预测任务。
它们的性能受到后处理步骤的显着影响，这些步骤用于折叠近乎重复的预测，锚集的设计以及将目标框分配给锚点的启发式方法[52]。
为了简化这些管道，我们提出了一种直接集预测方法来绕过代理任务。
这种端到端的理念在复杂的结构化预测任务（如机器翻译或语音识别）方面取得了重大进展，但在对象检测方面尚未取得重大进展：以前的尝试[43,16,4,39]要么增加了其他形式的先验知识，要么没有被证明在具有挑战性的基准上具有强大的基线竞争力。
本文旨在弥合这一差距。
我们通过将对象检测视为直接设置预测问题来简化训练管道。
我们采用了基于transformers的编码器-解码器架构[47]，这是一种流行的序列预测架构。
Transformer 的自注意力机制明确地对序列中元素之间的所有成对交互进行建模，使这些架构特别适用于集合预测的特定约束，例如删除重复预测。
我们的 DEtection TRansformer（DETR，见图 1）一次预测所有对象，并使用设置损失函数进行端到端训练，该函数在预测对象和地面实况对象之间执行二分匹配。
DETR 通过删除多个手工设计的组件（如空间锚点或非最大抑制）来编码先验知识，从而简化了检测管道。
与大多数现有的检测方法不同，DETR 不需要任何自定义层，因此可以在任何包含标准 CNN 和转换器类的框架中轻松重现。1 .

与之前大多数关于直接集预测的工作相比，DETR的主要特征是二分匹配损失和transformer与（非自回归）并行解码的结合[29,12,10,8]。
相比之下，以前的工作主要集中在使用RNN的自回归解码[43,41,30,36,42]。
我们的匹配损失函数唯一地将预测分配给地面实况对象，并且对于预测对象的排列是不变的，因此我们可以并行发射它们。
我们在最受欢迎的目标检测数据集之一 COCO [24] 上评估了 DETR 与极具竞争力的 Faster R-CNN 基线 [37] 。
更快的 R-CNN 经历了多次设计迭代，其性能自原始发布以来有了很大的提高。
我们的实验表明，我们的新模型实现了可比的性能。
更准确地说，DETR 在大型物体上表现出明显更好的性能，这一结果可能是由于转换器的非本地计算实现的。
然而，它在小物体上的性能较低。
我们预计未来的工作将像开发FPN [22]为Faster R-CNN所做的那样改进这一方面。
DETR 的训练设置在多个方面与标准对象检测器不同。
新模型需要超长的训练计划，并受益于变压器中的辅助解码损耗。
我们深入探讨了哪些组件对于所展示的性能至关重要。
DETR 的设计理念很容易扩展到更复杂的任务。
在我们的实验中，我们展示了一个简单的分割头在预训练的 DETR 之上进行训练，超越了 Panoptic Segmentation [19] 的竞争基线，这是一项具有挑战性的像素级识别任务，最近越来越受欢迎。
2 相关工作

我们的工作建立在几个领域的先前工作之上：用于集合预测的二分匹配损失、基于转换器的编码器-解码器架构、并行解码和目标检测方法。
2.1 集合预测

没有规范的深度学习模型可以直接预测集合。
基本的集合预测任务是多标签分类（参见 [40， 33] 以获取计算机视觉背景下的参考文献），对于该分类，基线方法（一对一与休息）不适用于诸如检测等问题，其中元素之间存在底层结构（即，几乎相同的盒子）。
这些任务的第一个困难是避免近乎重复。
目前大多数检测器都使用后处理（如非最大抑制）来解决这个问题，但直接集预测无需后处理。
他们需要全局推理方案来对所有预测元素之间的交互进行建模，以避免冗余。
对于恒定大小集预测，密集的全连接网络[9]就足够了，但成本高昂。
一般的方法是使用自回归序列模型，例如递归神经网络[48]。
在所有情况下，损失函数都应该通过预测的排列是不变的。
通常的解决方案是基于匈牙利算法[20]设计一个损失，以找到真值和预测之间的二分匹配。
这强制实施了置换不变性，并保证每个目标元素都具有唯一的匹配。
我们遵循二分匹配损失方法。
然而，与大多数以前的工作相比，我们放弃了自回归模型，并使用了具有并行解码功能的转换器，我们将在下面描述。
2.2 变压器和并联解码

变压器是由Vaswani等人介绍的。
[47] 作为机器翻译的一种新的基于注意力的构建块。
注意力机制 [2] 是聚合来自整个输入序列的信息的神经网络层。
Transformers引入了自注意力层，类似于非局部神经网络[49]，扫描序列的每个元素，并通过聚合来自整个序列的信息来更新它。
基于注意力的模型的主要优点之一是它们的全局计算和完美的内存，这使得它们比长序列上的RNN更适合。
在自然语言处理、语音处理和计算机视觉[8,27,45,34,31]的许多问题中，Transformer现在正在取代RNN。
Transformer 首先用于自回归模型，遵循早期的 sequence-to-sequence 模型 [44]，一个接一个地生成输出标记。
然而，令人望而却步的推理成本（与输出长度成正比，难以批量处理）导致了并行序列生成的发展，在音频 [29] 、机器翻译 [12， 10] 、单词表示学习 [8] 以及最近的语音识别 [6] 等领域。
我们还结合了转换器和并行解码，以在计算成本和执行集合预测所需的全局计算的能力之间进行适当的权衡。
2.3 物体检测

大多数现代物体检测方法都是相对于一些初始猜测进行预测的。
两级检测器 [37， 5] 预测箱 w.r.t.
建议，而单阶段方法则进行预测 W.R.T.
锚 [23] 或可能的对象中心网格 [53， 46] 。
最近的工作[52]表明，这些系统的最终性能在很大程度上取决于这些初始猜测的确切设置方式。
在我们的模型中，我们能够删除这个手工制作的过程，并通过直接预测绝对框预测的检测集来简化检测过程，而不是输入图像而不是锚点。
基于设置的损失。
几个目标检测器[9,25,35]使用了二分匹配损耗。
然而，在这些早期的深度学习模型中，不同预测之间的关系仅使用卷积层或全连接层进行建模，手工设计的NMS后处理可以提高其性能。
最近的检测器[37,23,53]在地面实况和预测之间使用非唯一分配规则以及NMS。
可学习的NMS方法[16,4]和关系网络[17]明确地对不同预测之间的关系进行关注建模。
使用直接设定损耗，它们不需要任何后处理步骤。
然而，这些方法采用了额外的手工制作的上下文特征，如提案框坐标，以有效地对检测之间的关系进行建模，同时我们正在寻找能够减少模型中编码的先验知识的解决方案。
复发检测器。
最接近我们的方法的是对象检测 [43] 和实例分割 [41， 30， 36， 42] 的端到端集预测。
与我们类似，他们使用二分匹配损失和基于 CNN 激活的编码器-解码器架构来直接生成一组边界框。
然而，这些方法仅在小型数据集上进行了评估，而没有根据现代基线进行评估。
特别是，它们基于自回归模型（更准确地说是 RNN），因此它们不会利用最近的并行解码转换器。
3 DETR模型

在检测中，有两个要素对于直接集合预测是必不可少的：（1）集合预测损失，强制预测框和地面实况框之间进行唯一匹配;（2） 一种架构，它（在一次传递中）预测一组对象并对其关系进行建模。

我们在图 2 中详细介绍了我们的架构。
3.1 目标检测集预测损失

DETR 在通过解码器的单次传递中推断出一组固定大小的 N 个预测，其中 N 被设置为明显大于图像中典型对象数。
训练的主要困难之一是根据基本事实对预测对象（类别、位置、大小）进行评分。
我们的损失在预测对象和地面实况对象之间产生最优的二分匹配，然后优化特定于对象（边界框）的损失。
让我们用 y 表示物体的真值集，ŷ = {ŷ i } N i=1 表示 N 个预测的集合。
假设 N 大于图像中的对象数量，我们将 y 也视为一组大小为 N 的集合，其中填充了 ∅（无对象）。
为了找到这两个集合之间的二分匹配，我们搜索 N 个元素σ ∈ S N 的排列，成本最低：
其中 L match （y i ， ŷσ（i） ） 是真值 y i 与索引为 σ（i） 的预测之间的成对匹配成本。
根据先前的工作（例如
[43] ).
匹配成本既考虑了类别预测，也考虑了预测框和真值框的相似性。
真值集的每个元素 i 都可以看作是 y i = （c i ， b i ），其中 c i 是目标类标签（可能是 ∅），b i ∈ [0， 1] 4 是一个向量，用于定义真值框中心坐标及其相对于图像大小的高度和宽度。
对于索引为 σ（i） 的预测，我们将类 c i 的概率定义为 pσ（i） （c i），将预测框定义为 bσ（i） 。
通过这些符号，我们将 L 匹配 （y i ， ŷσ（i） ） 定义为
这种寻找匹配的过程与用于将提案 [37] 或锚点 [22] 与现代检测器中的真值对象匹配的启发式分配规则具有相同的作用。
主要区别在于，我们需要找到一对一的匹配，以实现没有重复的直接集预测。
第二步是计算损失函数，即上一步中匹配的所有货币对的匈牙利损失。
我们对损失的定义类似于普通目标检测器的损失，即用于类预测的负对数似然和稍后定义的箱体损失的线性组合：
其中 σ 是在第一步 （1） 中计算的最优赋值。
在实践中，我们将 c i = ∅ 的对数概率项的权重降低 10 倍，以解释类不平衡。
这类似于 Faster R-CNN 训练程序如何通过子采样来平衡正/负提案 [37] 。
请注意，对象和∅之间的匹配成本不依赖于预测，这意味着在这种情况下，成本是一个常数。
在匹配成本中，我们使用概率 pσ（i） （c i ） 而不是对数概率。
这使得类预测项与L框（•，•）（如下所述）相当，并且我们观察到更好的经验性能。
边界框损失。
匹配成本和匈牙利损失的第二部分是 L 框 （•），它对边界框进行评分。
与许多将箱体预测作为∆ w.r.t. 的检测器不同。
一些初步的猜测，我们直接进行盒子预测。
虽然这种做法简化了实施，但带来了损失相对规模化的问题。
最常用的 1 损失对于小箱子和大箱子来说，即使它们的相对误差相似，也会有不同的尺度。
为了缓解这个问题，我们使用 1 损失和广义 IoU 损失 [38] L iou （•， •） 的线性组合，该损失是尺度不变的。
总的来说，我们的箱损是 L 箱 （b i ， bσ（i） ） 定义为
这两个损失通过批处理内的对象数量进行归一化。
3.2 DETR 架构

整体 DETR 架构非常简单，如图 2 所示。它包含三个主要组件，我们将在下面介绍：一个用于提取紧凑特征表示的 CNN 主干网、一个编码器-解码器转换器和一个用于进行最终检测预测的简单前馈网络 （FFN）。

与许多现代探测器不同，DETR 可以在任何深度学习框架中实现，该框架只需几百行即可提供通用的 CNN 主干网和 transformer 架构实现。
在 PyTorch [32] 中，DETR 的推理代码可以在不到 50 行的时间内实现。
我们希望我们方法的简单性能够吸引新的研究人员加入检测社区。
骨干。
从初始图像 x img ∈ R 3×H0×W0（具有 3 个颜色通道 2）开始，传统的 CNN 骨干生成分辨率较低的激活图 f ∈ R C×H×W 。
我们使用的典型值是 C = 2048 和 H， W = H0 32 ， W0 32 。
变压器编码器。
首先，1x1 卷积将高级激活映射 f 的通道维度从 C 减少到更小的维度 d。 创建一个新的特征映射 z 0 ∈ R d×H×W 。
编码器期望一个序列作为输入，因此我们将 z 0 的空间维度折叠为一维，从而产生 d×HW 特征图。
每个编码器层都有一个标准架构，由一个多头自注意模块和一个前馈网络（FFN）组成。
由于 transformer 架构是置换不变的，因此我们用固定的位置编码 [31， 3] 来补充它，这些编码被添加到每个注意力层的输入中。
我们将架构的详细定义留给补充材料，该定义遵循 [47] 中描述的定义。
变压器解码器。
解码器遵循转换器的标准架构，使用多头自和编码器-解码器注意力机制转换大小为 d 的 N 个嵌入。
与原始转换器的区别在于，我们的模型在每个解码器层并行解码N个对象，而Vaswani等[47]使用自回归模型，一次预测一个元素的输出序列。
我们建议不熟悉这些概念的读者阅读补充材料。
由于解码器也是置换不变的，因此 N 个输入嵌入必须不同才能产生不同的结果。
这些输入嵌入是学习的位置编码，我们称之为对象查询，与编码器类似，我们将它们添加到每个注意力层的输入中。
N 个对象查询由解码器转换为输出嵌入。
然后，它们通过前馈网络（在下一小节中描述）独立解码为框坐标和类标签，从而产生 N 个最终预测。
通过对这些嵌入的自理解和编码器-解码器的关注，该模型使用它们之间的成对关系对所有对象进行全局推理，同时能够使用整个图像作为上下文。
预测前馈网络 （FFN）。
最终预测由具有 ReLU 激活函数和隐藏维度 d 的 3 层感知器以及线性投影层计算。
FFN 根据输入图像预测框的归一化中心坐标、高度和宽度，线性层使用 softmax 函数预测类标签。
由于我们预测的是一组固定大小的 N 个边界框，其中 N 通常比图像中感兴趣的对象的实际数量大得多，因此使用额外的特殊类标签∅来表示在插槽中未检测到任何对象。
此类在标准对象检测方法中扮演类似于“背景”类的角色。
辅助解码损耗。
我们发现在训练期间在解码器中使用辅助损失 [1] 很有帮助，特别是可以帮助模型输出每个类的正确数量的对象。
我们在每个解码器层之后添加预测 FFN 和匈牙利损失。
所有预测 FFN 共享其参数。
我们使用一个额外的共享层范数来归一化来自不同解码器层的预测 FFN 的输入。
4 实验

结果表明，在COCO的定量评估中，与Faster R-CNN相比，DETR取得了有竞争力的结果。
然后，我们对结构和损失进行了详细的消融研究，并提供了见解和定性结果。
最后，为了证明 DETR 是一个通用且可扩展的模型，我们提出了全景分割的结果，仅在固定 DETR 模型上训练了一个小的扩展。
我们提供代码和预训练模型，以在 https://github.com/facebookresearch/detr 上重现我们的实验。
数据。
我们在COCO 2017检测和全景分割数据集[24,18]上进行了实验，包含118k训练图像和5k验证图像。
每张图像都用边界框和全景分割进行注释。
平均每个图像有 7 个实例，在训练集中，单个图像中最多有 63 个实例，在同一图像上从小到大不等。
如果未指定，我们将 AP 报告为 bbox AP，这是多个阈值的积分指标。
为了与 Faster R-CNN 进行比较，我们报告了最后一个训练时期的验证 AP，对于消融，我们报告了过去 10 个时期的中位数验证结果。
技术细节。
我们使用 AdamW [26] 训练 DETR，将初始变压器的学习率设置为 10 -4，将主干的学习率设置为 10 -5，将权重衰减设置为 10 -4。
所有 transformer 权重都使用 Xavier init [11] 进行初始化，主干使用 torchvision 的 ImageNet 预训练 ResNet 模型 [15] 和冻结的 batchnorm 层。
我们使用两个不同的主干网报告结果：ResNet-50 和 ResNet-101。
对应的型号分别称为 DETR 和 DETR-R101。
在 [21] 之后，我们还通过在主干网的最后一级添加一个扩张并从该阶段的第一个卷积中删除一个步幅来提高特征分辨率。
相应的型号分别称为 DETR-DC5 和 DETR-DC5-R101（扩张 C5 阶段）。
这种修改将分辨率提高了两倍，从而提高了小物体的性能，但代价是编码器的自注意力成本高出 16 倍，导致计算成本总体增加 2 倍。
表1给出了这些模型的FLOPs和Faster R-CNN的完整比较。
我们使用比例增强，调整输入图像的大小，使最短的边至少为 480 像素，最多为 800 像素，而最长的边最多为 1333 [50] 。
为了通过编码器的自我关注来帮助学习全局关系，我们还在训练过程中应用随机裁剪增强，将性能提高约 1 AP。
具体来说，火车图像以 0.5 的概率裁剪为随机矩形块，然后再次调整大小为 800-1333。
转换器使用默认 dropout 为 0.1 进行训练。
在推理时，某些插槽会预测空类。
为了针对 AP 进行优化，我们使用相应的置信度覆盖了这些得分第二高的类别的插槽预测。
与过滤掉空插槽相比，这将 AP 提高了 2 个百分点。
其他训练超参数可以在 A.4 部分中找到。
对于我们的消融实验，我们使用 300 个 epoch 的训练计划，200 个 epoch 后学习率下降 10 倍，其中单个 epoch 是一次遍历所有训练图像。
在 16 个 V100 GPU 上训练 300 个 epoch 的基线模型需要 3 天时间，每个 GPU 有 4 个图像（因此总批大小为 64 个）。
对于用于与 Faster R-CNN 进行比较的更长的时间表，我们训练了 500 个 epoch，学习率在 400 个 epoch 后下降。
与较短的计划相比，此计划增加了 1.5 AP。
4.1 与Faster R-CNN的比较

Transformer 通常使用 Adam 或 Adagrad 优化器进行训练，这些优化器具有很长的训练计划和辍学，DETR 也是如此。
然而，更快的 R-CNN 是用 SGD 训练的，数据增强最少，我们不知道 Adam 或 dropout 的成功应用。
尽管存在这些差异，我们仍试图使更快的 R-CNN 基线更强大。
为了使其与 DETR 保持一致，我们将广义 IoU [38] 添加到盒子损失中，相同的随机裁剪增强和已知的长时间训练可以改善结果 [13] 。
结果如表1所示。
在顶部部分，我们展示了来自 Detectron2 Model Zoo [50] 的更快 R-CNN 结果，用于使用 3x 计划训练的模型。
在中间部分，我们显示了相同模型的结果（带有“+”），但使用 9 倍计划（109 个时期）和所描述的增强功能进行训练，总共增加了 1-2 个 AP。
在编码器层数的最后一部分。
我们通过改变编码器层数来评估全局图像级自注意力的重要性（表2）。
如果没有编码器层，总体 AP 会下降 3.9 分，在大型物体上会下降 6.0 AP。
我们假设，通过使用全局场景推理，编码器对于解开对象很重要。
在图 3 中，我们可视化了经过训练的模型的最后一个编码器层的注意力图，重点关注图像中的几个点。
编码器似乎已经分离了实例，这可能会简化解码器的对象提取和定位。
解码器层数。
我们在每个解码层之后应用辅助损耗（参见第 3.2 节），因此，预测 FFN 通过设计进行训练，以从每个解码器层的输出中预测对象。
我们通过评估在解码的每个阶段预测的对象来分析每个解码器层的重要性（图4）。
AP 和 AP 50 在每一层之后都会有所改善，在第一层和最后一层之间总共提高了 +8.2/9.5 AP。
凭借其基于设置的损失，DETR 在设计上不需要 NMS。
为了验证这一点，我们运行一个标准NMS程序，每个解码器之后的输出都使用默认参数[50]。
NMS 提高了来自第一个解码器的预测性能。
这可以通过以下事实来解释：转换器的单个解码层无法计算输出元素之间的任何互相关，因此它容易对同一对象进行多次预测。
在第二层和后续层中，激活的自注意力机制允许模型抑制重复预测。
我们观察到，随着深度的增加，NMS带来的改进逐渐减弱。
在最后一层，我们观察到 AP 中有少量损失，因为 NMS 错误地删除了真正的阳性预测。
与可视化编码器注意力类似，我们在图 6 中可视化解码器注意力，以不同的颜色为每个预测对象的注意力图着色。
我们观察到，解码器的注意力是相当局部的，这意味着它主要关注物体的肢体，如头部或腿部。
我们假设，在编码器通过全局注意力分离实例后，解码器只需要关注极端以提取类和对象边界。
FFN 的重要性。
Transformer 内部的 FFN 可以看作是 1 × 1 个卷积层，使编码器类似于注意力增强卷积网络 [3] 。
我们试图完全消除它，只把注意力放在变压器层上。
通过将网络参数的数量从41.3M减少到28.7M，变压器中仅留10.8M，性能下降2.3 AP，从而得出结论，FFN对于获得良好的结果非常重要。
位置编码的重要性。
我们的模型中有两种位置编码：空间位置编码和输出位置编码（对象查询）。
我们试验了固定编码和学习编码的各种组合，结果可以在表3中找到。输出位置编码是必需的，不能删除，因此我们尝试在解码器输入时传递一次它们，或者在每个解码器注意力层添加到查询中。

在第一个实验中，我们完全删除了空间位置编码，并在输入处传递了输出位置编码，有趣的是，该模型仍然实现了超过 32 个 AP，比基线损失了 7.8 个 AP。
然后，我们像在原始转换器[47]中一样，在输入处传递一次固定正弦空间位置编码和输出编码，并发现与直接在注意力中传递位置编码相比，这导致了1.4 AP的下降。
传递给注意力的学习空间编码会产生类似的结果。
令人惊讶的是，我们发现在编码器中不传递任何空间编码只会导致 AP 下降 1.3 AP。
当我们将编码传递给注意时，它们将在所有层之间共享，并且输出编码（对象查询）始终被学习。
鉴于这些消融，我们得出结论，变压器组件：编码器中的全局自注意力、FFN、多个解码器层和位置编码，都对最终目标检测性能有显着贡献。
损失消融。
为了评估匹配成本和损失的不同组成部分的重要性，我们训练了几个模型，这些模型可以打开和关闭它们。
损失由三个部分组成：分类损失、1 边界框距离损失和 GIoU [38] 损失。
分类损失对于训练是必不可少的，并且不能关闭，因此我们训练一个没有边界框距离损失的模型和一个没有 GIoU 损失的模型，并与基线进行比较，训练所有三个损失。
结果见表4。GIoU 自身账户损失 表 3：与基线（最后一行）相比，不同位置编码的结果，基线具有固定的正弦位置。

在编码器和解码器的每个注意层传递的编码。
学习的嵌入在所有层之间共享。
不使用空间位置编码会导致 AP 显著下降。
有趣的是，在解码器中传递它们只会导致轻微的 AP 丢失。
所有这些模型都使用学习的输出位置编码。
对于大多数模型性能，仅损失了 0.7 AP 到基线，并增加了综合损失。
使用不带 GIoU 的 1 显示结果不佳。
我们只研究了这些点是用颜色编码的，因此绿色对应于小盒子，红色对应于大水平盒子，蓝色对应于大垂直盒子。
我们观察到，每个插槽都学会了通过多种操作模式专注于某些区域和盒子大小。
我们注意到，几乎所有的插槽都具有预测大图像宽度框的模式，这在COCO数据集中很常见。
对不同的损失进行简单的消融（每次使用相同的权重），但将它们组合的其他方法可能会获得不同的结果。
4.3 分析

解码器输出时隙分析 在图 7 中，我们可视化了 COCO 2017 val set 中所有图像的不同时隙预测的框。
DETR 为每个查询槽学习不同的专业化。
我们观察到每个插槽都有几种操作模式，专注于不同的区域和盒子大小。
具体而言，所有插槽都具有预测图像范围框的模式（可见为在图中间对齐的红点）。
我们假设这与COCO中物体的分布有关。
泛化到看不见的实例数量。
COCO 中的某些类不能很好地表示在同一图像中同一类的许多实例。
例如，训练集中没有超过 13 只长颈鹿的图像。
我们创建了一个合成图像3来验证DETR的泛化能力（见图5）。
我们的模型能够在图像上找到所有 24 只长颈鹿，这显然是分布不全的。
此实验证实，每个对象查询中都没有很强的类专用化。
4.4 用于全景分割的 DETR

全景分割[19]最近引起了计算机视觉界的广泛关注。
与将Faster R-CNN [37]扩展到Mask R-CNN [14]类似，DETR可以通过在解码器输出顶部添加一个掩模头来自然地扩展。
在本节中，我们演示了这样的头部可用于通过处理事物和事物类来产生全景分割[19]
图8：全景云台的图示。
为每个检测到的对象并行生成一个二进制掩码，然后使用像素级 argmax 合并掩码。以统一的方式。

我们对 COCO 数据集的全景注释进行了实验，该数据集除了有 80 个事物类别外，还有 53 个事物类别。
我们训练 DETR 使用相同的配方来预测 COCO 上 stuff 和 things 类周围的框。
要使训练成为可能，就必须预测框，因为匈牙利匹配是使用框之间的距离计算的。
我们还添加了一个掩码头，用于预测每个预测框的二进制掩码，参见图 8。
它采用转换器解码器的输出作为每个对象的输入，并在编码器的输出上计算此嵌入的多头（有 M 头）注意力分数，以小分辨率为每个对象生成 M 个注意力热图。
为了做出最终预测并提高分辨率，使用了类似 FPN 的架构。
我们将在补充中更详细地描述体系结构。
掩模的最终分辨率为步长 4，每个掩模都使用 DICE/F-1 损失 [28] 和焦点损失 [23] 独立监控。
掩模头既可以联合训练，也可以分两步过程进行训练，我们只训练箱子的 DETR，然后冻结所有权重并仅训练掩模头 25 个时期。
从实验上讲，这两种方法给出了相似的结果，我们使用后一种方法报告结果，因为它导致总时钟时间训练更短。
为了预测最终的全景分割，我们只需在每个像素的掩模分数上使用 argmax，并为生成的掩模分配相应的类别。
此过程保证了最终的掩模没有重叠，因此，DETR 不需要通常用于对齐不同掩模的启发式 [19]。
培训详细信息。
我们按照边界框检测的配方训练 DETR、DETR-DC5 和 DETR-R101 模型，以预测 COCO 数据集中事物和事物类别周围的框。
新的掩模头经过 25 个时期的训练（有关详细信息，请参阅补充）。
在推理过程中，我们首先过滤掉置信度低于 85% 的检测结果，然后计算每个像素的 argmax 以确定每个像素属于哪个掩码。
然后，我们将同一事物类别的不同掩码预测合并为一个，并过滤空的掩码预测（小于 4 个像素）。
主要结果。
定性结果如图 9 所示。
在表 5 中，我们将我们的统一全景隔离方法与几种不同的处理事物和事物的既定方法进行了比较。
我们报告全景质量 （PQ） 以及事物 （PQ th） 和事物 （PQ st） 的故障。
我们还报告了掩码 AP（在事物类上计算），然后再进行任何全景后处理（在我们的例子中，在采用像素级 argmax 之前）。
我们发现，DETR 的表现优于 COCO-val 2017 上公布的结果，以及我们强大的 PanopticFPN 基线（使用与 DETR 相同的数据增强进行训练，以便进行公平比较）。
结果分解表明，DETR 在 stuff 类上尤其占主导地位，我们假设编码器注意力允许的全局推理是该结果的关键因素。
对于事物类，尽管与掩模 AP 计算的基线相比，存在高达 8 mAP 的严重赤字，但 DETR 仍获得了具有竞争力的 PQ th。
我们还在COCO数据集的测试集上评估了我们的方法，得到了46个PQ。
我们希望我们的方法能够激发在未来的工作中探索完全统一的全景分割模型。
5 结论

我们介绍了 DETR，这是一种基于变压器和二分匹配损耗的目标检测系统的新设计，用于直接集合预测。
该方法在具有挑战性的 COCO 数据集上取得了与优化的 Faster R-CNN 基线相当的结果。
DETR 易于实施，并具有灵活的架构，易于扩展到全景分割，并具有竞争力的结果。
此外，与Faster R-CNN相比，它在大型物体上的性能明显更好，这可能要归功于自注意力对全局信息的处理。
这种新的探测器设计也带来了新的挑战，特别是在小物体的训练、优化和性能方面。
目前的探测器需要几年的改进才能应对类似的问题，我们预计未来的工作能够成功地解决 DETR 的问题。
A 附录

A.1 初步：多头注意力层

由于我们的模型基于 Transformer 架构，因此我们在这里提醒我们用于穷举性的注意力机制的一般形式。
注意机制遵循 [47] ，除了位置编码的细节（参见公式 8）[7] 之后。
多头 多头注意的一般形式，其中 M 个维度为 d 的头部是具有以下特征的函数（使用 d = d M，并在下括号中给出矩阵/张量大小） mh-attn ： X q d×Nq ， X kv
其中 X q 是长度为 N q 的查询序列，X kv 是长度为 N kv 的键值序列（为了便于说明，具有相同的通道数 d），T 是计算所谓的查询、键和值嵌入的权重张量，L 是投影矩阵。
输出的大小与查询序列的大小相同。
为了在给出详细信息之前修复词汇，多头自注意力 （mh-s-attn） 是特例 X q = X kv ，即
mh-s-attn（X， T， L） = mh-attn（X， X， T， L） .
多头注意力只是 M 个单头注意力的串联，然后是带有 L 的投影。通常的做法[47]是使用残差连接、丢弃和层归一化。

换句话说，表示 Xq = mh-attn（X q ， X kv ， T， L） 和 X（q） 注意头的串联，我们有 X q = [attn（X q ， X kv ， T 1 ）; ...; attn（X q ， X kv ， T M ）]
（5） Xq = 层范数 X q + dropout（LX q ） ，
其中 [;] 表示通道轴上的串联。
单头 权重张量 T ∈ R 3×d ×d 的注意头，用 attn（X q ， X kv ， T ） 表示，依赖于附加的位置编码 P q ∈ R d×Nq 和 P kv ∈ R d×N kv 。
它首先计算所谓的查询、键和值嵌入，然后添加查询和键位置编码 [7]：
其中 T 是 T 1 ， T 2 ， T 3 的串联。
然后，根据查询和键之间的点积的softmax计算注意力权重α，以便查询序列的每个元素都参与键值序列的所有元素（i 是查询索引，j 是键值索引）：
在我们的例子中，位置编码可能是学习的或固定的，但在给定查询/键值序列的所有注意力层之间是共享的，因此我们不会明确地将它们写成注意力的参数。
在描述编码器和解码器时，我们将提供有关它们的确切值的更多详细信息。
最终输出是由注意力权重加权的值的聚合：第 i 行由 attn i （X q ， X kv ， T ） = N kv j=1 给出 α i，j V j 。
前馈网络 （FFN） 层 原始转换器交替使用多头注意和所谓的 FFN 层 [47]，它们实际上是多层 1x1 卷积，在我们的例子中具有 M d 输入和输出通道。
我们考虑的 FFN 由两层 1x1 卷积和 ReLU 激活组成。
两层之后还有一个残差连接/丢/层范，类似于等式 6。
A.2 损失

为了完整起见，我们详细介绍了我们方法中使用的损失。
所有损失都通过批处理内的对象数量进行归一化。
对于分布式训练必须格外小心：由于每个 GPU 都接收一个子批次，因此通过本地批处理中的对象数量进行归一化是不够的，因为通常子批次在 GPU 之间不平衡。
相反，重要的是通过所有子批次中的对象总数进行归一化。
框损失 与 [41， 36] 类似，我们在损失中使用了 Intersection over Union 的软版本，并在 b 上使用 1 损失：
其中 λ iou ， λ L1 ∈ R 是超参数，L iou （•） 是广义 IoU [38] ：
|.|表示“区域”，框坐标的并集和交点用作框本身的简写。

并集或交集的面积由 b σ（i） 和 bi 的线性函数的最小值/最大值计算，这使得损失对于随机梯度来说表现得足够好。
B（b σ（i） ， bi ） 表示包含 b 的最大框 σ（i） ， bi （涉及 B 的区域也是根据框坐标的线性函数的最小值/最大值计算的）。
DICE/F-1损失 [28] DICE系数与交集与并集密切相关。
如果我们用 m 表示模型的原始掩码 logits 预测，m 表示二元目标掩码，则损失定义为：
其中 σ 是 sigmoid 函数。
这种损失通过对象的数量进行归一化。
A.3 详细架构

图 10 给出了 DETR 中使用的转换器的详细说明，并在每个注意力层传递位置编码。
来自 CNN 骨干网的图像特征通过 transformer 编码器传递，以及空间位置编码，这些空间位置编码被添加到每个多头自注意力层的查询和键中。
然后，解码器接收查询（最初设置为零）、输出位置编码（对象查询）和编码器内存，并通过多个多头自注意和解码器-编码器注意生成最终的预测类标签和边界框集合。
可以跳过第一解码器层中的第一个自注意力层。
FLOPS 计算 鉴于 Faster R-CNN 的 FLOPS 取决于图像中的提案数量，我们报告了 COCO 2017 验证集中前 100 张图像的平均 FLOPS 数量。
我们使用 Detectron2 [50] 中的工具翻转计数运算符计算 FLOPS。
我们在 Detectron2 模型中不加修改地使用它，并将其扩展为在 DETR 模型中考虑批量矩阵乘法 （bmm）。
A.4 训练超参数

我们使用 AdamW [26] 训练 DETR，改进了权重衰减处理，设置为 10 -4。
我们还应用梯度剪裁，最大梯度范数为 0.1。
主干网和变压器的处理方式略有不同，我们现在讨论两者的细节。
Backbone ImageNet 预训练骨干 ResNet-50 是从 Torchvision 导入的，丢弃了最后一个分类层。
在训练过程中，主干批量归一化权重和统计数据被冻结，遵循对象检测中广泛采用的做法。
我们使用 10 -5 的学习率对主干网进行微调。
我们观察到，让骨干网的学习率比网络的其他部分小大约一个数量级对于稳定训练很重要，尤其是在最初的几个时期。
Transformer 我们用 10 -4 的学习率训练 Transformer 。
在每次多头注意力和层归一化之前的 FFN 之后应用 0.1 的加性丢弃。
权重是使用 Xavier 初始化随机初始化的。
损失：我们使用 1 和 GIoU 损失的线性组合进行边界框回归，分别使用 λ L1 = 5 和 λ iou = 2 权重。
所有模型均使用 N = 100 个解码器查询槽进行训练。
基线：我们增强的 Faster-RCNN+ 基线使用 GIoU [38] 损失以及标准 1 损失进行边界框回归。
我们进行了网格搜索以找到损失的最佳权重，最终模型仅使用 GIoU 损失，权重为 20 和 1，分别用于箱形和提案回归任务。
对于基线，我们采用与 DETR 中使用的相同数据增强，并使用 9× 计划（大约 109 个周期）进行训练。
所有其他设置都与Detectron2模型库中的相同模型相同[50]。
空间位置编码 编码器激活与图像特征的相应空间位置相关联。
在我们的模型中，我们使用固定的绝对编码来表示这些空间位置。
我们采用了原始Transformer[47]编码到2D情况[31]的推广。
具体来说，对于每个嵌入的两个空间坐标，我们独立使用具有不同频率的 d 2 正弦和余弦函数。
然后，我们将它们连接起来，得到最终的d通道位置编码。
A.5 其他结果

图 11 显示了 DETR-R101 模型全景预测的一些额外定性结果。
增加实例数 根据设计，DETR 预测的对象数不能超过其查询槽的数量，即在我们的实验中为 100 个。
在本节中，我们将分析 DETR 在接近此限制时的行为。
我们选择给定类的规范方形图像，在 10 × 10 网格上重复该图像，并计算模型遗漏的实例百分比。
为了测试少于 100 个实例的模型，我们随机屏蔽了一些单元格。
这确保了无论有多少可见对象，对象的绝对大小都是相同的。
为了解释掩码的随机性，我们用不同的掩码重复实验100次。
结果如图 12 所示。
这种行为在各个类之间是相似的，虽然模型在最多 50 个可见时检测到所有实例，但它随后开始饱和并错过越来越多的实例。
值得注意的是，当图像包含所有 100 个实例时，模型平均只检测到 30 个实例，这比图像仅包含所有检测到的 50 个实例时要少。
模型的反直觉行为可能是因为图像和检测与训练分布相去甚远。
请注意，此测试是设计上对分布外泛化的测试，因为具有大量单个类实例的示例图像很少。
从实验中很难解开两种类型的域外泛化：图像本身与每个类的对象数量。
但是，由于很少有或没有 COCO 图像只包含同一类的大量对象，因此这种类型的实验代表了我们尽最大努力了解查询对象是否过度拟合数据集的标签和位置分布。
总体而言，实验表明，该模型不会过度拟合这些分布，因为它可以产生多达 50 个物体的近乎完美的检测。
A.6 PyTorch 推理代码

为了演示该方法的简单性，我们在清单 1 中包含了带有 PyTorch 和 Torchvision 库的推理代码。
该代码使用 Python 3.6+、PyTorch 1.4 和 Torchvision 0.5 运行。
请注意，它不支持批处理，因此它仅适用于使用 DistributedDataParallel 进行推理或训练，每个 GPU 一个图像。
另请注意，为清楚起见，此代码在编码器中使用学习的位置编码，而不是固定的，并且位置编码仅添加到输入中，而不是在每个转换器层上。
进行这些更改需要超越转换器的 PyTorch 实现，这会妨碍可读性。
用于复制实验的整个代码将在会议召开前提供。
清单 1：DETR PyTorch 推理代码。
为了清楚起见，它在编码器中使用学习过的位置编码，而不是固定的，并且位置编码仅添加到输入中，而不是在每个转换器层上。
进行这些更改需要超越转换器的 PyTorch 实现，这会妨碍可读性。
用于复制实验的整个代码将在会议召开前提供。
图 1：DETR 通过将通用 CNN 与转换器架构相结合，直接（并行）预测最终的检测集。
在训练期间，二分匹配唯一地将预测分配给地面实况框。
没有匹配项的预测应生成“无对象”（∅） 类预测。
图 2：DETR 使用传统的 CNN 骨干网来学习输入图像的 2D 表示。
该模型将其展平，并使用位置编码对其进行补充，然后再将其传递到转换器编码器中。
然后，转换器解码器将少量固定数量的学习位置嵌入作为输入，我们称之为对象查询，并额外参与编码器输出。
我们将解码器的每个输出嵌入传递给共享前馈网络 （FFN），该网络预测检测（类和边界框）或“无对象”类。
图 3：一组参考点的编码器自注意力。
编码器能够分离单个实例。
在验证集图像上使用基线 DETR 模型进行预测。
图 4：AP 和 AP50 在每层解码器之后的性能。
评估单个长计划基线模型。
DETR 在设计上不需要 NMS，此图验证了这一点。
NMS 降低了最后一层的 AP，去除了 TP 预测，但提高了第一解码器层的 AP，消除了双重预测，因为第一层没有通信，并略微提高了 AP50。
图5：稀有类的分布外泛化。
尽管训练集中没有图像超过 13 只长颈鹿，但 DETR 可以毫不费力地推广到同一类的 24 个或更多实例。
图 6：可视化每个预测对象的解码器注意力（来自 COCO val 集的图像）。
使用 DETR-DC5 模型进行预测。
对于不同的对象，注意力分数用不同的颜色进行编码。
解码器通常处理物体的肢体，例如腿和头部。
最好用彩色观看。
图 7：在 DETR 解码器中，COCO 2017 val 集的所有图像上的所有框预测的可视化，其中 N = 100 个预测槽中有 20 个。
每个框预测都表示为一个点，其中心的坐标在 1×1 正方形中由每个图像大小归一化。
这些点采用颜色编码，因此绿色对应于小框，红色对应于大水平框，蓝色对应于大垂直框。
我们观察到，每个插槽都学会了通过多种操作模式专注于某些区域和盒子大小。
我们注意到，几乎所有的插槽都具有预测大图像宽度框的模式，这在COCO数据集中很常见。
图 9：DETR-R101 生成的全景分割的定性结果。
DETR 以统一的方式为事物和事物生成对齐的掩码预测。
图11：全景预测的比较。
从左到右：地面实况、带 ResNet 101 的 Panop-ticFPN、带 ResNet 101 的 DETR
图 12：根据 DETR 中存在的数量，分析 DETR 遗漏的各种类别的实例数量。
我们报告平均值和标准差。
随着实例数接近 100，DETR 开始饱和并遗漏越来越多的对象
DETR（nn.模块）：

我们只从 ResNet-50 模型 11 中获取卷积层 self.backbone
= nn。Sequential（*list（resnet50（pretrained=True）.children（））[：-2]） 12 self.conv

DETR 模型获得的结果与经过大量调整的 Faster R-CNN 基线相当，具有较低的 APS，但大大提高了 APL。
我们使用 torchscript、Faster、R-CNN 和 DETR 模型来测量 FLOPS 和 FPS。
名称中没有 R101 的结果对应于 ResNet-50。
编码器尺寸的影响。
每行对应一个模型，该模型具有不同数量的编码器层数和固定数量的解码器层数。
随着编码器层数的增加，性能会逐渐提高。
表 1 显示了多个 DETR 模型的结果。为了在参数数量上具有可比性，我们选择了一个具有 6 个转换器和 6 个解码器层的模型，宽度为 256，带有 8 个注意力头。与带有 FPN 的 Faster R-CNN 一样，该模型有 41.3M 参数，其中 23.5M 在 ResNet-50 中，17.8M 在变压器中。尽管 Faster R-CNN 和 DETR 仍有可能随着更长的训练而进一步改进，但我们可以得出结论，在相同数量的参数下，DETR 可以与 Faster R-CNN 竞争，在 COCO val 子集上实现 42 AP。DETR 实现这一目标的方式是通过改进 AP L （+7.8），但请注意，该模型在 AP S （-5.5） 中仍然落后。具有相同参数数量和相似 FLOP 计数的 DETR-DC5 具有更高的 AP，但在 AP S 中也仍然明显落后。更快的 R-CNN 和带有 ResNet-101 骨干网的 DETR 也显示出类似的结果。Transformer 解码器中的注意力机制是建模不同检测的特征表示之间关系的关键组件。

在我们的消融分析中，我们探讨了架构的其他组件和损耗如何影响最终性能。
在这项研究中，我们选择了基于 ResNet-50 的 DETR 模型，该模型具有 6 个编码器、6 个解码器层和 256 个宽度。
该模型参数为 41.3M，在短时间和长时间上分别达到 40.6 和 42.0 AP，运行速度为 28 FPS，类似于具有相同骨干的 Faster R-CNN-FBN。
损耗分量对AP的影响。
我们训练了两个模型，关闭了 1 个损失和 GIoU 损失，并观察到 1 本身给出的结果很差，但当与 GIoU 结合使用时，可以改善 APM 和 APL。
我们的基线（最后一行）结合了这两种损失。
[18]在COCO val数据集上使用最先进的方法UPSNet[51]和Panoptic FPN[18]，我们使用与DETR相同的数据增强重新训练了PanopticFPN，以进行公平的比较。UPSNet 使用 1x 时间表，UPSNet-M 是具有多尺度测试时间增强的版本。型号Backbone PQ SQ RQ PQ th SQ th RQ th PQ st SQ st RQ st AP

