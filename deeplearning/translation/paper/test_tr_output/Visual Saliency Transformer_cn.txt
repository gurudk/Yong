视觉显著性转换器

现有的最先进的显著性检测方法严重依赖于基于 CNN 的架构。
或者，我们从无卷积序列到序列的角度重新思考这项任务，并通过建模长程依赖性来预测显著性，这是卷积无法实现的。
具体来说，我们开发了一种基于纯Transformer的新型统一模型，即Visual Saliency Transformer（VST），用于RGB和RGB-D显著目标检测（SOD）。
它将图像补丁作为输入，并利用转换器在图像补丁之间传播全局上下文。
与Vision Transformer（ViT）中采用的传统架构不同，该算法利用多级令牌融合，在Transformer框架下提出了一种新的令牌上采样方法，以获得高分辨率的检测结果。
我们还开发了一种基于令牌的多任务解码器，通过引入任务相关的令牌和新颖的补丁-任务-注意力机制，同时执行显著性和边界检测。
实验结果表明，该模型在RGB和RGB-D SOD基准数据集上均优于现有方法。
最重要的是，我们的整个框架不仅为SOD领域提供了新的视角，而且为基于Transformer的密集预测模型展示了一种新的范式。
代码可在 https://github.com/nnizhang/VST 处获得。
1. 引言

SOD旨在检测吸引人们眼球的物体，并可以帮助许多视觉任务，例如[58,19]。
近年来，RGB-D SOD对深度数据中的额外空间结构信息也越来越感兴趣。
目前最先进的SOD方法以卷积架构为主[28]，在RGB和RGB-D数据上都是如此。
他们通常采用编码器-解码器CNN架构[47,57]，其中编码器将输入图像编码为多级特征，解码器将提取的特征集成以预测最终的显著性图。
基于这种简单的架构，大多数努力都是为了构建一个强大的解码器，以预测更好的显著性结果。
为此，他们引入了各种注意力模型[37,80,7]，多尺度特征集成方法[24,49,16,43]和多任务学习框架[67,77,82,69,25]。
对RGB-D SOD的另一个要求是有效地融合跨模态信息，即外观信息和深度线索。
现有研究提出了多种模态融合方法，如特征融合[22， 4， 16， 18， 89]、知识蒸馏[53]、动态卷积[48]、注意力模型[31， 78]和图神经网络[43]。
因此，基于CNN的方法取得了令人印象深刻的结果[66,88]。
然而，所有以前的方法在学习全局长程依赖性方面都受到限制。
长期以来，全球背景[21,83,56,44,37]和全球对比度[75,2,8]已被证明对显著性检测至关重要。
然而，由于CNN在局部滑动窗口中提取特征的固有局限性，以前的方法很难利用关键的全局线索。
尽管一些方法利用全连接层 [36， 22] 、全局池化层 [44， 37， 65] 和非本地模块 [38， 7] 来整合全局上下文，但它们仅在某些层中这样做，并且基于 CNN 的标准架构保持不变。
最近，Transformer [61] 被提出用于模拟机器翻译词序列之间的全局长程依赖关系。
其核心思想是自注意力机制，它利用查询键相关性来关联序列中的不同位置。
Transformer 在编码器和解码器中多次堆叠自注意力层，因此可以对每一层的长程依赖关系进行建模。
因此，将 Transformer 引入 SOD 是很自然的，并始终利用模型中的全局线索。
在本文中，我们首次从新的序列到序列的角度重新思考SOD，并基于纯Transformer为RGB和RGB-D SOD开发了一种新型的统一模型，该模型被命名为Visual Saliency Transformer。
我们遵循最近提出的ViT模型[12,74]，将每个图像划分为多个斑块，并在斑块序列上采用Transformer模型。
然后，Transformer 在图像补丁之间传播长程依赖关系，而无需使用卷积。
但是，arXiv：2104.12099v2
[cs.CV] 2021 年 8 月 23 日 将 ViT 应用于 SOD 并不简单。

一方面，如何基于纯Transformer执行密集预测任务仍然是一个悬而未决的问题。
另一方面，ViT 通常会将图像标记到非常粗糙的比例。
如何使ViT适应SOD的高分辨率预测需求也尚不清楚。
为了解决第一个问题，我们通过引入任务相关的令牌来学习决策嵌入，从而设计了一个基于令牌的转换器解码器。
然后，提出了一种新的补丁-任务-注意力机制来生成密集预测结果，为在密集预测任务中使用Transformer提供了一种新的范式。
受先前利用边界检测提高SOD性能的SOD模型[82,87,79,25]的启发，我们通过引入显著性令牌和边界令牌，构建了多任务解码器，可同时进行显著性和边界检测。
该策略通过简单地学习与任务相关的标记来简化多任务预测工作流程，从而大大降低了计算成本，同时获得了更好的结果。
为了解决第二个问题，受Tokens-to-Token（T2T）变换[74]的启发，我们提出了一种新的反向T2T变换，通过将每个Token扩展为多个子Token来对Token进行上采样。
然后，我们逐步对补丁标记进行上采样，并将它们与低级标记融合，以获得最终的全分辨率显著性图。
此外，我们还利用跨模态Transformer深入研究了RGB-D SOD的多模态信息之间的交互作用。
最后，在RGB和RGB-D数据上，我们的VST在参数数量和计算成本上都优于现有的最先进的SOD方法。
我们的主要贡献可以归纳为以下几点：
• 我们首次从序列到序列建模的新角度，为RGB和RGB-D SOD设计了一种基于纯Transformer架构的新型统一模型。
• 我们设计了一种多任务 transformer 解码器，通过引入任务相关的令牌和 patch-task-attention 来共同进行显著性和边界检测。• 提出了一种新的基于Transformer框架的代币上采样方法。

• 我们提出的 VST 模型在 RGB 和 RGB-D SOD 基准数据集上都取得了最先进的结果，这证明了其有效性和基于 transformer 的 SOD 模型的潜力。
二、相关工作

2.1. 基于深度学习的SOD

基于CNN的方法已成为RGB和RGB-D SOD的主流趋势，并取得了可喜的性能。
大多数方法[24,65,49,84,16]通过使用UNet来利用多级特征融合策略
[57]或HED式[71]网络结构。
一些作品引入了注意力机制来学习更多的判别特征，包括空间和通道注意力[52,80,16,7]或像素级上下文注意[37]。
其他工作[36,64,11,42,6]试图设计循环网络以逐步完善显著性图。
此外，一些工作引入了多任务学习，例如注视预测[67]、图像标题[77]和边缘检测[54,82,69,79,25]，以提高SOD性能。
对于RGB-D SOD，许多方法都设计了各种模型来融合RGB和深度特征，并取得了显著的效果。
一些模型[4,5,18]采用了简单的特征融合方法，即串联、求和或乘法。
其他一些[81,30,52,31]利用深度线索来产生空间或通道注意力，以增强RGB特征。
此外，还采用动态卷积[48]、图神经网络[43]和知识蒸馏[53]来实现多模态特征融合。
此外，[38， 39， 7]采用了交叉注意力机制来传播RGB和深度线索之间的长距离跨模态交互。
与之前基于CNN的方法不同，我们是第一个从序列到序列的角度重新思考SOD的方法，并提出了基于纯Transformer的RGB和RGB-D SOD的统一模型。
在我们的模型中，我们遵循 [54， 82， 69， 79， 25] 来利用边界检测来提高 SOD 性能。
然而，与这些基于CNN的模型不同，我们设计了一种新型的基于令牌的多任务解码器，在Transformer框架下实现了这个目标。
2.2. 计算机视觉中的变压器

Vaswani等[61]首先提出了一种用于机器翻译的Transformer EncoderDecoder架构，其中多头自注意力和逐点前馈层被多次堆叠。
近年来，越来越多的工作将Transformer模型引入各种计算机视觉任务，并取得了优异的效果。
一些工作将CNN和Transformer组合成混合架构，用于目标检测[3,91]、全景分割[62]、车道形状预测[40]等。
通常，他们首先使用 CNN 来提取图像特征，然后利用 Transformer 来合并长程依赖关系。
其他作品设计了纯Transformer模型，从序列到序列的角度处理图像。
ViT [12] 将每幅图像划分为一系列扁平化的 2D 斑块，然后采用 Transformer 进行图像分类。
Touvron 等人。
[60]引入了一种师生策略来提高ViT的数据效率，Wang等[68]提出了一种金字塔架构来适应ViT的密集预测任务。
T2T-ViT [74]采用T2T模块对局部结构进行建模，从而生成多尺度代币特征。
本文以T2T-ViT为骨干，提出了一种新型的多任务译码器和一种反向译码器，然后采用转换器将补丁标记转换为解码器空间，并对RGB-D数据进行跨模态信息融合。
最后，解码器通过提出的任务相关标记和补丁-任务-注意力机制同时预测显著性图和边界图。
还提出了 RT2T 变换，以逐步对补丁令牌进行上采样。
虚线代表 RGB-D SOD 的专用组件。
T2T 代币上采样方法。
值得注意的是，我们对任务相关令牌的使用与以前的模型不同。
在[12， 60]中，通过在令牌嵌入上采用多层感知器，类令牌直接用于图像分类。
但是，我们无法直接从单个任务令牌中获取密集的预测结果。
因此，我们建议在补丁标记和任务令牌之间执行补丁任务注意，以预测显著性和边界图。
我们相信，我们的策略也将激发未来变压器模型用于其他密集预测任务的灵感。
与我们的另一个相关工作是[86]，它将transformer引入语义分割任务。
作者采用视觉转换器作为骨干，然后将标记序列重塑为2D图像特征。
然后，他们使用卷积和双线性上采样预测了全分辨率分割图。
他们的模型仍然属于混合架构类别。
相比之下，我们的模型是一个纯粹的Transformer架构，不依赖于任何卷积运算和双线性上采样。
3. 视觉显著性转换器

图 1 显示了我们提出的 VST 模型的整体架构。
主要组件包括基于T2T-ViT的Transformer编码器、用于将补丁标记从编码器空间转换为解码器空间的Transformer转换器以及多任务Transformer解码器。
3.1. 变压器编码器

与其他基于CNN的SOD方法类似，这些方法通常使用预训练的图像分类模型（如VGG [59]和ResNet [23]）作为其编码器的骨干来提取图像特征，我们采用预训练的T2T-ViT [74]模型作为骨干，详述如下。
3.1.1 令牌到令牌

给定上一层长度为l的补丁标记T序列，T2T-ViT迭代应用由重构化步骤和软分裂步骤组成的T2T模块对T中的局部结构信息进行建模，得到新的标记序列。
重组。
如图 2 （a） 所示，首先使用转换器层对令牌 T 进行转换，以获得新的令牌 T ∈ R l×c：
其中MSA和MLP分别表示原始Transformer中的多头自注意力和多层感知器[61]。
请注意，层归一化 [1] 在每个块之前应用。
然后，将 T 重塑为 I ∈ R h×w×c 的二维图像，其中 l = h × w，以恢复空间结构，如图 2 （a） 所示。
软分裂。
在重组步骤之后，我首先被分割成 k ×k 个 s 重叠的补丁。
P 零填充也用于填充图像边界。
然后，将图像块展开为一系列标记
，其中序列长度 L o 的计算公式为：
（2）与ViT[12]不同，T2T-ViT采用的重叠斑块分裂在相邻斑块内部引入了局部对应关系，从而带来了空间先验。
T2T变换可以迭代多次进行。
在每次中，重组步骤首先将以前的令牌嵌入转换为新的嵌入，并在所有令牌中集成远程依赖项。
然后，软拆分操作将每个 k × k 邻居中的令牌聚合到一个新的令牌中，该令牌已准备好用于下一层。
此外，当将 s 设置为 k -1 <时，令牌的长度可以逐渐减少。
我们按照[74]首先将输入图像软分割成补丁，然后两次采用T2T模块。
在三个软分割步骤中，面片大小设置为 k = [7， 3， 3]，重叠大小设置为 s = [3， 1， 1]，填充大小设置为 p = [2， 1， 1]。
因此，我们可以获得多级代币
给定输入图像的宽度和高度分别为 H 和 W，则 、 和

我们按照 [74] 设置 c = 64，并在 T 3 上使用线性投影层将其嵌入维度从 c 转换为 d = 384。
3.1.2 带T2T-ViT骨干的编码器

将最终的标记序列T 3与正弦位置嵌入[61]相加，以编码2D位置信息。
然后，利用L E transformer层对T 3之间的长程依赖性进行建模，以提取强大的补丁标记嵌入T E ∈ R l3×d。
对于RGB SOD，我们采用单个Transformer编码器从每个输入RGB图像中获取RGB编码器贴片标记T E r ∈ R l3×d。
对于 RGB-D SOD，我们遵循双流架构，进一步使用另一个 transformer 编码器以类似的方式从输入深度图中提取深度编码器补丁标记 T E d，如图 1 所示。
3.2. 变压器转换器

我们在Transformer编码器和解码器之间插入一个转换器模块，将编码器补丁令牌T E *从编码器空间转换为解码器空间，从而得到转换后的补丁令牌T C ∈ R l3×d。
3.2.1 RGB-D转换器

我们在 RGB-D 转换器中融合了 T E r 和 T E d，以整合 RGB 和深度数据之间的互补信息。
为此，我们设计了一种跨模态转换器（CMT），它由L C交替的跨模态注意层和自注意力层组成。
跨模态-注意力。
在纯Transformer架构下，我们修改了标准的自注意力层，使图像和深度数据之间传播长距离的跨模态依赖性，从而得到交叉模态-注意力，具体如下。
首先，与[61]中的自我关注类似，
，并通过三个线性投影将 V r ∈ R l3×d 值值。
同样，我们可以从 T E d 中获取深度查询 Q d ， 键 K d 和值 V d 。
接下来，我们计算来自一种模态的查询与来自另一种模态的键之间的“缩放点-乘积注意力”[61]。
然后，将输出计算为值的加权和，公式为：
(3)
我们遵循[61]中的标准Transformer架构，在交叉注意力中采用多头注意力机制。
还使用了相同的位置前馈网络、残差连接和层归一化 [1]，形成了我们的 CMT 层。
在每次采用所提出的CMT层后，我们在每个RGB和深度补丁令牌序列上使用一个标准transformer层，进一步增强了它们的令牌嵌入。
在交替使用 CMT 和 transformer 进行 L C 次后，我们通过串联将获得的 RGB 令牌和深度令牌融合，然后将它们投影到最终转换的令牌 T C 中，如图 1 所示。
3.2.2 RGB转换器

为了与我们的 RGB-D SOD 模型保持一致，对于 RGB SOD，我们只需在 T E r 上使用 L C 标准变压器层来获得转换后的补丁令牌序列 T C。
3.3. 多任务Transformer解码器

我们的解码器旨在将补丁标记 T C 解码为显著性图。
因此，该文提出一种具有多级令牌融合和基于令牌的多任务解码器的新型令牌upsam-pling方法。
3.3.1 Token Upsampling 和多级 Token 融合

我们认为，由于T C的长度相对较小，即l 3 = H 16 × W 16，因此直接从T C预测显著性图不能获得高质量的结果，这对于密集预测是有限的。
因此，我们建议先对补丁令牌进行上采样，然后再进行密集预测。
大多数基于CNN的方法[84,82,38,18]采用双线性上采样来恢复大规模特征图。
或者，我们提出了一种在Transformer框架下的新的代币上采样方法。
受 T2T 模块 [74] 的启发，该模块聚合了相邻代币以逐渐减少代币的长度，我们提出了一种反向 T2T （RT2T） 变换，通过将每个代币扩展为多个子代币来上采样代币，如图 2（b） 所示。
具体来说，我们首先对输入补丁标记进行投影，以将其嵌入维度从 d = 384 减少到 c = 64。
然后，我们使用另一个线性投影将嵌入维度从 c 扩展到 ck 2 。
接下来，类似于 T2T 中的软拆分步骤，每个标记都被视为 k × k 图像块，相邻块有 s 重叠。
然后，我们可以使用 p 零填充将令牌折叠为图像。
输出图像大小可以使用（2）反向计算，即，给定输入补丁标记的长度为h o × w o，则输出图像的空间大小为h × w。
最后，我们将图像重塑为大小为 l o × c 的上采样标记，其中 l o = h × w。
通过将 s 设置为 < k -1，RT2T 转换可以增加令牌的长度。
在T2T-ViT的激励下，我们使用RT2T三次，设置k = [3， 3， 7]，s = [1， 1， 3]，p = [1， 1， 3]。
因此，补丁标记的长度可以逐渐上采样到 H × W，等于输入图像的原始大小。
此外，受现有SOD方法中多级特征融合的广泛成功[24,49,84,16,43]的激励，我们利用T2T-ViT编码器中较长的低级标记，即T 1和T 2，来提供准确的局部结构信息。
对于 RGB 和 RGB-D SOD，我们只使用 RGB 转换器编码器的低级标记。
具体来说，我们通过串联和线性投影逐步将 T 2 和 T 1 与上采样的补丁标记融合。
然后，我们采用一个transformer层来获得每个层次i的解码器令牌T D i，其中i = 2,1。
整个过程制定为：
其中 [， ] 表示沿令牌嵌入维度的串联。
“线性”是指线性投影，用于将串联后的嵌入维度减少到 c。最后，我们使用另一个线性投影将 T D i 的嵌入维度恢复到 d。

3.3.2 基于令牌的多任务预测

受现有的纯 transformer 方法 [74， 12] 的启发，该方法在补丁令牌序列上添加一个类令牌以进行图像分类，我们还利用与任务相关的令牌来预测结果。
然而，我们不能像 [74， 12] 中所做的那样，直接在任务令牌嵌入上使用 MLP 来获得密集的预测结果。
因此，我们建议在补丁令牌和任务相关令牌之间执行patch-task-attention来执行SOD。
此外，受SOD模型[82， 69， 79， 25]中广泛使用的边界检测的激励，我们还采用多任务学习策略共同进行显著性和边界检测，从而利用后者来帮助提升前者的性能。
为此，我们设计了两个任务相关的代币，即显著性代币 t s ∈ R 1×d 和边界代币 t b ∈ R 1×d 。
在每个解码器级别 i 上，我们在补丁令牌序列 T D i 上添加显著性和边界标记 t s 和 t b，然后使用 L D i 转换器层处理它们。
因此，这两个任务令牌可以从与补丁令牌的交互中学习依赖于图像的任务相关嵌入。
在此之后，我们将更新的补丁令牌作为输入，并在（4）中执行令牌上采样和多级融合过程，得到上采样的补丁令牌T D i-1 。
接下来，我们在下一个级别 i -1 中重用更新的 t s 和 t b 来进一步更新它们和 T D i-1 。
我们重复此过程，直到我们达到 1 4 刻度的最后一个解码器级别。
对于显著性和边界预测，我们在最终解码器补丁标记 T D 1 和显著性和边界标记 t s 和 t b 之间执行 patchtask-attention。
对于显著性预测，我们首先将 T D 1 嵌入到查询 Q D s ∈ R l1×d 中，并将 t s 嵌入到 R 1×d ∈键 K s 和值 V s ∈ R 1×d 中。
同样，对于边界预测，我们将 T D 1 嵌入到 Q D b 中，并将 t b 嵌入到 K b 和 V b 中。
然后，我们采用patch-task-attention来获取任务相关的补丁令牌：
在这里，我们使用 sigmoid 激活进行注意力计算，因为在每个方程中我们只有一个键。
由于 T D s 和 T D b 处于 1 4 尺度，我们采用第三次 RT2T 变换将它们上采样到全分辨率。
最后，我们应用两个带有 sigmoid 激活的线性变换，将它们投影到 [0， 1] 中的标量，然后分别将它们重塑为 2D 显著性图和 2D 边界图。
整个过程如图1所示。
4. 实验

4.1. 数据集和评估指标

对于RGB SOD，我们在六个广泛使用的基准数据集上评估了我们的VST模型，包括ECSSD [72]表1。
我们提出的模型的消融研究。
“哔哔哩”表示双线性上采样。
“F”表示多级代币融合。
“TMD”表示我们提出的基于令牌的多任务解码器，而“C2D”表示使用传统的双流解码器来执行显著性和边界检测，而不使用与任务相关的令牌。
最佳结果标记为蓝色。
我们采用四个广泛使用的评估指标来全面评估我们的模型性能。
具体来说，Structure-measure S m [13] 评估区域感知和对象感知的结构相似性。
最大 F 测量 （maxF） 共同考虑最佳阈值下的精度和召回率。
最大增强对准度量 E max ξ [14] 同时考虑像素级误差和图像级误差。
平均绝对误差 （MAE） 计算像素级平均绝对误差。
为了评估模型的复杂性，我们还报告了乘法累加运算 （MAC） 和参数数量 （Params）。
设置 NJUD [26] DUTLF-深度 [52] STERE [46] LFSD [33]

4.2. 实现细节

为了公平比较，我们遵循大多数以前的方法，使用 DUTS 的训练集来训练我们的 VST 进行 RGB SOD，并使用来自 NJUD 的 1,485 张图像、来自 NLPR 的 700 张图像和来自 DUTLF-Depth 的 800 张图像来训练我们的 VST 用于 RGB-D SOD。
我们遵循[82]，使用清醒的算子从GT显著性图中生成边界地面实况。
对于深度数据预处理，我们将深度图归一化为 [0,1]，并将它们复制到三个通道。
最后，我们将每个图像或深度图的大小调整为 256 × 256 像素，然后随机裁剪 224 × 224 个图像区域作为模型输入，并使用随机翻转作为数据增强。
我们使用预训练的 T2T-ViT t -14 [74] 模型作为我们的骨干，因为它具有与 ResNet50 [23] 相似的计算复杂性。
该模型在 T2T 模块中使用高效的 Performer [10] 和 c = 64，并设置 L E = 14。
在我们的转换器和解码器中，我们根据实验结果设置 L C = L D 3 = 4 和 L D 2 = L D 1 = 2。
对于 RGB 和 RGB-D SOD，我们将批量大小设置为 11 和 8，总训练步数为 40,000 和 60,000。
对于两者，Adam[27]都被用作运算同步器，二元交叉熵损失用于显著性和边界预测。
初始学习率设置为 0.0001，并在总步长的一半和四分之三处分别减少 10 倍。
深度监督也用于促进模型训练，其中我们使用补丁任务注意力来预测每个解码器级别的显著性和边界。
我们使用 Pytorch [50] 实现了我们的模型，并在 GTX 1080 Ti GPU 上进行了训练。
4.3. 消融研究

由于我们的 RGB-D VST 是通过添加一个额外的变压器编码器和基于我们的 RGB VST 的额外 CMT 构建的，而两个模型的其他部分是相同的，因此我们基于 RGB-D VST 进行烧蚀研究，以验证我们提出的所有模型组件。
在NJUD、DUTLF-Depth、STERE和LFSD四个RGB-D SOD数据集上的实验结果如表1所示。
我们从 RGB-D VST 中删除了转换器转换器和解码器作为基线模型。
具体来说，它使用双流 transformer 编码器提取 RGB 编码器补丁标记 T E r 和深度编码器补丁令牌 T E d，然后直接将它们连接起来，在每个补丁令牌上使用 MLP 预测 1/16 尺度的显著性图。
CMT的有效性。

对于跨模态信息融合，我们在 transformer 编码器之后部署我们提出的 CMT，以替换基线模型中的串联融合方法，如表 1 中的“+CMT”。
与基线相比，CMT带来了性能提升，特别是在NJUD和LFSD数据集上，因此证明了其有效性。
RT2T 的有效性。
在“+CMT”模型的基础上，我们进一步简单地使用双线性上采样（“+CMT+Bili”）逐步上采样到全分辨率，然后预测显著性图。
结果表明，使用双线性上采样提高显著性图的分辨率可以大大提高模型性能。
然后，我们用我们提出的RT2T令牌上采样方法（“+CMT+RT2T”）替换双线性上采样。
研究发现，与使用双线性上采样相比，RT2T具有明显的性能提升，验证了其有效性。
多级代币融合的有效性。
我们在解码器（“+CMT+RT2T+F”）中逐步融合 T 1 和 T 2，以提供低级细粒度信息。
我们发现这种策略进一步提高了模型的性能。
因此，在 transformer 中利用低级令牌与在基于 CNN 的模型中融合低级特征一样重要。
多任务转换器解码器的有效性。
在“+CMT+RT2T+F”的基础上，我们进一步使用我们的基于令牌的多任务解码器（TMD）来联合进行显著性和边界检测（“+CMT+RT2T+F+TMD”）。
研究表明，使用边界检测可以在四个数据集中的三个数据集上为 SOD 带来进一步的性能提升。
为了提高我们基于令牌的预测方案的有效性，我们尝试直接使用传统的双流解码器（C2D），通过使用“+RT2T+F”架构两次，通过MLP预测显著性图和边界图，而不使用与任务相关的令牌。
该模型在表1中表示为“+CMT+RT2T+F+C2D”。
TMD 与 C2D 的参数和 MAC 分别为 17.22 M 和 20.35
M 和 17.70 G 与 28.27
G，分别。
结果表明，在四分之三的数据集上使用我们的 TMD 可以比使用 C2D 获得更好的结果，并且计算成本也大大降低。
这清楚地表明了我们提出的基于令牌的Transformer解码器的优越性。
4.4. 与最新方法的比较

对于 RGB-D SOD，我们将我们的 VST 与 14 种最先进的 RGB-D SOD 方法进行了比较，即 A2dele [53] 、JL-DCF [18] 、SSF-RGBD [79] 、UC-Net [76]、S 2 MA [38] 、PGAR [6]、DANet [85] 、cmMS [29]、ATSA [78] 、CMW [31] 、Cas-Gnn [43] 、HDFNet [48] 、CoNet [25] 和 BBS-Net [16] 。
对于RGB SOD，我们将VST与12种最先进的RGB SOD模型进行了比较，包括GateNet [84] ， CSF [20] ， LDF [69] ， MINet [49] ， ITSD [87] ， EGNet [82] ， TSPOANet [41]， AFNet [17] ， PoolNet [35] ， CPD [70] ， BASNet [55] 和 PiCANet [37] 。
表2和表3分别显示了RGB-D和RGB SOD的定量比较结果。
结果表明，我们的VST在RGB和RGB-D基准数据集上都优于以前所有最先进的基于CNN的SOD模型，具有相当的参数数量和相对较小的MAC，因此证明了我们的VST的巨大有效性。
我们还在图 3 中展示了性能最佳模型之间的视觉比较结果。
结果表明，我们提出的VST可以在非常具有挑战性的场景中准确检测突出物体，例如，大的突出物体、杂乱的背景、前景和外观相似的背景等。
5. 结论

在本文中，我们首次从序列到序列的角度重新思考SOD，并开发了一种基于纯变压器的新型统一模型，适用于RGB和RGB-D SOD。
为了解决在密集预测任务中应用trans-former的困难问题，该文在Transformer框架下提出了一种新的Token上采样方法，并融合了多级补丁Token。
我们还通过引入任务相关的令牌和一种新颖的补丁-任务-注意力机制来设计多任务解码器，以共同执行显著性和边界检测。
我们的 VST 模型在不依赖大量计算成本的情况下，在 RGB 和 RGB-D SOD 上都取得了最先进的结果，从而显示出其巨大的有效性。
我们还为如何在密集预测任务中使用 transformer 的开放性问题设定了新的范式。
表 4 .我们提出的模型在RGB SOD数据集上的消融研究。

“RC”表示RGB转换器。
“Bili”表示双线性上采样，“F”表示多级令牌融合。
“TMD”表示我们提出的基于令牌的多任务解码器，而“C2D”表示使用传统的双流解码器来执行显著性和边界检测，而不使用与任务相关的令牌。
最佳结果标记为蓝色。
设置

图 1.
我们提出的RGB和RGB-D的VST模型的整体架构 SOD.It 首先使用编码器从输入图像补丁序列生成多级令牌。
然后，采用转换器将补丁令牌转换为解码器空间，并对RGB-D数据进行跨模态信息融合;
最后，解码器通过提出的任务相关标记和补丁-任务-注意力机制同时预测显著性图和边界图。
还提出了 RT2T 变换，以逐步对补丁令牌进行上采样。
虚线代表 RGB-D SOD 的专用组件。
图2.（a） T2T模块将相邻的代币合并为一个新的代币，从而减少了代币的长度。

（b） 我们提出的反向 T2T 模块通过将每个代币扩展为多个子代币来对代币进行上采样。
图3.与最先进的RGB-D（左）和RGB（右）SOD方法的定性比较。

（GT：地面实况）
图4.与最先进的RGB SOD方法的定性比较。

（GT：地面实况）
我们提出的 VST 与其他 14 种 SOTA RGB-D SOD 方法在 9 个基准数据集上的定量比较。
红色和蓝色分别表示最佳和次优结果。
“-”表示代码或结果不可用。
我们提出的 VST 与其他 12 种 SOTA RGB SOD 方法在 6 个基准数据集上的定量比较。
“-R”和“-R2”分别表示 ResNet50 和 Res2Net 主干网。
.874 0.939 0.039 0.925 0.932 0.966 0.032 0.871 0.845 0.897 0.068 0.851 0. 861 0.899 0.068 +RC+RT2T+F+TMD 0.896 0.877 0.939 0.037 0.928 0.937 0.968 0.030 0.873 0.850 0.900 0.067 0.854 0.866 0.902 0.065 +RC+RT2T+F+C2D 0.891 0.870 0.937 0.040 0.924 0.931 0.966 0.033 0.869 0.844 0.896 0.069 0.852 0.860 0.898 0.067 VST模型中使用不同数量的变压器层数的比较。
最终模型设置标记为蓝色。
