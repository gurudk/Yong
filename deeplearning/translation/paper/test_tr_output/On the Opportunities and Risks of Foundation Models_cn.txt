论基础模型的机遇与风险
人工智能正在经历范式转变，模型(例如BERT、DALL-E、GPT-3)在广泛的数据上训练(通常使用大规模的自我监督)，可以适应广泛的下游任务。
我们称这些模型为基础模型，以强调其关键的核心但不完整的特征。
本报告全面介绍了基础模型的机遇和风险，从它们的能力(如语言、视觉、机器人操作、推理、人类互动)和技术原理(如模型架构、训练程序、数据、系统、安全、评估、理论)到它们的应用(如法律、医疗保健、教育)和社会影响(如不平等、滥用、经济和环境影响、法律和伦理考虑)。
虽然基础模型是基于标准的深度学习和迁移学习，但它们的规模导致了新的涌现能力，它们在许多任务中的有效性激励了同质化。
同质化提供了强大的杠杆作用，但也需要谨慎，因为基础模型的缺陷会被下游的所有适应模型所继承。
尽管基础模型的广泛应用迫在眉睫，但我们目前对它们是如何工作的，何时失效，以及由于它们的紧急属性，它们甚至能够做什么都缺乏清晰的理解。
为了解决这些问题，我们相信许多关于基础模型的关键研究将需要与其基本的社会技术性质相称的深入的跨学科合作。
1介绍
本报告研究了一种新兴的范例，用于构建基于一般类型模型的人工智能(AI)系统，我们称之为基础模型。2
基础模型是在广泛的数据上训练的任何模型(通常使用大规模的自我监督)，可以适应(例如，微调)到广泛的下游任务;目前的例子包括BERT [Devlin等人，2019]、GPT-3 [Brown等人，2020]和CLIP [Radford等人，2021]。
从技术的角度来看，基础模型并不新鲜——它们基于深度神经网络和自监督学习，这两者已经存在了几十年。
然而，过去几年基础模型的规模和范围已经超出了我们对可能性的想象;例如，GPT-3有1750亿个参数，可以通过自然语言提示进行调整，在广泛的任务中完成合格的工作，尽管没有经过明确的训练来完成许多任务[Brown et al. 2020]。
与此同时，现有的基础模型有可能加剧危害，而且它们的特征通常不太清楚。
考虑到它们即将被广泛应用，它们已经成为人们密切关注的话题[Bender et al. 2021]。
1.1涌现和同质化
基础模型的意义可以用两个词来概括:涌现和同质化。
涌现意味着系统的行为是隐性诱导的，而不是明确构建的;它既是科学兴奋的源泉，也是对意外后果的焦虑。
同质化表明在广泛的应用中构建机器学习系统的方法的整合;它为许多任务提供了强大的杠杆作用，但也造成了单点故障。
为了更好地理解涌现和同质化，让我们回顾一下它们在过去30年里在人工智能研究中的崛起。
图1。
人工智能的故事一直是一个不断涌现和同质化的故事。
随着机器学习的引入，任务的执行方式从示例中浮现(自动推断);通过深度学习，用于预测的高级特征出现;有了基础模型，甚至高级功能，如上下文学习也会出现。
同时，机器学习使学习算法同质化(如逻辑回归)，深度学习使模型架构同质化(如卷积神经网络)，基础模型使模型本身同质化(如GPT-3)。
机器学习。
今天的大多数人工智能系统都是由机器学习驱动的，其中预测模型是根据历史数据进行训练的，并用于对未来进行预测。
机器学习在人工智能领域的兴起始于20世纪90年代，代表着人工智能系统构建方式的显著转变:不是指定如何解决任务，而是学习算法将基于数据诱导它-即如何从学习的动态中出现。
机器学习也代表了向同质化迈出的一步:现在，广泛的应用程序可以由单一的通用学习算法(如逻辑回归)来驱动。
尽管机器学习在人工智能中无处不在，但在自然语言处理(NLP)和计算机视觉(如问答或对象识别)中，语义复杂的任务(输入是句子或图像)仍然需要领域专家执行“特征工程”，即编写特定于领域的逻辑，将原始数据转换为更适合流行机器学习方法的高级特征(例如SIFT [Lowe 1999])。
深度学习。
2010年左右，以深度学习(deep learning)的名义复兴的深度神经网络[LeCun et al. 2015]开始在机器学习领域获得关注。
更大的数据集、更多的计算(特别是gpu的可用性)和更大的胆识推动了深度学习。
深度神经网络将在原始输入(例如，像素)上进行训练，更高层次的特征将通过训练出现(这一过程被称为“表征学习”)。
这导致了在标准基准测试上的巨大性能提升，例如，AlexNet [Krizhevsky等人，2012]在ImageNet数据集上的开创性工作[Deng等人，2009]。
深度学习还反映了向同质化的进一步转变:与其为每个应用程序定制特征工程管道，不如将相同的深度神经网络架构用于许多应用程序。
基础模型。
基础模型在NLP中已经形成了最强烈的形态，所以我们现在把我们的故事集中在这里。
也就是说，就像深度学习在计算机视觉领域普及，但在它之外还存在一样，我们将基础模型理解为人工智能的一般范例，而不是以任何方式特定于NLP。
到2018年底，NLP领域即将经历另一场翻天覆地的变化，标志着基础模型时代的开始。
在技术层面上，基础模型是通过迁移学习[Thrun 1998]和规模实现的。
迁移学习的思想是将从一个任务(例如，图像中的对象识别)中学到的“知识”应用到另一个任务(例如，视频中的活动识别)。
在深度学习中，预训练是迁移学习的主要方法:模型在代理任务上进行训练(通常只是作为达到目的的手段)，然后通过微调适应下游感兴趣的任务。
迁移学习使基础模型成为可能，但规模使它们变得强大。
规模需要三个要素:(i)计算机硬件的改进-例如，GPU吞吐量和内存在过去四年中增加了10倍(§4.5:系统);(ii) Transformer模型架构的开发[Vaswani et al. 2017]，它利用硬件的并行性来训练比以前更具表现力的模型(§4.1:建模);(三)更多训练数据的可用性。
数据的可用性和利用数据的能力的重要性不容低估。
使用带注释的数据集进行迁移学习已经有至少十年的历史了，例如，在计算机视觉社区中，对ImageNet数据集进行预训练[Deng et al. 2009]用于图像分类。
然而，标注的巨大成本限制了预训练的好处。
另一方面，在自监督学习中，预训练任务是从未注释的数据中自动导出的。3
例如，用于训练BERT的掩模语言建模任务[Devlin等人，2019]是根据句子周围的上下文(例如，我喜欢豆芽)预测句子中缺失的单词。
自监督任务不仅更具可扩展性，只依赖于未标记的数据，而且它们被设计成迫使模型预测部分输入，使它们比在更有限的标签空间上训练的模型更丰富，可能更有用。
自监督学习的进展可以追溯到词嵌入[Turian et al. 2010;Mikolov et al. 2013;Pennington et al. 2014]将每个单词与上下文无关的向量关联起来，为广泛的NLP模型提供了基础。
此后不久，基于自回归语言建模的自监督学习(根据前一个单词预测下一个单词)[Dai and Le 2015]开始流行。
这产生了在上下文中表示单词的模型，如GPT [Radford等人，2018]，ELMo [Peters等人，2018]和ULMFiT [Howard和Ruder, 2018]。4
自监督学习的下一波发展-BERT [Devlin等人，2019]GPT-2 [Radford等人，2019]，RoBERTa [Liu等人，2019]，T5 [Raffel等人，2019]，BART [Lewis等人，2020a]紧随其后，采用Transformer架构，结合更强大的深度双向句子编码器，并扩展到更大的模型和数据集。
虽然人们可以纯粹从自我监督学习的角度来看待这最后一波技术发展，但在引入BERT的过程中，出现了一个社会学的转折点。
在2019年之前，使用语言模型的自监督学习本质上是NLP的一个子领域，它与NLP的其他发展并行发展。
2019年之后，使用语言模型的自监督学习更像是NLP的基础，因为使用BERT已经成为常态。
单个模型可以用于如此广泛的任务的接受标志着基础模型时代的开始。
基础模型已经导致了前所未有的同质化水平:现在几乎所有最先进的NLP模型都改编自少数基础模型之一，例如BERT、RoBERTa、BART、T5等。
虽然这种同质化产生了极高的杠杆作用(基础模型中的任何改进都可能导致所有NLP的直接收益)，但它也是一种负担;所有人工智能系统都可能继承一些基础模型的相同问题偏差[Bolukbasi et al. 2016;Caliskan et al. 2017;Abid等人。2021，除其他外])-见§5.1:公平，§5.6:道德以作进一步讨论。
我们也开始看到整个研究团体的同质化。
例如，类似的基于transformer的序列建模方法现在应用于文本[Devlin等人，2019;Radford et al. 2019;rafael et al. 2019]，图像[Dosovitskiy et al. 2020;Chen et al. 2020d]、语音[Liu et al. 2020d]、表格数据[Yin et al. 2020]、蛋白质序列[Rives et al. 2021]、有机分子[Rothchild et al. 2021]、强化学习[Chen et al. 2021b;Janner et al. 2021]。
这些例子指出了一个可能的未来，我们有一套统一的工具来开发跨多种模式的基础模型[Tamkin et al. 2021b]。
除了方法的同质化，我们还看到跨研究社区的实际模型以多模态模型的形式同质化-例如，基于语言和视觉数据训练的基础模型[Luo et al. 2020;Kim et al. 2021a;Cho et al. 2021;Ramesh et al. 2021;Radford et al. 2021]。
数据在某些领域自然是多模态的。、医学图像、结构化数据、医疗保健中的临床文本(§3.1:医疗保健)。
因此，多模态基础模型是融合关于一个领域的所有相关信息的一种自然方式，并且适应跨越多个模态的任务(图2)。
基础模型也导致了惊人的出现，这是规模的结果。
例如，GPT-3 [Brown et al. 2020]与GPT-2的15亿个参数相比，具有1750亿个参数，允许上下文学习，其中语言模型可以通过简单地为其提供提示(任务的自然语言描述)来适应下游任务，这是一种既没有专门训练也没有预期出现的紧急属性。
同质化和涌现以一种潜在的令人不安的方式相互作用。
同质化可能会为许多特定任务数据非常有限的领域带来巨大的收益——看看几个这样的领域所呈现的机会(例如，第3.1条:医疗保健，第3.2条:法律，第3.3条:教育);另一方面，模型中的任何缺陷都被所有适应的模型盲目地继承(§5.1:公平，§5.6:道德)。
由于基础模型的力量来自于它们的涌现特性，而不是它们的明确构造，现有的基础模型很难理解(§4.4:评估，§4.10:理论，§4.11:可解释性)，并且它们具有意想不到的失效模式(§4.7:安全性，§4.8:鲁棒性)。
由于出现产生了基础模型的能力和缺陷的大量不确定性，通过这些模型进行激进的同质化是有风险的。
从伦理(第5.6节:伦理)和人工智能安全(第4.9节:人工智能安全)的角度来看，降低风险是基础模型进一步发展的核心挑战。
1.1.1命名。
我们引入术语基础模型来填补描述我们正在目睹的范式转变的空白;我们简要叙述一下我们作出这一决定的一些理由。
现有的术语(例如，预训练模型，自监督模型)部分地捕获了这些模型的技术维度，但未能以机器学习以外的方式捕获范式转换的意义。
特别是，基础模型指定了一个模型类，这些模型类在其社会学影响以及它们如何赋予人工智能研究和部署的广泛转变方面是独特的。
相比之下，预训练和自我监督的形式在技术上预示了基础模型，但未能阐明我们希望强调的实践转变。
3 ． 在对基础模型的社会影响进行推理之前，重要的是要理解它们是从数据创建到部署的更广泛的生态系统的一部分。
在这两个方面，我们都强调了人作为基础模型训练的最终数据来源的作用，以及作为任何利益和损害的下游接受者的作用。
深思熟虑的数据管理和适应应该是任何人工智能系统负责任开发的一部分。
最后，请注意调整基础模型的部署是与它们的构造分开的决定，这可以用于研究。
此外，虽然在撰写本文时许多标志性的基础模型都是语言模型，但是术语语言模型对于我们的目的来说太狭窄了:正如我们所描述的，基础模型的范围远远超出了语言。
我们还考虑了诸如通用模型和多用途模型之类的术语，它们捕获了这些模型可以服务于多个下游任务的重要方面，但两者都未能捕获其未完成的特征和适应的需要。
诸如任务不可知模型之类的术语可以捕获训练的方式，但不能捕获对下游应用程序的重要含义。
我们选择了新的术语“基础模型”来识别作为本报告主题的模型和新兴范式。
特别地，“基础”一词指定了这些模型所扮演的角色:基础模型本身是不完整的，但是作为许多特定任务的模型通过适应而建立的公共基础。
我们还选择了术语“基础”来暗示体系结构稳定性、安全性和安全性的重要性:构造不良的基础是灾难的配方，而执行良好的基础是未来应用程序的可靠基石。
目前，我们强调，我们并没有完全理解基础模型所提供的基础的性质或质量;我们无法确定这个基金会是否值得信赖。
因此，对于研究人员、基础模型提供者、依赖基础模型的应用程序开发人员、政策制定者和整个社会来说，这是一个关键问题。
1.2社会影响与基础模型生态系统
基础模型由于其令人印象深刻的性能和能力而在科学上很有趣，但使它们成为研究的关键的事实是，它们正在迅速集成到对人类产生深远影响的人工智能系统的现实部署中。
例如，拥有40亿用户的谷歌搜索，现在依赖BERT [Devlin et al. 2019]等基础模型作为其信号之一。5
因此，我们必须停下来问:这种社会影响的本质是什么?
在本报告中，我们解决了这个问题的许多方面:社会不平等的潜在加剧(第5.1节:公平)，由于能力增加而产生的经济影响(第5.5节:经济学)，由于计算需求增加而产生的环境影响(第5.3节:环境)，放大虚假信息的潜在担忧(第5.2节:滥用)，由于强大的生成能力而产生的法律后果(第5.4节:合法性)，同质化导致的伦理问题，以及基础模型被开发和部署的更广泛的政治经济学(§5.6:伦理)。
考虑到基础模型的千变万化性质和它们未映射的能力，我们如何负责任地预测和处理它们引起的伦理和社会考虑?
一个反复出现的主题是，对部署到特定用户的特定系统的社会影响进行推理比对基础模型的社会影响进行推理更容易，基础模型可以适应任何数量的不可预见的下游系统。
在尝试回答这些问题之前，我们需要打下一些基础。
首先，让我们区分基础模型的研究和基础模型的部署。
大多数被公众所知的是基础模型研究——通过学术论文、演示和排行榜上的进展。
虽然知识的生产可以在塑造未来方面发挥至关重要的作用，但直接的社会影响是通过这些模型的实际部署来实现的，这些模型通常是由私有数据的专有实践控制的。
有时是通过新产品进行部署——例如，GitHub的Copilot 6基于OpenAI的Codex模型[Chen et al. 2021f]，但通常是通过升级现有产品(例如，使用BERT的Google搜索)。
研究模型往往没有经过广泛的测试，可能有未知的失效模式;对于不适合部署的研究模型，应该贴上警告标签。
另一方面，实际影响人们生活的已部署的基础模型应该服从更严格的测试和审计。
为了进一步理解基础模型的研究和部署，我们必须缩小并考虑这些基础模型所处的完整生态系统，从数据创建到实际部署。
需要注意的是，基础模型只是AI系统的一个组件(尽管是一个越来越重要的组件)。
简而言之，我们可以根据阶段序列来考虑基础模型的生态系统，将之前的训练和适应阶段延伸开来。7
正当我们对社会影响感兴趣时，人们占据了管道的两端。
这个生态系统视图允许我们看到关于基础模型的不同问题(例如，基础模型是否合乎道德)实际上应该根据不同的阶段来回答。
(1)数据创建:数据创建基本上是一个以人为中心的过程:所有数据都是由人创建的，大多数数据至少隐含地与人有关。
有时数据是由人们以电子邮件、文章、照片等形式为其他人创建的，有时它是对人的测量(例如，基因组数据)或对人们生活环境的测量(例如，卫星图像)。
重要的是要注意，所有数据都有一个所有者，并且都是为了一个目的而创建的(这个目的可能包括也可能不包括训练基础模型)。
(2)数据整理:然后将数据整理成数据集。
数据没有单一的自然分布;即使是最宽松的互联网抓取也需要一些选择和后过滤。
在尊重法律和道德约束的同时确保数据的相关性和质量至关重要，但也具有挑战性。
虽然这在工业界得到了认可，但在人工智能研究中却没有得到充分的重视(§4.6:数据)。
(3)训练:在这些精心策划的数据集上训练基础模型8是人工智能研究中著名的核心，尽管它只是众多阶段中的一个。
(4)适应:在机器学习研究的背景下，适应是指在基础模型的基础上创建一个新的模型来执行一些任务(例如，文档摘要)。
对于部署，适应性是关于创建一个系统，它可能需要许多不同的模块、自定义规则(例如，对输出空间的限制)或分类器(例如，用于毒性分类)，并与其他互补信号相结合(例如，问答模型生成的答案将根据相关文档进行验证)。
例如，如果在下游采取适当的预防措施，能够产生有毒内容的有问题的模型可能是可以容忍的。
额外的特定于应用程序的逻辑对于减轻危害至关重要。
(5)部署:人工智能系统的直接社会影响发生在它被部署到人们身上的时候。
虽然我们不希望在有问题的数据上部署潜在有害的基础模型，但允许它们在研究中推进科学理解可能仍然有价值，尽管人们仍然必须谨慎行事。
更一般地说，在大规模部署中进行逐步发布是标准实践，其中部署发生在越来越多的用户身上;这可以部分减轻任何潜在的危害。
虽然本报告是关于基础模型的，但重要的是要注意，许多影响来自管道中其他阶段的决策，每个阶段都需要周到的监测和干预。
虽然大型组织可能拥有整个管道，但每个阶段都可以由不同的组织执行，例如，专门为应用程序开发人员可以使用的各种领域创建自定义基础模型的公司。
思考生态系统，行动模式。
虽然社会影响取决于整个生态系统，但考虑到许多研究人员和实践者的范围仅限于培训阶段，能够对基础模型的社会影响进行推理仍然很重要。
这是困难的，因为基础模型是未完成的中间对象，可以适应许多下游应用程序，有时由完全不同的实体用于不可预见的目的。
我们需要的是两件事:(i)潜在下游评估的代表性集合的替代指标(§4.4:评估)，以及(ii)承诺记录这些指标[Mitchell等人，2019]，类似于金属和塑料等材料的数据表，可以适应许多下游用例。
描述基础模型潜在的下游社会影响是具有挑战性的，需要对技术生态系统和社会都有深刻的理解。
如果不认识到基础模型将如何部署，就无法全面评估其危害(§5.1:公平性)，也无法在不考虑丰富的社会和历史背景的情况下定义自动指标。
1.3基础模型的未来
基础模型已经展示了原始的潜力，但我们仍处于早期阶段。
尽管这些模型在现实世界中得到了应用，但它们在很大程度上是研究原型，很难被理解。
甚至围绕基础模型的专业规范——Robert Merton称之为科学精神[Merton 1979]——也是不发达的。
例如，在一些基本问题上缺乏共识，比如什么时候发布模型是“安全的”，或者社区应该如何对方法不当作出反应。
考虑到基础模型的未来充满了不确定性，一个大问题是:谁将决定这个未来?
学科的多样性。
基础模型背后的技术是基于机器学习、优化、自然语言处理、计算机视觉和其他领域数十年的研究。
这些技术贡献来自学术界和工业研究实验室。
然而，关于构建基础模型本身的研究几乎只发生在行业中——大型科技公司，如谷歌、Facebook、微软或华为，或初创公司，如OpenAI或AI21实验室，尽管AI2是一个明显的例外[Peters et al. 2018;Zellers et al. [2019b]。
技术进步的迅猛步伐和中央集权造成的堑壕引发了强烈的担忧，除了技术专家之外，还需要人文主义者和社会科学家的关注。
我们不应该依赖于事后对道德和社会后果的审计，只有在技术架构和部署决策做出之后才进行审计。
相反，我们需要从一开始就将社会考虑和伦理设计深入到基础模型及其周围生态系统的技术开发中。
学术机构的独特之处在于，它们在一个屋檐下容纳了最广泛的学科，从而将计算机科学家、社会科学家、经济学家、伦理学家、法律学者等聚集在一起。
考虑到学科多样性在理解和解决结合了技术、伦理、法律、社会和政治维度的问题方面的重要性[Hong and Page 2004;所罗门2006;Steel等人。2018]，因此，我们认为学术界在开发基础模型方面发挥着至关重要的作用，以促进其社会效益并减轻其社会危害，以及确定生态系统(§1.2:生态系统)的每个阶段(从数据管理到部署)的行动应严格禁止的背景。
激励措施。
设计、开发和部署基础模型的政治经济学为每个阶段的决策提供了不可避免的激励结构。
人们和机构如何对激励作出反应是经济学的一个基本课程。
市场驱动的商业激励可以很好地与社会利益相结合:使基础模型更加准确、可靠、安全和高效，同时寻找各种各样的潜在用例可以产生大量的社会效用。
然而，在股东无法获得创新价值的领域，商业激励也可能导致市场失灵和投资不足。
正如制药行业没有动力投入大量资源用于疟疾治疗的研发，因为穷人负担不起药物，9科技行业也没有动力投入大量资源用于旨在改善穷人和边缘化人群状况的技术[Reich et al. 2021]。
此外，商业激励可能导致企业忽视社会外部性[Acemoglu 2021;Reich et al. 2021]，如劳动力的技术取代、民主所需的信息生态系统的健康、计算资源的环境成本，以及向非民主政权出售技术的利润驱动。
最后，任何给定的公司都没有什么动力去创建一个开放的、去中心化的生态系统来开发鼓励广泛参与的基础模型。
相比之下，大学的长期和根深蒂固的研究使命是生产和传播知识和创造全球公共产品[Kerr 2001;Rhoten and Calhoun 2011;Nussbaum 2010]。
我们相信，学术界在塑造基础模型的发展方面具有独特的地位，以确保我们抓住具有潜在巨大社会效益的方向，否则这些方向可能不会被工业优先考虑。
丧失可访问性。
不幸的是，由于缺乏可访问性，学术界未能以尽可能充分的方式参与其中。
深度学习革命的一个经常被忽视的影响是可重复性和开放科学的增加:公开发布代码和数据集越来越成为常态，TensorFlow [Abadi等人，2016]和PyTorch [Paszke等人，2019]等软件包使人们更容易协作和构建彼此的工作。
ML可再现性挑战10 (ML Reproducibility Challenge 10)以及主要会议采用的可再现性检查清单(Pineau et al. 2020)等倡议，以及CodaLab Worksheets 11等平台，帮助推进了可再现性的社区标准。
这导致了技术创新和进步的激增。
基础模型开始逆转这一积极趋势。
有些模型(例如，根本没有发布(只有API可以访问有限的人)。
甚至数据集(例如GPT-2)也没有发布。
虽然训练过的模型可能是可用的(例如BERT)，但由于计算成本高得多和复杂的工程要求，绝大多数人工智能研究人员无法实际训练基础模型。
一些有意义的研究仍然可以通过在学术预算范围内训练更小的模型来完成，而且缩放定律预测的令人惊讶的规律性[Kaplan等人，2020]使这成为一种可行的策略，用于规模造成的差异是定量的(例如，准确性上升)。
然而，由于这些基础模型的突现性，一些功能，如上下文学习，只在足够规模的模型中得到了证明，所以甚至需要规模来提出正确的问题。
也可以有效地研究已经发布的预先存在的模型;事实上，这导致了NLP内部一个大型的子社区来探索这些模型[Rogers et al. 2020;Manning et al. 2020]。
拥有对现有模型的访问权限对于推动下游应用程序或识别缺陷(例如，偏差)是有用的，但是这对于我们设计更好的体系结构或为可以修复这些缺陷(例如，减轻偏差)的基础模型训练目标可能是不够的。
值得反思的是，今天有多少NLP研究是基于BERT(一个特定的(有些武断的)基础模型)。
考虑到需要将社会意识和道德设计注入到这些模型的构建中，我们可能需要建立与现有模型完全不同的基础模型。
这将需要大规模的密集试验。
EleutherAI 12和hug Face的BigScience项目13等社区努力正在尝试训练大型基础模型，但行业可以训练的私人模型与向社区开放的模型之间的差距可能会继续拉大。
此外，今天的初创公司(OpenAI, Anthropic, AI21 Labs等)比学术界拥有更多的资源，因此仍然可以负担得起训练最大的基础模型(例如OpenAI的GPT-3)。
然而，大型科技公司在资源方面处于完全不同的水平，特别是在基础设施、用户和数据方面，这些都来自于它们的市场地位。
基础模型的基本集中特性意味着开发它们的进入门槛将继续上升，因此即使是初创公司，尽管它们很敏捷，也会发现很难与之竞争，这一趋势反映在搜索引擎的发展中[Radinsky 2015]。
缩小资源差距的一个方法是政府投资于公共基础设施。
我们可以把哈勃太空望远镜和大型强子对撞机等大型科学项目作为灵感，在这些项目中，大量的投资使原本不可能的基础科学发现成为可能。
人们可以想象类似的计算基础设施，基础模型的学术研究将从中受益匪浅。
在美国，刚刚起步的国家研究云计划(National Research Cloud initiative)就是朝这个方向迈出的一步。
另一种补充方法是依靠志愿计算，在这种方法中，数十亿计算设备(节点)中的任何一个都可以连接到中央服务器并贡献计算。
Folding@home项目已经成功地实现了这种方法来模拟蛋白质动力学[Beberg et al. 2009]。
最近，Learning@home项目正试图利用志愿者计算来训练基础模型[Ryabinin and Gusev 2020]。
节点之间的高延迟连接和训练基础模型的高带宽需求使这成为一个开放的技术挑战。
总结。
推动基础模型的能力和规模有巨大的经济动机，因此我们预计未来几年技术将稳步进步。
但是，一项主要依赖于突发行为的技术是否适合广泛应用于人类尚不清楚。
很明显，我们需要谨慎，现在是建立专业规范的时候了，这将使基金会模型的负责任的研究和部署成为可能。
学术界和工业界需要在这方面进行合作:工业界最终对如何部署基础模型做出具体决定，但我们也应该依靠学术界，其学科多样性和围绕知识生产和社会效益的非商业激励，为基础模型的开发和部署提供独特的指导，这在技术和道德上都是有根据的。
1.4本报告概述
2021年3月，我们在斯坦福大学创建了一个非正式社区，聚集了对基础模型某些方面感兴趣的学生、教师和研究人员。15
从一开始，社区不仅包括人工智能研究人员，还包括那些渴望将基础模型应用于他们的领域(例如，医疗保健和法律)的人，以及那些对社会问题(例如，伦理和经济学)感兴趣的人。
随着讨论的进行，我们注意到在相互理解方面存在许多差距——技术如何工作，行业如何开发基础模型，如何考虑伦理问题，等等，而现有的文献只涵盖了一小部分。
因此，我们想要提供基础模型的更全面的画面，识别机会和风险，并为基础模型的未来负责任的发展建立一个建设性的愿景。
这份报告的撰写是一个实验:我们让100多名来自不同背景的人聚集在一起，撰写了一份涵盖基础模型广泛方面的报告。
这份报告的很大一部分是对现有工作的调查，但经过多次讨论，我们将其统一在一份报告中，以突出所有跨学科的联系。
结构。
报告分为26个部分，每个部分讨论基础模型的一个方面。
这些部分被分成四个部分:能力(第2节:能力)、应用(第3节:应用)、技术(第4节:技术)和社会(第5节:社会)，尽管各部分之间有许多联系。
这些联系突出了一种综合方法，在这种方法中，技术和功能的开发方式对现实社会问题很敏感，同时受到应用程序的启发和基础。
虽然我们已经设法捕获了围绕基础模型的大多数重要主题，但这份报告将不可避免地不完整，特别是随着该领域的迅速发展。
例如，许多应用程序(例如，自然科学，音乐，金融，农业)不包括在内，尽管它们与我们选择讨论的应用程序一样可能受到影响。
这也会很有趣
纸的路线图
图4。
该报告分为四个部分:能力、应用程序、技术和社会，其中每个部分包含一组章节，每个章节涵盖基础模型的一个方面。
研究基础模型如何与神经科学、认知科学和心理学的研究联系起来，以解释智力，并帮助计算社会科学的努力来理解社会。
Percy Liang提出并构思了整个报告的框架和结构。
他和Rishi Bommasani一起领导分散的写作工作，并在个别章节提供指导。
Drew A. Hudson创建了报告中的所有数据，并与每个部分的作者讨论了它们的结构和内容。
本报告的26个部分中的每一部分都是由一部分作者撰写的，他们的名字列在每一部分的开头。
然而，有许多讨论跨越了多个部分，因此对每个部分的实际贡献通常来自更广泛的集合。
最后，我们注意到，并非本报告中所表达的所有观点都代表所有作者的观点。
1.4.1功能概述。
基础模型获得各种能够增强应用程序的能力。
我们选择讨论五种潜在的能力:处理不同模式的能力(例如，语言，视觉)，影响物理世界的能力(机器人技术)，进行推理的能力，以及与人类互动的能力(互动)。
最后，我们以对其能力的潜在限制进行哲学讨论来结束。
§2.1:语言。
自然语言处理作为一个领域为基础模型开辟了道路。
虽然这些模型主导了标准基准，但这些模型目前获得的能力与那些将语言描述为人类交流和思考的复杂系统的能力之间存在明显的差距。
针对这一点，我们强调了语言变化的全范围(例如，不同的风格，方言，语言)，这带来了机遇和挑战，因为一些变体是数据有限的。
此外，儿童语言习得比基础模型训练更具样本效率;我们研究了文本和接地之外的信号如何帮助弥合这一差距。
语言的这两个特点为未来基础模型的研究提供了明确的方向。
§2.2:愿景。
计算机视觉引领了人工智能中深度学习的采用[Russakovsky等人，2015]，表明在大型带注释的数据集上预训练的模型可以转移到许多下游设置。
现在，在网络规模的原始数据而不是精心策划的数据集上进行预训练，基础模型在计算机视觉领域正在崛起[例如，Radford et al. 2021]。
这些模型在该领域的标准任务(如图像分类和目标检测)中显示出了有希望的结果，并且在图像之外的多模态和具体数据上进行训练可能会在重大挑战(例如，3D几何和物理理解，常识推理)上取得进展。
我们还讨论了建模(例如，有效扩展到视频的能力)和评估(例如，高阶功能的测量)以及应用(例如，医疗保健的环境智能)和社会考虑(例如，监视)中的一些关键挑战，这些挑战将决定基础模型对计算机视觉未来的影响。
§2.3:机器人技术。
机器人研究的一个长期目标是开发能够在不同物理环境中执行无数任务的“通才”机器人。
与语言和视觉不同，由于大量的原始数据可以训练这些模型，并且可以使用虚拟应用程序将这些模型应用于基础模型，因此机器人技术面临着基于物理世界的基本挑战。
开发新型机器人基础模型的主要挑战是获取有利于学习的适当形式的足够数据(例如，人类的通用视频等)，这些数据不是特定于特定环境的，而是跨模式(例如，语言，视觉)，可以帮助弥合这一差距。
这些新的机器人基础模型可以允许更容易的任务规范和学习，引入新的应用(例如，更好的机器人辅助家庭任务)，并提高鲁棒性和安全性的重要性(例如，正式的安全评估)。
§2.4:推理和搜索。
推理和搜索问题，如定理证明和程序合成，一直是人工智能的长期挑战。
组合搜索空间使得传统的基于搜索的方法难以解决。
然而，众所周知，即使在最数学化的领域，人类也能直观地进行操作[Lakoff和Núñez 2000]，事实上，像AlphaGo这样的现有工作已经表明，深度神经网络可以有效地指导搜索空间。
但人类也会跨任务传递知识，促进更有效的适应和更抽象的推理能力。
基础模型提供了缩小这一差距的可能性:它们的多用途特性以及强大的生成和多模态能力为控制搜索固有的组合爆炸提供了新的杠杆。
§2.5:交互。
基础模型显示出改变人工智能系统开发人员和用户体验的明显潜力:基础模型降低了原型化和构建人工智能应用程序的难度门槛，因为它们具有适应的样本效率，并提高了新用户交互的上限，因为它们具有多模态和生成能力。
这提供了一种我们鼓励向前发展的协同作用:开发人员可以提供更适合用户需求和价值的应用程序，同时引入更动态的交互形式和反馈机会。
§2.6:知性哲学。
基础模型如何理解它所训练的数据?
以自然语言为例，我们确定了理解本质的不同立场，并探讨了它们与我们的中心问题的相关性。
我们的初步结论是，怀疑未来基础模型理解自然语言的能力可能为时过早，特别是在模型是在多模态数据上训练的情况下。
1.4.2应用概述。
目前，基础模型的研究主要局限于计算机科学和人工智能领域，基础模型的影响及其支持的应用主要集中在科技行业。
展望未来，基础模型呈现出明显的潜力，可以将人工智能的影响范围扩展到科技行业以外的许多领域，这表明人工智能对人们生活的影响将更加普遍。
虽然有许多应用程序和领域需要考虑，但我们选择了三个应用程序-医疗保健，法律和教育-因为它们代表了我们社会的基本支柱。
为了使基础模型对这些应用领域做出重大贡献，模型将需要特定的能力(§2:能力)以及技术创新(§4:技术)来考虑每个领域中的独特考虑。
此外，由于这些领域对社会功能至关重要(§5:社会)，在这些领域应用基础模型需要深入参与社会技术问题，例如与数据(§4.6:数据)、隐私(§4.7:安全)、可解释性(§4.11:可解释性)、公平性(§5.1:公平性)和伦理(§5.6:伦理)有关的问题。
§3.1:医疗保健和生物医药。
医疗保健任务(例如，通过疾病治疗对病人进行护理)和生物医学研究(例如，新疗法的科学发现)需要有限且昂贵的专家知识。
基础模型在这些领域提供了明确的机会，因为在许多模式(例如，图像，文本，分子)中有丰富的数据来训练基础模型，以及由于专家时间和知识的成本而提高的适应样本效率的价值。
此外，基础模型可能允许改进界面设计(§2.5:交互)，使医疗保健提供者和患者与人工智能系统交互，并且它们的生成能力表明了开放式研究问题(如药物发现)的潜力。
同时，它们也有明显的风险(例如，加剧医疗数据集和试验中的历史偏差)。
为了负责任地释放这一潜力，需要深入参与数据源和隐私以及模型的可解释性和可解释性等社会技术问题，同时对医疗保健和生物医学基础模型的使用进行有效监管。
§3.2:法律。
法律申请要求律师阅读并写出长而连贯的叙述，其中包含不断变化的背景，并解读模棱两可的法律标准。
基础模型可以在这个领域提供好处:大量的数据以法律文件的形式存在，它们的生成能力非常适合法律中需要的许多生成任务，但是需要对基础模型进行重大改进，以便能够可靠地对各种信息来源进行推理，以生成真实的长篇文档。
正如医疗保健(§3.1:医疗保健)中的护理一样，考虑到专家在法律领域的时间和知识成本，基础模型的适应样本效率具有更高的价值，这可能允许将专门知识重新分配给紧迫的司法和政府服务问题。
法律基础模型的负责任发展将需要特别考虑隐私，并强调现有基础模型的核心局限性，这将需要在其行为的来源方面取得根本性的进步，并保证其生成的真实性。
§3.3:教育。
教育是一个复杂而微妙的领域;有效的教学涉及对学生认知的推理，应反映学生的学习目标。
基础模型的性质在这里展示了人工智能教育领域尚未实现的前景:虽然教育中的某些数据流过于有限，无法训练基础模型，但利用领域外(例如，互联网)的相关数据和跨多种模式(例如，教科书、数学公式、图表、基于视频的教程)的数据的能力共同为广泛适用于教育任务的基础模型提供了希望。
如果基础模型导致教育相关能力的显著改善，那么与基础模型的开放式生成(例如，问题生成)和交互式(例如，对教师的反馈)方面相一致的新应用就有明显的潜力;基础模型的样本有效适应表明了更强的适应性和个性化学习能力。
在这种情况下，需要重新考虑将技术应用于教育的标志(例如，学生隐私)，以及某些问题变得更加关键(例如，在教育中获取技术的不平等，技术辅助抄袭)。
1.4.3技术概述。
现在我们讨论构建更好的模型架构、训练和适应程序，当然还有扩大系统规模背后的技术。
一个重要但经常被忽视的话题是数据——数据从何而来?数据的构成又是什么?
此外，我们希望基础模型对于分布转移是健壮的，并且对攻击者是安全的。
最后，我们希望了解为什么基础模型从数学角度和经验角度都有效。
§4.1:建模。
什么样的结构特性产生了基础模型?
在建模部分，我们将探索基础模型背后的底层体系结构，并确定5个关键属性。
首先，我们首先讨论计算模型的表达性(捕获和吸收现实世界的信息)和可伸缩性(熟练地处理大量高维数据)。
现有的架构(如变压器网络[Vaswani et al. 2017])已经成功地实现了这些特性，这些架构支撑着迄今为止大多数基础模型。
然后，我们继续讨论可能对下一代模型至关重要的属性，包括:多模态(用于消费、处理和潜在地产生来自不同来源和领域的内容)，记忆容量(用于有效存储和检索获得的知识)，以及最后的组合性(用于促进对新设置和环境的成功概括)。
我们相信，实现基础模型的全部潜力将取决于模型的进步，以满足这些愿望。
§4.2:培训。
训练目标以数学方式指定模型应该如何从训练数据中学习和获得能力。
目前训练基础模型的现状涉及特定于模态的目标(例如，用于文本的掩模语言建模[Devlin等人，2019]和用于图像的SimCLR [Chen等人，2020c])，这些目标通常是启发式选择的。
我们设想，基础模型的未来训练目标将反映两个变化:从系统证据和评估(§4.4:评估)中得出的原则性选择，以及跨数据源和模式提供丰富、可扩展和统一的训练信号的领域通用性。
我们还讨论了重要的设计权衡，包括生成与判别训练，输入数据表示的选择，以及涉及目标明确表示的未来训练目标的潜力。
§4.3:适应。
基础模型是中间资产;它们是未完成的，通常不应该直接使用，而是需要针对特定的下游任务进行调整。
事实上，适应的方法是微调，最近的工作表明，轻量级的微调替代方案和基于提示的方法可能实现有利的准确性和效率权衡。
展望未来，我们设想了一种更广泛的适应观点，它不仅仅是专门的基础模型来执行感兴趣的任务:适应将减轻独立基础模型的缺陷(例如，反映世界随时间变化的时间适应)或引入约束(例如，与被遗忘权相关的GDPR合规性;§4.7:安全);这种关于适应的更广泛的观点与需要新的评估协议(§4.4:评估)相一致，该协议在控制适应所涉及的资源(例如，运行时、内存)和访问要求的同时，系统地评估适应方法。
§4.4:评估。
评估通过提供跟踪进度、理解模型以及记录它们的能力和偏差的方法，为基础模型提供了上下文。
基础模型挑战了机器学习中标准评估范式实现这些目标的能力，因为它们离特定任务只有一步之遥。
为了设想适合基础模型的评估新范式，我们讨论了(a)直接评估基础模型以衡量其固有能力，并告知基础模型是如何训练的，(b)通过控制适应资源和获取来评估特定任务的模型，以及(c)更广泛的评估设计，以提供更丰富的背景，而不是测量准确性(例如，稳健性(第4.8节:稳健性)，公平性(第5.1节:公平性)，效率(第4.5节:系统)、环境影响(§5.3:环境))。
评估实践的改革将允许评估充分地服务于基础模型范例中涉及的不同目标和涉众。
§4.5:系统。
训练数据(§4.6:数据)决定了基础模型可用的理论信息，模型架构(§4.1:建模)和训练目标(§4.2:训练)决定了可以提取多少信息，计算机系统决定了实际可实现的内容。
就数据和模型大小而言，系统是扩展的关键瓶颈，这两者似乎都可靠地跟踪功能的改进。
为了确保我们能够有效地训练下一代基础模型(在时间和成本方面)，我们将需要算法、模型、软件和硬件的协同设计。
这种协同设计已经开始以各种形式出现，从精心调整的并行策略到新的架构，如基于检索和专家混合模型。
除了训练之外，我们还要考虑在基础模型之上部署应用程序所需要的东西(例如，有效的推理)。
§4.6:数据。
数据是基础模型的命脉;这些模型的训练数据在很大程度上决定了这些模型可以获得哪些能力。
数据的中心性并不是基础模型所独有的;最近对以数据为中心的人工智能的呼吁[Press 2021;[2021]表明了管理、理解和记录用于训练机器学习模型的数据的普遍重要性。
特别是对于基础模型，目前的操作方式是使用未指定或不明确的原则来选择训练数据，并且通常缺乏关于训练数据性质的透明度。
我们认为需要一种替代方法来重新构想围绕基础模型的数据生态系统:我们利用数据可视化和管理方面的工作，为基础模型提出一个数据中心。
我们阐明了该建议如何与基础模型的许多相关的以数据为中心的考虑因素相关联:选择、管理、文档、访问、可视化和检查、质量评估以及法律法规。
§4.7:安全和隐私。
目前，基础模型的安全性和隐私性在很大程度上是未知的。
从根本上说，基础模型是一个高杠杆的单点故障，使它们成为攻击的主要目标:现有的工作展示了这些模型的各种安全漏洞(例如，生成不希望的输出的对抗性触发器)或隐私风险(例如，训练数据的记忆)。
此外，基础模型的通用性使这些问题复杂化，加剧了功能蠕变或双重使用的风险(即，用于意想不到的目的)。
对于安全性，我们将基础模型看作类似于传统软件系统中的操作系统;我们讨论了安全基础模型的步骤，如果实现了，将为可靠的ML应用程序提供强大的抽象层。
对于隐私，通过利用来自公共数据的知识转移，基础模型可以使样本更有效地适应敏感数据分布，也就是说，在使用基础模型构建隐私保护应用程序时，可能会导致更少的准确性下降。
§4.8:对分布移位的鲁棒性。
标准机器学习的一个主要限制是，它产生的模型对分布变化不具有鲁棒性，其中训练分布与测试分布不匹配(对于下游任务)。
现有的研究表明，在广泛的未标记数据上训练的基础模型可以提高适应模型在各种变化中的鲁棒性。
这为改进鲁棒性基础模型的训练和适应开辟了一套新的有希望的方向。
然而，我们不认为基础模型是鲁棒性的灵丹妙药——诸如跨时间外推和虚假相关性等挑战不太可能得到充分解决。
§4.9:AI安全和对齐。
在考虑这些模型潜在的实际应用时，确保基础模型是可靠的(第4.5节:系统)、健壮的(第4.8节:健壮性)和可解释的(第4.11节:可解释性)变得越来越重要。
除了关键和直接的考虑之外，我们还考虑了基础模型与更大规模的风险、危害和危害之间的关系，随着模型能力的不断发展，这些风险、危害和危害可能会增加相关性。
例如，我们考虑对齐基础模型的重要性，这样它们就不会被错误指定的目标或价值所部署。
我们还讨论了预测基础模型的紧急行为(例如，欺骗或战略计划的能力)的相关性，这可能会使它们适应特定任务的尝试复杂化，并且可能需要新的可解释性方法(§4.11:可解释性)或评估方法(§4.4:评估)。
§4.10:理论。
学习理论为应用机器学习中遇到的各种情况提供了广泛的基础;理论提供了理解、原则和保证，以补充实证研究结果。
目前对基础模型的研究主要是经验性的，标准监督学习理论虽然比较成熟，但不足以充分解释基础模型。
具体来说，基础模型体系中训练阶段和适应阶段之间的差异指出了现有理论的不足，因为这些阶段对应(可能)完全不同的任务和数据分布。
尽管如此，我们努力在理论上的进步，以解决这种差异，即使在简单的，有限的设置，将提供有用的见解。
§4.11:可解释性。
可解释性为基础模型提供了清晰度:支持基础模型的深度神经网络的不透明性，以及基础模型的预期普遍性，提高了理解这些模型及其功能的需求。
目前的可解释性方法一般是为解释和解释特定任务模型的行为而设计的;基础模型的性质(即，这些模型有利于广泛的任务以及它们获得的意想不到的紧急属性)为可解释性研究带来了新的挑战。
为了讨论基础模型的可解释性，我们提出了一个模型-多个模型范式，其目的是确定一个模型(基础模型)和它的许多模型(其适应的衍生物)共享决策构建块的程度。
除了解释所涉及的决策组件外，我们还进一步讨论了基础模型背景下的可解释性(例如，模型产生的事后解释的有效性)以及驱动模型行为的机制(这可能阐明理解基础模型可以扩展到理解其适应衍生品的程度)。
鉴于可解释性在基础模型研究中的关键作用，我们最后评估了可解释性和不可解释性的社会影响。
1.4.4社会概况。
我们认为，基础模式的迅速发展，适应和部署到各种应用中，将对社会的健康产生广泛的影响。
这些模型既令人兴奋又令人不安的是它们的任务不可知性。
当我们谈论部署给用户的特定系统时，社会影响更容易理解和推理(但仍然是非琐碎的)，但是在开发基础模型时，我们如何考虑所有可能的系统和用例的社会影响呢?
§5.1:不公平和公平。
在许多情况下，机器学习已被证明会加剧或潜在地放大社会不平等。
基础模型可能会扩展这一趋势，即进一步对历史上受到歧视的人进行不公正的对待。
然而，理解不平等和基础模型之间的关系需要考虑基础模型的抽象;基础模型是适用于影响用户的应用程序的中间资产。
因此，我们描述了内在偏差，即基础模型中预示危害的属性，以及外在危害，即使用基础模型构建的特定应用环境中产生的危害。
我们对产生这些偏差和危害的各种来源(例如，训练数据，基础模型开发人员之间缺乏多样性，更广泛的社会技术背景)进行分类，强调来源追踪的重要性和技术难度，以理解伦理和法律责任。
在基础模型范式中，我们并不认为不公平是不可避免的:为了解决基础模型产生的不公平结果，我们双重考虑了主动干预(例如，反事实数据增强等技术方法)和被动追索权(例如，反馈传播机制和道德/法律责任归因)。
§5.2:滥用。
我们将基础模型滥用定义为使用基础模型，因为它们在技术上是有意的(例如，生成语言或视频)，但目的是造成社会危害(例如，生成虚假信息，开发深度伪造骚扰)。
我们认为，基础模型的进步将导致更高质量的机器生成内容，这将更容易创建和个性化用于误用目的。
例如，虚假信息行动者可能会使用它们来快速生成针对不同人口群体(例如，国籍，政党，宗教等)的文章集合。
虽然这些新功能可能会限制现有的人类对有害内容的检测方法(例如，跟踪不同来源的类似文本)，但基础模型本身可能提供有希望的自动化误用检测器。
§5.3:环境。
基础模型是计算昂贵的训练制度的副产品，现有的轨迹更倾向于更密集的模型;这种训练所需的能量与向大气中释放更多碳和环境退化相吻合。
目前，当前的讨论集中在这些巨大的一次性培训成本和在重复使用中摊销这些成本的潜力上。
我们试图通过确定塑造基础模型的环境影响演算的假设来澄清这些讨论。
此外，我们设想围绕基础模型的生态系统需要多方面的方法:(a)更高效的计算模型、硬件和能源网格都可以减轻这些模型的碳负担，(b)环境成本应该是一个明确的因素，告知如何评估基础模型(§4.4);评估)，使基础模型可以更全面地与更环保的基线并列，以及(c)围绕环境影响的成本效益分析需要在整个社区进行更多的文件编制和测量。
§5.4:合法性。
目前的基础模型依赖于脆弱的法律基础;法律如何影响这些模型的发展和使用在很大程度上是不清楚的。
将需要专门针对基础模型的法律和监管框架，以及更广泛地针对人工智能技术的法律和监管框架，以影响、约束甚至促进研究、开发和部署方面的实践。
以美国的法律环境为中心，现有的算法工具的考虑仍然广泛不确定，我们强调了模型预测的责任和对模型行为的保护的相关问题。
关于这两个问题，我们描述了在给定基础模型的中介地位(与面向用户的特定任务模型相对)的情况下，法律标准将如何被提升以解决这些问题。
§5.5:经济学。
基础模型由于其新颖的能力和在各种行业和职业中的潜在应用，可能具有实质性的经济影响。
我们考虑基础模型的发展和使用对美国和全球经济未来的影响，重点关注生产率、工资不平等和所有权集中。
§5.6:规模伦理。
除了增加不平等的风险，如§5.1:公平所讨论的，基础模型的广泛采用带来了其他伦理、政治和社会问题。
我们讨论了与基础模型应用规模相关的伦理问题，如同质化和权力集中，以及解决这些问题的规范和释放策略。
2功能
基础模型获得能力，其中一些从它们的学习过程中出人意料地出现，这些能力为下游应用程序提供动力(§3:应用程序)。
具体来说，我们讨论了语言(第2.1节:语言)和视觉(第2.2节:视觉)能力以及影响物理世界(第2.3节:机器人)，执行推理和搜索(第2.4节:推理)以及与人类交互(第2.5节:交互)的能力。
此外，我们讨论了自我监督(用于学习大多数最新基础模型的技术方法)如何在哲学上与理解能力相关(§2.6:哲学)。
2.1语言
作者:Isabel Papadimitriou, Christopher D. Manning
人类语言的本质。
语言是大多数人类交流和互动的基础。
然而，它不仅仅是人类实现共同目标的一种手段:语言是人类思想的核心，是社会和情感关系的形成，是我们如何在社会和个人中识别自己，是人类如何记录知识和发展社会智能的核心。
口语或手语出现在每个人类社会，世界上的语言在表达和组织信息的方式上都有着令人难以置信的多样性，同时在构成语言的丰富性方面也表现出惊人的一致性[Comrie 1989]。
语言是非常复杂而有效的系统，儿童在短时间内不断获得语言，并随着语言社区不断变化的需求和条件而发展。
由于语言在人类活动中的中心地位，语言的理解和生成是人工智能研究的一个关键因素。
自然语言处理(NLP)是与语言相关的人工智能的子领域，与自动语音识别(ASR)和文本到语音(TTS)的相关领域一起，其目标是使计算机能够以与人类相同的方式理解和生成人类语言。
到2021年为止，NLP是受基础模型影响最深刻的领域。
第一代基础模型展示了令人印象深刻的各种语言能力，以及对大范围语言情况的惊人适应性。
自2018年引入早期基础模型ELMo [Peters et al. 2018]和BERT [Devlin et al. 2019]以来，NLP领域已经主要集中在使用和理解基础模型上。
该领域已经转向使用基础模型作为主要工具，并将更广义的语言学习作为中心方法和目标。
在本节中，我们回顾了NLP中基础模型的最新成功，详细介绍了基础模型如何改变了训练语言机器学习模型的整体过程和心态，并讨论了基础模型在应用于更广泛的语言和更现实和复杂的语言情境时面临的一些理论和实践挑战。
2.1.2基础模型对自然语言处理的影响。
基础模型对自然语言处理领域产生了巨大的影响，现在是大多数自然语言处理系统和研究的核心。
首先，许多基础模型是熟练的语言生成器:例如，Clark等人[2021]证明，非专家很难区分由GPT-3编写的短格式英语文本和人类编写的短格式英语文本。
然而，基础模型在NLP中最具影响力的特征不是它们的原始生成能力，而是它们惊人的通用性和适应性:为了实现许多语言任务，单个基础模型可以以不同的方式进行调整。
历史上，NLP领域一直专注于为具有挑战性的语言任务定义和工程系统，并认为擅长这些任务的模型将为下游应用程序提供合格的语言系统。
NLP任务包括整个句子或文档的分类任务(例如，情感分类，如预测电影评论是积极的还是消极的)，序列标记任务，其中我们对句子或文档中的每个单词或短语进行分类(例如，预测每个单词是动词还是名词，或者哪个词的范围指的是一个人或一个组织)，跨度关系分类，(例如，关系提取或解析，比如一个人和地点是否通过“当前居住地”关系联系在一起，或者一个动词和一个名词是否通过“主-动词”关系联系在一起)和生成任务，产生有条件的新文本。世界上有超过6000种语言，由于构成一种独立语言的内在不确定性，估计会有所不同[Nordhoff和Hammarström 2011]。
这张地图显示了世界上的语言，每个点代表一种语言，其颜色表示顶级语系。
数据来自Glottolog [Hammarström et al. 2021]。
我们在地图上标出几种语言作为例子。
对输入(例如，生成文本的翻译或摘要，识别或生成语音，或在对话中响应)的影响很大[Jurafsky和Martin 2009]。
在过去，NLP任务有不同的研究团体，他们开发了特定于任务的架构，通常基于不同模型的管道，每个模型执行一个语言子任务，如标记分割、语法解析或共同引用解析。
相比之下，执行每个任务的主要现代方法是使用单个基础模型，并使用特定于每个任务的相对少量的注释数据(情感分类、命名实体标记、翻译、摘要)对其进行稍微调整，以创建一个适应的模型。
这已经被证明是一种非常成功的方法:对于上面描述的绝大多数任务，稍微适应任务的基础模型大大优于先前专门为执行该任务而构建的模型或模型管道。
仅举一个例子，在基础模型之前，2018年回答开放式科学问题的最佳系统可以在纽约大学八年级科学考试中获得73.1%的成绩。
一年后的2019年，一个改编的基础模型得分为91.6% [Clark et al. 2019]。
基础模型的出现在很大程度上是为了生成语言而训练的，这构成了语言生成在NLP中角色的重要转变。
直到2018年左右，生成通用语言的问题被认为是非常困难的，基本上无法解决，除非通过其他语言子任务[Paris et al. 2013]。
相反，NLP研究主要集中在语言分析和理解文本。
现在，有可能用一个简单的语言生成目标来训练高度连贯的基础模型，比如“预测这个句子中的下一个单词”。
这些生成模型现在构成了完成语言机器学习的主要工具——包括曾经被认为是生成的先决条件的分析和理解任务。
基础模型所展示的成功生成也导致了对语言生成任务(如摘要生成和对话生成)的研究蓬勃发展。
基础模型范式的兴起已经开始在口语和书面语中发挥类似的作用。
现代自动语音识别(ASR)模型，如wav2vec 2.0，仅在语音音频的大型数据集上进行训练，然后对具有相关转录的音频进行调整，以完成ASR任务[Baevski等人，2020]。
由于基础模型范式带来的变化，NLP研究和实践的重点已经从为不同的任务定制架构转向探索如何最好地利用基础模型。
对适应方法的研究已经蓬勃发展(参见§4.3:适应，了解适应的详细情况)，基础模型的惊人成功也导致研究兴趣转向分析和理解基础模型(参见§4.11:基础模型的可解释性和分析的可解释性)。
2.1.3语言变异和多语性。
尽管基础模型对于从预训练中获得的语言知识具有惊人的通用性，但这种适应性存在局限性:目前尚不清楚基础模型在处理语言变化方面有多成功。
语言差异很大。
除了世界上有成千上万种不同的语言这一事实之外，即使在一种语言或一个说话者的内部，语言也会发生变化。
举几个例子，非正式谈话与书面语言表现不同，人们与朋友交谈时使用的语法结构与与权威人士交谈时使用的语法结构非常不同，使用同一种语言的社区使用不同的方言。
社会和政治因素嵌入在如何看待和评价语言变化中，以及在NLP研究中代表了多少不同的品种中(例如，参见Blodgett和O'Connor[2017]关于非裔美国英语NLP的失败，以及§5.1:关于基础模型不平等的更深入讨论的公平性)。
由于基础模型具有学习语言信息和灵活适应这些知识的能力，因此有望扩展NLP以涵盖更多的语言多样性。
了解是否有可能建立基础模型，使其强大而公平地代表语言的主要和微妙变化，并对每种语言变体的独特之处给予同等的重视和灵敏度，这仍然是一个开放的研究问题[提出和解决这个问题的研究包括Ponti等人。2019;本德2011;Joshi et al. 2020]。
继英语基础模型的成功之后，多语言基础模型已经发布，将这种成功扩展到非英语语言。
对于世界上超过6000种语言中的大多数，可用的文本数据不足以训练大规模的基础模型。
举个例子，西非语言富拉语(Fula)的使用者超过6500万，但富拉语的NLP资源很少(如果有的话)[Nguer et al. 2020]。
多语言基础模型通过同时对多种语言进行联合训练来解决这个问题。
迄今为止的多语言基础模型(mBERT、mT5、XLM-R)都是在大约100种语言上训练的[Devlin等人，2019;Goyal et al. 2021;薛等。2020]。
联合多语言训练依赖于一个合理的假设，即语言之间的共享结构和模式可以导致从高资源语言到低资源语言的共享和迁移，从而使我们无法训练独立模型的语言的基础模型成为可能。
使用和分析多语言基础模型的实验表明，在多语言基础模型中，不同语言之间的迁移和并行编码确实存在惊人的数量[Wu and Dredze 2019;Choenni和Shutova 2020;Pires等人，2019;libovickhu et al. 2019;Chi et al. 2020;Papadimitriou et al. 2021;Cao et al. 2019]。
然而，这些模型在多大程度上是健壮的多语言仍然是一个悬而未决的问题。
目前尚不清楚在这些数据上训练的模型有多少可以代表与英语截然不同的语言方面，或者很少有语言资源可用[Wu and Dredze 2020]，以及它们明显的多语言表现是否更多地依赖于同化[Lauscher et al. 2020];Virtanen et al. 2019;Artetxe et al. 2020]。
多语言模型在与其训练数据中资源最高的语言相似的语言中表现出更好的性能，并且已有研究表明，多语言模型中的语言会竞争模型参数，因此不清楚单个模型中可以适应多少变化[Wang et al. 2020d]。
一个突出的问题源于我们用于训练多语言基础模型的数据:在许多多语言语料库中，英语数据不仅比低资源语言丰富得多，而且通常更干净、更广泛，并且包含展示更多语言深度和复杂性的示例[Caswell等人，2021](参见Nekoto等人[2020]关于构建参与性和鲁棒性多语言数据集)。
然而，答案并不仅仅在于创造更平衡的语料库:语言变化的轴线太多了，创造一个在所有方面都平衡和具有代表性的语料库是不可实现的。
基础模型的未来、多功能性和公平性都取决于在数据不平衡的情况下稳健地处理语言变化[例如，Oren等人，2019]。
当前多语言基础模型的原始形式，以及幼稚的无监督多语言训练作为一种方法，可能无法充分模拟语言和语言变体的微妙之处。
然而，它们对于一些多语言应用仍然有用，例如，通过调整多语言模型来适应原始训练集之外的低资源语言[Wang等人。2020b]。
此外，(非公开的)GShard神经机器翻译模型的结果显示，在最低资源语言的单语言基线上，收益最大，收益随着模型规模的增加而增加[Lepikhin et al. 2021]。
研究界应该批判性地检查基础模型如何处理语言变化，了解基础模型在为NLP带来公平和代表性方面的局限性，而不是满足于推广消除语言变化的基础模型，并且在其训练数据中大部分符合语言多数。
2.1.4来自人类语言习得的启示。
尽管基础模型在创建更像人类的NLP系统方面取得了巨大的进展，但它们获得的语言系统以及学习过程仍然在很大程度上不同于人类语言。
理解机器和人类语言学习之间的这种差距的含义是发展一个了解基础模型的语言局限性和可能性的研究社区的必要组成部分。
人类语言习得非常有效:像GPT-3这样的基础模型所训练的语言数据比大多数人听过或读过的语言数据多三到四个数量级，当然也比孩子们在语言能力基本具备时接触到的语言数据要多得多。
基础模型和人类语言习得之间的一个显著区别是，人类语言是基于现实世界的[Saxton 2017]。
例如，婴儿和看护人在语言发展过程中会指向物体[Colonnesi et al. 2010]，婴儿在学习语言系统的许多其他方面之前，会先学习涉及常见物体的单词的基本含义[Bergelson and Swingley 2012]。
另一方面，NLP中使用的大多数基础模型都是从原始的、没有基础的文本的分布信息中学习，并且(与人类学习者相反)Zhang等人[2021]表明RoBERTa模型在可用意义之前表达抽象的句法特征。
强大的非基础统计学习确实也存在于婴儿中[Saffran等人，1996]，因此它无疑是习得的一个重要因素。
然而，推进基础模型的基础语言学习仍然是提高人类习得效率的重要方向[Dupoux 2018;Tan and Bansal 2020;Zellers等人。2021a，除其他外](参见§2.2:视觉和§2.3:基础模型的多模态潜力的机器人，以及§2.6:哲学，讨论基础模型是否可以在没有基础的情况下理解语言)。
另一个重要的方向是检查基础中的归纳偏差(图6)。
人类语言习得与基础模型。
虽然人类大脑和基础模型之间肯定存在不同的归纳偏差，但他们学习语言的方式也非常不同。
最明显的是，人类与一个物质和社会世界互动，在这个世界中，他们有各种各样的需求和愿望，而基础模型主要是观察和模拟他人产生的数据。
模型以及它们如何与人类思维中的归纳偏见(包括语言学习特有的偏见和人类认知的普遍偏见)相关联[Linzen和Baroni 2021]。
尽管人类大脑在结构上可能更专门于有效的语言习得，但基础模型并不是空白学习者[Baroni 2021]，理解和调整这些语言归纳偏差是基础模型研究的一个重要未来方向。
语言习得效率的一个重要因素是人类习得了一个系统的、可概括的语言系统。
尽管关于人类语言系统产生哪些类型的理论抽象存在许多不同的理论[例如，Comrie 1989;乔姆斯基2014;克罗夫特2001;Jackendoff(2011)]，人们普遍认为，人类学习语言的方式使他们能够轻松地将新知识插入现有的抽象概念中，并有效地创造新的语法句子。
例如，一个十岁的孩子已经获得了很多关于他们的语言是如何工作的抽象概念，尽管他们实际产生的单词和结构在接下来的十年中将发生巨大的变化。
另一方面，基础模型通常不能获得我们期望从人类那里获得的系统抽象。
例如，当基础模型一次准确地产生语言结构时，不能保证该结构的未来使用将基本一致，特别是在主题发生重大领域转移之后[检查基础模型在系统性方面局限性的工作示例包括Lake和Baroni 2018;Kim和Linzen 2020;Bahdanau et al. 2018;Chaabouni et al. 2021]。
NLP面临的挑战是在基础模型的获取中开发某种系统性，而不回归到过于依赖严格语言规则的系统。
语言学习将伴随使用者的一生:人类语言的语法会不断进化，人类会灵活地适应新的语言情境[Sankoff 2018]。
例如，当成年人生活中出现新的术语和概念时，他们可以相对容易地在语法句子中使用它们，并且人类经常调整他们的语法模式以适应不同的社会群体[Rickford et al. 1994]。
另一方面，基础模型的语言系统大多由训练数据设置，相对静态[Lazaridou et al. 2021;Khandelwal等。2020]。
虽然适应方法可以为不同的任务初始化基础模型(参见§4.3:adaptation)，但如何在不进行大量训练的情况下改变基础模型更基本的语言基础，目前还不清楚。
构建自然反映人类语言适应和语言进化的适应性模型是基础模型未来的一个重要研究领域。
基础模型极大地改变了自然语言处理的研究和实践。
基础模型为社区带来了许多新的研究方向:理解生成是语言的一个基本方面，研究如何最好地使用和理解基础模型，理解基础模型可能增加NLP不平等的方式，研究基础模型是否能令人满意地涵盖语言变异和多样性，以及寻找利用人类语言学习动态的方法。
在基础模型之前，研究界关注的大多数复杂的NLP任务现在都可以使用少数公开发布的基础模型来处理，达到接近人类的水平。
然而，在这种性能和在复杂的下游环境中有效和安全部署基础模型的需求之间仍然存在显著的差距。
2.2视觉
作者:Shyamal Buch, Drew A. Hudson, Frieda Rong, Alex Tamkin，张锡昆，Bohan Wu, Ehsan Adeli, Stefano Ermon, Ranjay Krishna, Juan Carlos Niebles，吴家军，
通过大规模地利用自我监督，视觉基础模型有可能将原始的、多模态的感觉信息提炼成视觉知识，这可能有效地支持传统的感知任务，并可能在具有挑战性的高阶技能上取得新的进展，如时间和常识推理(§2.2.1:视觉能力)。
这些输入可以来自不同的数据源和应用领域，这表明在医疗保健和具体化的交互式感知设置中的应用前景(§2.2.2:视觉挑战)。
图片来源[Zamir et al. 2018;Haque et al. 2020]。
视觉是生物体理解环境的主要方式之一。
视觉的能力使我们能够近乎恒定、远距离地收集密集的信号，这是在进化的时间尺度上在各种生命形式中发展起来的一种关键能力[Parker 2003;Zhang and Shu 2021]。
即使是简单的生物也可以毫不费力地完成一项技能，但将同样的能力转移到机器上已经证明是非常具有挑战性的，1988年，计算机视觉和机器人研究人员汉斯·莫拉维克(Hans Moravec)观察到一个悖论:在人工智能中，(我们认为的)困难的问题很容易，同样简单的问题也很难，其中“最简单”的问题是我们每天使用的视觉灵敏度，它可以在几毫秒内不断地解释复杂的场景[莫拉维克1988;Thorpe et al. 1996;飞飞等。2007]。
在这一艰巨挑战的另一端，是计算机视觉所掌握的大量变变性应用的关键:可以将通勤者从交通堵塞中解放出来的自动驾驶汽车(第2.3条:机器人技术)，可以通过检测罕见医疗事件(第3.1条:医疗保健)来帮助过度劳累的专家的救生人工智能工具，用于多媒体创作和编辑的下一代工具(第2.5条:交互)等等。
反思人类感知是工具的应用和设置，可以让我们了解计算机视觉可以帮助和改变的潜在领域。
计算机视觉领域和我们定义的挑战在许多方面都从人类感知能力中汲取灵感。
几个经典理论[例如，Biederman 1972;McClelland and Rumelhart 1981;Marr 1982]认为，人类可能通过将部分作为一个更大的整体来感知现实世界的场景，并指出了计算机视觉技术以不断增长的抽象水平逐步模拟物理世界的方式[Lowe 1992;Girshick et al. 2014]。
Gibson[1979]认为人类的视觉具有内在的体现，互动的生态环境可能在其发展中发挥关键作用。
这些想法继续激励着计算机视觉系统的持续发展，迭代到上下文，交互和具体化的世界感知。
在计算机视觉的背景下，基础模型将来自不同来源和传感器的原始感知信息转化为视觉知识，可以适应多种下游设置(图7)。
在很大程度上，这种努力是过去十年来该领域出现的关键思想的自然演变。
ImageNet的引入[Deng et al. 2009]和监督预训练的出现导致了计算机视觉中深度学习范式的转变。
这一转变标志着一个新时代的到来，我们超越了早期的经典方法和特定于任务的特征工程[Lowe 2004;Bay等人，2006;Rosten和Drummond, 2006]朝着可以在大量数据上训练一次的模型，然后适应各种各样的任务，如图像识别，目标检测和图像分割[Krizhevsky等人，2012;Szegedy et al. 2015;He et al. 2016a;Simonyan and Zisserman 2015]。
这个想法仍然是基础模型的核心。
通往基础模型的桥梁来自于之前范例的局限性。
传统的监督技术依赖于昂贵且精心收集的标签和注释，限制了它们的鲁棒性、泛化和适用性;相比之下，自监督学习的最新进展[Chen et al. 2020c;他等人[2020]提出了开发基础模型的另一种途径，该模型可以利用大量原始数据来获得对视觉世界的上下文理解。
相对于该领域更广泛的目标，视觉基础模型的当前能力目前处于早期阶段(§2.2.1:视觉能力):我们已经观察到传统计算机视觉任务的改进(特别是在泛化能力方面)[Radford et al. 2021;Ramesh et al. 2021]并预计近期的进展将延续这一趋势。
然而，从长远来看，基础模型减少对显式注释依赖的潜力可能会导致基本认知技能(例如，常识推理)的进步，这在当前的全监督范式中已被证明是困难的[Zellers等人，2019a;Martin-Martin et al. 2021]。
反过来，我们讨论了基础模型对下游应用的潜在影响，以及必须解决的核心挑战和前沿(§2.2.2:视觉挑战)。
2.2.1关键能力和方法。
在高层次上，计算机视觉是人工智能的核心子领域，它探索如何赋予机器解释和理解视觉世界的能力。
它包含了大量的任务、子领域和下游应用，在过去的几十年里，社区在这些领域取得了持续的进步[Zamir等人，2018]。
示例任务16:(1)语义理解任务，旨在发现视觉场景中实体之间的属性和关系;这些包括图像分类、目标检测、语义分割、动作识别和场景图生成等[例如，Krizhevsky等人，2012;He et al. 2016a;Krishna et al. 2017;Russakovsky et al. 2015;Krizhevsky et al. 2009;Kay et al. 2017;Lin et al. 2014]。
(2)几何、运动和3D任务，寻求表示静止或运动物体的几何、姿态和结构，包括深度估计、运动结构、表面法线检测、曲率线和关键点估计等任务[例如，Laina et al. 2016;Agarwal et al. 2011;Wang et al. 2015a;Zamir et al. 2018;Ullman 1979]。
(3)多模态整合任务，将语义和几何理解与自然语言等其他模态相结合;这些包括，例如，视觉问题回答，图像字幕和指令跟随[例如，安托尔等人，2015;Chen et al. 2015b;Anderson et al. 2018;Goyal等，2017b;哈德逊和曼宁2019b;Johnson et al. 2017;Luo et al. 2020;Akbari et al. 2021;黄等。2021c;Tsimpoukelli et al. 2021]。
我们在图7中突出显示了传统核心任务的一个子集。
在2010年代早期ImageNet [Deng et al. 2009]的出现推动下，解决这些任务的主要范式倾向于围绕一个熟悉的核心思想:首先，在大量精心注释的数据集上预训练模型[Russakovsky et al. 2015]，并使用完全监督的训练任务，如图像分类。
然后，在任务特定的数据集和领域下游调整模型[Lin et al. 2014;Chen et al. 2015b;Antol et al. 2015]通过微调达到最先进的性能[Krizhevsky et al. 2012;Simonyan and Zisserman 2015;He et al. 2016a;Xu and Saenko 2016]。
在我们现在考虑的基础模型的定义中，这种预训练之后的适应的概念仍然存在(§1:介绍)。
这种完全监督范式的局限性激发了向基础模型的过渡:对外部监督注释的依赖限制了以前方法的上限能力，无法以可扩展、鲁棒和可推广的方式捕获视觉输入的各种光谱。
视觉合成和无监督学习领域的最新发展提供了一个令人信服的选择。
例如，GANs学习生成高保真度、真实感和多样性的视觉内容，其特点是两个相互竞争的网络，一个生成器和一个判别器可以从图像集合中相互监督[例如，Goodfellow et al. 2014;Hudson and Zitnick 2021]。
其他神经模型通过采用变分自编码、对比学习或其他自监督技术，在没有明确注释的监督下推断物体和场景的视觉属性[例如，Kingma和Welling 2014;Chen et al. 2020c;他等人。2020]。
例如，He等人[2021]建立在先前使用掩码图像编码的表示学习工作的基础上[例如，Pathak等人2016;Vincent et al. 2008]通过部分地结合柔性架构的最新进展(例如，视觉变压器[Dosovitskiy et al. 2021;Zhai et al. 2021])。
有了基础模型，这种自我监督技术的发展使得在更大尺度的视觉数据上进行训练成为可能[Changpinyo et al. 2021]，无论是在范围上还是在潜在的多样性上。
因此，我们已经看到了传统视觉任务在标准精度度量和少量泛化方面取得进展的早期指标。
对于图像分类和目标检测，自监督技术已经报告了与之前的全监督方法竞争的性能[He et al. 2019;Chen et al. 2020c;Radford et al. 2021;hsamnaff et al. 2021]，在训练过程中没有明确的注释，在适应过程中具有更高的样本效率。
对于视觉合成，值得注意的例子包括DALL-E [Ramesh et al. 2021]和clip引导生成[Radford et al. 2021;Galatolo et al. 2021]，研究人员利用多模态语言和视觉输入来呈现引人注目的视觉场景。
在短期内，随着训练目标的细化，我们预计这些基础模型的能力将沿着这些方向继续提高[Chen等人，2020a;hsamnaff et al. 2021;Selvaraju et al. 2021]，架构设计纳入了额外的模式[Jaegle et al. 2021b]。
值得注意的是，相对于NLP(§2.1:语言)，当前计算机视觉的基础模型还处于萌芽阶段:有希望的早期努力仍然主要集中在RGB图像输入和核心传统视觉任务的子集上。
然而，该领域继续在以具身和交互式感知设置为中心的更广泛挑战上取得进展(对于机器人的基础模型至关重要[Bohg等人，2017，§2.3:机器人])。
我们在图7中注意到这些高阶目标的一个子集，包括物理场景理解、对视觉常识和时间事件的推理以及对社会启示的感知。
这些都是全监督系统的目标，但由于大规模注释这些任务的困难，已经被证明具有挑战性。
例如，用于视觉问题回答的标准系统很难回答需要常识性理解的问题，因为这些问题通常需要超出像素本身的外部知识[Zellers et al. 2019a]。
以稳健的方式感知人类凝视和社会支持仍然是交互式代理中具身视觉系统面临的挑战[Martin-Martin et al. 2021]。
通过减少对显式注释的依赖，基础模型可能比以前可行的更能实现这些目标。
语言基础模型(§2.1:language)的相关进展已经能够捕获语言事件的一定程度的常识[Brown et al. 2020]，也表明了在多模态视觉输入上实现类似能力的潜在途径。
虽然如何在基础模型中实现这些能力的确切路线图仍然是一个悬而未决的问题，但新的高效和灵活的体系结构(第4.1节:建模)、大规模训练(第4.5节:系统)、自我监督技术(第4.2节:训练)和少量适应方案(第4.3节:适应)的组合可能会打开通往迄今为止难以达到的能力的大门。
2.2.2中心研究挑战。
我们对研究挑战的讨论是由下游应用领域驱动的，其中基础模型可能进一步集成和影响视觉模型。
我们强调了一些这样的领域:(1)医疗保健和家庭环境的环境智能:基于这些环境中环境智能的现有方法[Haque等人，2017;Lyytinen and Yoo 2002;Hong and Landay 2004]，基础模型可能提供更好地检测细颗粒人类活动和医疗事件的潜力，以及改善临床医生、患者和日常消费者的辅助互动(另见§3.1:医疗保健)。
(2)移动和消费者应用:具有更强多模态基础的基础模型可以使移动设置中的服务具有更强的交互性，并且从视觉和语言输入生成能力的根本改进可以使计算摄影和内容编辑应用受益[Delbracio等人，2021];Ramesh et al. 2021;Park等人。2019](另见§2.5:相互作用)。
(3)具身的交互式代理:感知模型已经被证明是有效的输入[Sermanet et al. 2018]和奖励函数[Chen et al. 2021c];Shao等人。2020]机器人设置;在大量以自我为中心的(真实/模拟、人类/机器人)视觉数据集上训练的基础模型[Damen等人，2018;Chen等人[2021e]可能会通过捕获更广泛的视觉场景、物体和动作来进一步推进这一进展(参见§2.3:机器人技术)。
基础模型可能进一步影响这些应用程序设置的程度取决于§2.2.1:视觉功能实现的程度。
为了弥合当前、短期和长期预期能力之间的重大差距，我们必须解决当前远景基础模型的局限性，包括它们的训练和评估。
下面是相应关键挑战的子集:语义系统性和感知稳健性。
人类在将视觉理解推广到看不见的构图以及对新物体和场景的物理和几何特性进行推理方面具有非凡的能力[Lake et al. 2015]。
虽然目前的基础模型在图像合成方面表现出了很好的能力，并且在细粒度语言输入的泛化方面也取得了早期成果，但这些模型仍然难以泛化到简单形状和颜色的组合[Ramesh et al. 2021;Radford et al. 2021;荣2021]。
概括性也超越了语义学;视觉场景和物体的物理动力学和几何特性具有自然的规律性。
基础模型已经显示出理解场景和物体几何的早期迹象[Ramesh et al. 2021]。
此外，感知模型中对物理场景和几何理解的早期努力可能为正在进行的基础模型开发提供指导[Yi等人，2019;Bakhtin et al. 2019;李等。2020b]。
事实上，在基础模型中持续整合多种模式(例如音频)可能有助于实现这些目标[Zhang等人，2017;Gao等。2020b;[j]。
然而，对于基础模型来说，将初始观测能力稳健地推广到人类水平的广泛自然场景和对象的具体技术仍然是一个开放的研究挑战。
计算效率和动力学建模。
人类在处理物体、场景和事件的连续视觉流以支持对事件动态的理解方面具有惊人的效率[Zacks等人，2001;Tversky and Zacks 2013]。
语言的基础模型(§2.1:语言)已经显示出对事件的长期一致性建模的初步步骤;在视觉输入中捕捉远程时间相关性和因果一致性的类似能力将有利于机器人等下游环境[Dai等人，2019;Alyamkin et al. 2019;Goel et al. 2020b;Feng等。2019，§2.3:机器人[j]。
然而，相对于语言中的单词令牌级输入，低级计算机视觉输入是非常高维的:单个1080p帧包含超过200万像素。
在这种情况下，在远程视频序列中建模更丰富的事件动态似乎是一项艰巨的努力，特别是有额外的模式(例如，语音，光流等)和不断增加的分辨率。
可以理解的是，完全处理每个像素的naïve方法可能是令人望而却步的。
当前的视觉模型[例如，Radford et al. 2021;Sun et al. 2019a;Tan and Bansal 2019;Kim等人[2021a]经常通过处理嵌入来解决这个问题，这些嵌入汇总了图像补丁甚至帧组，但这有失去细粒度细节的潜在缺点[Ramesh等人]。
除了考虑原始输入空间之外，视觉的基础模型可能需要重新审视基础架构原语的设计(§4.1:建模)，以实现高效和有效的建模:3D卷积的替代方案可能更好地解决其立方复杂性[Fan et al. 2020;Sitzmann et al. 2019]，而基于粒子的表示可能更有效地建模物理动力学[Bear et al. 2021]。
此外，将这些视觉模型部署到下游应用程序设置也将需要系统设计的进步(§4.5:系统)。
综上所述，大规模、动态视觉输入的高效和有效建模的瓶颈仍然是一个必须解决的多方面的研究方向。
培训、环境和评估。
实现基础模型的潜力同样重要的是训练和评估它们的支持元素。
目前的视觉基础模型主要集中在图7所示的一小部分模式上(例如，RGB图像和文本的数据集)，因为这些可能是最容易访问的[Changpinyo et al. 2021;Radford et al. 2021]。
这促使开发和使用额外的大规模训练数据集，这些数据集包含广泛模式的各种输入集合。
虽然额外的注释可能不是严格必要的，但输入质量会影响模型的学习效率;利用其他类型的基础模型(例如，语言)来帮助提高质量的技术是一条很有前途的道路[Zellers et al. 2021b]。
我们还想考虑静态数据集之外的环境:经典研究表明，人类的感知理解与其体现和互动生态环境有关[Gibson 1979]。
作为实现体现和交互的长期能力的垫脚石(§2.3:机器人技术)，通过多种模式和视角捕捉物理、视觉和生态现实主义的模拟环境的持续发展可能在为这一目标提供可扩展和高保真的视觉输入方面发挥重要作用[Kolve等人。2017a;Savva et al. 2019b;Gan et al. 2020;沈等。2021a;Srivastava等。2021]。
最后，还有度量的问题:我们如何评估生成基础模型输出相对于语义的可靠性?
标准指标，如fr<s:1> Inception Distance，存在已知缺陷[Bińkowski et al. 2018];这些问题与自然语言处理中的问题类似(例如，BLEU等指标与人类的因果判断无关)。
将人工判断作为评估的一部分可能是一种途径，但会产生巨大的成本，并且可能不具有可扩展性[Zhou et al. 2019;Khashabi et al. 2021]。
围绕视觉基础模型的训练(§4.2:训练)、数据(§4.6:数据)和评估(§4.4:评估)设置的突出和开放的挑战确实非常微妙，并且将成为未来研究的核心领域。
结束语。
在本节中，我们探讨了计算机视觉背景下的基础模型，从识别以前的计算机视觉范式的根源，到将其当前和预期的能力置于背景中，再到提出未来的研究方向。
最后，我们简要讨论了计算机视觉基础模型及其持续发展的一些更广泛的社会影响(参见§5:社会)。
相机在我们的社会中无处不在，这意味着计算机视觉技术的进步具有巨大的破坏性影响潜力;这带来了相应的责任负担，需要仔细考虑其风险。
计算机视觉模型中学习偏差的历史记录很好，导致代表性不足的群体的准确性较低和相关错误，因此不适当和过早地部署到一些现实世界的环境中[例如，Buolamwini和Gebru 2018，§5.1:公平性]。
许多相同的潜在问题在当前的基础模型中继续存在[Agarwal等人，2021]。
随着来自其他传感器模式(例如，可穿戴或环境传感器，图7)的数据被纳入这些基础模型，围绕隐私和监视的担忧变得至关重要(参见§5.6:道德)。
此外，随着视觉基础模型的语义和生成能力不断增长，生成的深度假图像和错误信息会带来更大的风险[Dolhansky et al. 2020;Ramesh等。2021，§5.2:误用]。
尽管计算机视觉和基础模型面临着巨大的挑战和机遇，但同时解决这些挑战和相关风险仍然至关重要。
2.3机器人
作者:Siddharth Karamcheti, Annie Chen, Suvir Mirchandani, Suraj Nair, Krishnan Srinivasan, Kyle Hsu, Jeannette Bohg, Dorsa Sadigh, Chelsea Finn图8构建机器人的新型基础模型需要跨越不同环境和行为的海量数据集。
仿真、机器人交互、人类视频和自然语言描述都可以成为这些模型的有用数据源。
尽管在获取数据方面存在挑战，但开发新的机器人基础模型对于任务规范和机器人学习中的各种问题表述具有巨大的潜力。
图片来源:[Finn et al. 2016b;Szot et al. 2021]。
机器人研究的一个长期挑战是赋予机器人处理它们在现实环境中遇到的无数条件的能力。
在本节中，我们将讨论基础模型背后的想法如何潜在地帮助实现“通才”机器人，例如，这些机器人可以在新房子里用新厨房做一顿新饭。
为了实现这一目标，现有的基础模型是不够的。
我们需要在大量数据源上训练的新型模型，从机器人交互数据到人类执行任务的视频等等。
我们的重点是如何将这些基础模型应用于机器人控制其自身物理体现以成功执行不同任务的问题。
这是一个高维的闭环决策问题:机器人采取的行动直接影响它接下来感知到的东西，而感知到的东西又反过来影响机器人的下一个行动。
这种闭环方面在语言和计算机视觉中没有传统的研究，在这些领域，大型离线数据集占主导地位，基础模型已经取得了成功。
我们的重点是如何在这种新的闭环数据制度中利用基础模型(大规模、自我监督学习)所展示的好处。
新型机器人基础模型的前景在于它能够放大机器人的潜力，以改善日常生活的关键方面，从制造业[1999年;Sanneman et al. 2020]， construction [Khoshnevis 2004;Bock 2007]，自动驾驶[Thorpe et al. 1988;Badue et . 2020]，到家庭援助[Thrun and Mitchell 1995;布鲁克斯2002年;Dillmann 2004;Goodrich and Schultz 2007;Gupta et al. 2018;Shridhar et al. 2020]和个人援助[Dragan and Srinivasa 2013;Javdani et al. 2018]等。
我们在本节中的讨论主要集中在用于家庭任务的移动操作机器人上，但我们希望其本质广泛适用于上面列出的机器人技术的其他用例。
在构建新型机器人基础模型的关键道路上，要抓住任务规范和任务学习方面的机遇，同时应对数据采集、安全性和鲁棒性方面的挑战。
考虑以下机器人学习范例:从一个任务的描述开始，捕捉用户可能希望机器人做什么(例如，“做早餐”)-学习相应的策略来生成所需的机器人动作。
虽然策略可以以不同的方式参数化，但一种常见的选择是将任务表示和环境观察(例如，来自固定或自我中心相机的场景图像，或来自激光雷达等替代传感器的输入)映射到机器人动作的函数[Andrychowicz et al. 2017;Nair et al. 2018]。
当机器人以任务条件的方式行动时，随后的状态被反馈给策略，产生更多的动作，直到任务被满足。
然而，在实践中实现这样的范例是困难的。
首先，描述一个人的目标的正确界面是什么?
对于给定的用户来说，“做早餐”意味着一顿完整的早餐，包括煎鸡蛋、烤面包和一杯橙汁;对于另一个用户来说，“做早餐”可能意味着懒散的桑巴和一大杯过滤咖啡。
一般来说，像这样的高级上下文相关目标并不是单独存在的，而且可能会引入大量的模糊性。
如何以足够清晰的方式指定目标(以及相应的子目标)，以解决这些模糊性，并允许机器人在给定的任务上取得进展?
此外，我们如何制作通用任务表示来帮助类似目标的泛化(例如，取一杯牛奶而不是橙汁)。
更进一步，我们如何建立方法来帮助机器人学习新任务和新环境的策略(在这种情况下，一个全新的厨房，有新的器具，电器，布局等)?
最近在语言和视觉基础模型应用方面的突破(§2.1:语言和§2.2:视觉)表明大规模、自监督预训练对提高泛化有几个潜在的好处。
利用不同的数据流来学习有意义的代表性先验(类似于BERT和GPT-3等模型所学习的先验)的能力，为学习强大的机器人基础模型以进行任务规范提供了希望。
不同的机器人交互数据可用于学习动作条件动力学模型或策略，索引一般和语义上有意义的技能，从而为任务学习带来希望。
然而，尽管存在这些机会，关键的障碍是收集正确的数据。
与语言和视觉数据不同，机器人数据既不丰富，也不能代表足够多样化的实施方式、任务和环境——我们(作为一个领域)仍然没有融合在对实现通才机器人(例如，离线演示、人类的第三人称录音、以自我为中心的视频、自主体验、再加上获得适当规模和多样性数据的问题是确保安全性和稳健性的问题:我们如何在新环境中表现而不造成损害?
因此，为机器人技术构建新型基础模型包括机遇和挑战的二分法:任务规范和学习的机会与数据收集和安全部署的挑战相平衡。
本节通过展示机器人基础模型如何帮助我们开发多面手机器人的图片来进行探讨，这种方式不仅有意义地解决了与构建此类系统相关的挑战，而且还包含了多模态的潜力-包括感知，驱动和语言-以及用于规范和学习的人机交互。
2.3.1机会。
机器人基础模型可以有多种形式:机器人技术中的问题不容易符合一个通用的模型，因为不同的问题有不同的输入输出签名——这与NLP等领域形成鲜明对比，在NLP中，许多问题可以被归为一个通用的“文本输入，文本输出”签名。
我们专注于泛化任务规范和跨任务、环境和机器人实施例学习的机会。
任务规范的基础模型。
在机器人学会如何以通用的方式解决任务之前，它们必须理解想要完成的任务是什么:例如，为了在一个新的厨房里发挥作用，机器人需要知道我们希望它做什么，以及我们希望它避免的行为。
因此，开发多面手机器人的必要的第一步是建立一种新的可靠的任务规范的基础模型，即任务目标、偏好和约束的直观和有效的沟通。
我们将任务规范形式化为将人类提供的任务描述转换为测量机器人任务完成和进度的定量度量的过程，例如奖励函数。
该信号对于优化机器人行为、诊断故障和提示人类反馈至关重要。
由于描述任务的最自然方式可能因用户、环境或任务而异，任务规范的机器人基础模型应该接受各种描述方式，例如目标状态[Fu等人，2018;Singh et al. 2019]，自然语言[MacGlashan et al. 2015;Karamcheti et al. 2017;Misra et al. 2017b;Co-Reyes et al. 2019;Shao et al. 2020]，人类视频[Shao et al. 2020;Chen et al. 2021c;Liu et al. 2018]，两两比较或排名比较[Biyik and Sadigh 2018]，互动修正[Co-Reyes et al. 2019;Karamcheti et al. 2020]和物理反馈[Ross et al. 2011;Bajcsy et al. 2017]。
任务规范的通用模型的一个重要需求是能够转移到新的环境和任务。
将任务描述可靠地转换为机器人学习的可推广的奖励信号仍然是一个悬而未决的问题[Taylor等人，2016]——机器人基础模型可以说非常适合这个问题。
当应用于任务规范时，这样的模型应该通过从大型和广泛的数据集学习，甚至利用上面列出的多种描述模式，提供更健壮(§4.8:鲁棒性)的奖励信号。
任务规范的新基础模型的一个可能实例可能是从任意(语言，当前观察)对中学习映射，通过在不同的语言和视觉数据集上训练来奖励信号[Bahdanau等人，2019;Fu et al. 2019;陈等。2021c]。
通过从这些广泛而多样的数据集中学习信息先验，这样的模型可以推广到看不见的语言指令和看不见的环境中的观察。
一般来说，新的基础模型能够巧妙地连接各种模式，并广泛地泛化，这使得它们对通用任务规范具有吸引力。
任务学习的基础模型。
除了实现更通用的任务规范之外，机器人基础模型可以使学习解决新任务更加有效和可靠。
在这种情况下，这些新类型的基础模型可能采取行动、传感器观察、奖励和其他感兴趣的属性的联合分布的形式。
对这个联合分布的不同维度进行条件反射，可以得到不同的推理问题，每个问题对应一个不同的签名:
•动态建模:(未来观察|动作，过去观察)[Finn and Levine 2017;Hafner et al. 2019;吴等。[2021d]。
•政策学习:(行动|观察，目标)[Kaelbling 1993;Schaul et al. 2015;Ding et al. 2019]。
•逆强化学习:(奖励函数|观察，行动)[Ng和Russell 2000;Ziebart et al. 2008;Finn et al. 2016a]。
机器人基础模型的一个貌似合理的训练目标是以自回归的方式预测上述联合分布的不同元素[Janner et al. 2021;Chen et al. 2021b，§4.1:modeling [j]。
然而，这些并不是唯一的选择。
特别是，机器人数据集包含大量未标记的数据，这些数据由来自许多不同传感器模式(例如，RGB和深度相机，触觉传感器，麦克风等)的同步观察组成，以及机器人为生成这些观察而执行的一系列动作。
除了上述目标之外，还可以训练机器人基础模型来预测一种传感器模态的观测结果，或者预测两种感官观测结果是否来自同一时间段。
这些类型的自我监督目标可以利用多模态对应来生成高维数据的低维表示，甚至可以与上述目标结合起来，在这些表示的基础上生成模型、策略和奖励。这些目标可能有助于从未标记的数据中训练强大的机器人基础模型，只要数据表现出多样化、有意义的行为。
2.3.2§:
《机器人挑战》讨论了进一步收集此类数据的挑战。
在语言和视觉方面，基础模型已经证明了从大型、不同的数据集中学习广泛适用的先验的能力，这些先验随后可以适应下游任务(§2.1:语言，§2.2:视觉)。
与现有的语言和视觉模型相比，机器人基础模型利用不同的数据、自我监督的目标和模式，具有类似的潜力，能够对新环境、任务和实施例进行少量的感知和控制适应。
考虑一下我们厨房跑步的例子。
要在新厨房里做饭，机器人需要适应特定的环境——它的空间布局、可用的设备等等。从人类、机器人交互、文本和/或模拟的离线视频中学到的经验可能会对厨房的一般方面进行编码，例如炉子通常靠墙，必须打开才能产生热量。
这些常识性知识、物理先验和视觉先验可以使样本更有效地适应新环境。
类似地，为机器人任务学习开发一个新的基础模型，可以在其训练数据集中使用大量烹饪视频，以适应一项常见技能的策略，例如“煎鸡蛋”，从少量演示中适应特定用户的偏好，从而实现样本有效适应。
最后，由于具有学习前面描述的跨模态表示的潜力，机器人基础模型可以帮助实现对新实施例的适应。
适应的这一方面对于使这些模型广泛使用至关重要。
2.3.2挑战与风险。
尽管这一愿景令人振奋，但仍需克服多重挑战。
为了实现上述泛化，我们必须收集足够规模和多样性的机器人数据集。
此外，我们需要一些机制来确保我们能够在现实世界中安全地部署学习到的行为。
数据需求与挑战。
对于通过传感器感知环境状态并采取行动来完成任务的机器人，学习策略通常需要机器人在现实世界中交互的大量数据集。
另一方面，计算机视觉和自然语言处理中的许多学习任务依赖于可以从网络上轻松抓取的大型和多样化的离线数据集。
受现有基础模型在语言和视觉方面的进步的激励，我们对利用大型离线数据源来训练新的机器人基础模型的可能性感到兴奋。
实现这一目标的一个途径是收集用于离线学习的大型数据集，例如使用远程操作[Mandlekar等人，2019]，动觉教学[Sharma等人，2018]或自主方法[Pinto和Gupta, 2016;Gupta et al. 2018;Levine et al. 2018;Dasari et al. 2019;卡拉什尼科夫等人2021;Chen et al. 2021d]，在泛化方面显示出一些有希望的迹象。
将机器人数据收集扩展到视觉和语言数据集的大小[Deng et al. 2009;Krishna et al. 2017;rafael et al. 2019;Gao等人。2020a]仍然是一个开放的挑战，机器人数据集的规模和质量的增加表明它们可以在学习机器人的基础模型中发挥重要作用。
此外，由于机器人有能力主动和自主地塑造它们的环境，它们应该能够大规模地产生有针对性的未标记数据。
考虑到学习控制具有挑战性的闭环特性，收集与视觉和语言中使用的数据集大小相当的数据集可能不足以用于机器人。
一个令人兴奋的选择是额外利用外部非机器人数据源，如人类视频或现有的视觉和自然语言数据集。
这些数据多种多样，在网络上大量存在[Deng et al. 2009;Lee et al. 2012;Heilbron et al. 2015;Goyal et al. 2017a;Damen et al. 2018;Gao等。2020a;Grauman et al. 2021]，如果适当利用，可以提供广泛推广的可能性。
优雅地解决机器人的领域与视频或网络语言之间的差距仍然是一个公开的挑战;然而，域适应的最新进展[Smith et al. 2019;Schmeckpeper等人。2020]并在机器人中使用预训练的视频和语言模型[Lynch and Sermanet 2020;Shao et al. 2020;Chen等人。2021c]提出了缩小这一差距的有希望的方向。
最后，仿真提供了丰富的交互数据的无限来源，机器人可以从中学习，包括一系列传感器模式，如渲染视觉，点云和模拟触摸/音频。
然而，主要的挑战在于弥合模拟和现实世界之间的差距，无论是在基础物理还是在环境和任务的语义分布方面。
最近的研究表明，通过使用广泛的领域随机化，从飞行[Sadeghi and Levine 2017]到接触丰富的操作[Mahler et al. 2017;OpenAI et al. 2019]和运动[Peng et al. 2020;hwang et al. 2019]在模拟中学习的技能可以成功地转移到真实的机器人上，并且可以通过将真实世界扫描到模拟中来模拟真实世界的语义和视觉分布[Chang et al. 2017;Kolve et al. 2017b;Savva et al. 2019a;Szot et al. 2021;沈等。2021a]。
虽然这些都是缩小模拟到真实差距的有希望的步骤，但有效和普遍的模拟到真实的操作和运动技能学习仍然是一个开放的挑战。
仿真数据、真实机器人数据、人类视频和自然语言数据对于学习机器人基础模型都是必不可少的。
安全性和稳健性。
使机器人新基础模型的开发进一步复杂化的是，在训练或在现实世界中部署它们时，要确保它们的安全性和鲁棒性。
我们可以预期，这些机器人模型的安全风险与它们的语言对应体不同，因为具体化的代理被授权在物理世界中直接操纵和与周围环境互动。
基于学习的系统的一个核心安全挑战是先有鸡还是先有蛋的问题，即需要在收集数据之前指定系统安全约束，之后可能会出现需要额外约束的不可预见的不安全行为。
例如，智能体适应训练分布之外的新厨房需要足够的安全保证来确保安全的数据收集，这可能会对任务性能产生不利影响，或者导致智能体以新的方式失败。
解决这个问题的一种方法是限制环境的复杂性或增加机器人的复杂性，以便通过构造避免不可恢复的状态或不安全的动作。
机器人还可以自动重置环境，以促进从大规模数据收集中不间断的学习(或适应)[Eysenbach等人，2017;古普塔等。[2021b]。
这意味着要确保厨房里没有任何东西是易碎的，或者确保并更换代理在试图收集数据时可能会损坏的物品。
为了解决机器人基础模型无法对新刺激进行泛化或产生意外行为所带来的风险，潜在的未来方向包括开发代理的因果分析[dsametang等人，2021]，新的正式安全评估工具和逼真的模拟环境[Corso等人，2020;Dreossi et al. 2017;Julian and Kochenderfer 2019]。
最后，推导机器人基础模型的正式安全保证，例如安全集的Hamilton-Jacobi可达性[Chow et al. 2018;Fisac et al. 2019;Herbert et al. 2021]或为人类操作员开发可解释的学习安全边界(§4.11:可解释性)，可以帮助降低此类模型带来的风险[Berkenkamp et al. 2017]。
随着这些新型基础模型的开发和研究的进展，解决这些挑战将是至关重要的。
结论。
虽然机器人基础模型的前景很多——跨越从任务规范到任务学习的机器人流水线的多个层次——但挑战也是重大的。
在涵盖各种环境和大规模实施的物理世界中收集数据是一个相当大的障碍，确保此类系统的安全性和稳健性同样紧迫。
尽管如此，我们的乐观主义占上风;现在，在开发模型之前，解决这些挑战为我们提供了机会，可以确定从正确的来源、以正确的规模收集正确数据的方法，从而构建具有我们期望的功能的安全可靠的机器人基础模型。
支撑这一节的是多模式的主题。
机器人基础模型——所有可能的实例——已经并将继续受益于人工智能的其他子领域，如语言和视觉(§2.1:语言，§2.2:视觉)。
然而，当我们考虑纳入来自其他领域的这些扩展时，就会出现跨学科的挑战，触及基础模型的其他方面:用于训练和部署实时机器人模型的系统创新(§4.5:系统)，用于鲁棒人机交互的界面创新(§2.5:交互)，以及在我们更好地掌握这些模型的安全性和鲁棒性时要纳入的教训(§4.9:ai-safety，§4.8:鲁棒性)。
建立一个可靠的生态系统，围绕一般的基础模型，特别是机器人基础模型，进行深思熟虑的研究实践，是实现这些目标的关键。
2.4推理与搜索
作者:吴玉怀，Frieda Rong，任宏宇，Sang Michael Xie，李学辰，Andy Shih, Drew A. Hudson, Omar Khattab多模态可以允许基础模型不仅使用正式的符号语言进行推理，而且还可以利用问题的视觉方面，如等效性，对称性和欧几里得几何，修剪无限的搜索空间并找到解决方案的有希望的结构(§2.4.1:推理任务)，模仿人类对几何问题的推理方式。
推理和搜索一直是人工智能历史上的一个中心主题。
从战略游戏到抽象的数学发现，经典的智力测试成为鼓舞人心的门柱，推动了“机器智能”的极限，需要设计出更智能的方法来寻找获胜的解决方案。
在早期，符号方法是推理的主要方法[Russell and Norvig 2020]，但所涉及的工程工作和形式化启发式来解决棘手的搜索空间的需求很快被证明是麻烦的。
最近，使用神经网络的数据驱动方法显示了令人鼓舞的结果——例如，通过利用统计结构和学习有用的启发式，在围棋中击败了最好的人类[Silver et al. 2016]，围棋是一种比经典的国际象棋挑战具有更大的动作空间的棋盘游戏。
本节概述了现有的推理任务，这些任务需要扩展到更大的搜索空间并广泛地理解世界(§2.4.1:reasoning-tasks)。
然后，我们在§2.4.2:推理角色中提出，基础模型应该在一般推理中发挥核心作用，作为捕获无界搜索空间(生成性)的统计规律的工具，允许跨任务和场景的积极迁移(普遍性)，并利用多模态环境中的知识基础(接地)。
2.4.1当前的任务是什么?
许多推理问题提出了无界搜索空间，其中系统必须处理许多种类的开放式替代方案。
考虑试着证明等腰三角形∠和∠是相等的，用=(图9)。
系统可以在推理的每个步骤中执行任意数量的操作。
例如，系统可以用任意结构添加一个新的辅助点，比如一条垂直线、一条平行线或一个切线圆，随着图表变得更复杂，搜索空间只会变得更大。
证明该定理的一种方法是，画一条线作为角的平分线，用两个三角形△和△的相余来表示∠=∠，但如果不进行广泛的搜索，系统怎么能找到这个定理呢?
更一般地说，数学家不局限于搜索图表结构和欧几里得定理:数学家可以应用数学各个分支的大量定理，做出高级猜想，形式化新的数学概念，或者找到反例。
这与更结构化的人工智能挑战(如围棋)形成鲜明对比，后者的搜索空间被认为要小得多。17
除了定理证明之外，许多现实世界的问题都涉及无界搜索空间，例如程序合成[Gulwani et al. 2017]，药物发现[Drews 2000]，化学合成[Segler et al. 2018]，计算机辅助设计[Haigh 1985]，组合优化[Bengio et al. 2021]等等。
这些推理问题往往表现出类似的结构，如药物发现中的反合成和命题逻辑中的定理证明之间的双向映射，如图10所示:在这两个问题中，一个是构建一个合成树，其节点一边是化学产品，另一边是命题，叶子节点一边是产品，另一边是结束公理。
在这些问题中，通常提供一个模拟环境，它允许求解器运行几个搜索线程来构建解决方案树。
模拟器通常提供中间反馈，例如，在证明被认为完成之前，将剩余的命题告知求解器。
求解器依次需要选择最有希望的搜索线程，并根据中间反馈进行求解。
最近，人们对应用基于学习的方法来解决推理问题的兴趣激增。
为了克服无界搜索空间的挑战，研究人员首先从约束搜索空间开始，使问题易于处理[Huang et al. 2018;Bansal et al. 2019]。
但这种方法受到求解器所能发出的行动种类有限的影响。
例如，求解器只能应用已知数据库中的定理来证明目标定理，而不能综合新的定理和引理。
由于大型语言模型提供了一种将输出空间建模为序列的通用方法，因此它们很快成为更有利的选择，允许生成任意类型的操作。
研究人员已经将这些基于语言模型的方法应用于各种应用，例如预测蛋白质结构[Senior et al. 2020]，证明形式定理[Polu和Sutskever 2020;Han et al. 2021]，猜想定理[Urban and Jakubuv 2020;Rabe et al. 2021;Li et al. 2021b]，自然语言合成程序[Chen et al. 2021f];Ling et . 2016]，修复、生成和理解代码[Yasunaga and Liang 2021;Lu et al. 2021b;Guo et al. 2020;Svyatkovskiy et al. 2020;Kim et al. 2021b;z<s:1> gner等。2021]。
也有研究表明，缩放模型大小可以显著提高推理能力[Polu和Sutskever 2020]，此外，语言建模的标准技术，如预训练，也可以大大提高这些任务的性能[Rabe等人，2021;Polu and Sutskever 2020]。
2.4.2
基础模型的作用是什么?
繁殖。
我们相信基础模型的生成能力对于有效推理是必不可少的。
由于搜索空间无界，枚举各种可能性变得非常困难。
相反，使用基础模型，可以对最优决策的分布进行建模，并生成合适的候选方案，以进行下一步。
特别是，由于基础模型提供了一种将输出空间建模为序列的通用方法，因此下一个决策生成是完全不受约束的，因此是通用的。
这种灵活性对于我们讨论的许多推理挑战至关重要，可以在数学推测[Li et al. 2021b]和合成新程序[Chen et al. 2021f]等领域产生创造性。
随着基础模型的扩大，捕获此类统计结构的能力也会大幅增长[Polu和Sutskever 2020]。
普遍性。
正如我们在上一节中提到的，许多推理问题表现出类似的潜在结构。
我们相信，由基础模型施加的统一框架可以跨任务传递和共享重要的启发式，范围从概括适用于一个任务的低级技术到新场景，一直到直接发现适用于多种问题的元技术。
此外，由于基础模型是跨多个领域训练的，因此它可以跨任务和领域积极地转移基础模型权重中编码的元知识[Papadimitriou and Jurafsky 2020;Wu等。2021f;[j]。
基础模型训练和适应框架鼓励关注点分离，其中基础模型训练学习元知识，如药物反合成和命题逻辑证明之间的共享搜索树结构，而适应阶段可以专注于学习任务特定词汇。
因此，基础模型可以在适应阶段降低学习问题的复杂性，提高样本复杂度和泛化能力。
接地。
推理问题通常很容易用符号语言表达(例如，数学、代码、分子的SMILE表示)。
然而，这些符号有着深刻的潜在语义——说“等腰三角形”在人们的脑海中描绘了一个生动的形象。
基础模型可以实现深层的基础和语义含义。
首先，其他形式的基础表征，如视觉或物理，对于在推理任务中掌握抽象概念并赋予它们具体意义至关重要[Larkin和Simon 1987;[j]。
由于模型可以在多种模态上进行训练，基础模型可以帮助理解一系列数据源(例如，图像、文本)。
因此，在几何实例中，基础模型通过对自然图像中学习到的几何形状的理解，可以有效地利用问题的图解表示。
然而，在推理中对齐的多模态数据是稀缺的，基础模型能否以无监督的方式发现不同模态之间的联系(例如，发现与相应代数方程的交换图)仍然是一个悬而未决的问题。
此外，即使在符号领域内，符号也可以有不同层次的解释。
例如，高级编程语言可以翻译成低级汇编代码。
基础模型可以学习包含这些不同视图的共享表示。
过去的研究表明，自我监督任务[Han et al. 2021;Peng et al. 2021;Li et al. 2021a]允许模型理解高级代码脚本背后的内部工作原理，并进一步协助下游任务。
2.4.3未来的推理挑战。
由于这些问题本身的困难，高质量的带注释的数据相对于原始图像和文本来说是稀缺的，更难收集。
为缓解这一问题已经进行了几次尝试。
在数学方面，研究人员提出生成综合定理，希望将其推广到现实定理[Wang and Deng 2020;吴等。2021a;Firoiu et al. 2021;周等。2021c]。
另一种方法是设计自监督任务来增强数据集[Yasunaga and Liang 2020;Ren et al. 2020;Han et al. 2021;rozi<e:1>等人，2021;Yasunaga and Liang 2021]，或者更好的预训练目标[Wu et . 2021f]。
然而，我们仍然缺乏设计自我监督任务的一般原则方法，因为大多数现有作品都是针对特定的问题设置量身定制的[Yasunaga和Liang 2020;Ren和Leskovec 2020;Han et al. 2021]。
建立一个基础模型将鼓励一个统一的框架来构建一套可以应用于所有推理问题的自我监督任务。
此外，交互性(§2.5:交互)可以通过足够的可扩展性，通过将人类带入循环，以最低限度地指导学习课程或数据增强过程，例如，选择要添加的公理或要探索的猜想，从而缓解数据稀缺问题，而交互工具本身是对基础模型的激励使用推理[Han等人。2021;Chen et al. 2021f]在认知要求最高或最费力的方面帮助人们。
解释性友好的交互工具可以在教育中找到进一步的应用，通过帮助人类在高性能基础模型的帮助下学习(§3.3:教育)。
提高高级推理能力是现有基础模型面临的核心挑战。
人类在处理困难的问题解决任务时进行抽象推理和高级规划[Miller et al. 1960]。
例如，当构建一个软件工具或证明一个定理时，我们通常从一个高级草图开始，然后再深入研究低级细节[Koedinger and Anderson 1990]。
现有的基础模型没有经过训练来生成这样的高级计划。
相反，他们往往只专注于预测下一个低水平的步骤[Polu和Sutskever 2020;Han et al. 2021;陈等。2021f]。
不幸的是，为了训练基础模型来模拟类似人类的推理，我们再次面临数据收集的挑战。
虽然在有限的环境中确实存在这样的数据[Li et al. 2021b]，但总的来说，用于高级推理的数据是稀缺的，很难收集。
一条研究路线是让抽象和模块化的层次结构在学习过程中自行出现[Ellis等人。2021;Hong et al. 2021]，但如何将这些方法扩展到更一般和现实的环境中仍然是一个悬而未决的问题。
除了这些挑战之外，还有许多悬而未决的问题，这些问题对于其他部分讨论的主题也是必不可少的。
什么构成了可靠推理的良好架构(§4.1:建模)?
我们如何能从理论上理解和解释这些模式(§4.10:理论性和§4.11:可解释性)?
我们能否训练健壮的推理模型，可以推广到域外问题(§4.8:鲁棒性和§4.3:适应性)?
我们相信，对这些前沿的基础模型的研究可以极大地扩大它们对推理领域的影响。
2.5交互
作者:Joon Sung Park, Chris Donahue, Mina Lee, Siddharth Karamcheti, Dorsa Sadigh, Michael S. Bernstein
基础模型将通过降低构建人工智能应用程序的难度门槛，为开发人员带来重大机遇，并通过提高可实现的交互类型的上限，为应用程序用户带来重大机遇。
在某些情况下，开发人员和用户之间的界限将开始模糊，用户可能能够轻松地开发自己的人工智能应用程序，例如使用自然语言。
早期形式的基础模型，如GPT-3 [Brown等人，2020]和DALL•E [Ramesh等人，2021]，在让非机器学习专家构建强大的人工智能注入应用程序原型的能力以及无缝集成从文本到图像等模式的能力方面，都表现出了高度的多功能性。
随着基础模型的发展成熟，模型的能力将继续扩大，它们的多功能性可能最终导致我们如何与人工智能交互的根本变化，允许我们快速原型化并构建高度动态和生成的人工智能注入的应用程序。
在本节中，我们将从两个重要利益相关者的角度讨论这些变化带来的机会:(1)将与基础模型交互以设计用户体验的应用程序开发人员，以及(2)将使用或受基础模型驱动的人工智能注入应用程序影响的最终用户。
最后，我们考虑的场景是，今天严格区分开发人员和最终用户的界限可能开始模糊，为创建更能满足用户需求和价值的人工智能应用程序提供了新的机会。
2.5.1对人工智能应用开发者开发过程的影响。
基础模型将如何改变开发人员创建人工智能应用程序的方式?
尽管机器学习算法和系统基础设施取得了巨大进步，但有人指出，设计新颖而积极的人机交互形式仍然很困难[Dove等人，2017;Cooper et al. 2014]。
创建强大的特定于任务的模型所需的大量数据、计算资源和技能经常与引出和满足用户需求和价值所需的迭代原型过程相冲突[Yang等人，2016]。
人工智能的反应可能是不可预测的，而模型可能产生巨大的生成输出空间，这使得人们很难为自己的表现建立有效的心理模型，这进一步加剧了这一挑战。
在应对这些挑战方面，交互式机器学习方面的工作已经取得了一些进展(例如，Crayon [Fails and Olsen 2003]， Regroup [Amershi et al. 2012])，以及将人工智能中的不确定性传达给最终用户的设计框架(例如，混合主动原则[Horvitz 1999])。
然而，要克服这些障碍，还需要做更多的工作[Yang et al. 2020]。
基础模型为解决上述许多挑战提供了重要的机会。
例如，基于语言的基础模型能够将自然语言作为输入，并将其推广到许多下游任务，这可以显著降低应用程序开发的难度“阈值”[Myers等人，2000]，也就是说，无需收集大量数据并从头开始训练大型模型，就可以开发复杂的模型。
这甚至可以让非机器学习专家快速创建人工智能应用的原型。
与此同时，基础模型强大的生成和潜在的多模态能力可以提供更高的“天花板”[Myers等人，2000]，就其质量和多样性而言，我们将在下面讨论哪些类型的交互是可以实现的。
然而，我们如何成功地利用这些能力将取决于我们如何有效地将基础模型转换成更易于应用程序开发人员管理的形式。
不幸的是，赋予基础模型优势的通用性和高上限也会使这些模型难以使用，因为它们可能比单一用途的人工智能模型更加不可预测和复杂。
事实上，最近的研究表明，很难让像ggt -3这样的模型始终如一地执行预期任务[Reynolds和McDonell 2021]，而了解它的能力仍然是一个活跃的研究领域[Hendrycks等人，2021a]。
为了提高人工智能注入应用程序的可靠性和可信度，我们建议未来的工作应该继续研究如何从基础模型(例如，通过微调，或者在主要交互模式是自然语言提示的情况下，通过提示工程)中实现更可预测和更稳健的行为[Reynolds和McDonell 2021;Liu et al. 2021d]，校准[Zhao et al. 2021]，或预格式化特定于任务的端点。18
请参见§4.8:鲁棒性了解更多细节)。
2.5.2对终端用户与人工智能应用交互的影响。
除了开发人员可能创建人工智能应用程序的新方法之外，基础模型将为与这些应用程序交互的最终用户带来哪些变化?
正如Douglas Engelbart [Engelbart 1963]所描述的那样，开发面向用户的人工智能应用程序的现有设计框架侧重于增强(而不是取代)用户的能力，我们希望这些框架应该并且将继续与未来人工智能应用程序的开发相关。
例如，维护用户的代理和反映他们的价值将继续是基础模型驱动的应用程序的中心主题。
此外，允许人工智能代理主动采取行动并自动化用户日常工作的好处与等待用户直接操作的好处[Shneiderman和Maes 1997]将需要仔细权衡[Horvitz 1999]。
此外，用户的价值应该通过参与式[Lee等人，2019]和价值敏感设计[Smith等人，2020]等过程直接收集和反映，这些过程主张在设计人工智能注入的应用程序期间积极参与所有利益相关者。
这些问题在基础模型中可能会变得特别突出，因为模型的行为方式可能会让用户和社区感到惊讶和失望。
生成能力可能会暴露出与社区目标相反的偏见或观点，或者更阴险地在社区不知情的情况下在其行为中利用这种关联。
这将给利用基础模型来监视其模型行为的小组带来很大的负担，并在可能的范围内，使它们以适当的方式行动。
虽然考虑注入人工智能的应用程序以增强用户能力的设计框架应该保持不变，但由于基础模型强大的生成和多模态能力，可以实现的实际交互形式可能会大大多样化。
早期几代可以被认为是基础模型驱动的多媒体创作和编辑软件工具已经开始推动一个新的前沿，使新手内容创作者能够从粗糙的、直观的规范中生成高质量的多媒体(例如，作家的协作创作[Lee et al. 2022]，数字艺术家的文本到图像生成，音乐家的母版，程序员的代码完成)。21
改进的基础模型可能会实现更雄心勃勃的工具(例如，一个粉丝可能会为一首歌曲提供主题材料，然后以他们最喜欢的乐队的风格生成，或者一个企业主可能会提供他们产品的简单描述，这将用于创建一个完整的网站)。
此外，基础模型将用于丰富静态多媒体(例如，自动将遗留多媒体内容重新制作成新格式，或为新视频游戏中的每个玩家生成独特的体验)，甚至可能导致使用混合不同模式的界面的多模式交互的新形式，例如视觉和基于手势的交互。
我们开始看到基础模型如何在从AI Dungeon 22到Microsoft PowerApps 23和CoPilot等应用程序中实现具体的交互。24
当我们开始设想新的互动形式时，批判性地思考这些互动对个人用户和社会的潜在影响，以最大限度地发挥其积极影响，对我们来说越来越重要。
例如，基础模型驱动的应用程序将如何改变我们彼此通信的方式?
一个强大的模型会代替我们写电子邮件吗?如果是这样，这将如何重塑人们的信任、信誉和身份，因为作者可能不是自己写的电子邮件，这将如何改变我们的写作风格[Hancock et al. 2020]?
谁将拥有模型生成内容的作者身份，以及如何滥用转移的责任和同意所有权[Weiner 2018](参见§5.5:经济学进行更深入的讨论)?
基础模型将对我们的工作、语言和文化产生什么长期影响[Hancock et al. 2020;Buschek et al. [2021]?
与最后一个问题特别相关的事实是，基础模型是根据观察到的数据训练的，并不一定告诉我们因果关系。
因此，我们如何确保使用基础模型将我们引向期望的未来，而不是重复过去?
虽然这些问题不一定是基金会模型所独有的，但随着基金会模型加速创建有效的人工智能应用程序，它们将被放大并变得更加普遍。
2.5.3模糊开发人员和最终用户之间的界限。
今天，人工智能模型开发人员和最终用户之间的界限是严格的——很少有最终用户拥有数据、计算资源和专业知识，能够开发出适合自己价值观和需求的新模型。
虽然在某些情况下，通用模型(即，不特定于特定用户或社区的模型)可能就足够了，但近年来，这种模型无法为用户服务的场景越来越多。
例如，设计用于识别一个在线社区的问题评论的文本分类模型可能对该社区有效，但在其他规范和文化可能存在显着差异的社区中可能会失败(例如，Reddit上的NSFW社区可能对某些内容更宽容，而科学社区可能会拒绝看似平凡的轶事，而不是基于科学研究)[Chandrasekharan等人，2018]。
在另一个例子中，为一个目标人群设计的人工智能传感器和机器人工具如果不能快速适应具有不同能力和需求的用户，可能会失败[Karamcheti et al. 2021]。
虽然最近的工作为最终用户如何通过手动提供模型参数或数据集来共同创建人工智能模型的未来研究提供了有希望的途径(例如，WeBuildAI [Lee et al. 2019])，但结果仍然是初步的，并且通常侧重于基本模型。
如果基础模型能够充分降低构建人工智能应用程序的难度门槛，则可以通过允许用户积极参与模型的开发过程，将用户的需求和价值观与模型的行为更紧密地结合起来，这是一个重要的机会。
例如，最近的研究表明，当在其自然语言提示中给出足够的任务描述时，GPT-3可以以少量甚至零射击的方式稳健地执行分类任务[Brown et al. 2020]。
一个试图调节其自身内容的在线社区可能能够利用这种能力来创建定制的AI分类器，根据社区已同意的分类任务描述过滤内容(当然，这种能力也可能被滥用来压制社区内某些成员的声音-我们指出§5.2:误用以进一步讨论此主题)。
此外，基础模型将展示的强大的上下文学习能力可能允许基础模型驱动的应用程序在每个用户的基础上更有效地优化它们的接口。
这可能为解决人机和机器人交互中的许多突出问题打开大门，例如在混合自治设置中平衡用户直接操作和自动化的力量。
当然，为了真正实现模糊用户和开发者之间界限的潜力，我们仍然需要克服一些重要的挑战。
这些挑战包括减轻基础模型中现有的偏见，以及使模型的行为更加健壮和易于管理，即使对于非ML专家来说也是如此(与ML专家相比，非ML专家可能更难理解基础模型的全部能力和机制，这可能导致开发周期中意想不到的陷阱[Yang等人，2018])。
未来的工作应该探索如何将基础模型置于交互式机器学习的背景下，并研究我们如何支持那些机器学习经验有限的人以稳健的方式利用这些模型。
尽管如此，最终用户参与开发人工智能应用程序的能力是一个令人兴奋的机会，它可以为我们未来如何与这些应用程序交互引入一种新的范式。
2.6理解哲学
作者:Christopher Potts, Thomas Icard, Eva Portelance, Dallas Card, Kaitlyn Zhou, John Etchemendy基础模型如何理解它所训练的数据?
对这个问题的回答将对基础模型为智能系统做出贡献的总体能力提供极其丰富的信息。
在本节中，我们将重点关注自然语言的情况，因为语言使用是人类智能的标志，也是人类经验的核心。
目前最好的基础模型能够以惊人的流畅性消费和产生语言，但它们总是陷入不连贯的状态，这表明它们只是“随机鹦鹉”[Bender等人，2021]。
这些失误是固有局限性的证据，还是未来的基础模型真的能理解它们处理的符号?
本节的目的是澄清这些问题，并帮助围绕这些问题组织辩论。
我们首先解释我们所说的基础模型是什么意思，特别注意基础模型是如何训练的，因为训练制度限定了模型获得的关于世界的信息。
然后，我们解释了为什么澄清这些问题对于这些模型的进一步发展是重要的。
最后，我们试图澄清我们所说的理解是什么意思，阐述理解是什么(形而上学)以及我们如何可靠地确定一个模型是否已经实现了理解(认识论)。
最后，我们得出结论，怀疑未来模型理解自然语言的能力可能为时过早。
单靠基础模型就能实现理解，这绝不是显而易见的，但我们也不知道有明确的理由认为它们不能实现理解。
2.6.1什么是基础模型?
基础模型并没有一个精确的技术定义。
更确切地说，这是一个大型模型家族的非正式标签，并且这个模型家族很可能随着时间的推移而增长和变化，以响应新的研究。
这对推断它们的基本性质提出了挑战。
然而，所有基础模型都有一个共同的特征:它们都是自我监督的。
我们关注的是自我监督是模型唯一正式目标的情况。
在自我监督中，模型的唯一目标是学习它所训练的符号序列中的抽象共现模式。
这项任务也使许多这些模型能够生成可信的符号串。
例如，许多基础模型的结构是这样的:人们可以用“三明治中含有花生”这样的序列提示它们，并要求它们生成一个延续——比如，“黄油和果冻”。
其他模型的结构是为了更好地填补空白;你可能会提示一个模型"三明治里有什么什么和果冻”，并希望它填上“花生酱”。
这两种能力都来源于这些模型从训练数据中提取共生模式的能力。
这种自我监督并不能明显地告诉模型符号的含义。
直接给出的唯一信息是关于哪些单词倾向于与哪些单词同时出现的信息。
从表面上看，知道“三明治含有花生”很可能会接着说“黄油和果冻”，但这并不能说明三明治是什么，果冻是什么，这些物体将如何组合，等等。
这似乎暗示了基础模型所能实现的内在限制。
然而，我们不需要限制模型只看到文本输入。
一个基础模型可以在各种不同的符号上进行训练:不仅仅是语言，还有计算机代码、数据库文件、图像、音频和传感器读数。
只要它只是在学习它所接触到的序列的共现模式，那么根据我们的定义，它就可以算作一个基础模型。
作为这种学习的一部分，该模型可能会表示给定文本片段与特定传感器读数之间，或像素值序列与数据库条目之间的强烈关联。
这些联想可能反映了我们所居住的世界的重要方面以及我们用来谈论这个世界的语言。
2.6.2利害关系是什么?
在考虑分析什么是理解之前，有必要反思一下为什么我们可能会关心基础模型是否能够实现理解的问题。
这些模型可以部署在具有各种功能的众多用途上。
我们在部署中的一些目标可能只有在模型能够理解的范围内才能实现。
这里我们列出了一些这样的目标:
信任:有人可能会说，我们不能信任一个系统的语言行为，除非它理解它正在使用的语言。
当然，我们目前相信工程系统可以做一些事情(例如，制造汽车零件)，甚至不会产生理解的问题，但是语言在这方面可能是特殊的，因为它是人类独有的。
此外，语言可以用来欺骗和歪曲，所以理解本身显然并不意味着信任。
因此，在语言使用的语境中，理解可以被看作是信任的必要条件。
•可解释性:如果真正的自然语言理解在某种程度上涉及维护和更新世界的内部模型(包括，例如，语音上下文)，并且如果我们(作为工程师)能够分析语言输入和输出如何与这个内部模型接口，那么可以在这些系统的可解释性，可预测性和控制方面获得实质性的收益。
•问责制:与前面的几点不无关系，在未来，我们可能会发现希望让人工智能体以某种方式对它们产生的语言负责[the HAI Adaptive agents Group 2021]。
根据我们如何看待问责制、责任、代理等概念，语言理解可能会成为一个先决条件。
仅仅是理解在这些问题中扮演不可或缺的角色的可能性，就提供了强有力的动机来发展一个关于它的理论化框架。
2.6.3什么是理解?
我们的核心问题是基础模型是否能够理解自然语言。
有了以上这些，我们现在可以把它清晰化:自我监管是否足以理解，同时要记住，这种监管所使用的数据没有限制?
为了解决这个问题，我们首先需要定义理解的含义。
作为开始，我们发现明确区分有时在讨论该主题时混淆的区别是有帮助的。
区别在于形而上学和知性认识论。
形而上学关注的是主体实现理解的意义(“原则上”)。
相比之下，认识论关注的是(“在实践中”)我们如何能够知道一个主体已经获得了相关类型的理解。
简而言之，形而上学更多的是关于我们的最终目标，而认识论更多的是关于我们如何(如果有的话)知道我们何时到达它。
因此，我们的认识论在某种程度上取决于我们的形而上学。
知性的形而上学。语言哲学为……提供了许多选择
理解自然语言意味着什么。25
为了简洁起见，以下三大类观点都与AI和NLP的研究方向有关
•内部主义:语言理解相当于对语言输入的正确内部表征结构的检索。
因此，如果没有丰富的正确的内部概念库，语言理解甚至是不可能的。
参考主义:粗略地说，当一个智能体能够知道该语言中的不同句子(相对于上下文)是正确的时候，他们就能理解语言。
也就是说，词语有指称物，(陈述性)话语是可真实评价的，理解涉及到一种相对于情境或场景的呈现来评价它们的能力。
•实用主义:理解不需要任何内部表征或计算方式，真理和参考不是基本的。
更确切地说，重要的是代理人是否倾向于以正确的方式使用语言。
这可能包括对推理或推理模式的倾向、适当的对话动作等等。
至关重要的是，相关的语言能力构成了理解能力。27
虽然这是可能性空间的简化图，但我们已经看到它们是如何以不同的方式与上述目标联系在一起的。
例如，在实用主义的观点中，实现语言理解并不意味着我们信任或解释系统的能力，因为它不能保证代理的内部结构或它与(非语言)世界的关系。
相比之下，在内部主义者的观点中，至少强烈提出了一种相当强大的内部/因果可解释性。
基础模型能否理解语言的问题，根据我们所采用的形而上学特征的不同，有着不同的特征。
内在主义和指称主义都可以被视为定义映射问题:将语言符号与“意义”或“语义价值”联系起来。
对于内部主义，这将是一个表示或概念，一个计算值的程序，或其他类型的内部对象。
对于引用主义，它可能是从一个单词到一个外部引用的映射，或者是从一个情境到一个真值的映射(都相对于上下文)。
自我监督是否足以在基础模型中实现期望的映射?
在这里，训练示例的性质可能是相关的。
如果模型只接收语言输入，那么它学习这种映射的能力可能会从根本上受到限制，从而阻止它学习相关意义上的参考。
(事实上，Merrill等人[2021]确定了一些理论上的限制，尽管是在非常强烈的假设下学习符号的含义。)
然而，如果输入符号流包括世界上各种事物的数字痕迹——图像、音频、传感器等——那么共出现模式可能包含足够的信息，使模型能够为所需的映射诱导高保真代理。28
对于引用主义，还有一个进一步的问题，即这些代理如何与实际世界联系起来，但同样的问题也出现在人类语言用户身上。
Bender和Koller[2020]给出了一个有趣的论点，将参考主义与实用主义结合起来。
他们设想有一个智能体O，它拦截两个人之间的交流，使用一种自然语言l。O生活在一个与人类截然不同的世界，因此不具备以指称主义要求的方式来理解人类话语所需要的那种经验。
尽管如此，O从人类的话语模式中学习，甚至可以成功地假装成人类中的一员。
Bender和Koller随后试图激发这样一种直觉，即我们可以很容易地想象O无法将L根植于人类世界的情况，而这反过来又会揭示O不理解L。指导性假设似乎是世界的复杂性如此之大，以至于任何文字交流都无法完全覆盖它，差距最终会显露出来。
在我们已经定义的术语中，无法参考被认为意味着主体没有处于正确的理解倾向状态。
从根本上说，本德和科勒所描述的情况是，一些对理解至关重要的信息被认为是缺失的，一个简单的行为测试揭示了这一点。
我们可以同意这个评估，但不认为基础模型通常无法理解。
这再次将我们带回到所涉及的训练数据的细节。
如果我们修改Bender和Koller的设想，使传输包括数字编码的图像、音频和来自人类世界的传感器读数，并且O能够学习这些数字痕迹和语言单位之间的联系，那么我们可能会更加乐观——可能存在一个关于O获得足够数据进行概括的能力的实际问题，但可能不是原则上限制O可以实现的。29
我们暂时得出结论，没有简单的先验理由认为属于我们三种立场中的任何一种的各种理解不能以相关的方式学习。
由于这种可能性仍然存在，我们面临着艰难的认识论挑战，即澄清我们如何希望评估潜在的成功。
理解的认识论。
实用主义的一个积极特征是，通过将成功与具体行为的表现联系起来，就不存在如何测试成功的重大概念难题。
我们只需要说服自己，到目前为止，我们对系统行为的有限观察表明，对于我们作为目标的更一般的行为类别，我们有一个可靠的倾向。
当然，就合适的目标达成一致是非常困难的。
当提出具体的建议时，它们总是遭到反对，通常是在证明了假定的成功之后。
图灵测试的历史在这里很有启发意义:尽管许多人工智能体已经通过了真正的图灵测试，但它们都没有被广泛认为是智能的。
同样，近年来，NLP中的一些基准任务被提出来评估理解的特定方面(例如，回答简单问题，执行常识推理)。
当系统超出了我们对人类表现的估计时，社区的反应通常是测试有缺陷，而不是达到了目标。
可能有一些行为是我们真正的目标，但它很难限定或变成一个实际的测试。30.
再一次，这可能揭示了内在主义或参照主义是我们一直以来的想法。
如果我们以内在主义或参照主义作为最终目标——我们理解的黄金标准——那么作为评估理解是否已经实现的一种手段，行为测试充其量是不完美的。
缺陷是双重的。
首先，行为测试总是有漏洞，可能会让不成熟的模型漏网。
其次，系统可能已经实现了这些视图所需要的映射，但是我们可能无法用行为测试来显示这一点。
最近使用GPT-3模型的经验表明，这可能会变得多么具有挑战性:根据使用的提示，人们可以看到令人惊讶的连贯输出或完全无意义的输出，因此提示工程需要深入的专业知识[Rong 2021]。
因此，内部主义和指称主义都需要结构性评估方法，使我们能够研究它们的内部表征，探索它们的信息[Tenney等人，2019;Manning et al. 2020]，研究它们的内部动力学[Sundararajan et al. 2017]，并可能根据支持因果推理的特定实验协议积极操纵它们[Vig et al. 2020];Geiger et al. 2020]。
我们可以从复杂基础模型的内部运作的实际实验中学到的东西可能存在根本的限制，但很明显，只要我们的目标与内部主义或参考主义保持一致，这些方法就会有用。
2.6.4推进讨论。
很明显，对于基础模型是否能够理解语言这个问题，没有简单的答案。
为了开始解决这个问题，我们必须解决一个困难的形而上学问题，关于这个问题，有许多截然不同的观点。
这个形而上学的问题就变成了一个认识论的问题，这个问题提出了许多实际的挑战。
尽管如此，上面的讨论确实引出了一个实际的结论:如果基础模型被作为人工智能体语言理解的途径，那么多模态训练机制可能是最可行的策略，因为它们似乎最有可能为模型提供必要的信息。
自我监督是否足够是一个完全悬而未决的问题。
3应用程序
基础模型的能力(第2节:能力)表明，它们有潜力改变各个部门和行业，扩大人工智能在社会中发挥的作用(第5节:社会)。
在基础模型可能应用的无数应用中，我们将重点关注三个学科-医疗保健(第3.1节:医疗保健)，法律(第3.2节:法律)和教育(第3.2节:法律)-这些都是社会功能的基础。
在每个领域中，我们讨论了基础模型为该领域带来的机会以及挑战(例如，可解释性;§4.11:可解释性)和关注点(例如，隐私;§4.7:安全性)。
3.1医疗保健和生物医药
作者:Michihiro Yasunaga, Jing Huang, Camilo Ruiz, Yuhui Zhang, Giray Ogut, Saahil Jain, William Wang, Yusuf Roohani, Hongyu Ren, Antoine Bosselut, Ehsan Adeli, Jure Leskovec, Russ Altman图12医疗保健和生物医学的基础模型。
我们可视化了一个交互式框架，其中基础模型在接受医疗保健生态系统中各种来源生成的多模态数据的训练后，可以实现医疗保健和生物医学领域的各种任务。
第一栏列出了几个数据来源，包括医疗服务提供者、支付方、机构(大学、非营利组织和政府)、制药公司、可穿戴设备和医学出版物/论坛。
第二列显示了数据源生成的几个数据模式。
它们包括图像(如胸部x光片)、视频(如超声波)、化合物图、电子健康记录(EHRs)表格、文本(如临床记录)、时间序列(如心电图)和遗传数据。
第三列显示了在这些数据上训练的基础模型，然后将其应用于第四列中列出的医疗保健和生物医学下游任务。
这个过程可以生成新的数据，这些数据将进一步改进基础模型，从而在基础模型和任务之间建立双向关系。
例如，医疗保健和生物医学是一个巨大的社会应用领域，其支出占美国国内生产总值(GDP)的17% [Swensen等人，2011;van Hartskamp et al. 2019;Keehan et al. 2020]。
医疗保健(侧重于通过诊断、治疗和健康管理向患者提供护理)和生物医学研究(侧重于对疾病的科学理解和新疗法的发现)都需要大量的费用、时间和全面的医学知识[Yu et al. 2018;Korngiebel and Mooney[2021]。
我们设想，基础模型可以成为医学知识的中央存储，该存储可以在医学数据的不同来源/模式上进行训练[Krumholz等人，2016;Soltanian-Zadeh 2019;Suresh et al. 2020](图12左)，医疗专业人员可以交互式地查询/更新(例如，医疗保健提供者和生物医学研究人员可以访问已发表的研究结果并上传新的出版物)[Ionescu et al. 2020]，公众也可以查询。
由于基础模型具有较强的适应能力(如微调、提示[Brown et al. 2020])，因此可以有效地适应医疗保健和生物医学中的各种个体任务(如患者使用的问答应用程序[Klasnja and Pratt 2012];Zhu et al. 2019;Daniel et al. 2019;Liu et al. 2020a]、临床试验匹配系统[Ni et al. 2015];Harrer et al. 2019;Beck et al. 2020]由研究人员和患者查阅;图12右)。
通过这种方式，基础模型可以成为一个中心接口，支持医疗保健和生物医学领域的数据、任务和人员之间的各种交互，从而提高医疗保健/生物医学应用的效率和准确性[Elbattah et al. 2021]。
我们在§3.1.1中阐述了这些机会:
医疗保健任务和§3.1.2:
biomed-tasks。
与此同时，医疗保健/生物医学应用带来了独特的挑战，激发了基础模型的进一步研究，例如在医疗保健/生物医学中整合多模式数据[Miura等人，2021;Liu等。2021a]并遵守医学伦理和法律法规(隐私、安全和可解释性)[Guan 2019;Xu et al. 2019]。
我们在§3.1.3中详细阐述了这些挑战:
healthcare-biomed-challenge。
3.1.1医疗保健领域的机会。
基础模型可以通过医疗保健提供者和医院改善对患者的护理。
目前，医疗成本每年都在增加[Keehan et al. 2020]，研究估计，由于行政效率低下和可预防的医疗差错，30%的医疗支出可能被浪费[Kocher 2021]。
此外，随着对医疗保健需求的增加，社会面临着医疗保健提供者的严重短缺[Kirch和Petelle 2017]。
医疗保健的低效率和短缺需要为医疗保健提供者和患者开发快速准确的界面，例如用于诊断/治疗、患者记录摘要和患者问题回答的自动辅助系统[Davenport和Kalakota 2019;Nie et al. 2018;Wang等。2021b]。
特别是在COVID-19等紧急大流行危机中，快速诊断/筛查(如胸部x线图像自动分析)以及针对患者(如症状检查和护理)和公众(如疾病预防)的自动问答对于减少疾病传播和为危重患者分配医疗资源，挽救更多生命至关重要[Lalmuanawma et al. 2020]。
由于基础模型具有强大的集成知识库功能，因此可以对其进行查询和调整，以适应医疗保健中的各种单独任务。
以下是将受益于基础模型的医疗保健领域重要任务的示例。
用于医疗保健提供者的接口。
基础模型可以提高提供者护理的效率和准确性。
医疗保健提供者花费不必要的时间编辑电子健康记录(EHRs) [Kocher 2021]，可预防的医疗错误(例如，医院再入院，手术错误)导致医疗保健浪费[shrinket al. 2019];Shah et al. 2020]。
基础模型可以作为电子病历(临床记录、实验室价值史和成像文件)的高效、准确的接口[Li et al. 2020c;Steinberg et al. 2021;Percha 2021]，帮助医疗保健提供者创建患者访问摘要[Krishna等人，2020]，检索相关病例和文献，并建议实验室测试、诊断、治疗和出院[Zhang等人，2019b];Rasmy et al. 2021]。
基础模型也可以用来帮助手术机器人监测并实现精确的手术[Diana and Marescaux 2015;Agrigoroaie and Tapus 2016;Yu et al. 2019]。
关于机器人基础模型的更多讨论，请参见§2.3:robotics。
患者界面。
基础模型可以作为患者的界面，提供有关临床预约的相关信息[Bates 2019]，回答与预防保健相关的患者问题[Demner-Fushman et al. 2020]，以及相关的医疗解释信息(例如，解释条件的文本和图形)[Chaix et al. 2019]，并帮助患者辅助护理机器人[Jeong et al. 2015];Abdi et al. 2018]。
关于用户交互的基础模型的更多讨论，请参见§2.5:交互。
基础模型还可以作为与公众的接口，回答与公共卫生和大流行预防(如COVID-19病例)相关的问题[Bharti等人，2020;Herriman et al. 2020]。
同时，我们注意到，界面必须保证事实准确性，以确保公众对医疗建议的信任[Kreps和Kriner 2020](见§3.1.3:
healthcare-biomed-challenge)。
3.1.2生物医药领域的机会。
基础模型可以促进生物医学研究，如药物发现和疾病理解，最终转化为改进的医疗保健解决方案[Hanney et al. 2015]。
目前，生物医学发现需要大量的人力资源、实验时间和财务成本。
例如，药物开发涉及一个复杂的过程，从蛋白质靶点识别和有效分子发现的基础药物研究到临床开发(如临床试验)，再到最终的药物批准，通常需要10年以上的时间，成本超过10亿美元[Wouters et al. 2020]。
利用现有数据和已发表的发现促进和加速生物医学发现是生物医学领域的一个迫切问题[Yu et al. 2018]。
特别是，COVID-19等新型疾病爆发造成数百万人死亡和数万亿美元的损失[Lalmuanawma et al. 2020;McKibbin et al. 2020];如果我们能加快新疾病的药物开发，那将非常有帮助。
基础模型在两个方面对生物医学发现特别有帮助。
首先，基础模型具有强大的生成能力(例如，GPT-3中的连贯文本生成)，这可以帮助生物医学研究中的生成任务，例如根据现有数据生成实验方案(临床试验)和设计有效分子(药物发现)[Kadurin等人，2017;Harrer et al. 2019]。
其次，基础模型有可能整合医学中的各种数据模式，从而能够从多个尺度(使用分子、患者和群体水平的数据)和多个知识来源(使用成像、文本和化学描述)调查生物医学概念(例如疾病)。
这有助于使用单模态数据难以获得的生物医学发现[Lanckriet et al. 2004;Aerts等人，2006;Kong et al. 2011;Ribeiro et al. 2012;Wang et al.， 2014;;Ruiz et al. 2020;吴等。[2021h]。
基础模型还可以跨模式转移知识。
Lu等人[2021a]展示了基于自然语言(一种数据丰富的模态)训练的变压器模型如何适用于其他基于序列的任务，如蛋白质折叠预测，这是生物医学中一个长期研究的预测任务[Jumper等人。2020]。
以下是将受益于基础模型的生物医学重要任务的例子。
药物发现。
为了发现一种治疗疾病的药物或疗法，研究人员必须首先确定一个目标(例如，与疾病有因果关系的蛋白质、基因、RNA)，然后必须寻找与目标结合并治疗疾病的分子(例如，化合物、抗体)。
[Whirl-Carrillo et al. 2012;Tatonetti et al. 2012;ger蜇等，2017;Grinfeld et al. 2018;Adam et al. 2020]。
基础模型在整合从电子病历[Rajkomar等人，2018]到医学成像[Bera等人，2019]的多模式患者数据方面具有独特的强大功能;欧阳等。2020]到药物和分子测量[Gottlieb等。2011;Ruiz et al. 2020]进行最优预测。
临床影像[Chaitanya et al. 2020]，基因[Ji et al. 2021]，蛋白质[Jumper et al. 2020])，而不是联合学习不同的模式。
为了从这些不同的多模态医学数据中学习跨模态和跨模态信息，我们需要在基础模型的训练中研究特征级和语义级的融合策略。
如果做得有效，这有可能统一生物医学知识并促进§3.1.2中讨论的发现:
biomed-tasks。
Explainability。
可解释性——为决策提供证据和逻辑步骤——在医疗保健和生物医学领域至关重要[Holzinger等人，2019]，并且在《通用数据保护条例》(GDPR)下是强制性的。
例如，在诊断和临床试验中，必须将患者症状和时间相关性作为证据加以解释。
这有助于解决系统和人类专家之间潜在的分歧。
医疗保健中的知情同意也需要可解释性[Amann等人，2020]。
然而，目前基础模型的训练目标不包括可解释性，这需要未来在这个方向上进行研究[Linardatos et al. 2021]。
结合知识图谱可能是进一步提高模型可解释性的一步[Roberts等人，2020;Xu et al. 2020;Jin et al. 2021]。
读者可以参考§4.11:可解释性以获得关于可解释性的更多讨论。
法律和道德规范。
医疗保健应用程序必须遵守具有保障的法律和道德规范，例如患者安全、隐私和公平性。
例如，关于安全性，基础模型所做的预测必须与已有的医学知识在事实上准确，并且必须量化不确定性，或者在不确定时选择服从专家[Challen et al. 2019;Mozannar and Sontag 2020]。
对于隐私，患者健康记录的使用必须遵守隐私法，例如美国的HIPAA[1996年法案]。
联邦学习是在基础模型训练中保持原始敏感数据私密性的一种潜在解决方案[Chamikara et al. 2021]。
为了公平起见，研究人员需要注意常见的陷阱，否则可能会加剧现有的社会不平等[Chen et al. 2019;Wiens et al. 2019;陈等。2020b]。
它们必须确保基础模型的培训和评价数据充分代表不同的性别、种族、民族和社会经济背景;一个医学数据集和临床试验长期存在偏见的领域[Martinez-Martin et al. 2020;Kaushal et al. 2020]。
在代表性数据稀缺的情况下，还需要对模型进行去偏见和正则化研究，以确保公平性[Zhao et al. 2020a]。
基础模型开发人员还需要咨询伦理和法律研究人员，并在部署它们的特定环境(例如，国家、地区)中遵守法规。
我们还请读者参阅§4.7:安全性，§4.8:鲁棒性，§5.1:公平性，§5.4:合法性，以了解有关隐私，鲁棒性，公平性和合法性的详细信息。
外推法。
生物医学发现的过程涉及外推。
例如，基础模型必须能够快速适应新的实验技术(例如，新的分析方法，新的成像技术，如高分辨率显微镜)或新的设置(例如，新的目标疾病，如COVID-19) [Jaroch等人，2018;Benam et al. 2019]。
利用现有数据集并推断新设置的能力是生物医学中关键的机器学习挑战[Snell等人，2017;马等。[2021b]。
虽然GPT-3表现出一些外推行为(例如，生成以前未见过的新文本)，但其机制尚不清楚，仍处于起步阶段。
需要进一步的研究来提高基础模型的外推能力，特别是考虑到医疗保健和生物医学固有的各种数据模式和任务，但在当前的GPT-3和相关模型中尚未得到普遍研究。
参见§4.8:健壮性。
3.2法律
作者:Peter Henderson, Lucia Zheng, Jenny Hong, Neel Guha, Mark Krass, Julian Nyarko, Daniel E. Ho基础模型在法律中扮演什么角色?32
一个重要的承诺是，基础模式可以通过消除法律服务的程序和财务障碍，改善获得司法和政府服务的机会。
法律应用带来的挑战反过来可以激发基础模型的基础研究问题。
许多法律应用程序对计算解决方案提出了独特的挑战。
法律语言是专门化的，法律结果往往依赖于将模棱两可和不明确的标准应用于各种以前看不见的事实模式。
同时，由于成本高，标记训练数据是稀缺的。
根据具体的任务，这些特性可能对传统模型的成功部署构成不可逾越的障碍。
相比之下，它们的灵活性和从少数例子中学习的能力表明，基础模型可以独特地定位于解决上述挑战。
在本节中，基础模型可以采用多种形式作为证据:审判过程中的音频，发现过程中的视频和图像，以及进行法律研究的文本。
然而，大多数依赖基础模型的法律任务都涉及基于文本的输入和输出。
因此，我们主要关注基于文本的领域，而对其他领域仅作简要讨论。
为了展开讨论，图13描述了美国民事诉讼的各个阶段，以及基础模型可能在这个过程中发挥作用的地方。
图14显示了生成法律摘要一段的一部分所需要的逻辑流，它可以作为基础模型某一天可能被用于的任务的具体示例。
一个重要的考虑。
在继续之前，我们注意到在§5.6:伦理、§5.4:合法性和§5.1:公平性中展开的伦理、法律和公平考虑特别重要，因为在应用法律或政府环境中使用基础模型之前，这些应用通常会对受影响的人产生重要的、现实世界的后果[Surden 2020]。
基础模型也必须在部署之前进行彻底的检查，如§4.4:评估中所讨论的。
例如，法律体系特别强调——甚至可能强制要求——透明度、问责制和可解释性。
因此，目前的模式是否能够解决许多最紧迫的法律问题是值得怀疑的。
尽管如此，扩大和改善获得法律和政府服务的机会的需求为基金会模式提供了一个有价值的目标。
3.2.1法律方面的机会。
法律应用的范围可以从在政府环境中使用机器学习[Engstrom等人，2020;Coglianese and Ben Dor 2020;Re and Solow-Niederman 2019]协助律师提供法律服务[Zheng et al. 2021;黄等，2021b;Ostendorff et al. 2021;沃尔德和康拉德2021]。
我们注意到，之前的工作也调查了基于文本领域的机器学习辅助法律任务[Zhong et al. 2020;Chalkidis et al. 2020]，尽管有人注意到，最近的法律人工智能研究主要集中在美国以外的地理区域[Zheng et al. 2021]。
虽然我们在这里讨论的许多主题可能适用于不同的法律体系，但由于我们团队的专业知识，我们主要关注美国。我们特别关注可能受益于美国法律体系基础模型的三大类法律应用:私法或民事司法(因合同、财产或侵权而引起的私人之间的索赔)、刑法(即对个人犯罪行为的起诉)和(非刑事的)公法(例如政府机构对私人行为的监管)。
民法。
在美国民事诉讼中，当事人通常必须找到并支付律师费。
因此，许多人，尤其是那些低收入的人，很难获得足够的法律代理[Rhode, 2004]。
基础模式有可能通过降低成本、提高质量和扩大法律服务的覆盖面来改善诉诸司法的机会。
在图13中，我们描述了在美国法院提起民事诉讼的过程，其中基金会模型可以在帮助律师和法官方面发挥作用。
甚至在律师参与法律程序之前，客户就可以从基金会模型的部署中受益。
最近的工作使用机器学习模型来识别客户提供的事实的简单语言描述中包含的相关法律问题。33
像这样的工具可以帮助提供解决手头问题所需的法律行动类型的建议，或者推荐专门的律师。
其他一些类似的努力也力求通过提供适合当事人特殊需求的信息来增加诉诸司法的机会[Cabral等人，2012;Brescia et al. 2014;Queudot et al. 2020;Westermann et al. 2019]。
一旦客户与律师交谈，在民事诉讼之前，律师可能会寻求避免昂贵的审判。
在这个阶段，他们可以依靠基础模型来评估合同，审查服务条款，查找相关专利，并进行其他诉讼前流程，以确保他们的客户处于优势地位[Betts and Jaep 2017;Elwany et al. 2019;Lippi et al. 2019;Lee and Hsiang 2019;hendricks et al. 2021c;Hegel et al. 2021]。
值得注意的是，最近的工作描述了使用基础模型进行合同审查的挑战和好处[Leivaditi等人，2020;黑格尔等人2021;hendricks等。2021c]。
除了审查和起草法律文件外，还可以翻译客户互动和文件，以降低提供法律服务的成本和障碍[cusamllar 2019]。
但法律文件的翻译需要准确性和对高度技术性语言的理解，这使得收集培训数据的成本很高。
此外，翻译客户陈述或审判程序通常需要了解当地方言和语言。
这也使得很难收集足够的真实翻译数据来进行训练。
因此，传统的监督方法很少达到法律领域所需的准确度[Vieira et al. 2020]。
在这些资源匮乏的环境中，通过快速适应，基础模型可以在完全监督机制的基础上提高这一领域的性能。
在诉讼过程中，基础模型可以帮助律师进行法律研究，起草法律语言，或评估法官如何评估其索赔[Zheng et al. 2021;黄等，2021b;Ostendorff et al. 2021;沃尔德和康拉德2021;Chalkidis et al.， 2019 [j]。
这可能会降低法律服务的成本并改善法律服务。
例如，最近的工作利用预训练模型在编写法律文本时推荐相关引用和持有声明[Zheng等人，2021;黄等，2021b;Ostendorff et al. 2021]。
其他工作使用预训练模型来改进法律问题回答，为常用的法律搜索引擎提供动力，并帮助律师进行法律研究[Vold and Conrad 2021]。
各种各样的工作也检查了自动合同起草和审查，这项任务同样可以从基础模型中受益[Hendrycks等人。2021c;Betts and Jaep 2017]。
也许最引人注目的是，基础模型可以帮助律师生成法律摘要(书面论点)。
这些模型可能会在律师撰写的摘要中发现新颖的论点或识别问题。
例如，Tippett等人[2021]根据从归档摘要中提取的特征预测法律程序的结果。
基础模型可以利用原始语言作为输入，而不是提取的特征。
这可能会为律师提供更翔实的建议，如何改进他们的简报，以确保有利的结果。
在打开和回复简报后，各方开始发现过程，这已经使用了十年的大部分时间的简单机器学习模型[Grossman和Cormack 2010]。
律师使用这些系统来标记是否应该向对方出示文件。
文档本质上是多模态的，通常包含视频、图像、音频和文本。
目前的系统成本很高，因为它们使用监督学习和主动学习来标记文件为响应性[Grossman and Cormack 2010;Oard et al. 2018;Yang et al. 2021]。
相反，基础模型可能具有的少量或零次文档检索功能将有助于减轻对当前过程的大量成本的关注。34
为了避免在发现过程中出现博弈的可能性，Cui[2018]提出了一种零枪(或少枪)适应过程，该过程只能通过使用基础模型来实现。
在发现之后，一旦审判开始，基础模型可以通过预测法官在询问过程中可能关注的内容来帮助当事人为审判做准备[Dickinson 2018]，适应法官先前发表的意见的当前背景。
在法庭上，基础模型可用于检查法庭诉讼的音频和视频，以确定结果是否因种族或方言而对被告有偏见。35
一旦审判结束，基础模型可以帮助法官和法律助理使用类似的技术正确评估双方的法律主张，或者使用基础模型的上下文嵌入可能有助于法律解释[Nyarko和Sanga 2020;崔2020]。
最近的工作(不依赖基础模型或NLP)研究了是否可以从一组提取的特征(如引用计数和关键词的出现)中预测上诉决定[Katz et al. 2017;Boniol et al. 2020]。
有可能使用基础模型改进这些模型，并应用于帮助法官通过标记其意见中的明显错误来起草决定，正如在裁决机构的背景下所讨论的那样[Engstrom等人，2020;Ray and Lubbers 2014]。
它们还可以用来识别法律意见中的种族偏见，并帮助法官相应地修改他们的意见[Rice et al. 2019]。
刑法。
一个特别有争议的领域是在政府机构中使用风险评分，特别是在刑法中。
有些人可能希望使用基于语言的基础模型来帮助根据给定的基于文本的事件叙述做出收费决定或假释决定。
由于潜在的偏差，在使用基础模型进行风险评分之前必须仔细考虑，特别是当包含语言数据时[Bender et al. 2021;Berk et al. 2021;Laufer 2020]。
但基础模型可能在刑事司法的许多其他方面发挥作用。
上述民事诉讼中使用的工具也可以被检察官和辩护律师使用。
这可以帮助指定的律师更有效地执行他们的工作，减少不必要的开销。
因此，他们可能能够更有效地平衡已经繁重的案件量。
例如，公设辩护人往往被视为工作过度和资金不足，这将导致本可避免的程序错误。36
基础模型可以通过识别错误和自动化简单任务来帮助减少这些资源约束。
然而，它们本身并不是解决方案。
在其他领域，基础模型可以作为一种监督机制来减少结构性不平等。
预训练模型已被用于处理假释听证记录，以发现异常结果的实例[Bell等人，2021]。
最近的研究还删除了警方报告中有关嫌疑人种族的语言线索，以促进种族歧视的指控决定，避免种族偏见的起诉[Chohlas-Wood等人，2020]。
其他工作有助于识别不尊重的警察通信[Voigt et al. 2017]。
在这些上下文中，标记数据的成本非常高，因为注释者必须获得对敏感数据的访问权限，并且通常需要进行适当的背景检查。
为了降低这些成本，基础模型可以用于预训练并快速适应标签稀缺的下游任务。
公共法律。
政府机构管理着社会的大部分，基金会模型在公法中具有广泛的潜在适用性。
这包括:在通知-评论过程中分析公众意见，协助专利审查，根据《信息自由法》的要求检索相关文件，协助大规模裁决等等。
最近的工作调查了这些政府在各种情况下的应用，我们建议读者参考相关资料进行深入讨论[Engstrom等人，2020;Coglianese and Ben Dor[2020]。
在许多这样的应用中，基础模型可以提高政府服务的质量、效率、效用和可访问性:标签是稀缺的，资源是受限的，环境是不断变化的。
因此，通常需要基础模型的适应性和灵活性来提高效率和性能。
为了给出一个这样的应用程序的说明性示例，现有的工作已经利用NLP在公共评论论坛中促进审核。
在这个用例中，预测模型帮助外行用户改进论证并识别评论中的错误陈述。
这样的系统已经部署在美国交通部的规则制定过程中[Park等人，2012]，尽管它可能会通过基础模型的语言推理能力得到改进。
但政府机构必须遵守宪法、法律和行政义务(见§5.4:合法性)，因此在这些情况下需要额外的注意。
3.2.2基础模型有何独特之处?
上述法律应用的例子在几个方面是独特的。
首先，标注数据的成本非常高。
通常，制作高质量标签的专业知识只能在律师身上找到，他们可能每小时收费数百美元。
即使在获得标签之后，某些数据也可能是敏感的，不能集中在一起训练一个大的语言模型。
鉴于最近在few-shot学习方面的进展[Brown et al. 2020]，基础模型是具有有限注释的学习模型中最有前途的路径之一。
其次，法律决策需要不同尺度的背景:对所有历史决策和标准的了解，对与当前相关的判例法的了解，以及对手头个案的细微差别的了解。
基础模型具有独特的潜力，可以学习历史和法律背景的共享表示，并且具有为单个案例建模的语言能力和准确性。
3.2.3还缺少哪些基础模型需要进一步研究?
为了说明当前基础模型需要克服的缺陷，以便实际部署，我们以自动创建提交给法院的法律摘要为例。
在听证会前向法官陈述案情摘要。
一旦一方当事人提交了开庭摘要，对方当事人就会提交答辩。
然后，法官评估案情摘要，并在听证会上向双方提出问题，然后再作出决定。
图14显示了这样一个法律摘要的结构及其一些特征。
自动摘要生成机制可以将相关文档和案件事实(由律师指定)以及期望结果的大致草图作为上下文。
然后，它将生成一份包含复杂法律论据的法律摘要，提交给法院。
长篇文件和叙述。
为了实现这一目标，模型必须能够阅读较长的上下文并产生较长的叙述。
法律文件往往比任何其他情况下的文件都要长得多。
美国最高法院的意见书平均约有4700字，向最高法院提交的案情摘要可能多达1.5万字，一篇法律评论文章通常有2万至3万字，假释记录可能长达数百页[Bell et al. 2021]，审判记录可能更长。
当前的基础模型已经在这样长的上下文和输出中挣扎(参见§4.1:建模以获得更多讨论)。
检索，概念漂移，论点形成和逻辑推理。
除了阅读特定案例的文件外，基础模型还必须检索相关的判例法，了解哪些判例法仍然有效，哪些已被推翻，并考虑到自训练以来潜在的概念漂移。
随着判例法的发展，需要在基础模型中编辑基础信息方面做更多的工作[De Cao et al. 2021]。
使用检索到的法律标准，基础模型必须了解如何将它们编织成一个有说服力的论点。
新兴研究已经研究了使用基础模型来测量、检测和生成有说服力的文本的方法[Duerr and Gloor 2021;Li et al. 2020a;Longpre et al. 2019]，这可能为实现这一目标提供了垫脚石。
但法律简报也需要识别相关法律规则并确定如何将其应用于新情况的能力。
例如，Holzenberger等人[2020]提供了一个数据集和方法，用于评估基于税法的当前模型中的这种能力。
给定一组税法规则和对某人情况的描述，该模型必须确定这些规则如何适用于个人，以便确定他们的纳税义务是什么。40
Holzenberger等人[2020]发现，领域专家可以将相关税法表达为prolog求解器，以非常准确地确定个人的纳税义务。
然而，他们发现，仅仅使用BERT模型，即使是对领域内的法律文本进行了预训练，也不足以让模型推断和应用相关规则，这一过程也被称为“法律推理”。
”
这些挑战可以通过与GPT-3的真实交流进一步说明，表明当前的模型甚至无法执行涉及法律推理的相对简单的任务。
用GPT-3进行法律推理。
“违约金”是双方在协议中规定的对违约行为的一种货币补偿形式。
一般来说，美国法院不会执行违约金条款，如果合同中规定的金额太高，作为一种惩罚形式，而不仅仅是补偿。
首先，我们看看该模型是否编码了关于违约金条款执行的正确规则。
问:
违约金条款是否可执行?
答:
除非规定的金额过高或不合理，违约金条款通常是可强制执行的。
这条规则似乎是正确的。
然后我们把这个规则应用到一组事实中。
在这里，模型需要推断100万美元的损害赔偿对于丰田卡罗拉来说是过高的，因此不应该强制执行违约金条款。
问:
X和Y签订了一份销售丰田花冠的合同。
他们的合同规定，如果X不能按时交付汽车，X必须支付Y 100万美元的赔偿金。
X没有按时交货。
Y有权要求损害赔偿吗?
如果有，数量是多少?
答:
是的，Y有权获得100万美元的损害赔偿。
损害赔偿是为了补偿受害方因违约而遭受的损失。
由于X没有按时交货，Y损失了100万美元。
该模型无法推断丰田卡罗拉100万美元的价格过高。
即使在提示符中为模型提供了抽象规则及其对上下文的精确翻译(因此应用程序应该是微不足道的)，gpt -3仍然无法进行所需的逻辑推理。
问:
X和Y签订了一份销售丰田花冠的合同。
他们的合同明确规定，如果X不能按时交付汽车，X必须向Y支付100万美元的高额赔偿金。
X没有按时交货。
法律规定，除非规定的金额过高或不合理，否则必须执行违约金条款。
Y有权要求损害赔偿吗?
如果有，数量是多少?
答:
是的，Y有权获得违约金。
在这种情况下，金额并不过高，因为价格是由当事人自己确定的。
精度。
基础模型也不能在过程中创造虚假事实，这是当前模型中出现的问题[Gretz et al. 2020;Zellers et al. [2019b]。
在法律环境中，特异性和真实性是非常重要的，不精确的陈述可能会产生严重的、意想不到的后果，而虚假的陈述可能会导致对律师的制裁。
Few-shot学习。
除了上述例子中描述的，基础模型面临的一些挑战包括仍处于起步阶段的few-shot学习[Perez et al. 2021]。
研究几次学习技巧和增加对法律语料库的访问可以同时进行。
因为基础模型需要精确，不仅在如上所述的事实真相中，而且在技术法律语言中，从一个语料库获得的信息在多大程度上可以在不同领域的另一个语料库中使用仍然不清楚。
因此，短时学习仍然很重要。
适应。
在未标记的法律语料库上进行领域自适应预训练已经取得了一些成果。
当预训练语料库与下游任务高度相关并且标记的训练数据有限时(法律中常见的设置)，这些收益似乎最为明显[Zheng等人，2021]。
尚未全面研究这是否扩展到各种法律任务，但是利用未标记的领域特定语料库进行基础模型的自我监督训练可能会为少量方法提供补充改进。
访问清洁域内数据。
最近的一些努力试图通过自动化[Zheng et al. 2021]或由志愿法律专家手工注释[Hendrycks et al. 2021c]来创建大型标记数据集，以完成更具挑战性的法律基准任务。
这些努力表明，与在其他环境中观察到的有限收益相比，在更多数据上预训练的更大的语言模型在某些具有挑战性的任务上实现了性能提升[Chalkidis et al. 2020;Elwany et al. 2019;钟等。2020]。
这项工作表明，可能需要更大的法律基准数据集来观察将迁移学习技术应用于基础模型的进一步收益。
然而，从NLP的角度来看，为具有法律意义和困难的任务创建基准数据集本身就具有挑战性，因为人类专家注释可能是昂贵的，而且利用传统标记化和句子分割技术的自动化方法可能无法解释法律文本的独特方面，例如法律引用的结构[Bommarito等人，2018;Savelka et al. 2017]。
作为这些挑战的结果，许多现有的法律领域特定的标记数据集很小，不公开可用，或者反映了通过通常早于基础模型开发的方法已经解决的更简单的任务。41
这种现有的法律数据也可能不具有代表性。
由于只有一小部分案件最终形成了法律意见，因此尚不清楚公开数据中的争议是否代表了实践中呈现给模型的典型争议[Priest and Klein 1984]。
对于更具代表性的场景，昂贵的培训数据可能集中在最大的律师事务所。
这些律师事务所可能有能力保留和积累许多案件和客户的数据。
一个值得关注的问题是，基础模型可能会将更多的权力集中在少数几个拥有资源的参与者身上，这些参与者可以在领域内的数据上训练模型——除非模型可以很好地泛化。
可靠性。
最后，我们再次注意到，即使基础模型可以成功地执行法律领域的所有任务，部署仍然是一个主要的挑战:法律基础模型的失败将对客户和律师产生真实的、破坏性的后果(参见§5.1:公平、§5.4:合法性和§5.6:道德中关于公平、合法性和道德的讨论)。
由于这个原因，机器翻译软件在一些法庭上已经被视为不可靠的证据，尽管在其他法庭上它继续被依赖。43
考虑到所有这些复杂性，法律简报和推理可能超出了当前模型的能力，但似乎在未来的可能性范围内。
因此，它们作为正在进行的基础模型开发的潜在的矿脉之星。
3.3教育
作者:Ali Malik, Dorottya Demszky, Pang Wei Koh, Moussa Doumbouya, Drew A. Hudson, Allen Nie, Hamed Nilforoshan, Alex Tamkin, Emma Brunskill, Noah Goodman, Chris Piech
教育中的基础模型可以在多个数据源上进行训练，以学习教育所需的能力:对各种主题和不同教学技术的理解。
这些基础模型可以以通用的方式应用于一系列任务和目标，例如理解学生、协助教师和生成教育内容。
2000年，世界各国领导人在联合国千年首脑会议上举行了规模最大的一次聚会，对未来的理想愿景进行了思考。
代表们的结论是，首要重点应该是教育，并宣布教育是“人类实现、和平、可持续发展、经济增长、体面工作、性别平等和负责任的全球公民身份的基础”。
这一讨论最终被重新纳入联合国可持续发展目标，即“确保包容和优质的全民教育，促进终身学习”[2015年联合国大会]。
然而，大规模提供高质量的包容性教育带来了困难的社会和经济挑战。
每个学生的教育成本的增长速度超过了整个经济的成本[Bowen 2012]，这限制了支持学生学习的可用资源。
在美国，一个症状是学生持有的私人教育债务已达到1.6万亿美元，超过了信用卡债务总额[Friedman 2020]。
考虑到提供成人再培训的需求日益增长，对教育的需求与我们提供教育的能力之间的差距惊人地大，并且在受保护的人口中存在令人担忧的成就差异。
随着数字时代的到来和数字学习的快速增长，计算教育方法在提高学习者和教师的效率方面显示出了希望。
人工智能在教育中的潜在影响应用已经出现了几个核心方向[Woolf et al. 2013]，例如可以向学生提供有意义的反馈的系统[Malik et al. 2021]，帮助教师提高[Jensen et al. 2020];Demszky et al. 2021;Suresh et al. 2021]，甚至创造个性化和适应性的学习体验，使学习过程适合个别学生的需求和性格[Connor 2019]。
尽管存在这种潜力，但事实证明，建立有效扩大包容性和提高教育质量的技术解决方案异常困难。
一个特别的挑战是，现有的工作集中在定制解决方案上，以解决高度特定的任务，而这些任务必须从头开始收集大量的训练数据。
由于创建大型数据集的难度和成本，使用这种方法独立解决每个教育任务从根本上是有限的。
相反，是否有可能创建跨各种任务和主题可重用的通用方法?
基金会模式已经开始提高教育领域某些特定旗舰任务的绩效。
最近的例子包括使用MathBERT [Shen et al. 2021b]来支持“知识追踪”——根据学生过去的回答跟踪学生随着时间的理解的挑战，以及“反馈挑战”，其中算法必须解释学生对结构化开放式任务的答案，例如编码问题[Wu et al. 2021e]。
基础模型可以在这个领域中导致更多的变革性的变化吗?
将基础模型应用于教育的已知和想象的风险是什么?
在本节中，我们首先围绕伦理考虑构建对话。
然后，我们将讨论建立在两个具体任务上:(1)了解学生的误解，(2)通过指导提高学生的理解。
3.3.1基础模型在教育研究中定位的重要关注。
人工智能教育的未来是令人兴奋的，尤其是在基础模型的背景下。
然而，我们提醒读者要特别考虑应用于教育的任何人工智能研究的影响。44
教育的目标与复杂的、长期的社会影响深深地交织在一起。
在我们积极努力改善数字教育的同时，我们必须投入大量的精力来尝试和想象这个领域的任何破坏的复杂性[Piech和Einstein 2020]。
伦理挑战包括数据偏见、法律约束和数字社会化的影响等问题。
这些问题并不是基础模型所独有的，但随着人工智能在教育领域的研究取得实质性进展，这些问题值得定期反思。
当研究开始问“新的人工智能技术能带来什么?”时，反思影响尤为重要。
§5.6:ethics中的许多问题也适用于教育。
例如，与许多其他领域一样，基础模型训练数据中的小偏差可能很难追踪[Dixon等人，2018;Bolukbasi et al. 2016]，但对教育机会公平具有重要意义。
此外，这些系统可能经历高度的“反馈”，其中收集的数据不断加强模型的决策。
这种偏见问题超出了收集数据的范围，还包括对研究人员选择从事的应用程序的关注。
下面，我们将讨论其他与教育相关的问题。
许多问题都围绕着这样一个问题:“谁受益?”
新技术是为谁创造的?
数字教育的目标之一，尤其是基于人工智能的数字教育，是提高学习体验的生产率，以便在单位时间或单位成本下实现更多的学习。
可以想象，决策者可以利用这种提高的生产力将人类教师从循环中移除。
这些决定的长期影响很难先验地知道。
与一个优化为最大化“学习”的教育系统互动是否会对社会情感技能的发展产生不利影响?
这会不会减少图16的机会?
该图展示了一个系统，该系统将来自各种形式(图像、语音、符号、文本)和语言的信号嵌入到一个通用的特征空间中。
这样的特征空间允许思想跨模态和语言联系起来。
与教学相关的链接类型包括类比(跨语言的相似性)和对比(跨语言的不同概念)，两者都可以在相同的模态中发生，也可以在不同的模态中发生。
与他人互动?
年轻一代的孤独感正在上升[Cigna 2018]，而教师是一种调节压力的力量，这是人工智能研究人员可能无法想象的。
这项工作是由学习者完成的还是由基础模型完成的?
另一个挑战是如何有效地教授那些能够使用基于基础模型的工具的学生。
例如，如果学生与一个强大的生成模型一起工作，教师要理解学生的贡献程度，或者规范无效的合作和检测剽窃，将会复杂得多。
Visual Studio最近发布了GitHub CoPilot，这是一个基于GPT-3构建的AI结对编程器[Chen et al. 2021f]。
这将如何改变计算机科学教育?
新手程序员面临的许多挑战对于CoPilot或其技术后继者来说可能是微不足道的，这可能会破坏新手的学习体验。
研究技术进步扰乱某些学科教育的其他例子，比如数学课堂上的计算器和语言课程上的谷歌翻译，这两者现在都与传统教学共存，这将是有益的。
隐私和安全。
在教育中使用人工智能的一个重要的伦理问题是关于学生工作隐私的严格法律准则。
例如，在美国，学生信息受到《家庭教育权利和隐私法》(FERPA)的保护。
这些法律法规对13岁以下的儿童尤为重要，他们的数据隐私和安全受到《儿童在线隐私保护法》的额外保护。
除此之外，FERPA还限制教师分享个人身份的学生作业。
这可以直接影响共享用于培训和评估基础模型的数据的计划。
此外，还有一个悬而未决的问题，即基础模型的权重是否会以某种方式泄露它所训练的(可能是私人的)数据[Nasr et al. 2018;Song et al. 2017]。
这些问题及其相应的方法与§3.1:医疗保健中描述的挑战类似。
此列表并非详尽无遗，伦理考虑因项目而异。
3.3.2学生思维的基础模式。
在为包容和快乐的教育构建人工智能工具时，基础模型可以在许多任务中发挥作用。
其中许多任务都要求我们首先了解我们要帮助的学习者，特别是在开放式工作的背景下。
一个基础模型要怎样才能推断出学生的理解能力呢?
很容易想象一个基础模型被用来正确地回答一个数学问题，但是如何建立一个可以根据学生的答案诊断学生理解错误的模型就不太清楚了。
为了探索这个主题，我们考虑了一个案例研究，为那些正在完成开放式任务的学生提供反馈，比如写一个短段落，画一个物理图，或者写代码。
这个“反馈挑战”举例说明了基础模型如何对学习者有现成的帮助，也展示了基础模型研究的开放领域。
为了有效地向学生提供反馈，需要两个核心能力:(1)理解任务的主题(例如，物理或编码)，以及(2)“注意”的诊断能力:一个教育中的技术术语，用于推断学生犯错误的原因。
对于典型课堂中典型的学生互动，没有足够的数据让人工智能模型从零开始学习这两种核心能力。
即使对于拥有数百万学生的大规模课程，监督算法也很难理解即使是简短的四行程序背后复杂的学生推理[Malik et al. 2021]。
因此，反馈任务本质上需要从外部数据和经验中转移理解。
基础模型，正如它们目前存在的那样，对这些能力中的第一个有直接的帮助:理解一个特定的主题。
例如，当学习为简短的编程问题提供反馈时，像GPT-3这样的基础模型可以通过几个例子有效地理解流畅的代码是什么样子的。
这方面的一些研究已经开始探索能够快速适应新主题领域问题的基础模型[Wu et al. 2021e;Condor et al. 2021]。
类似地，基础模型也可以集成多种模式的信息，例如任务提示的文本、问题中的图表，甚至是提供给助教的评分标准的内容。
这种统一的表示能力可以帮助基础模型通过更丰富的信息源来理解主题。
作为一个具体的案例研究，许多这些见解被用作一种算法的核心组成部分，该算法能够在斯坦福大学的计算机科学入门期中评分，其效果与人类助教相同[Wu et al. 2021e]。
在这种情况下，主题编码是建立在一个基础模型上的，这个模型是根据GitHub代码改编的，每个问题的主题都有一个相应的小数据集。
一般来说，我们可以想象利用各种数据源来调整基础模型以适应不同的主题。
例如，数学适应可以使用数学网站或教科书[Shen et al. 2021b]或Gradescope等平台上的历史学生答案;口语理解可以利用广播档案或播客;而像创意写作这样的领域可以参考像古腾堡计划这样的大型数字档案。
与主题相比，将基础模型用于将观察到的错误映射到学生思维过程中的缺陷，这一任务的探索要少得多。
教师“注意到”学生犯特定错误背后的原因的能力是反馈挑战的关键组成部分。
想象一下，例如，一个学习两位数加法的学生回答了“26 + 19等于几”这个问题，得到的答案是“315”。
花点时间，试着猜猜他们为什么会给出这个答案，他们有什么误解。45
．
这种注意能力可以作为基础模型的适应任务(§4.3:适应)，或者甚至可以作为推理任务(§2.4:推理)。
虽然很难，但训练人工智能系统去注意是一个可以实现的目标。
跨教室，跨给定领域的学习任务，在学生如何得出答案方面存在一般化的模式。
可以直接用于此适应任务的标记数据，例如[Wu等人。2021e]中教师对学生作业的书面反馈，通常由教师在不同的数据集中私下持有。
然而，可以公开访问的数据，例如StackOverflow交互，也可以创造性地用于调整基础模型以引起注意。
一些研究还探索了从教师那里提取学生如何犯错误的生成描述的有效方法[Malik等人，2021;Gulwani和Singh 2013]——这些手写的生成模型也可以用来生成适应数据，以帮助基础模型诊断学生的错误。
3.3.3教学基础模型。
推理学生的理解是实现第二个目标的重要一步:提供包容的、高质量的教学。
教学的计算方法侧重于不同的任务，如内容个性化[Connor 2019]，问题生成[Guo et al. 2016];Willis et al. 2019;Srivastava and Goodman 2021]，适应性课程设计[Mandel et al. 2014;Doroudi等。2017]，并预测教师干预[Chandrasekaran and Kan 2019;Alrajhi et al. 2021]。
在本小节中，我们将讨论基础模型在教学中如何发挥作用。
由于有效的教学需要对学生的理解进行推理，所以前面关于理解主题和“注意”的讨论是非常相关的。
然而，提供有效的教学需要额外的能力:理解教学法的能力[McKenzie 2003]。
这包含了对指导学生的技巧的有效理解，例如问苏格拉底式问题或提供类比/对比案例;使用鼓励或支持的语言;为学生量身定制问题的难度;并生成与学生的兴趣和背景相关的例子。
如何调整基础模型来理解良好的教学方法?
一种想法是考虑使用数据源进行适应，其中指令是主要角色。
例如，来自像StackOverflow这样的问答论坛的数据可以用来建立一个可以模仿常见苏格拉底问题的导师。
类似地，基于百科全书(如维基百科)的基础模型可能能够给出(通常)事实正确的学生问题的答案。
还有一些公共数据源，如教科书、讲座视频、课程计划和分级反馈，它们共同包含了重要的教学行为，可以被基础模型所适应(图15)。
基于基础模式的教学的另一个适应性挑战是学习如何像老师一样与学生交谈。
教师使用的语言往往不同于普通大众使用的语言。
理想情况下，教师应接受培训，以尊重的方式与学生交谈，并有意帮助他们对所学科目形成积极的认同[Truax 2018]。
一些值得警惕的例子，比如微软2016年推出的Twitter机器人“Tay”，这个聊天机器人在上线24小时内就开始产生仇恨言论，向我们展示了在教育中明确考虑这一因素的重要性。
为了训练一个在课堂上更受专业教师影响的语言模型，我们也许可以将基础模型适应于讲座视频或录制的办公时间视频等数据源。
不同的教育环境在语言类型上有很大的不同，这一事实使上述适应问题更加复杂:例如，五年级科学课的有效教学与大学物理课的教学完全不同，更不用说大学文学课了。
这就提出了在典型的NLP领域转移设置中所面临的技术挑战(例如，基于新闻文章与Reddit帖子的问答)，因为基础模型需要在语气和语言方面具有流畅的适应性，而不仅仅是它生成的事实内容。
除了健全的教学技术和教学语言，基础模型如何提供更有洞察力的教学形式?
这篇论文的语言突出了一个事实，即婴儿可以在很短的时间内习得非常复杂的语言。
正如作者所指出的，基础模型训练和人类语言习得之间的一个显著区别是，“人类语言是建立在现实世界的基础上的:例如，婴儿的看护人在谈论物体时指着物体。”
同样的见解也可以启发关于如何将基础模型用于生成教育的想法。
当面对现实世界的类比和对比时，人类似乎学得很好，这些类比和对比可能是他们当前环境和过去经验之间的交叉。
例如，在教授手语时，老师可能会使用类似的类比，如“单词'morning'的手部形状看起来像太阳升起”，或者注意“你刚刚做出的手部形状看起来与另一个单词非常相似，所以让我们专注于差异。”
另一个例子是，当教授斯瓦希里语给一个已经懂阿拉伯语和英语的学习者时，老师可以指出斯瓦希里语中的8(发音为name)是一个“假朋友”，发音类似于英语中的9(发音为nine)。
能够整合多模态数据的基础模型有潜力进行这些丰富的类比和比较，这在儿童语言学习中是典型的(图16)。
4技术
基础模型的技术基础产生了决定其潜力的能力(§2:能力)。
为了理解开发中使用的技术，我们考虑用于训练(4.2:训练)的数据(第4.6节:数据)、模型架构(4.1节:建模)和系统(4.5节:系统)，并进一步适应(4.3节:适应)这些模型以及应该开发的理论(4.10节:理论)来理解这种范式。
为了理解所得到的模型，我们讨论了如何评估(§4.4:评估)和解释(§4.11:可解释性)以及鲁棒性(§4.8:鲁棒性)、安全性和隐私性(§4.7:安全性)和长期人工智能安全性(§4.9:人工智能安全性)的重要性，以确保这些模型在社会中部署时的可靠性(§5:社会)。
4.1建模
作者:Drew a . Hudson, Antoine Bosselut, Alex Tamkin, Omar Khattab, Jared Quincy Davis, Jiaxuan You, Trevor Gale在过去的几年里，基础模型的新兴范式在人工智能领域取得了令人印象深刻的成就，像BERT [Devlin等人，2019]这样的模型在广泛的语言理解任务中大出神彩:从文本分类和推理到问答和阅读理解，GPT-3编写了关于独角兽的丰富而流畅的故事[Brown等人，2020]，而DALL-E显示了视觉创造力的迹象，从无到有地生成了非常逼真的鳄梨椅图片[Ramesh等人，2021]。
最近基础模型的这些和其他实例不仅在众多不同的下游任务和应用程序中实现了卓越的性能[Rajpurkar等人，2018;Wang et al. 2019a]，但也表现出值得注意的可解释性行为[Karras et al. 2020]、鲁棒性行为[Devlin et al. 2019]、可控性行为[Patashnik et al. 2021]和泛化行为[Brown et al. 2020]。
一个模特怎样才能表现出这些品质呢?
什么样的架构能够消耗大量潜在的多模式信息，并将其转化为丰富的世界知识?
总的来说，一个网络应该拥有什么样的理想属性才能产生一个基础模型?
在这里，我们确定并讨论了五个这样的属性，跨越表达性、可扩展性、多模态、记忆容量和组合性，我们认为这是基础模型必不可少的，以便:(1)从各种来源和领域提取和积累知识，(2)将其组织成有效和可扩展的表示，以及(3)灵活地将其推广到新的环境中。
对于每一个属性，我们都激发了它们的必要性，提供了结合它们的当代模型的例子，并探索了未来研究和开发的关键挑战和有希望的途径。
请参见图17查看概览图。
以下4.4.1表现性。
表达性关注的是网络的理论和实践能力，以对其训练的数据分布进行建模，并以灵活的方式表示它。
先前的工作已经提出了形式表达性度量来表征网络可以计算的函数的复杂性，或者更准确地说，近似，这主要受其深度，宽度，连通性和结构模式的影响[Raghu等人，2017]。
正如没有免费的午餐定理所表明的那样，没有一个模型或算法最适合所有情况[Wolpert和Macready 1997]，因此，就我们的目的而言，我们特别感兴趣的是确定哪些模型可以有效地捕获自然信息的各个方面，例如人类语言或现实世界的图像[Goodfellow等人，2016]。
这些模态要么是连续的(如视觉)，要么是离散的(如语言)，具有明显的层次和高维，并在其组成元素(无论是像素、文字还是物理对象)之间呈现出一组复杂的关系和相互作用。
事实上，最近在生成建模方面的突破为神经网络的高表达性提供了强有力的证据，因为它们成功地表达了文本的分布[Brown et al. 2020;Devlin et al. 2019;Lieber et al. 2021;Wang and Komatsuzaki 2021]，听觉[van den Oord等，2016]，视觉[Karras等，2020;Brock et al. 2018]域，并生成高保真度、多样性和真实感的样本。
归纳的偏见。
在过去十年中，神经网络在自然数据建模方面的成功很大程度上归功于网络的高深度，这可以通过它们所组成的堆叠非线性层的数量或它们在推理链中所采取的计算步骤的数量来粗略衡量。
大深度在增强网络的表达能力方面起着至关重要的作用，使它们能够形成强大的分层和分布式表示，可以从训练数据推广到新的未见过的示例[He et al. 2016b;Levine et al. 2020]。
通用近似定理[Lu et al. 2019b]确实指出，即使是简单的多层感知器(mlp)也可以表示广泛的函数集，而不同的归纳偏差，如在循环神经网络(rnn)或卷积神经网络(cnn)中实现的[Goodfellow et al. 2016]，可以提高学习效率并增强给定网络对不同形式信息建模的能力:前者是语言、语音和时间序列中常见的顺序数据，后者是图像或视频等空间不变信息。
变压器网络&注意。
与此同时，最近引入的变压器网络[Vaswani等人，2017]证明了捕获元素之间的远程依赖关系和成对或高阶相互作用的重要性。
它们建立在自我注意机制的基础上[Vaswani et al. 2017;Bahdanau等人[2014]，它可以缩短计算路径，并提供直接的方法来比较输入数据中的元素(例如句子中的代词及其先行词，或者两个句子中提到的是同一个主题)。
从另一个角度来看，在注意力和门接结构(如lstm [Hochreiter and Schmidhuber 1997]或mix -of- experts [Shazeer et al. 2017])中体现的乘法交互为mlp和cnn的刚性固定权重计算提供了更灵活的替代方案，动态地使计算适应手头的输入。
这证明特别有用的建模语言,例如,给定一个句子像X”“她吃冰淇淋,而前馈网络总是以同样的方式处理它,一个attentionbased模型可以调整其计算输入基本的上下文表示“吃”这个词如果介词短语(PP)附件X是“勺子”,或相反链接它如果X“冰淇淋”的意思,例如,对“草莓”(Zavrel et al . 1997年)。
通用计算。
与之前的架构相比，注意力的最后一个显著优势源于其更强的通用性，它不像卷积的局部接受域或循环网络的顺序假设那样，与特定的任务或领域紧密相连，两者都分别反映了视觉和语言模式的固有属性。
我们假设，注意力和转换器的通用性质有助于它们广泛适用于广泛的研究问题和应用[Liu等人，2019;Dosovitskiy et al. 2020;Hudson and Zitnick 2021]。
这种对比抓住了任务专业化和表达性之间更普遍的权衡:具有更强结构先验的模型可以利用它们来提高从这些假设中受益的特定任务的样本效率;相反，集成了较弱归纳偏差的模型学习得更慢，但反过来可以扩展到更大的数据量，并适应不同的领域，因为它们不依赖于限制性或特定任务的假设。
随着数据和计算变得更容易获取，我们观察到，对具有最小归纳偏差集的模型的探索，可以“让数据为自己说话”，似乎是该领域未来研究的更有前途的方法。
挑战与未来方向。
尽管神经网络总体上取得了巨大的进步和成就，尤其是基础模型，但在表达能力方面，仍然存在着显著的挑战。
领先的方法[Choromanski et al. 2020;Dosovitskiy等人。2020]一直在努力建模极度长期的依赖关系，例如那些发生在书籍，电影甚至DNA序列中的依赖关系，这可能归因于当代基于变压器的方法的二次计算[Wang等人。2020c;Lin et al. 2021]。
这一挑战本质上反映了效率和表达能力之间的权衡:一方面，通过短而直接的计算路径对远程交互进行显式建模，提高了表达能力，但另一方面，由于连接性增加所带来的计算，以牺牲可扩展性为代价[Child等人，2019;Kitaev et al. 2020;Choromanski et al. 2020]。
GANformer [Hudson and Zitnick 2021]和Perceiver [Jaegle et al. 2021b,a]等模型探索了平衡这两种特性的方法，并提出了依赖于双部或瓶颈注意力的线性复杂性变压器，从而在保持高表达性的同时提高计算效率。
我们相信，确定这两个目标之间的有效平衡为未来的研究提供了一个有趣的途径。
另一个重要的研究方向与基础模型的扩展有关，到目前为止，基础模型主要集中在语言领域[Peters et al. 2018;Devlin et al. 2019;Brown et al. 2020]，到不同的模式，如结构[Scarselli et al. 2008;veli<e:1> koviki et al. 2017]和perceptual [Tolstikhin et al. 2021;Jaegle等。2021b;Tan和Le 2021]，每个都涉及一组独特的相关挑战。
同样，我们认为探索推理架构(§2.4:推理)，这需要迭代计算链和与符号信息的交互，构成了未来基础模型研究的一个有价值的目标。
4.1.2可伸缩性。
与模型的表现力密切相关的是可伸缩性的概念。
随着来自不同来源的丰富数据变得更容易获得，计算资源变得更强大、更高效(§4.5:系统)，我们应该寻找与这种进展速度相匹配的方法，并利用它来提高人工智能的能力和多功能性。
为了使基础模型有效地适应图像或文本的复杂和高维分布，它们应该在所有维度上都是可扩展的:包括模型的深度和宽度，以及它们的训练时间、参数数量和它们可以处理的数据量。
优化。
具体来说，基础模型应该:(1)易于训练(§4.2:训练)，对数据中的噪声或缺陷具有弹性，并且对消失等不稳定性具有鲁棒性[Helfrich等人，2018;gloot and Bengio 2010]或爆炸梯度[Hochreiter and Schmidhuber 1997;Nair and Hinton 2010]，而且(2)易于适应(第4.3节:适应)，通过克服灾难性遗忘现象[Kirkpatrick et al. 2017]和支持少镜头学习[Sung et al. 2018]。
我们仍然处于理解驱动学习算法可扩展性的原理的早期阶段，虽然最近的工作已经开始阐明这些主题[Liu et al. 2020c;Kuditipudi等人，2019;Nakkiran et al. 2019]，还有很多工作要做。
硬件的兼容性。
除了健壮性和优化方面，基础模型也应该是实际有效的(§4.5:系统)，并利用当代和未来的硬件[Hooker 2020]。
其中一个例子是并行性，这是表征gpu支持的计算的一个重要属性。
事实上，许多变形器比以前占主导地位的循环方法取得的巨大成功是由它们更高程度的并行性驱动的。
展望未来，鉴于系统发展的快速发展，我们应进一步确保模型的设计能配合未来硬件的发展。
因此，基础模型在理想情况下应该适用于分布式训练等方案，这种方案越来越受欢迎，例如混合专家，并可能利用诸如计算或表示的稀疏性等属性，例如Longformer [Beltagy等人2020]，BigBird [Zaheer等人2020]和Sparse Transformer [Child等人2019]方法，并且可能在未来的硬件和处理器中变得更加核心。
4.1.3多峰性。
传统上，计算机视觉、机器人技术和NLP领域以独立的方式取得了进展，不同的社区开发适合每种模式的特定方法。
深度学习的兴起带来的一个有益的结果是，它帮助在人工智能的各个社区和研究领域之间建立了桥梁，因为看起来不同的问题现在可以通过密切相关的方法来解决，并且原本遥远的主题的研究开始收敛到一个共同的基础上。
这一突破开辟了一系列新的可能性，促进了对多模态主题的开创性探索，包括语言基础[Lynch and Sermanet 2020]，视觉语义[Conser et al. 2019]，体现环境[Savva et al. 2019a]和交互式代理[Gray et al. 2019]等多种领域。
从本质上讲，多模态是智力的关键组成部分，是发展对世界的全面和广泛理解的关键因素。
具体来说，语言学习在有基础的环境中比在真空中更有效。
相反，从视觉的角度来看，语言鼓励抽象的出现，将低级感知信号和统计数据与对象、属性、代理和动机的语义概念联系起来，从而丰富和提升视觉表征。
根据这些观察结果，我们认为基础模型应该理想地将不同的模态连接在一起，将它们的具体信息提炼成一个共享的多面表示，并捕获它们之间的全部相互联系和关系，从而提供广泛的功能(参见§2.1:语言，§2.2:视觉，§2.3:机器人，§2.4:推理)。
通用性和专门化。
多模态基础模型的一个重要设计选择是专业化程度，或负责每个模态的模块之间的结构共享。
自然，不同领域的数据表现出不同的结构和性质，例如，语言是离散的，而视觉是连续的。
乍一看，这种变化暗示，为每种模式量身定制的专门归纳偏见可能会有所帮助。
然而，随着训练规模的扩大，模型有机会更少地基于结构先验而更多地基于数据本身进行学习，事实证明，仅保留少数广泛的一般假设的一般方法比特定任务的替代方法要成功得多。
因此，最近通用模型(如跨不同模态的变压器)的成功证实了这一点——无论是语言学上的[Liu et al. 2019;Lan et al. 2019]和visual [Dosovitskiy et al. 2020;Hudson和Zitnick 2021]，我们看到通用性对于提高人工智能能力至关重要。
多通道交互。
多模态模型的另一个关键考虑因素与权重共享有关:各种模态是否从为各自的组件使用相同或不同的参数中受益?
先前的研究表明，有效的转移当然可以跨模式发生，但理想的共享程度仍然不清楚，因此发现它的原则方法的存在也是不清楚的。
最后，一个主要的设计问题与模型支持的多模态交互形式有关，具体情况和示例之间差异很大:交叉模态或后期融合模型，如ConVIRT [Zhang等人，2020a]和CLIP [Radford等人，2021]为每个数据源保持完全独立的编码器，并且仅在最终计算阶段比较它们的空间，例如使用简单的点积。
同时，早期融合模型，如ViLBERT [Lu et al. 2019a;Cho等人[2021]，共同对视觉推理和问答任务所需的多种模式进行推理。
确定合并各自向量空间的最佳阶段和形式[Nagrani et al. 2021]仍然是一个开放的研究问题。
总体而言，虽然多模态的重要性在社区内似乎已经达成共识，但超越视觉和语言的浅对齐的模型尚未存在，并且在具体环境中基础语言学习的主题仍有很大的探索空间。
4.1.4记忆。
到目前为止，我们已经讨论了基础模型的目标，即在大尺度上从不同的模态中收集和积累信息。
这种知识既包括对世界的广泛理解，也包括对特定主题或特定事实的具体掌握。
表示如此庞大的学习信息体绝不是微不足道的，并且导致了有关访问，存储，检索和操作特定项目或记忆的有效机制的有趣问题。
显式存储。
实现这些理想的一个重要设计原则是将计算从内存中分离出来[Weston et al. 2014;Graves et al. 2016;Hudson and Manning 2018,2019a]通过将先前获得的抽象技能应用于新的具体环境来增强模型转移知识的能力。
在这种情况下，区分显性事实(可以存储在外部存储器中)和隐性知识(通过网络的可训练权重反映出来)是很重要的。
与通过网络权重将所有信息隐式编码在一起相比，这种显式和隐式知识的解耦具有许多优点。
这种分离减轻了模型大小和存储不断增长的知识所需的参数数量的膨胀[Guu等人，2020]，通过增加其知识来源提高模型的信任和可靠性[Cheney等人，2009]，最值得注意的是，它是记忆更新、操作或适应的关键[Lewis等人，2020b](§4.3:适应)，这反过来又可以实现对新上下文和下游任务的概括。
事实上，在过去几年里，记忆和计算之间的分离一直是深度学习和NLP研究中反复出现的目标，包括记忆网络(memory Networks)等模型[Weston et al. 2014;Sukhbaatar et al. 2015]，神经图灵机[Graves et al. 2014] [Graves et al.， 2016]]，神经状态机[Hudson and Manning 20119a]， MAC [Hudson and Manning 2018]。
此外，使用键值结构[Miller et al. 2016]访问外部记忆已被证明对建模长期依赖关系非常有效[Henaff et al. 2016];Bosselut et al. 2018;Lample et al. 2019]。
变形金刚是迄今为止大多数基础模型的著名架构，同样展示了涉及键值存储器访问和上下文单词表示之间计算的操作[Geva等人，2020]。
信息检索。
一旦模型在训练后完成了信息收集，就有多种方法来检索下游应用程序和任务所需的特定事实或记忆。
有些采用显式提示技术，通过输入序列查询模型的知识[Petroni等人，2019;Kassner et al. 2021;Jiang et al. 2020]而其他方法涉及内隐回忆和通过适应阶段重塑先验知识[Bosselut et al. 2019;Hwang et al. 2021]。
第三类方法更进一步，将基于神经的计算与符号聚合和从非结构化文本存储库中检索信息相结合[Karpukhin等人，2020;Lewis et al. 2020b;Khattab et al. 2020]甚至结构化资源，如知识图谱[Zhang et al. 2019a;Peters et al. 2019;Liu et al. 2020e;Verga et al. 2020;Yasunaga et al. 2021]。
然而，在检索机制提供的强记忆技能和在存在信息瓶颈时学习到的更丰富的表征之间存在权衡。
事实上，过度依赖检索减少了学习如何以紧凑和抽象的方式表示信息的机会，也减少了从模型暴露的大量输入信息中提取关键见解和概念的机会，并且基本上是将小麦从糠中分离出来。
例如，GPT-3的上下文学习能力可能是通过其有限内存架构强制网络表示输入顺序数据的副产品[Brown等人，2020]。
总的来说，虽然它们确实有一些优点[Guu等人，2020]，但依赖于外部检索机制的模型可能无法像有界、紧凑和抽象的表示那样有效地学习泛化。
操作知识。
最后，当考虑长时间的大规模学习时，注意到知识的动态特性是至关重要的，事实的正确性和有效性会随着世界的不断发展而随着时间的推移而改变——昨天是正确的或相关的东西明天可能就不是这样了。
因此，对于模型来说，以一种支持有效更新或操作事实的方式来表示其知识是至关重要的，这是其终身学习的一部分。
4.1.5组合性。
组合性可以定义为一种原则，根据这种原则，整体的意义来源于其组成部分的意义，以及用于组合它们的规则[Janssen and parte 1997;[2014]。
它是人类智能的一个重要组成部分[Lake等人，2017]，是我们从少量例子中轻松有效地进行计划、推理和学习的能力的基础。
组合性可能是实现分布外泛化或组合泛化的关键。
借鉴符号人工智能的经典思想，它鼓励并增强了神经网络中的理想属性，如可解释性、可控制性和数据效率[Lake等人，2017]，并且可以采取不同的形式，表征各种元素:
模型。
组合性可以体现在模型层面，体现在其架构属性、结构和模块化程度上，这可以提高大型神经模型的训练和推理效率[Shazeer等人，2017]。
它还与可解释性和多模态的主题有关，因为它与模型组成的不同模块之间的接口、它们采用的交互模式以及它们的透明度有关。
计算。
模块网络[Andreas等人，2016]和混合专家[Shazeer等人，2017]等模型沿着这个方向走得更远，不仅展示了结构模块化，还展示了组合计算，由子网络的专业化支持不同的操作，以一种适应和定制模型行为的方式输入。
虽然一些方法依赖于手工设计模块的连接[Andreas等人，2016]，但替代方法使网络专业化能够通过学习自然出现[Shazeer等人，2017]。
其他模型，如MAC [Hudson and Manning 2018]和动态记忆网络[Xiong et al. 2016]执行显式迭代计算，其中给定任务被分解为多个推理步骤，一个接一个地执行，表现出从一组初始事实到新的推断和结论的顺序进展。
培训与数据。
不仅模型或其计算可以是合成的，数据或训练过程也可以是合成的[Andreas 2020]。
与其在一个完整的数据集上训练一个模型，不如将其分割或分解为子集，在每个子集上独立训练不同的模型，并最终通过各种集成技术在测试时重新组合它们[Dietterich 2000]。
这样的方法可能对基础模型的训练和部署过程产生深远的影响，无论是在实践方面还是在社会方面。
表示。
我们讨论了不同元素的组合性，如模型、计算、训练方案或数据。
但最值得注意的是，在模型训练和适应过程中出现的学习表征本身也可以是组合的[Andreas 2019]。
事实上，通过结构化的、可能基于图的、面向对象的表示来表示知识是一种很有前途的方式[Zhang等人，20119a;Wang等。2021a]，其核心是识别实体和事件节点，并在它们之间形成连接、类比和关系边。
它反映了一种组织世界信息的自然方式，来自不同模式的输入可以围绕语义多面概念进行引导和聚合。
这种表示可以支持多跳推理和推理[Washington et al. 1995;Sun等。2020b;Yu等。2020c]，也可能通过重组实现更强的分布外泛化。
然而，组合性也会阻碍表征的表现力，并阻碍其解释特质、例外和上下文相关性的能力[Misra等人，2017a]。
换句话说，整体有时可能大于部分的总和，例如，红酒和红洋葱是不一样的。
但是，尽管在过去十年中占据主导地位的许多方法倾向于主要关注频谱的一端，并学习整体分布式表示，但我们相信，探索在情境性和组合性之间达到更好平衡的方式是未来研究的一个有希望的途径。
4.1.6总结。
我们已经介绍了我们认为对下一代基础模型至关重要的五个属性，以便有效地提取我们周围的大量信息，从而成功地处理下游任务:表现力——灵活地捕获和吸收现实世界的信息;可扩展性——熟练地处理大量高维数据;多模态——消费、处理和潜在地产生来自不同来源和领域的内容;记忆能力——有效地存储和检索获得的知识;最后，组合性——促进对新任务、设置和环境的成功概括。
我们相信，正如在本报告中所设想和详细讨论的那样，基础模型的全部潜力的实现将依赖于对新架构和建模进步的研究来实现这些期望。
4.2培训
作者:Alex Tamkin
训练目标是描述如何将模型架构和大量广泛数据转换为基础模型的数学函数。
例如，GPT-3是用语言建模目标进行训练的，该目标会奖励正确预测下一个单词的模型[Shannon 1948]。
我们首先列出这些训练方法的一些目标，描述当前方法中的重要设计权衡，并概述未来道路的重要目标。
4.2.1培训目标的目标。
鉴于近年来这些方法和模型的快速发展，我们概述了训练算法的一些关键目标。46
利用广泛的数据。
自我监督学习算法的兴起释放了互联网规模数据集的力量，这些数据集很难手工注释。
这种广泛的数据有多种形式，包括图像、录音和视频(§2.2:视觉);机器人和传感器数据(§2.3:机器人技术);而文本，要么是孤立的，要么是与其他形式如图像(§2.1:语言)相结合的。
由于这些数据缺乏外部注释，研究人员的主要关注点是设计定制的自监督算法，利用每种数据中的独特结构为基础模型产生训练信号。
域完整性。
基础模型训练算法的一个重要目标是领域完备，从某种意义上说，解决训练任务需要对领域中的下游任务广泛有用的能力(参见§2.1:语言，§2.2:视觉，§2.3:机器人)。
这个属性对于基础模型的通用性是至关重要的。
例如，语言建模可能需要模型在学习预测文档中的下一个单词时获得广泛的功能，如共指、情感和翻译。
相比之下，像情感分类这样的监督学习任务可能会导致更狭窄的能力集(参见§2.1:语言)。
同样重要的是，它不是明显的先验任务将导致一个领域完整的能力，或者甚至如何评估一个模型的能力的全部广度(参见§4.4:评估和§4.10:理论)。
扩展和计算效率。
训练基础模型的过程必须可靠地将数据、模型架构和计算转换为具有广泛能力的模型。
为了使基础模型的能力最大化，我们可以识别这个过程中的瓶颈，并提出新的训练算法来消除它们。
自监督算法的兴起使得模型大小和计算资源的瓶颈日益突出[Kaplan et al. 2020;Henighan等人。2020]，导致了一种转变，即模型的评估不仅取决于它们的能力，还取决于达到这些能力所需的计算量和类型(§4.4:评估)。
训练目标的效率可能会有很大的不同，这充分说明了在给定固定计算预算的情况下，训练方法的设计对于强大功能的出现是多么重要。
因此，训练研究者的一个主要目标是设计具有更丰富的训练信号的训练目标，从而使模型学习更快，获得更强的能力。48
帮助这一发展的一个力量是对不同类型的架构、数据大小和计算的能力如何扩展的惊人可预测性[hetness等人，2017;Kaplan et al. 2020]，这是一个惊人的现象，它使模型开发人员能够根据更清晰的趋势做出选择，而不是更昂贵的随机搜索。
4.2.2当前SSL方法的设计权衡。
目前用于训练基础模型的自监督学习(SSL)方法多种多样，但它们的共同点是，它们从未标记的数据中产生预测问题，而不需要人工注释器。
SSL目标通过精心设计的约束(数据本身(例如，编校或噪声)或模型能够表示或处理数据的方式(例如，潜在瓶颈)，从这些数据中生成丰富的训练信号。
在某种程度上，这些约束“嵌入”了在使模型适应下游任务时所需的各种能力(第4.3节:适应)。49
在这里，我们描述了当前模型探索的三种重要的设计选择，以及它们各自在最终功能方面的权衡。
我们应该在什么抽象层次上建模?
一个基本的问题是基础模型的输入表示应该是什么。
一种选择是在原始字节级别对输入进行建模。
然而，这种高维可能会导致模型专注于预测输入的较少语义方面，从而减慢了它获得更通用功能的速度。
当训练像变压器(Vaswani et al. 2017)这样的模型时，这些方法也变得棘手，这些模型的计算成本随着输入大小呈二次增长。51
另一种选择是使用领域知识来减少模型的输入空间——这种策略包括补丁嵌入[Dosovitskiy等人，2020]以及固定或学习标记化[Schuster和Nakajima 2012;Sennrich et al. 2016;工藤和理查森2018;van den Oord et al. 2017;Ramesh et al. 2021]。
这些方法可能会减轻生成方法面临的一些挑战，但它们可能会在输入中丢弃可能有用的信息。52
连续输入与离散输入的选择也有适应的权衡(§4.3:适应);需要做更多的工作来捕捉这两种方法的好处。
生成式训练方法在概念上是优雅而强大的——它们训练模型在训练输入上学习联合分布或条件分布。
两大类生成基础模型包括自回归基础模型[van den Oord et al. 2016;Radford and Narasimhan 2018;Chen et al. 2020d;Yang et al. 2019;Ramesh et al. 2021]，它逐块生成输入，并对基础模型进行去噪[Devlin et al. 2019;rafael et al. 2019]破坏然后恢复输入。
在训练过程中执行的特定生成类型决定了可用的交互性(第4.6节:数据)，这仍然是许多领域的主要挑战，包括机器人(第2.3节:机器人)和低资源语言(第2.1节:语言)49例如，用于训练GPT-3的因果语言建模目标[Brown et al. 2020]启用了通过前缀调节它。
在对比学习过程中使用的颜色抖动增强[Chen等人。2020c]鼓励对被认为对下游任务无用的属性的不变性。
更好地理解这些约束的特定选择和结构如何影响模型获得的能力是未来工作的一个重要领域(§4.10:理论)。50
例如，草叶、音频压缩人工制品或单词的拼法51参见§2.2:视觉和§4.1:建模，讨论高维序列(如图像和视频)的训练成本52例如，在适应过程中，对文本进行标记可能会使学习押韵、双关语或其他受益于字符级信息的任务变得更加困难[Branwen 2020] 53(参见§2.5:交互和§4.3:适应)，未来的模型可能会实现更丰富的交互集。54
虽然生成式训练方法有其优点，但最近也有几种判别式训练方法获得了关注。
这些方法不支持基于生成的交互，但它们可以在图像、音频和视频等高维连续设置中为基于分类或回归的任务提供更有效的学习。
这些方法中的大多数都输出输入(部分)的向量，这些向量被训练成对于输入的不同“视图”相似[Wu等人，2018;van den Oord et al. 2018;Chen et al. 2020c;He et al. 2020;Grill等人，2020;Caron等人，2021;张等。2020a;Radford et al. 2021]或用于预测部分输入是真还是假[Clark et al. 2020;Iida et al. 2021]。
更好地理解生成训练和判别训练之间的权衡，以及捕捉这两种方法的优点，仍然是未来研究的有趣途径。
捕获多模态关系。
另一个日益重要的研究领域是捕捉多种数据之间的关系。
这意味着什么可能会根据上下文和建模者的目标而有所不同。
例如，CLIP [Radford et al. 2021]和ViLBERT [Lu et al. 2019a]都是多模态视觉语言，但在多模态的精确方式上有所不同。55
前者将图像和文本分别编码为向量，使用户能够从单一模态中检索，评分或分类来自另一模态的示例。
第二种方法是在模型的早期阶段联合处理图像和文本，从而实现下游应用，如视觉问答，其中提供了对相关图像和文本(例如，图像和关于它们的问题)的推理。
多模态基础模型是一个新兴的研究领域;关于模型可以多模态的不同方式，以及更好地理解这些额外模态带来的功能，还有很多尚未探索的地方。
4.2.3前进路径。
最后，我们提出了基础模型训练未来的一些重要目标。
目前，SSL的目标是高度特定于领域的:目前在自然语言处理、计算机视觉和语音处理中流行着不同的方法。
这有两个主要的缺点:首先，这些不同的技术使得掌握这些方法的共同线索和科学原理变得困难。
其次，这种领域特异性要求为每个新领域从零开始开发新的基础模型训练方法，包括医学、科学和新的多模式设置。
更普遍的目标是在任何类型的数据上有效地训练基础模型，这将是基础模型训练社区的一个重要里程碑[Tamkin et al. 2021b]。
很明显，并不是所有的训练目标都是相同的——有些训练目标比其他目标更有效，在给定的计算预算下，转化为更有能力的基础模型。
是否有比目前已知的训练方法更有效的数量级?
如果有，我们如何找到他们?
这些调查将受到许多力量的影响，包括未来软件和硬件的进步(§4.5:系统)使之成为可能。
我们也不需要将数据(§4.6:数据)和训练算法视为独立的因素:53例如，像GPT-3这样的自回归模型支持基于前缀的条件反射，而像T5或BERT这样的去噪模型有助于使用双向上下文来替换任意长度的跨度或修复错字。54
在基础建模环境中较少研究的其他类型的生成方法包括扩散和基于分数的模型[sol - dickstein等人，2015;Song and Ermon 2019;Ho et al. 2020]， VAEs [Kingma and Welling 2014]，流动模型[Dinh et al. 2015;Kingma and Dhariwal 2018]和gan [Goodfellow et al. 2014]——这些或其他未来的方法是否也能像自回归或去噪方法一样使学习具有各种各样的能力，还有待观察。55
虽然基础模型为处理多模态信息提供了一个强大的通用引擎，但在使用之前对基础模型进行调整对于某些应用程序是必要的。
广义地说，一个适应过程通过在附加信息上调节基础模型来产生一个适应的模型，或者通过在其输入中包含新数据或提示来启动基础模型，或者通过更新部分或全部基础模型的参数来反映新信息。
例如，在文本摘要中，通过将TL;DR等提示附加到输入文章中，可以作为基础模型的任务规范，从而提高基础模型的性能[Radford et al. 2019]。
另外，使用组织的内部、领域特定的数据对基础模型的参数进行微调，可以通过添加与组织的用例相关的信息来提高模型的准确性。
在本节中，我们描述了现有的适应方法，以及决定特定适应程序是否适合特定环境的几个因素。
我们还描述了基础模型适应的各种用例，包括相对较好的研究设置，如将基础模型专一化到特定任务或领域，以及更多的推测设置，如测试时间数据删除[Bourtoule等人，2019]和在特定输入上编辑模型行为[Sinitsin等人，2020]。
最后，提出了未来基础模型适应研究的长远目标。
4.3.1基础模型自适应方法。
目前已经提出了许多自适应基础模型的方法，这使得对特定问题或计算环境使用哪种自适应过程变得困难。
我们强调从业者在选择适应过程时要考虑的三个特别重要的因素:(1)计算预算(特别是存储和内存);(2)可获得的特定任务数据量;(3)接近基础模型梯度的程度。
因素1:计算预算。
对于具有数十亿或数万亿参数的基础模型，微调所有模型参数可能需要非常大的内存。
此外，对许多任务进行单独的微调可能会产生不可接受的存储成本。
已有许多研究提出了减少基础模型自适应存储的方法，我们将这类轻量级自适应方法称为低存储自适应。
通常，该类中的方法冻结了大多数预训练的基础模型参数，只学习相对较少数量的特定于任务的参数(通过微调一些预训练的参数或通过添加完全新的模块)，从而减少了每个任务的存储开销。
这种算法的关键设计决策是要适应的参数的选择。
也许最简单的方法是只调整预训练模型的最后一层，而其他作品只调整模型的偏差向量[Zaken等人，2021]，模型权张量的低秩残差[Hu等人，2021]，或模型参数的掩模[Zhao等人，2020b]。
另一项研究曲调“软”提示[李和梁2021;Qin and Eisner 2021;刘等。2021e;Lester et al. 2021;Hambardzumyan等人。2021]，对应于任意参数向量的序列，而不是模型词汇表的嵌入，并通过在输入层或所有层与输入激活连接，将这些提示作为基础模型的条件。
另一种方法是冻结所有模型参数，并在现有模型层之间交错使用具有可训练参数的新MLP模块[Houlsby等人，2019]。
虽然这些轻量级自适应技术似乎在下游任务中权衡了参数效率和性能，但它们有时可以达到与完全微调相当的性能，尽管更新的参数减少了1000倍[Zaken等人。2021;李梁2021;Hu et al. 2021]。
Lester等人[2021]展示了一个实例，当完全微调和轻量级自适应之间的性能差距随着模型大小的增加而消失。
我们仍然不确定轻量级适应技术如何随着模型尺寸的增加而扩展[Aghajanyan et al. 2020]。
因为GPU内存通常是比磁盘存储更有限的资源，所以低内存适应过程可能比低存储适应过程对基础模型的民主化更为关键。
已经提出了各种用于低内存模型训练的技术，这些技术可以直接应用于基于微调的适应过程(§4.5:系统)。
然而，一些低内存的过程，如梯度检查点[Chen et al. 2016]权衡了计算和内存，潜在地加剧了基础模型的显著能耗[Bender et al. 2021]。
因素2:数据可用性。
任务专门化通常需要特定于任务的标记数据作为训练信号。57
然而，注释的成本在不同的任务和语言之间差异很大;例如，注释MRI数据需要专业的医学知识，而标记英语文本的情感只需要常识判断。
当适应数据丰富时，我们可能会应用传统的微调方法或轻量级的对应方法。
在低资源的基于语言的设置中，提示和微调相结合已被证明是一个有前途的方向[Schick和sch<e:1> 2021a,b;Gao et al. 2020c;Perez et al. 2021;IV等人。2021;Min et al. 2021]。
Le Scao和Rush[2021]表明，一个经过精心调整的提示值约为100个训练样例，并且对精心提示的基础模型进行微调的数据效率明显高于对无条件基础模型进行微调。
因素3:访问基础模型梯度。
尽管基础模型在一些研究领域产生了重大影响，但对于大型基础模型(超过500亿个参数)，分布实践的标准化很少。
随着我们逐渐意识到滥用基础模型的潜在危险(参见§5.2:滥用)，提供对所有基础模型参数的访问以进行微调可能会导致道德问题。
此外，大多数用户没有足够的计算资源来充分利用其访问权限。
例如，基础模型的内存需求可能会妨碍它们对许多组织和机构进行直接微调。
因此，未来的基础模型提供者可能会限制对模型全部参数的访问，而提供代理API访问，如早期的基础模型GPT-3所示。
在一个极端情况下，如果基础模型提供者只允许访问模型输出(例如，提示的文本延续，生成的图像，或评估图像与文本描述之间一致性的分数)，则可以使用上下文学习来适应基础模型[Brown等人。2020]。
上下文学习冻结基础模型参数，并通过一个(典型的自然语言)提示来控制基础模型的输出，这个提示可能由任务指令或演示组成。
为了提高上下文学习的性能，需要通过手动搜索或自动过程来仔细设计提示[Jiang等人，2020;Shin et al. 2020]，并在适应数据上进行了验证。
在另一种极端情况下，如果基础模型提供者授予对模型参数的梯度的访问权，则可以应用完全微调，其中更新所有模型参数以提高下游任务的性能。
作为中间地带，我们可能只获得对基础模型输入的梯度访问，58这比基础模型参数的维度低得多。
在这种情况下，我们可以部署轻量级适应技术[Liu et al. 2021e;李梁2021;Lester et al. 2021]，冻结模型参数并为每个任务优化连续前缀或提示。
4.3.2适配用例。
每当模型的期望用例不同于用于基础模型训练的相对一般的训练目标(§4.2:训练)时，适应性是有用的。
最常考虑的情况是，基础模型被用于执行特定任务(例如，文本摘要或从图像中对动物进行分类)，从而缩小了模型的范围。
实际上，本节前面描述的绝大多数现有方法都针对这种设置。
然而，其他形式的适应是有用的，例如对局部模型进行编辑，以纠正对特定输入的不期望的预测，或者向训练的基础模型添加隐私约束，这是与任务无关的。
在本小节中，我们描述了适应的各种用例，最适用于它们的方法，以及在解决这些设置时仍然存在的挑战。
任务分工。
基础模型适应最广泛研究的案例是任务专门化，其中基础模型被适应以优化特定任务或一组任务的性能。
例如，专门化摘要任务将诱导基础模型行为从输入文档中提取关键思想，并将它们重新组织成简短的摘要句子。
各种适应程序已被证明对任务专业化是有效的，显示出比未适应模型的性能有显着改善[Howard and Ruder 2018;Brown et al. 2020]。
除了对特定任务的专门基础模型的设置进行了相对广泛的研究之外，由于其规模和计算需求，其他与任务无关的适应问题对基础模型来说变得越来越具有挑战性(但同样重要)。
例如，训练基础模型的成本使得随着时间的推移持续训练以保持模型的预测与当前事件保持同步的成本特别高。
此外，收集用于训练基础模型的大量匿名数据集(第4.6节:数据)的挑战使得个人信息泄露到训练集的可能性变得不可忽视;因此，有效地从基础模型中删除训练数据的机制是可取的。
时间适应。
理想情况下，基础模型存储的知识紧密地代表了世界的状态，独立于模态。
然而，世界在不断变化;新的国家元首当选，服装风格改变，社会规范和信仰改变(§5.6:道德)，语言的使用也在演变，导致输入分布、目标预测分布或两者都发生变化。
由于诱导的分布转移，这种时间转移提出了一个具有挑战性的统计问题，如§4.8:稳健性所讨论的。
对于基础模型，时移也提出了特别困难的计算问题;由于训练基础模型的计算要求很高[Shoeybi等人，2019;Brown et al. 2020]，频繁的从头开始重新训练可能会带来不可接受的财务或环境影响[Bender et al. 2021](§5.3:environment)，或者只是需要太长时间才能成为保持模型最新的可行方法。
在视觉领域，在中间时间点上对未标记数据进行逐渐的自我训练可以跨越很长一段时间，但仍然是一个昂贵的再训练过程[Kumar等人。2020a]。
在语言模型的背景下，临时划分的诊断数据集有助于量化大型语言模型过时的速度[Lazaridou等人，2021;Hombaiah et al. 2021;dhinggra et al. 2021]，表明经典技术，如重新加权训练数据和动态评估(在生产时用新数据更新模型参数[Mikolov et al. 2010])可以部分缓解，但不能完全解决这个问题。
根据语言模型打算建模的时间段显式地调节语言模型是另一种显示出一些前景的技术[dhinggra等人，2021]。
解决时间偏移的另一种策略是设计基于检索的(半参数)模型，该模型使用从大型人类可解释数据库(例如，维基百科文章)检索的额外上下文来增强模型输入[Karpukhin等人，2020;Lewis et al. 2020b;Guu et al. 2020;Khandelwal et al. 2020;Khattab et al. 2020]。
对于基于检索的模型，适应对应于更新数据库中的单个信息单元(例如，百科全书文章的单个段落)，而无需重新训练模型。
基于检索的方法虽然很有前途，但在训练准确的检索机制和对检索信息的准确调节方面仍然存在挑战[Lewis等人，2020b]。
在本节后面的部分中，我们将在持续学习的更一般的背景下重新讨论时间适应问题。
域专门化。
除了任务专门化之外，通常有必要将基础模型专门化到特定领域(例如法律文档)，而不限制基础模型可以完成的任务的广度。
这种专门化导致了基础模型训练和适应数据分布之间的不匹配(§4.8:鲁棒性)，这可能需要新的适应方法来处理。
先前的研究发现，多样化和通用的预训练数据会导致当前适应方法的负迁移。
例如，Cole等人[2021]表明，对仅在iNaturalist动物分类数据集上预训练的模型进行微调，比对在iNaturalist以及750K其他图像上预训练的模型进行微调提供更好的下游性能;类似地，LegalBERT [Chalkidis等人，2020]只在法律文件上进行预训练，比BERT [Devlin等人，2019]有所改进，BERT在法律文件中的文本分类和序列标记的下游任务上进行了更多样化的训练集训练。
领域专门化的一种方法是包括一个中间的适应步骤，在这个步骤中，基础模型继续对来自专门化领域的未标记数据进行训练。
例如，这种方法显著提高了卫星图像和专门文本主题的下游性能[Reed等人，2021;Gururangan et al. 2020]。
然而，在某些领域(如法律文件)，持续的基础模型训练可能比从头开始重新训练的效果更差[Chalkidis et al. 2020]。
阐明持续培训对绩效有利或不利的情况是未来工作的一个重要方向。
本地模型编辑。
在某些情况下，局部地调整基础模型是有用的，这意味着模型的预测分布应该只适应单个输入或单个输入周围的局部邻域，而不改变模型对不相关输入的行为。
例如，当一个基础模型对一个特定的输入短语和目标语言产生一个特别有问题的误译时，在不影响模型对不相关短语的行为的情况下纠正这个误译是可取的。
过去的工作研究了通过新的预训练目标将近似局部更新应用于大型神经网络的问题，这些目标可以轻松地使用标准梯度下降进行编辑[Sinitsin等人，2020]，预测底层模型参数编辑的高阶网络[Cao等人，2021;Mitchell et al. 2021;Hase et al. 2021]和约束微调程序[Zhu et al. 2020]。
然而，现有的方法在不破坏全局模型性能的情况下执行模型编辑的可靠性方面各不相同。
此外，将这些方法扩展到大规模基础模型并不简单，因为它们的大小和训练目标的计算成本需要计算高阶梯度[Sinitsin et al. 2020;Cao et al. 2021;Mitchell et al. 2021]。
应用约束。
在某些情况下，需要调整基础模型以满足隐私约束。
例如，Carlini等人[2021]证明了现有的基础模型能够记忆训练数据中的敏感信息，并且在通过标准api查询时可以反刍这些数据。
虽然这种现象要求改进数据管理，但制定适应程序以消除或减少特定数据示例对训练模型的影响将是一种补充解决方案。
在这个方向上改进的适应策略(以及更好的预训练方法)也将使根据《通用数据保护条例》(GDPR)使用基础模型的机构受益，因为该授权赋予用户被遗忘的权利。
而关于机器学习主题的研究[Bourtoule et al. 2019;Cao and Yang[2015]已经开始获得关注，但基础模型的问题尚未深入研究。
此外，研究表明，在未经整理的互联网数据上训练的基础模型会对特定群体产生有害的偏见(例如，性别和种族偏见)[Bender et al. 2021;Basta等人，2019;Kurita et al. 2019;Sheng等人，2019]并且可以产生有毒物质[Gehman等人，2020](第5.2节:误用)。
虽然诸如在精心策划的数据集上进一步微调基础模型(可能是多代)[Solaiman和Dennison 2021]或应用可控生成技术[Keskar等人2019]等策略在减轻有害行为方面取得了一些成功，但培训公平和安全基础模型的框架(§5.1):公平性)可能需要在数据收集、培训和适应阶段以及咨询领域专家的共同努力下进行进一步的研究。
4.3.3基础模型适应研究的长期目标。
在某种程度上，适应与有效地将模型的现有知识与新数据或目标相结合有关，适应的自然延伸是持续学习[McCloskey和Cohen 1989;Parisi et al. 2019]或持续适应。
不断适应基础模型的能力是可取的，无论是保持模型的知识不断地与世界事件或文化发展同步，不断地从全新的领域或模式中添加数据，或者随着社会价值观或法律的发展，不断地编辑模型的记忆以遵守隐私或法律约束。
然而，持续学习问题通常会导致灾难性遗忘[McCloskey and Cohen 1989;·拉特克利夫称1990;Kirkpatrick et al. 2017]在神经网络中，随着训练分布的变化，旧的任务或数据会迅速被遗忘。
我们认为基础模型的持续适应是未来基础模型适应研究的重大挑战。
解决这一挑战需要缩小在来自不同任务、领域或时间段的非平稳数据流上连续训练的基础模型与从i.i.d训练的相同基础模型之间的性能差距。
数据来自骨料混合物[Lopez-Paz and Ranzato 2017]。
为了做到这一点，在模型架构和训练目标方面的创新可能是必要的。
例如，虽然记忆机制长期以来一直被推测为成功持续学习的关键[French 1999]，并且已经显示出基础模型的一些希望[Lewis et al. 2020b;Guu et al. 2020;Borgeaud et al. 2021]，有效利用更新的记忆仍然是一个具有挑战性的问题[Zhang and Choi 2021]。
此外，在基础模型中定位知识以进行有针对性的参数更新的技术[Dai等人，2021a]或学习这种更新规则的技术[Cao等人，2021;Mitchell et al. 2021]可能有助于防止遗忘，但重复使用这种更新仍然会导致显著的遗忘[Hase et al. 2021]。
持续的基础模型适应可能还需要对灾难性遗忘问题在基础模型尺度上的表现有新的理解，以及开发新的元学习技术[Schmidhuber 1987;Santoro et al. 2016;Finn等人[2017]，以改善已学习信息向新设置的前向转移。
持续地训练由部署的基础模型所收集的经验，或者甚至训练由许多不同模型所收集的聚合经验，可能会加快基础模型开发的进度，但是会招致反馈循环的风险，以及模型行为和涉众价值的侵蚀。
尽管存在上述挑战，持续的基础模型适应有望更快地响应社会文化价值观的变化，更好地利用现有知识来学习新概念，减少环境影响，通过消除从头开始训练的计算负担来增加基础模型的可访问性，并减少以前学习的概念由于遗忘而必须重新学习的程度。
4.4评价
作者:Rishi Bommasani, Kawin Ethayarajh, Omar Khattab评估为机器学习模型提供了上下文:它作为(1)跟踪进度的一种手段，我们如何衡量模型的性能以及我们如何设计改进的模型(§4.1:建模);(2)理解模型表现出的行为(§4.11:可解释性)以及它们如何在不同的数据切片上执行(§4.8:鲁棒性);(3)文档-我们如何有效地总结模型行为并将其传达给不同的利益相关者。
对于基础模型，这些评估目的中的每一个都是至关重要的，但基础模型的性质引入了在其他AI或ML设置中通常不会遇到的新挑战:
(1)跟踪进度需要相对比较，但是比较基础模型是复杂的，因为基础模型必须适应(可能以不同的方式)来执行任务。
(2)理解需要特定的预先知识(例如，分类法)，但基础模型获得了在设计评估时难以预料的紧急技能(例如，情境学习)。
(3)文档需要明确的需求来有意义地告知决策，但是基础模型可以适应无数的应用程序，这使得全面的文档具有挑战性。
为了引导评估基础模型的讨论，我们区分了基于基础模型抽象产生的两类评估:基础模型的内在评估，由于这些模型的任务不可知论性，它本质上脱离了特定任务，以及特定任务模型的外在评估，它必然依赖于基础模型和适应机制。
此外，我们认识到，由于基础模型的预期影响和范围，各种利益相关者(例如，基础模型提供者和应用程序开发人员，审计师和政策制定者，从业者和研究人员)将需要对基础模型和特定任务衍生品进行评估，这些评估服务于不同的目的，并涉及基于利益相关者的不同需求。
考虑到这一点，评估机器学习模型的标准范式并不是为基础模型的设置而明确设计的。
因此，我们强调内在评估(§4.4.2:evaluationintrinsic)、适应在外在评估中的重要性(§4.4.3:评估-适应)和评估设计(§4.4.4:评估-设计)是迈向更适合基础模型的评估框架的明确步骤。
这种讨论有助于围绕机器学习系统评估的作用进行更广泛的对话[Galliers和Spärck Jones 1993;Lipton and Steinhardt 2019;Ribeiro et al. 2020;Linzen 2020;Kiela等人2021;Milli et al. 2021;Jacobs and Wallach 2021;鲍曼和达尔2021;Dehghani et al. 2021;Ma et al. 2021a，除其他外]并且，考虑到评估的复杂性，可能会受益于借鉴机器学习之外存在的测量和评估理论[Messick 1987;杰克曼2008;Loevinger 1957;梅西克1988;手2010;Brewer and Crano 2014]。
4.4.2内在评价。
机器学习系统的评估传统上是基于任务的，通常是那些被设想为对应用程序特别有用的功能(例如，翻译，对象识别)。
相反，由于基础模型是中间资产，必须进一步调整或专门化以执行有用的任务，因此必须改变标准评估范式，以促进对基础模型的直接理解和比较。
一种方法是根据与训练目标相关的任务来评估基础模型。
例如，像GPT-3这样的语言模型，通过预测给定前一个上下文的下一个单词来训练，可以根据它在给定前一个上下文的测试数据中分配单词的概率来评估(即语言建模基准的困惑，如LAMBADA [Paperno et al. 2016])。
到目前为止，这种方法在NLP中显示出了希望，但我们确定了它所表现出的两个基本限制。
首先，依靠训练目标进行评估缺乏通用性:使用不同的不相容目标训练的基础模型不能在一致的框架中容易地进行比较或理解。
其次，以这种方式进行的评估依赖于有意义的代理关系，即，根据训练目标进行的测量应该与其他更有意义和可理解的数量(例如，通过基础模型生成的内容的质量)相关联。
虽然这种代理关系在过去的一些环境中被证明是健壮的，但当评估基础模型的更多样化的能力时，它们在更多样化的环境或领域中的行为，以及超越领域内准确性的考虑(我们在§4.4.4:评估-设计中更广泛地讨论了这一点)，它可能会崩溃。
鉴于这些限制，我们预计需要考虑两种方法，以提供互补的好处。
从广义的外在评价中推导出内在评价。
评估基础模型的一个途径是使它们适应广泛的任务，并测量结果任务特定模型的性能。
由于基础模型是所有这些模型的共享基础，所以总体上的性能反映了这个共享基础的性质和质量。
目前，人工智能的许多子领域已经开始构建元基准，即整合多个不同任务或领域的单个评估的单一评估[Wang等人，2019b,a;Hu et al. 2020;Santurkar et al. 2020;Gehrmann et al. 2021;Hendrycks等人。2021b;Koh等人。2021;Tamkin等。[2021b]。
考虑到越来越多的人采用这种模式，以及它已确立的优势，这里我们注意到为什么它可能不足以完全满足与基础模型相关的评估目标。
元基准评估需要适应(最低限度地将基础模型专门化到元基准中的每个任务)，这使得关于基础模型本身的推理具有挑战性，因为涉及到添加过程(即适应)。
具体来说，这使进展问题变得复杂，无论是在跟踪方面(例如，性能是否归因于有效的基础模型或设计良好的适应实践)，还是在用于学习基础模型的过程中识别改进方面(例如，数据选择方面的基本改进(§4.6:数据)、训练目标(§4.2:训练)和模型架构(§4.1:建模)可能很难通过比较两个基础模型之间的元基准上的性能来识别。
此外，这种评估范式使得很难理解或记录特定于基础模型的属性和能力，这可能会使它难以向特定的涉众传达(例如，SuperGLUE的性能可能没有足够的信息，或者可能会误导决策者)，或者用作预测他们对新任务或领域的行为的依据。
内在性质的直接评价。
为了补充元基准的使用，我们还争论了为什么直接测量基础模型的属性(例如，特定的能力或偏差)是有价值的，与特定的任务分离。59
例如，我们可以尝试直接测量基础模型的语言能力，以识别句法有效和无效的句子。
为了激发这种方法的价值，我们回到评估的目的上来。
值得注意的是，59严格地说，这些直接的评估可能仍然涉及作为任务的表述和执行任务的基础模型专门化，但是目标更类似于探索(见§4.11:可解释性)，试图尽可能直接地度量基础模型。
阐明能力、技能和偏差的存在和强度，确定需要改进的具体领域(进展)，阐明当前的潜力(理解)，并有效地表达相关方面(文档)。
这种方法也有助于广泛理解的评价，即技术专家、非技术专家(例如政策制定者或社会科学家)和一般目的都能理解的评价。
例如，表征这些模型的说服力或修辞能力对于内化其虚假信息和误用的潜力可能特别直观(§5.2:误用)[Buchanan et al. 2021]。
直接评估属性也是更好地处理基础模型突发性属性的重要途径;为了证明这一点，我们将上下文学习作为案例研究。
特别是，Brown等人[2020]不仅证明了GPT-3稳健的上下文学习的标志性能力，而且还首次明确将上下文学习确定为适应模型并与模型交互的特定方式(通过他们对GPT-3的探索)。
传统的以任务为基础的外在评价并没有提供一种明确的方法来识别情境学习;在这种情况下，直接与基础模型交互似乎是必要的。
更一般地说，虽然通过对这些模型及其功能的非结构化或松散结构化的探索，似乎不可避免地会发现许多意想不到的现象，如情境学习，但我们认为应该寻找新的评估方法来组织这种探索，或者更雄心勃勃地提出新的属性，然后可以进行更严格的测试。
内在评价也可以降低展示基础模型潜力的门槛;如果基础模型的新方法证明了内在评价的改进，那么它们可能是足够有前途的，即使它们没有立即伴随着相应的适合的适应方法来引出外在评价中的这些能力。
如何实施内在评价是一个悬而未决的重大问题;这种评估的机制尚不清楚。
我们列举了一些可能有助于设计和执行内在评估的一般原则和注意事项。
(1)人类评价的启示。
我们对基础模型感兴趣的许多相关属性、能力和偏差也对人类感兴趣，这表明在人类中测量这些属性的方法可能被证明是有指导意义的，甚至可以直接翻译为评估基础模型。
例如，人类语言能力的心理语言学测量可以被修改以评估基础模型语言能力[Levy 2008;Frank et al. 2013;Linzen et al. 2016 (2) Human-in-the-loop评价。
人在环评估可能被证明是至关重要的，它为理解基础模型提供了一种更具探索性的方法，包括评估它们的生成或交互能力。
特别是，直接与基础模型的人类交互可以更好地识别其紧急能力和局限性，并且对基础模型的直接审计[例如，Raji和Buolamwini 2019，§5.6:伦理]可以推进文档和透明度的目标。
(3)内在措施的有效性。
虽然内在测量允许在源头上直接测量，即测量和评估独立于适应和特定任务的基础模型的属性，但它们对建立对评估有效性的信任提出了挑战[Messick 1987 [Messick, 1988]]。
特别是，外在评价结果在验证内在测量设计中也可能很重要，例如，内在测量的预测效度(即，它们(统计)预测相关下游结果的能力)可能被证明是一个中心标准。
4.4.3外部评价与适应。
评估特定于任务的模型在历史上涉及报告模型在特定测试集上的性能(通常意味着准确性)。
虽然这种范式可能部分地足以理解或记录一个模型，但它通常会在使用不同(并且可能是不平等的)资源产生的特定于任务的模型之间进行不公平的比较，从而使评估取得了多少进展变得困难。
在基础模型体系中，对不公平比较的担忧加剧了:不同的基础模型(例如BERT和GPT-3)可能构成不同任务特定模型的基础，这些基础模型可能涉及大量不同的训练数据和计算。
为了考虑达到特定绩效水平所需的资源，Linzen[2020]认为(预)培训资源应该在评估中得到确认和跟踪。
我们认为这是一个有科学原则的建议;在不考虑训练资源的情况下比较训练基础模型的不同方法可能会产生误导。
然而，考虑到创建基础模型的过程特别昂贵(例如，需要大量的人力和财政资本)，并且除了科学因素之外，还经常受到社会因素(例如，商业激励)的支配，可能会出现实践中的基础模型在提供的培训资源方面差异很大的情况，使得控制比较变得困难。
在这里，我们考虑了一种可能更普遍可行的替代方案，可以部分解释为补充Linzen[2020]提案所涉及的资源。
特别是，我们考虑了为什么外部评估应该承认适应资源，这对于确保外部评估能够识别最有效的适应方法至关重要(而内在评估从根本上无法做到这一点)。
我们提请注意，适应资源通常被解释为用于调整模型的数据，但其他资源[例如，用于选择适应方法的数据;Perez et al. 2021]和约束(例如，适应基础模型所需的访问水平;参见§4.3:适应和§5.6:进一步讨论的伦理)也应考虑在内。
适应资源核算。
考虑到为适应特定任务而调整基础模型所花费的资源，需要完全了解不同适应方法所使用的资源或限制，即，努力考虑这些资源的评估必须与适应中使用的资源的发展一起发展(§4.3:适应)。
在现有的特定于任务的评估中，大多数评估都指定了可用于使(基础)模型适应任务的数据量。
然而，Perez等人[2021]在这里发现了一个在过去的工作中被忽视的关键细微差别，即这应该包含用于通知适应的所有数据，即用于适应基础模型的数据和用于选择适应方法的数据。
此外，在基础模型体系中，不同适应方法的访问需求概念也是一个新的考虑因素，应该纳入评估。
具体地说，一些适应方法可能通常优于其他方法，但与其他方法相比，可能需要更大的访问或修改基础模型的能力(例如，微调需要基础模型梯度来修改基础模型，而提示可能只需要在指定输入时访问黑盒)。
考虑到适应所涉及的资源，可以丰富从特定任务模型的评估中合理得出的结论。
目前，特定于任务的评估可以为特定于任务的工件(即，被评估的确切模型)的某些类型的理解或文档提供足够的清晰度，但不能为不同的适应方法如何执行以及如何在给定的上下文中选择特定的适应方法提供明确的信号。
相比之下，通过考虑适应所涉及的资源和获取要求，评估能更好地使研究确定哪些适应方法或过程能最好地利用所提供的资源，即不仅为被评估的特定工件提供信号，而且为衍生这些工件的更一般的过程提供信号。
因此，拟议的评估议定书显然致力于确定应该使用哪些适应方法;我们注意到，所有这些结论都应该被视为特定于给定的基础模型，因为这种形式的评估并不能提供足够的证据来得出适应性方法在所有基础模型中都是一致的最佳方法。60
4.4.4评价设计。
理论上，评估的目标是衡量和表征各种理论结构(例如，准确性，稳健性(第4.8节:稳健性)，公平性(第5.1节:公平性)，效率(第4.5节:系统)，环境影响(第5.3节:环境))，服务于各种目的(即，进展，理解，文档)。
然而，在实践中，评估的效用将取决于如何设计和执行评估。
例如，基础模型的生成能力的自动测量(例如，它们的事实正确性)可能很难捕捉到这些品质的本质，相反，人在循环中的评估可能更好地将这些能力置于环境中。
在考虑评估设计时，我们为基础模型和它们的衍生品设想，我们从评估的机制开始。
传统上，机器学习模型的评估包括一个用于学习模型的大型训练集，一个用于设置超参数的可选验证集，以及一个用于评估学习模型对持有数据的泛化的测试集[Bishop 2006]。
因此，创建基准来评估模型历来需要大量数据，其中大部分数据被分配给训练，这使得某些诊断或细微评估的设计变得复杂，因为数据稀缺或成本高昂[Rogers 2020， [Rogers, 2021]]。
相比之下，因为基础模型的好处通常与适应的样本效率(即，少射或零射能力)和可能应用的多样性相一致，我们取而代之的是设想一个制度，其中单个任务的基准要小得多(因为需要提供的数据要少得多)作为“训练”，即适应。数据)，并且更加多样化(既可以捕捉内在评价的各种能力，也可以以生态有效的方式进行更有力的基础评价[Bronfenbrenner 1977;de Vries et al. 2020]进行外部评价)。
这表明基础模型的本质可能会导致基准的本质(以及那些构建基准的人的心态)的转变，不再强调数量作为基准的关键优先级，而不是质量和多样性。
NLP社区已经开始看到这样一种制度的开始，它具有广泛和多样化的基准，如BIG-Bench 61和FLEX [Bragg等人，2021];这种范例降低了基准设计的障碍，从而使更广泛的社区能够参与评估设计。62
除了评估的机制之外，评估结果的呈现和界面告知了这些结果将如何用于决策(例如，新的建模方法、模型选择、审计)。
排行榜已经成为机器学习领域的实际范例，即根据特定且单一的标准(通常是一种准确性形式)对模型进行排名。
随着时间的推移，这种方法通常会导致系统质量的显著而快速的进步[例如，Wang 60]。相反，目前的结果表明，不同的适应方法更适合不同类型的基础模型和训练目标[Liu et al. 2021e;Lester et al. 2021]。
61 https://github.com/google/BIG-bench 62
传统上，像ImageNet [Deng et al. 2009]和SQuAD [Rajpurkar et al. 2016]这样的基准测试的设计是由资源丰富的研究实验室进行的，这些实验室有能力通过众包来支付创建这些数据集的费用[Rogers 2020]。
等人。2019a]，但人们对这是否会产生更普遍的改进提出了重大关切[例如，Linzen 2020;Bowman and Dahl[2021]。63
与所有机器学习模型一样，基础模型及其衍生物的需求很少是单一的;相反，我们预计其应用的广度和社会影响需要高度考虑准确性以外的标准(例如，稳健性，公平性，效率和环境影响)。
为此，我们注意到基础模型的评估应该报告这些不同方面的测量结果;现有的基准越来越多地被设计为不仅仅反映准确性(例如，鲁棒性[Koh等人，2021;Goel et al. 2021]，公平性[Nadeem et al. 2021;Nangia et al. 2020]，效率和环境影响[Coleman et al. 2017])。
此外，我们注意到，如果以排行榜的形式对不同类别的表现进行报告，那么消除潜在权衡(以诱导排名)的机制将特别必要[Ethayarajh和Jurafsky 2020]。
特别是，由于不同的利益相关者会有不同的偏好(例如，他们赋予不同属性的权重)和价值观[Birhane等人，2020]，排行榜设计应该允许利益相关者互动并操纵排名如何与他们的价值观保持一致;Ma等人[2021a]通过使用基于用户指定效用函数的经济框架比较模型的效用，提出了实现这一目标的早期尝试。
4.4.5外卖。
评估执行对所有机器学习范例(包括基础模型范例)都至关重要的几个角色(即，进度、理解、文档)。
基础模型对现有评估框架提出了新的挑战;设计直接针对基础模型体系的评估，不仅可以更好地服务于评估的多种目的，还可以更好地服务于所涉及的无数涉众。
(1)虽然机器学习评估传统上考虑的是特定于任务的模型，但评估基础模型涉及到这些模型并不特定于任务的事实。
对这些模型的评估可能涉及整合两种互补的方法:(a)通过对特定任务衍生品的广泛评估来推断基础模型的属性，以及(b)在基础模型中直接测量这些属性。
(2)现有的评估框架往往没有考虑到创建被评估模型所需的资源，导致不公平的比较。
对于基础模型，我们讨论了一种强调考虑适应资源的评估范式(例如，适应中使用的所有数据，基础模型的访问要求)，这似乎会导致更有信息的评估，从而更好地塑造适应的实施方式。
(3)现有的评价设计往往在考虑指标的多样性方面受到限制，并且需要大的适应数据集。
对于基础模型，我们响应越来越多的呼吁，要求评估考虑更广泛的期望范围(例如，稳健性，公平性，效率，环境影响)，以捕获广泛的利益相关者价值/偏好，并强调适应模型的样本效率如何通过重新分配设计评估所涉及的资源来允许更多样化的评估。
4.5系统
作者:Deepak Narayanan, Trevor Gale, Keshav Santhanam, Omar Khattab, Tianyi Zhang, Matei Zaharia计算机系统是开发基础模型的最大瓶颈之一。
基础模型通常太大，无法容纳单个加速器(例如GPU)的主内存，并且需要大量的计算来训练(例如，对于GPT-3， > 1000 petaFLOP/s-days [Brown et al. 2020])。
此外，随着时间的推移，这些模型可能会变得越来越大:例如，最先进的语言模型的计算和内存需求在过去三年中已经增长了三个数量级，并且预计将继续以远远超过硬件能力的速度增长(图19)。
一旦经过训练，这些大型模型执行推理的成本很高，而且难以在生产应用程序中进行调试、监控和维护。
我们认为，基础模型的性能和可用性的进一步发展将需要在算法、模型、软件和硬件系统以及编程和部署ML应用程序的新接口之间进行仔细的协同设计。
在本节中，我们讨论了开发和生产大型基础模型的关键计算机系统挑战。
4.5.1通过协同设计提高性能。
如今，训练大规模基础模型[Brown et al. 2020;Rae et al. 2021;英伟达和微软2021;Wang和Komatsuzaki 2021]通常需要定制软件系统，如Megatron、DeepSpeed或Mesh Transformer JAX [Shoeybi et al. 2019;Rasley et al. 2020;Wang 2021]，建立在PyTorch、TensorFlow和JAX等标准框架之上[Paszke et al. 2019;Abadi et al. 2016;Bradbury et al. 2018]。
这些软件系统依赖于跨堆栈的许多创新来大规模高效地训练模型:新的并行化维度，如管道并行性[Huang等人，2019;Narayanan et al. 2019]在保持设备繁忙的同时限制通信，状态分片优化器减少内存使用[Rajbhandari et al. 2020]，即时(JIT)编译器优化计算图[PyTorch 2021]，以及优化库如cuDNN和NCCL [NVIDIA 2021]。
威震天和深度速度在特定规模上是有效的;例如，威天可以在一个具有一万亿参数的模型上使用大约3000个gpu提取高达现代硬件理论峰值吞吐量的52% [Narayanan et al. 2021b]。
然而，扩展到具有更多GPU的更大模型仍然具有挑战性，因为现有的并行化策略在更大的GPU数量下会崩溃。
数据并行性受到批处理大小的限制[Li et al. 2020e]，管道并行性受到模型层数的限制[Huang et al. 2019;Narayanan et al. 2019]，以及单个服务器上gpu数量的张量模型并行度[Shoeybi et al. 2019]。
虽然我们将继续从新硬件中实现性能提升，但大型模型资源需求的增长远远超过了硬件的改进[Brown等人，2020]。
为了促进模型能力的下一个重大飞跃，并使模型质量的进步民主化，协同设计训练算法、模型、软件和硬件将变得越来越重要，因为许多显著提高性能的途径改变了训练计算的语义。
例如，以较低精度(如fp16)执行操作可以帮助提高现代硬件的吞吐量(例如，V100和A100 gpu具有用于低精度矩阵乘法的专用张量核心单元)，但也会影响优化过程的数值[micicikevicius等人，2017]。
类似地，利用权稀疏性可以显著提高训练和推理时间[Elsen等人，2020;Gale et al. 2020]通过只对模型中的非零进行数学运算，但需要不同的训练算法[Jayakumar et al. 2021;Evci et al. 2020;Dettmers and Zettlemoyer 2019]。
协同设计的其他例子包括更有效地映射到硬件的模型架构[So et al. 2019;Child等人，2019;Wang et al. 2020c;Lee-Thorp et al. 2021;Kitaev et al. 2020;Beltagy等人，2020;Tay et al. 2020;Ren et al. 2021]，高效优化器[Anil et al. 2020;Shazeer and Stern 2018]，新的代币化替代方案[Xue et al. 2021;Tay et al. 2021]，专门架构的硬件训练平台[Jouppi et al. 2017;Mudigere et al. 2021;Selene 2021]，以及放宽权重更新语义的分布式并行化策略[Narayanan et . 2019] [Narayanan et .， 2021a]]。
案例研究:有效的知识表示。
作为成功协同设计的具体案例研究，基于检索的模型，如REALM、RAG、ColBERT-QA和RETRO [Guu et al. 2020;Lewis et al. 2020b;Khattab et al. 2020;Borgeaud等人[2021]采用了一种不同的方法来设计模型，而不是简单地增加模型参数的数量。
基于检索的模型不是试图从更大的数据集中直接将隐性知识积累到具有数十亿个参数的DNN模型中(如GPT-3)，而是以文本段落的形式存储模型参数之外的知识，通过密集的向量表示捕获段落内的知识。
然后，这些模型使用可扩展的顶部搜索机制提取与每个输入相关的知识，同时保持DNN模型本身的小(§4)。1.4:建模-内存)。
这种设计提高了模型在生产中的计算效率和可维护性:例如，开发人员可以通过替换文本段落来更新模型的知识，而无需重新训练大型DNN。
基于检索的模型通过利用几个新的跨职能思想取得了有希望的初步结果，包括在训练期间通过检索器反向传播损失[Guu等人，2020](这需要通过由数百万个段落组成的知识存储近似梯度)和建模查询和段落之间的细粒度交互[Khattab和Zaharia 2020;Khattab et al. 2020](这需要将计算分解为向量级最近邻搜索操作)。
这些技术允许基于检索的模型准确高效，但需要的功能不容易被流行的机器学习框架和最近邻索引(如FAISS)支持[Johnson et al. 2019]。
4.5.2自动优化。
系统中的另一个重要挑战是自动化跨算法、模型、软件和硬件的优化应用程序。
虽然许多优化和并行化策略是互补的，但确定最有效的优化组合是具有挑战性的，因为联合搜索空间是组合增长的，并且优化以非平凡的方式相互作用[Narayanan等人，2021b]。
基础模型增加了对自动优化的需求，因为在数千个gpu的规模下，手动实验非常昂贵且耗时。
该领域最近的工作集中在以语义保持优化为目标的系统上。
特别是，已经提出了自动发现数学等效图替换的系统[Jia et al. 2019a;Wang et al. 2021c]，通过高级api和低级编译器促进算子图的分布式执行[Rasley et al. 2020];曼迪普·贝恩斯2021;Bradbury et al. 2018;Shazeer et al. 2018;Lepikhin et al. 2020]，以及混合配送策略的自动化选择[Jia et al. 2019b;Santhanam et al. 2021]。
这些系统帮助在工业中部署了许多基础模型[Fedus等人，2021;m2m - 100 2020;图灵- nlg 2020]。
不幸的是，自动优化在组合语义saltering优化(§4.5.1:system -co-design)时变得更加困难，因为通常不清楚如何联合建模这些技术的统计影响(例如，需要多少训练迭代才能达到特定的精度?)。
因此，我们将需要新的软件工具、库和编译器来自动识别针对时间-精度等综合指标的优化组合[Coleman等人，2017;Mattson et al. 2020]。
构建这样的工具需要系统和机器学习专家之间的紧密合作。
4.5.3执行和编程模型。
基础模型独特的多任务特性提供了在许多应用程序上分摊训练和推理成本的机会。
特别是，适应等范式意味着模型实例之间的更多共享[rafael 2021]。
例如，来自同一预训练模型的两个经过前缀调优的模型[Li and Liang 2021]可以共享相同的模型“干”，从而减少了存储占用(共享的干只需要存储一次)，同时还可以在经过前缀调优的模型之间共享和批处理执行[Shen et al. 2019;Narayanan et al. 2018]。
因此，所使用的特定适应机制为系统优化提供了信息(§4.3:适应)。
关于应该使用什么编程接口来指定从相同预训练模型派生的各种适应模型(例如，模型和派生自相同预训练模型)，或者两个模型的各个组件共享参数(例如，两个模型和共享相同的树干到层)，这是一个悬而未决的问题。
Ludwig [Molino et al. 2019]和PyTorch的Module提供了在模型内组合功能的简单方法，但目前没有系统支持跨模型依赖关系。
让用户有机会提供注释将允许训练和推理系统更有效地优化和编排计算;如果没有这样的注释，系统将无法看到哪些计算和参数可以在模型实例之间共享。
模型的“适应历史”(这个特定的模型是从什么模型改编的)也可以用于调试:一个适应模型对特定类型输入的错误可能源于预训练模型，指出预训练过程与适应过程中的问题。
像PyTorch这样的框架，以及用于训练基础模型的软件库，如HuggingFace Transformers [Wolf et al. 2020]，不允许指定整个模型实例的细粒度沿袭信息。
构建和维护由数千个加速器组成的集群也需要付出巨大的努力。
新的培训模式，如Learning@Home [Ryabinin and Gusev 2020;Diskin等人。2021]探索利用互联网上的志愿者计算来协同训练基础模型。
这种全新的执行模型可以降低任何一个实体的培训成本，但需要跨多个不同领域的协作，比如安全(确保恶意志愿者不能显著改变培训过程)、分布式系统(在志愿者退出时处理容错问题)和众包。
4.5.4基础模型制作。
随着社区继续推动基础模型的能力，实现它们的潜力将需要解决与在生产中部署这些资源密集型模型相关的挑战。
这些挑战包括使用严格的延迟目标执行模型推理，以及确保以自动化的方式监控模型和数据。
对于具有严格成本和延迟约束的应用，模型压缩技术如蒸馏[Hinton et al. 2015;Li et al. 2020d;Sanh et al. 2019]，量化[Polino et al. 2018;Gholami et al. 2021;周等。2018]，修剪[LeCun等。1990;Gordon et al. 2020;McCarley et al. 2019;Wang et al. 2019c;Sajjad et al. 2020]和稀疏性[Gale et al. 2020;Elsen等人[2020]可以通过转换更大的模型来获得所需的推理时间属性来帮助部署。
这些技术最初用于低内存环境(例如，移动电话)中的较小模型(例如，BERT-L)，但现在必须处理数据中心部署中现代基础模型的极端规模。
并行化技术，如张量模型并行化[Shoeybi等人，2019]，传统上用于训练，也可能有助于减少推理延迟，并在gpu上提供额外的内存容量来拟合模型的参数。
除了这些实际限制之外，基础模型和用于训练它们的数据集的大小和复杂性的增加对模型和数据集生命周期管理提出了新的挑战。
由于具有大量参数的模型很难由人类手动检查，因此我们需要更好的系统来实现自动数据集管理(§4.6:data)和模型质量保证。
像行为测试[Ribeiro等人，2020]和模型断言[Kang等人，2020]这样的技术通过为部署在最终应用程序中的模型提供类似于单元测试、运行时监控(以测试时间断言的形式)和持续的模型改进(随着新输入的到来)来简化生产中的模型维护。
这些工具可以帮助解决公平性和偏见问题(§5.1:公平性)，并减少模型的错误预测。
4.6数据
作者:Laurel Orr, Simran Arora, Karan Goel, Avanika Narayan, Michael Zhang, Christopher r<s:1>基金会模型标志着一种范式转变，越来越多的大量数据被“馈送”到这些模型中，以提高适应性能[Devlin等人，2019;Radford et al. 2021;Tolstikhin et al. 2021]，首要的经验法则是“数据越多越好”[Kaplan et al. 2020]。
正如前面章节所提到的，对数据管理的关注引起了对基础模型数据生命周期的关注，包括(1)大规模管理数据(第1节:介绍)，(2)跨新模式集成数据(第2.3节:机器人，第3.1节:医疗保健)，(3)对许可和治理法规的推理-特别是考虑到基础模型训练中使用的大规模网络爬虫-(第3.1节:医疗保健，第5.4节);合法性)和(4)理解数据质量(§4.4:评估)。
虽然基础模型为这些挑战增加了新的和困难的方面，但我们看到这些问题与数据管理和数据分析以及工业ML管道等社区的核心挑战之间存在相似之处。
例如，数据管理长期以来一直在研究可扩展的声明式系统，用于数据分析、版本控制、来源和集成解决挑战(1)和(2)[Zaharia et al. 2012;cudr<s:1> - mauroux et al. 2009;Stonebraker and Weisberg 2013;Stonebraker and Ilyas 2018;Hellerstein and Stonebraker 2005]。
业界有管道来处理挑战(3)，以管理各种数据许可并帮助减轻数据违规。
有一个完整的研究和系统生态系统来应对挑战(4)，以支持交互式数据分析和可视化[Hohman等人，2020]。64
虽然这些解决方案不一定是“基础模型就绪”的，但是我们相信更好地管理基础模型数据生命周期的途径应该从这些现有的系统中获得灵感。
在本节中，我们讨论管理基础模型数据生命周期。
我们首先概述了四种需求，包括大规模数据管理、对异构数据源的支持、数据治理和数据质量监控。
然后，我们设想如何将所有这些需求集成到称为数据中心的整体数据管理解决方案中。
数据中心只是一个数据管理工具包，私营或公共部门可以使用它来更好地支持基础模型数据生命周期的交互式管理。
4.6.1数据管理需求。
基础模型开发的当前实践通常是在整个生命周期中从数据管理和数据文档到模型监控和修补的临时实践[Gebru等人，2018;班迪和文森特2021;Bender and Friedman 2018]。
数据管理社区的研究表明，定义良好的数据管理平台通过数据摄取、数据版本控制、数据来源、高效分析和模型监控，促进了机器学习模型的大规模开发[Hellerstein和Stonebraker 2005;Agrawal et al. 2019;Vartak et al. 2016;Ikeda and wisdom 2010]。65
从数据管理社区获得灵感，我们在为基础模型构建整体数据管理平台时考虑了核心需求。
(1)可伸缩性。
基础模型正在接受越来越多的海量数据的训练[Kaplan等人，2020]，WuDao 2.0模型正在接受4.9 TB的多模态数据的训练。66
由于大多数最新的模型主要是在面向公众的数据集上进行训练，因此这一规模预计会增加。
与每天收集并用于工业基础模型管道的pb级商业和个人数据相比，公共数据只占数据的极小部分[2017年3月]。
因此，对于能够处理多模态基础模型数据集的高度可扩展技术的需求日益增长。
(2)数据集成。
最近使用基础模型的工作表明，利用集成的结构化和非结构化数据可以帮助模型更好地泛化到罕见概念[Orr et al. 2020]，并提高事实知识的召回率[Orr et al. 2020];Logeswaran et al. 2019;Zhang et al. 2019a;Peters et al. 2019;Poerner et al. 2020]。
尽管最近取得了这些成功，但为基础模型集成数据集仍然是一个挑战。
许多作品使用非结构化文本数据与结构化实体知识或图像数据[Antol et al. 2015]。
越来越多的人需要整合不同模式的数据集，如文本、视频、眼球追踪[Hollenstein等人2020]和机器人模拟[Lynch和Sermanet 2021](见§2.3:机器人技术)。
我们需要能够在工业规模上应用于多种模式和多个领域(如政府、商业和科学)的数据集成解决方案。
(3)隐私和治理控制。
用于基础模型的训练数据可能有侵犯数据主体隐私的风险;他们的数据可能在未经他们同意的情况下被披露、收集或使用[Jo and Gebru 2020]，或者在最初获得同意的情况之外。
同意和使用的问题与基础模型特别相关，因为下游应用程序总是无法预测。
正如§5.4:合法性中所解释的，这些问题与基础模型训练的web抓取数据集的流行相结合。
由于关于如何管理和保护网络抓取数据的法律问题仍然存在，67对于公共和私营部门的基础模型提供者来说，使用网络数据的后果仍然不清楚。
我们需要工具来帮助基础模型提供者适应新出现的法规和指导方针，以确保安全和负责任的数据管理。
(4)了解数据质量。
数据质量影响模型性能[Lee et al. 2021b];然而，系统地、可扩展地理解训练数据和相关数据子集的工具包或方法仍处于起步阶段。
数据创建过程可能很混乱，数据可能包含不同类型的偏差[Blodgett et al. 2020;Bender et al. 2021](见§5.1:公平性)，并包含有毒的、虚假的或重复的信息[Chang et al. 2020;Carlini and Terzis 2021;Buchanan et al. 2021;[j]。
数据也在不断更新和提炼[Kiela等人，2021]，可能存在突现实体[Fetahu等人，2015]、分布转移[Chen等人，2021a]和概念意义转移[Kenter等人，2015;Lazaridou et al. 2021]。
此外，一旦部署，基础模型可能会在关键的、细粒度的数据子群上呈现不良行为，基础模型提供商需要检测和缓解这些行为[Goel等人，2021;Hohman et al. 2018;r<s:1>等人，2019;Oakden-Rayner et al. 2019]。
我们需要能够检测并潜在地减轻不同类型的不良数据的工具包，以交互和迭代的方式改进模型性能。
这些工具包还需要适应训练数据的动态特性。
4.6.2数据中心方案。
经过多年的数据管理、数据科学和数据分析工作，我们设想了一个基础模型生命周期数据管理解决方案，我们称之为数据中心。
而例子67最近来承担这些问题使用GitHub数据周围的辩论在副驾驶的法典工具来帮助开发人员代码https://www.pwvconsultants.com/blog/questions-around-bias-legalities-in-githubs-copilot/ ML-focused数据中心68年以及更传统的数据管理系统存在,69年他们要么(1)不要把数据集成作为一个一流的原始,(2)本地不支持端到端生命周期模型预测,或者(3)不允许交互驱动的数据管理和细化，其中基础模型提供者可以根据访问控制指南动态探索和更新可能的数据集。
现在我们讨论数据中心如何满足这四个需求。
数据规模。
为了应对大规模管理的挑战，数据中心将需要标准的数据管理解决方案[Armbrust等人，2009]，如存储和维护大规模数据集的基础设施，以及用于查询、选择和过滤数据集的可扩展接口。
集线器应该支持异构计算和云基础设施，以支持不同环境中的可伸缩解决方案。
数据集成。
该中心应以一流公民的身份整合数据。
它将需要先进的数据集成解决方案[Stonebraker和Ilyas 2018;Abiteboul 1997;Dong et al. 2020;Rekatsinas等人。2017a] 70允许跨模式和领域合并结构化和非结构化知识。
此外，这意味着集线器需要支持对异构数据集和数据源的存储和查询。
访问控制。
考虑到中心的访问控制，中心将需要支持各种文档，例如，数据表[Gebru等人，2018]或数据声明[Bender和Friedman, 2018]，以允许数据管理员反思他们的流程，并对其数据集的预期用例、潜在偏差和局限性保持透明。
数据中心将需要决定上传数据需要哪些文档(例如，数据源和数据描述)以及推荐哪些信息(例如，数据可用于哪些任务)。
此外，文档可能需要随着数据集的发展而更新[Goel et al. 2021]。
数据源通常与许可证相关，该中心需要将不同的数据源与不同的法律问题和条件相结合[Masur 2018]。71
此外，某些数据集有法律指导方针来保护数据主体的隐私。
该中心将需要确保数据集不会泄露个人身份信息(PII) 72，匿名或去身份化数据的聚合不会泄露PII 73，并且数据主体已对其数据的传播给予知情同意。74
借鉴数据集成的思想[Rekatsinas等人，2017b]，中心应该支持机制，以实现高效、安全的数据资源维护和共享。
特别是当某些公共数据集(例如，web转储)的合法性仍在决定中(§5.4:合法性)，中心迫切需要工具来帮助识别许可违规并减轻任何治理违规的影响。
由于某些违规可能与模型行为相关，我们需要系统来支持更好地理解模型行为，正如我们接下来描述的那样。
数据质量工具。
利用数据分析和探索领域，当用户交互式地选择、过滤和精炼用于培训或适应的数据时，该中心将需要工具来快速68。一些公共数据中心包括:https://data.world/;
https://dataverse.harvard.edu/dataverse/harvard,
https:// datacommons.org/,
https://www.data.gov/,
https://www.kaggle.com/,
https://huggingface.co/datasets, https://www.ldc。
upenn.edu/ 69
一些传统的基础模型数据管理系统包括:https://aws.amazon.com/big-data/datalakesand-analytics/，
https://eng.uber.com/michelangelo-machine-learning-platform/,
https://kafka.apache。
作为关键数据驱动决策系统的核心组件，机器学习模型必须解决各种安全和隐私威胁。78
这些威胁可以用传统的“CIA三位一体”计算机安全来描述。
机器学习系统应该保护用户数据的机密性，防止推理和重构攻击[Fredrikson et al. 2015;Shokri et al. 2017;Carlini et al.， 2019，[2021]。
此外，训练模型本身的保密性可能面临模型窃取攻击的风险[tram<e:1>等人，2016;Papernot et al. 2017]。
机器学习系统的完整性可能会受到对抗性示例的影响[Biggio等人，2013;Szegedy et al. 2014]和数据中毒攻击[Biggio et al. 2012;Chen et al. 2017]。
最后，资源枯竭攻击[Shumailov et al. 2020;Hong等人。2020a]会威胁机器学习系统的可用性。
考虑到这些威胁，我们认为基础模型在未来机器学习系统中的安全作用将类似于操作系统在传统软件系统中所扮演的角色。
由于其通用性和普遍性，基础模型可能成为单点故障，从而成为针对源自该模型的应用程序的攻击的主要目标。
然而，反过来，一个具有强大安全和隐私属性的基础模型可以成为设计各种安全可靠的ML应用程序的支柱。
当然，这些应用程序可能仍然需要设计为强制执行特定的安全和隐私保证(就像软件设计人员不能依赖安全的操作系统来防止所有安全风险一样)。
4.7.1风险。
单点故障。
适用于各种应用程序的基础模型表示这些应用程序的单点故障。
例如，对基础模型的数据中毒攻击(攻击者将恶意示例插入训练数据中)也可能影响所有已适应的应用程序。
类似地，针对基础模型的对抗性示例(即，导致模型输出非常不同特征的小输入扰动)可以更容易地转移到适应的应用程序中。
Wallace等人[2019]甚至发现，在任何输入中添加单个对抗性触发器都可以导致GPT-2等语言模型输出预定义的文本。
基础模型也可能成为数据隐私的单点故障。
如果一个基础模型是在公司的私有数据上进行预训练的，并且该模型记住了该数据的一部分，那么所有下游应用程序都可能面临暴露该数据的风险[Carlini et al. 2021]。
基础模型的提供者也可以是应用程序数据隐私的单一信任点。
例如，目前GPT-3的API要求所有用于微调或推理的(潜在敏感的)数据上传到OpenAI的服务器。
设计一个避免这种信任集中化的基础模型服务是一个有趣的问题。
如果基础模型的参数是公开的，则可以促进对已适应应用程序的模型窃取攻击，因为攻击者只需要对公共基础模型的“增量”进行逆向工程[Krishna等人，2019](例如，根据从公共冻结模型中提取的特征训练的线性模型)。
最后，对基础模型提供者的拒绝服务攻击也可能是一个问题，并且可能会通过使用特殊的高成本输入查询模型而加剧[Shumailov等人，2020]。
数据中毒。
迄今为止，成功的基础模型是在从网络上抓取的大型且通常未经整理的数据集上进行训练的[Radford et al. 2021] [Radford et al.， 2019]]。
这种宽松的数据收集加上缺乏直接的训练监督，容易对基础模型的训练数据进行毒化攻击(例如，将针对特定个人或公司的仇恨言论注入Reddit的几个出站页面)。
更糟糕的是，中毒攻击的力量可能会因当今模型的规模和准确性的不断增长而加剧[Carlini 2021]。
为了说明这一点，Schuster等人[2021]表明，在Github数据上使用GPT-2训练的代码自动完成系统可以通过注入少量恶意文件而被毒害，从而建议不安全的代码片段。
Carlini和Terzis[2021]进一步表明，针对CLIP-style [Radford et al. 2021]模型的针对性攻击只需要修改300万个训练样本中的两个。
功能蠕变和双重用途。
基础模型学习一般特征，使它们能够很容易地适应各种任务。
然而，这种灵活性引起了人们的关注，即基础模型的使用可能超出了它们最初预期的目的——一种通常被称为功能蠕变或双重使用的风险。
机器学习中功能蠕变的例子包括过度学习[Song and Shmatikov 2019]和对抗性重编程[Elsayed et al. 2018]。
为了说明这一点，CLIP最初被训练来解决预测图像-文本对的通用任务，但在此过程中也学会了捕捉丰富的面部特征[Goh等人，2021]。
虽然CLIP的“模型卡”79明确将面部识别和其他监控技术置于范围之外，但CLIP当然可以重新用于此类任务[Radiya-Dixit和tram<e:1> 2021]。
这个例子说明，在设计基础模型时，约束(甚至预见)其可能的恶意使用可能是具有挑战性的。
§5.2:误用提供了对基础模型双重(错误)使用的进一步讨论。
多通道不一致性。
多模态可能增加基础模型的攻击面，使攻击者能够利用模态之间的不一致性。
这种攻击的可能性在一个著名的例子中得到了证明:CLIP将一个带有“iPod”字样的苹果分类为iPod [Goh et al. 2021]。
更一般地说，每当一个概念可以使用不同的模式来表达时，这些模式之间的不一致性就可能被利用。
当基础模型适应于主要依赖于一种学习模式的任务时，这种不一致性尤其令人担忧。
例如，考虑使用从CLIP中提取的特征进行面部识别。
这是一项纯粹的视觉任务，但经过调整的模型的特征仍然对文本信号敏感(因此，攻击者可以通过穿着带有印记文本的衣服来逃避面部识别)。
或者，考虑一个自动驾驶系统(一种主要依赖于视觉的应用程序)，它看到一个写着“绿色”的广告牌，并错误地将其解释为绿灯。
4.7.2机会。
安全瓶颈。
如果经过调整的应用程序可以从基础模型继承漏洞，那么它们也可以继承理想的安全特性——例如对对抗性示例或中毒攻击的健壮性。
因此，基础模型可以作为安全瓶颈。
例如，对对抗示例具有鲁棒性的模型在适应其他任务时可以保持其鲁棒性[Shafahi等人，2019]。
类似地，可以(以某种方式)防御中毒、模型窃取或资源耗尽攻击的基础模型提供者可以为其客户的应用程序提供这样的安全保证。
基础模型作为单点故障或安全阻塞点的角色之间的权衡让人想起软件堆栈中其他抽象层(例如，操作系统、数据库系统或Web浏览器)中类似的安全权衡。
由于服务于许多不同的应用程序，抽象层是攻击的主要目标，但与任何单个应用程序相比，抽象层通常还可以利用更多的资源来增强其安全性。
更便宜的私人学习。
当前的基础模型通常是通过收集来自公开可用资源(例如，来自开放的Web)的大量数据来训练的。
这种做法可能会引起对隐私的担忧——从广义上讲，将用户数据从其预期的上下文中取出[Nissenbaum 2004;Carlini et al. 2021]。
虽然一些现有的工作旨在减轻模型记忆训练数据的倾向(例如，通过去重复训练数据[Lee等人，2021b]，或通过差分隐私下的预训练[Anil等人，2021])，但这些解决方案不太可能满足用户可能与文本数据相关联的广泛隐私期望[Brown等人，2022]。
另一方面，在处理稀缺和敏感数据的应用程序(例如医疗保健)中，公共预训练也可能最终成为用户隐私的胜利。
例如，考虑为医疗保健任务训练差异私有模型的问题[Dwork等人，2006]。
训练这样的模型“端到端”(即，不利用任何预训练)到一个体面的隐私-效用权衡目前需要大量的隐私敏感数据[McMahan等人，2018;Basu et al. 2021]。
相比之下，在许多情况下，对公共数据进行预训练的基础模型可以适用于使用较少机密数据执行特定任务[Bommasani等人，2019;tramtrir and Boneh 2021;Li et al. 2022;Yu et al. 2022]。
对大规模对抗例子的鲁棒性。
有证据表明，与标准训练相比，训练一个对对抗示例具有鲁棒性的模型需要更多的数据[Schmidt等人，2018]，但未标记的数据可能足以弥补这一差距[Carmon等人，2019;Uesato et al. 2019]。
此外，增加模型大小和容量(即过度参数化)也被证明是在某些情况下实现对抗性鲁棒性所必需的[Madry等人，2018;Bubeck and Sellke[2021]。
了解如何最好地利用过度参数化和未标记数据来实现对抗性鲁棒性是未来研究的一个重要方向。
考虑到它们前所未有的规模(在模型大小和训练集大小方面)，基础模型具有独特的定位，可以从这条调查线中受益。
尽管规模空前，但不幸的是，目前的基础模型在最坏情况下的对抗性扰动的稳健性方面收效甚微[Fort 2021;Wallace et al. 2019]。
然而，像CLIP这样的多模态模型对(非对抗性)分布变化具有惊人的鲁棒性(见§4.8:鲁棒性)。
这些在分布式稳健性方面的收益是否可以转化为对现实世界攻击的增强弹性，这是另一个令人兴奋的开放性问题。
特别是在攻击者受到各种约束(例如，有限的查询访问或计算预算)的情况下，有理由乐观地认为，增强的分布式鲁棒性可能会导致整体安全性的附带收益——即使基础模型仍然容易受到最坏情况下的“白盒”攻击。
4.8对分布移位的鲁棒性
作者:谢桑，库马尔，陶里罗汉，李托尼，佐川诗织，高庞伟，桥本辰典
现实世界的机器学习系统需要对分布变化具有鲁棒性——它们应该在不同于训练分布的测试分布上工作得很好。
高风险应用，如资源不足国家的贫困地图绘制[Xie et al. 2016;Jean et al. 2016]、自动驾驶汽车[Yu et al. 2020a;Sun et al. 2020a]，医学诊断[AlBadawy et al. 2018;Dai和Gool[2018]都要求模型能够很好地概括训练数据中没有出现的情况，例如，来自不同国家、不同驾驶条件下或不同医院的测试示例。
先前的研究表明，即使在最先进的模型中，这些类型的分布变化也会导致性能大幅下降[Blitzer et al. 2006;多乌姆诺尔(2007);Sugiyama et al. 2007;Ganin and Lempitsky 2015;Peng et al. 2019;Kumar等。2020a;Arjovsky et al. 2019;Szegedy et al. 2014;亨德里克斯和迪特里希2019;Sagawa等。2020a;Recht et al. 2019;阿布尼2007;Ruder and Plank 2018;Geirhos et al. 2018;Kumar等。2020b;Yu et al. 2020b;Geirhos et al. 2020;谢等。2021a;Koh et al. 2021]。
在本节中，我们考虑基础模型对分布移位的鲁棒性的作用。
基础模型在从分布预采样的大量不同的未标记数据集上进行训练，可以适应许多下游任务。
对于每个下游任务T，基础模型适应于从分布内(in-distribution, ID)训练分布T ID中采样的标记训练数据，然后对分布外(out- distribution, OOD)测试分布T OOD进行评估。
例如，一个贫困预测模型[Xie et al. 2016;Jean et al. 2016]可以对来自世界各地的未标记卫星数据进行预训练，以学习所有国家的有用特征，然后对来自尼日利亚的标记示例进行微调，最后在标记示例稀缺的马拉维进行评估。
我们认为1)基础模型是一种特别有前途的鲁棒性方法。
现有的研究表明，与许多鲁棒性干预相比，对未标记数据进行预训练是一种有效的、通用的方法，可以提高OOD测试分布的准确性。
然而，我们也讨论了为什么2)基础模型可能并不总是减轻分布变化，例如由于虚假的相关性或随时间的变化而引起的变化。
最后，我们概述了利用和改进鲁棒性基础模型的几个研究方向。
我们注意到，基础模型导致改进外推的方法之一是通过为自适应模型提供归纳偏差(通过模型初始化)，这是在扩展到下游训练数据之外的不同数据集上学习的。
然而，同样的归纳偏差也可以从预训练数据中编码有害的关联，并在分布转移的情况下导致代表性和分配损害。
关于此类危害和缓解方法的进一步讨论见第4.6节:数据和第5.1节:公平性。
4.8.1优势。
基础模型通过在一个大而多样的基础模型训练分布pre上学习表征，可以提高下游测试分布T OOD上自适应导数的精度。
OpenAI的CLIP模型是在多种图像和自然语言文档上训练的基础模型，已被证明对ImageNet上的一些基准分布变化具有鲁棒性[Radford et al. 2021]:例如，CLIP和标准ResNet50在ImageNet上的准确率都达到76%，但CLIP在ImageNetV2上的准确率提高了6% [Recht et al. 2019]，在ImageNet Sketch上的准确率提高了35% [Radford et al. 2021]，两者都与原始ImageNet训练分布相关，但又有所不同。
相比之下，许多其他稳健性干预措施，如对抗性训练[Madry等人，2018]，不变风险最小化[Arjovsky等人，2019]，图21。
各种配送班次的配送内(ID)和配送外(OOD)输入。
这里描述的预测任务是图像的图像分类和文本的事实验证。
尽管基础模型学习的表征提高了许多转变(例如，常见的腐败)的下游鲁棒性[Hendrycks和Dietterich 2019;谢等。2021a;Radford et al. 2021]，一些变化，如虚假相关性(其中草可以预测牛)[Beery et al. 2020]和跨时间的外推(随着时间的推移而变化的事实)[Lazaridou et al. 2021]，基础模型仍然可能无法解决。
或使用更大的模型对这些ImageNet任务的有效鲁棒性(定义为分布内和分布外性能之间的差距)几乎没有影响，特别是在没有明确的分布转移知识的情况下[Taori等人，2020;Santurkar et al. 2020;Radford et al. 2021;Miller et al. 2021]。
许多其他研究表明，对大型数据集进行预训练可以提高对常见图像损坏、标签移位和标签损坏的鲁棒性[Hendrycks等人，2019a,b];卫星图像任务的空间变化[Xie等。2021a;Kumar et al. 2022];以及自然语言理解任务中的跨主题转换[Hendrycks等人，2020;Fisch et al. 2019;Yogatama et al. 2019]。
另一个例子是，将基础模型训练数据多样化以包含多种语言(如在多语言BERT中[Liu et al. 2020b])可以显著提高未见语言对的性能。
4.8.2持续的挑战。
尽管有希望的迹象表明基础模型将导致对健壮性的实质性改进，但我们预计基础模型并不是分布转移的灵丹妙药。
我们将在下面两大类分布变化的背景下讨论这个问题。
虚假的相关性。
虚假相关是特征和标签之间的统计相关性，在训练分布上具有预测能力，但在测试分布上没有[Heinze-Deml and Meinshausen 2017;Arjovsky et al. 2019;Sagawa等。2020a]。
众所周知的例子包括物体识别依赖背景颜色[Xiao et al. 2020]，医疗诊断的外科标记[Winkler et al. 2019]，众包数据中的注释者偏差[土家2018;Gururangan et al. 2018;Poliak et al. 2018;Geva et al. 2019]和人口统计学偏差[Abid et al. 2021;Nadeem et al. 2021;Gehman et al. 2020]。
模型学习这些虚假相关性的主要原因是基础模型训练和适应数据表现出这些偏差[Nagarajan et al. 2020;Gehman et al. 2020]，这个问题不能简单地用更大的模型来解决[Sagawa et al. 2020b]。
基础模型可能加剧或减轻虚假相关性的影响，但这取决于特定下游任务的性质及其与基础模型训练数据和算法的关系。
通过使用不同的数据集进行训练，基础模型可以提高对仅在训练数据子集中发现的虚假相关性的鲁棒性:例如，现有研究发现，预训练的语言模型可以通过快速从反例中学习虚假相关性来避免虚假相关性[Tu et al. 2020]。
然而，基础模型也会通过引入基础模型训练数据中存在的偏差而加剧这一问题，正如在GPT-3和其他NLP模型中观察到的人口统计学偏差[Abid等人，2021;Nadeem et al. 2021;Gehman et al. 2020]。
此外，单独进行大规模训练并不需要完全解决识别和不依赖下游训练集而不是下游测试集的预测特征的根本问题[Heinze-Deml and Meinshausen 2017]。
解决这些挑战需要我们理解和管理来自基础模型训练的归纳偏差，并开发能够抵抗学习虚假相关性的自适应算法。
外推和时间漂移。
最后，基础模型的少射和零射能力将意味着这些模型将越来越多地应用于远远超出训练分布的领域。
虽然大规模的基础模型训练可以帮助对新分布进行某些形式的外推[Papadimitriou和Jurafsky 2020]，但它们的外推能力可能有限。
例如，如果不重新训练，现有的语言模型无法处理世界知识的变化或语言的变化[Lazaridou等人，2021;dhinggra et al. 2021]， CLIP中的零射击转移在卫星图像域中受到很大影响[Radford et al. 2021]，而ImageNet预训练并没有显著提高大型模型在医学图像上的性能[Raghu et al. 2019];Ke et al. 2021]。
我们相信基础模型不能被假定为在给定的模态(例如，所有的图像)内自动推断，并且它将变得越来越重要的是定义和分离由基础模型新启用的外推形式与那些仍然遥不可及的外推形式。
虽然现有的分布转移分类已经普遍提出[Quiñonero-Candela et al. 2009;Ye等人[2021]，充分理解和定义基础模型有效的分布转移类型是鲁棒性研究的一个主要开放性问题。
4.8.3机会。
基础模型作为一种通用的稳健性干预分布变化的方法具有很大的前景，并为稳健性研究开辟了新的途径。
我们在下面列出了一些机会和悬而未决的问题。
理解基础模型表示。
现有的基础模型稳健性研究主要是经验的，对稳健性增长背后的机制了解甚少。
Sun等人[2019b]假设预训练的表示将不同的域(如ID和OOD分布)更紧密地结合在一起，这反过来可以提高从标记ID数据到OOD数据的泛化[Ben- David等人，2010]。
在有预训练和没有预训练的情况下测量领域表示之间距离的控制实验可以阐明这一效应。
在描述基础模型训练(例如，对比学习作为谱图分解[HaoChen等人，2021a])及其归纳偏差方面有初步的有希望的方向[Saunshi等人，2020a];Lee et al. 2020a;张和桥本2020;谢等。2020]。
然而，这些理论是有限的，无法解决其他经验上有效的基础模型，如全生成语言模型(例如，GPT-3 [Brown等人2020]和image-GPT [Chen等人2020d])。
进一步了解这些归纳偏倚在分布移位下是如何有用的，可能会导致基础模型如何提高鲁棒性的更完整的理论(§4.10:理论)。
基础模型训练中的数据扩充。
虽然在不了解下游任务的情况下训练的基础模型可以避免一些特定于任务的偏差，并且经常提高健壮性，但是源于基础模型训练方式的某些统计偏差可能会持续存在。
作为一个具体的例子，许多当代的自我监督算法严重依赖于选择一组适当的数据增强[Chen等人。2020c]，这反过来又赋予了适应阶段不同类型的鲁棒性。
例如，Xiao等人[2021]表明，使用旋转增强的对比学习训练的视觉基础模型可以提高OOD在具有旋转不变性的自适应任务中的性能，但可能无法提高OOD泛化需要其他不变性的任务的鲁棒性。
进一步研究哪种类型的数据增强可以提高广泛下游任务的鲁棒性-包括从数据中学习的数据增强[Wong和Kolter 2020;Tamkin等人。2021c]或设计成普遍适用于各种数据模式[Verma等人。2021]-将为更好的基础模型训练算法提供信息(§4.2:training)。
基础模型训练中的编码结构。
总的来说，探索对数据中已知结构和不变性进行编码的新方法是基础模型训练的重要途径。
许多现实世界的任务都有额外的元数据(例如，在我们的贫困预测示例中，来自辅助卫星的空间位置坐标和气候信息)，这可能为OOD的推广(例如，跨地理区域)提供额外的结构[Xie et al. 2021a];Koh et al. 2021]。
例如，Xie等人[2021a]表明元数据可以作为预训练的目标，以提高下游OOD的准确性。
在语言方面，对HTML数据中的标签进行建模提供了额外的下游任务相邻监督，允许新的提示形式(例如，为标题建议填写<title>标签)，并提高了数据效率[Aghajanyan et al. 2021]。
虽然当前的数据增强方法对手工制作的知识进行编码，但其他途径(如利用元数据)可以提供更自动化的方法来确定将哪些结构和不变性纳入基础模型训练。
基础模型训练数据的专业化与多样性。
基础模型训练数据的选择具有下游效应——在更多样化的数据集上进行训练并不总是比在更专业的基础模型上进行训练对下游性能更好[Cole等人。2021;Chalkidis等人。2020](更详细的讨论见§4.3:适应)。
在一些领域，如卫星图像和专门的文本主题，在专门领域上继续预训练可以显著提高下游性能[Reed等人。2021;Gururangan et al. 2020]。
这是一个潜在的紧张来源:一方面，我们可能想要在一个大的、多样化的数据集上训练基础模型，以便在分布变化下具有更健壮的性能，而另一方面，我们可能需要专门的基础模型，以提高其在下游任务中的分布内和分布外性能。
更好地理解专门化如何影响基础模型的分布内和分布外性能，将允许我们设计和收集更有效的基础模型训练集。
适应气候变化的方法。
尽管基础模型提供了一个强大的起点，但自适应方法如何使用预训练信息会影响鲁棒性。
例如，语言模型的轻量级调优方法(例如，适配器/前缀/提示调优[Houlsby等人，2019;李梁2021;Lester等人。2021])，通过优化一小组参数(如连续提示)使模型适应新任务，同时保持其他基础模型参数冻结，似乎可以提供OOD性能优势(§4.3:适应)。
Xie等人[2021b]在一种特殊情况下解释了这一点，在这种情况下，用冻结的基础模型组成学习模型可以降低学习模型的复杂性，提高ID和OOD的泛化。
视觉数据集，Wortsman等[2021];Kumar等人[2022]发现冻结基础模型并只训练头部比微调整个模型更能获得更好的OOD性能。
Kumar等人[2022]从理论上解释了这一点，他们表明，即使在简单的设置(两层线性网络)中，完全微调也会扭曲预训练的特征。
然而，总的来说，为什么冻结参数似乎可以提高OOD的性能，人们仍然知之甚少。
最后，虽然目前的自适应方法可能足以实现良好的ID泛化，但这些方法在设计中没有明确地考虑到分布的变化。
作为第一步，我们可以研究分布转移的方法，如领域适应、领域泛化和半监督学习方法，在用于适应时如何与基础模型相互作用。
在这些方向上的进展可以导致适应方法，这些方法可以更好地利用基础模型来增强鲁棒性。
4.9人工智能安全和校准
作者:亚历克斯·塔姆金，杰夫·基林，杰克·瑞安，悉尼·冯·阿克斯
人工智能(AI)安全领域关注的是先进人工智能模型的潜在事故、危害和风险，特别是对社区或社会的大规模风险。
目前的基础模型可能远未构成这种风险;然而，它们的能力和潜在应用的广度是惊人的，并且与以前的ML范例有明显的转变。
虽然人工智能安全在历史上一直占据着人工智能研究的边缘位置，但目前向基础模型及其相应的普遍性的过渡为人工智能安全研究人员提供了一个机会，可以从新的角度重新审视该领域的核心问题，并重新评估它们当前或近期的相关性。80
4.9.1人工智能安全中的传统问题。
人工智能安全研究的一个主要分支涉及先进人工智能系统的影响，包括那些在广泛的认知任务中可能匹配或超过人类表现的系统[Everitt et al. 2018]。81
在这种背景下，安全研究的一个核心目标是减轻先进人工智能发展带来的大规模风险。这些风险可能比§5.2:误用、§4.8:稳健性和§4.7:安全性所考虑的风险更具有推测性;然而，它们的规模要大得多，并且至少在原则上可能来自未来的高性能系统。
特别值得关注的是全球灾难性风险:粗略地说，是全球性或跨代风险，造成死亡或以其他方式显著减少受影响者的福利(例如，核战争或迅速的生态崩溃)[Bostrom和Cirkovic 2011]。
因此，人工智能安全研究相当于一系列项目，旨在描述先进人工智能的发展所带来的灾难性风险(如果有的话)，并制定合理的技术解决方案，以减轻这些风险的可能性或严重性。
从人工智能安全的角度来看，最好的情况是控制问题的解决方案:如何开发一种先进的人工智能系统，使我们能够获得该系统的计算优势，同时让我们拥有足够的控制权，使系统的部署不会导致全球灾难[Bostrom和Cirkovic 2011]。
然而，技术解决方案不足以确保安全:确保安全算法实际上是在现实世界系统中实现的，并且不部署不安全的系统可能需要额外的社会技术措施和制度。
强化学习(RL)研究针对奖励优化的决策代理，在过去十年中一直是人工智能安全的主要焦点。
这里的问题在于，在不构成全球性灾难性威胁的最小意义上，为符合人类价值观的人工智能指定和实例化奖励功能的难度。83
虽然这个问题被称为价值一致性[Gabriel 2020;Yudkowsky 2016]，乍一看似乎微不足道，人类的价值观是多种多样的，84无定形的，并且难以定量捕捉。
因此，一个突出的问题是奖励黑客，即人工智能发现了一个不可预见的政策，该政策可以最大化人类福祉的代理奖励，但其错误规范会导致重大伤害。85
参见Amodei等人[2016]和Hendrycks等人[2021d]，以获得对人工智能安全中开放性问题的更广泛视角
这被一些人称为AGI或人工通用智能，尽管术语使用各不相同[例如，参见Karnofsky 2016]。
需要注意的是，这并不需要我们相信构建某种高级人工智能是一个理想的目标，甚至也不需要我们确定它是可以实现的。83
参见Hubinger等人[2019]对奖励规范和奖励实例化之间阈值出现的一些挑战的讨论。84
参见Gabriel[2020]对人类多样性、伦理和价值一致性问题的深入讨论。参见此电子表格，列出了现实世界中奖励黑客行为的示例，包括一种飞机着陆算法，该算法通过利用模拟器中的缺陷输出大量力量而获得了满分。
价值对齐问题的重点是最大限度地提高可纠错性，即一旦系统运行，系统设计中的错误就可以得到纠正[Soares等人，2015]。
这可能远不是直接的——在强化学习环境中，具有特定目标的智能体将被激励禁止改变该目标的尝试，因为任何改变该目标的尝试都可能不利于目标的实现[Omohundro 2008]。
然而，纯粹的强化学习并不是通往高级人工智能的唯一理论途径。基础模型也可以用简单的(自我)监督目标(如下一个令牌预测)进行训练，但仍然可以以交互式和目标导向的方式使用，无论是否需要额外的强化学习训练。
此外，似乎许多这些方法可以通过直接缩放计算、参数数量和数据集大小来提高能力[hetness等人，2017;Kaplan et al. 2020]。
在基础模型的更广泛的背景下，价值一致性和可纠错性等概念在几个方面与纯粹的RL情况有所不同，因此必须仔细地理论化。
4.9.2现有基础模型与AI安全性。
RL设置中的许多风险来自为实现目标而优化的模型。
然而，在最近的基础模型上进行人工智能安全研究的一个关键挑战是，尽管没有明确优化，但目标导向行为可能会出现(参见§4.2:训练)。
例如，可以在语料库上训练大型语言模型，其中代理以目标导向的方式使用语言，例如在有说服力的文本中。
为了很好地预测下一个标记，模型可能会获得推理和产生参数的一般能力，这些参数可能会在合适的上下文中出现。
在其他类型的人类数据上训练的基础模型可以捕获数据中存在的其他类型的目标导向行为;例如，在视频中训练模仿人类的机器人代理，如果他们的训练数据包括拳击比赛的视频，可能会试图击打或击倒他们的人类操作员。
最近的工作也试图直接训练代理人产生目标导向的行为;例如，Decision Transformer在带有回报的轨迹上训练序列模型[Srivastava等人，2019;Schmidhuber 2019;陈等。[2021b]。
然后，人们可以通过“提示”这个模型获得高回报，从而产生高回报轨迹，这从强化学习环境中提出了类似的奖励黑客问题。
然而，目标导向模型安全研究的一个主要目的是对智能体所追求的行为获得更多的原则性控制和可解释性，而不是依赖于来自黑箱神经网络的不可思议的决策。86
这使得当前的基础模型成为人工智能安全研究的一个令人兴奋的研究途径，因为对齐它们可能是对齐更先进模型的有用先驱[Christiano 2016;Cotra 2021;Kenton et al. 2021]。
其中一个挑战是基础模型的训练目标与期望行为之间的不一致;例如，可以训练语言模型来预测训练语料库中所有文档的下一个单词，而不考虑准确性，但用户可能希望模型只输出真实或有用的文本[Tamkin et al. 2021a]。
引导目标导向的智能体走向期望行为的一种潜在方法可能是用自然语言描述动作来训练它们——这可能使它们能够用语言来指导它们，并使它们能够输出描述它们“相信”自己正在执行的任务的可解释语言，类似于可控生成和源归因的方法[例如，Keskar等人，2019，参见§2.3:机器人，§2.5:交互和§4.11:可解释性]。
然而，需要进一步的进展来确保这些模型在野外的可靠性和自一致性(第4.8节:鲁棒性)，以及获得对这些模型如何运作的更机械的理解[Cammarata等人，2020，也见第4.11节:可解释性]。
即使基于自然语言的未来基础模型的控制能够更好地实现任务规范86关于理解和语义之间关系的更多信息，参见§2.6:哲学和监测，模型可能会从人类数据中获得欺骗性或其他不良行为-识别和中和这种行为是未来研究的另一个重要方向。
虽然前文所述的自监督目标训练模型以捕获数据中的人类行为，但新的训练范式可能产生目标导向的基础模型，能够在复杂环境中执行广泛的任务，并且在不同领域表现出优于人类的能力(参见§4.2:training)。
例如，目标导向的基础模型可以在开放式的自我游戏设置中进行训练，类似于AlphaGo，或者在大型多任务单智能体RL设置中进行训练。
这可能会导致紧急能力的出现，使让代理执行目标的努力复杂化，特别是如果许多代理在一个丰富的世界模拟器中一起训练，这鼓励了欺骗、误导、伪装、说服和战略规划等技能的发展。
除了打击欺骗行为外，如何有效地评估和控制非常有能力的模型的行为，也就是所谓的可扩展监督或校准(Amodei et al. 2016;Leike et al. 2018];例如，对化学基础模型提出的新反应进行评分(见§4.4:评价)。
因此，用于训练、控制、监控和理解这些模型的新的“人在循环”方法是令人兴奋的未来方向。
最后，即使在这些更先进的能力出现之前，近期人工智能安全的一个重要研究领域是描述和预测当前自我监督基础模型的能力。
这有三个方面的挑战。
首先，基础模型的通用性意味着它们可以以意想不到的方式应用于无数不同类型的应用程序。
列举基础模型的当前和计划的应用程序是不足以捕获它们可以被使用的全部方法的。
其次，即使在特定的应用程序中，模型功能也是突现的:随着模型的扩展，它们以意想不到的方式增长和变化。
例如，通过“提示”控制GPT-3的能力是一种紧急现象，在较小的GPT-2模型中只有最明显的瞥见[Radford等人，2019;Brown et al. 2020]。
未来基础模型的涌现特性会是什么样子是未知的。
第三，即使在特定的应用程序和规模中，模型的功能也不容易表征。
例如，一旦在输入中加入逗号，GPT-3执行加法的能力就会大大提高[Branwen 2020;Brockman 2020]。
同样，提示的小改动也会对任务性能产生很大的影响。
由于提示空间难以枚举，因此明确断言任何任务都超出当前基于提示的基础模型的范围是具有挑战性的——这是从基础模型推断可能的灾难性风险的主要挑战。
4.9.3未来地基模型可能带来的灾难性风险。
当前模型的广泛和快速增长的能力表明，试图从更先进的系统中描述可能的灾难性风险是有益的。
我们看到，先进的基础模型至少可以从两方面促成这样的结果。
灾难性的健壮性失败。
§4.8:鲁棒性讨论了当面对新类型的数据时，模型如何以意想不到的或有害的方式表现[Amodei et al. 2016];Yudkowsky et al. 2008]。
如果基础模型被集成到重要的系统中，利用基础模型的能力来快速适应许多不同的任务和情况，这些失败可能是特别重要的。
如果发生在战争系统(导致不必要的武器发射，可能引发冲突)，关键基础设施(意外破坏关键能源或农业能力)，或者如果它们对很大一部分经济活动至关重要(其意外故障可能导致生活水平突然崩溃和政治不稳定)，则故障可能是灾难性的;参见§5.5:经济学)。
事实上，与其他类型的人工智能相比，基础模型的灾难性鲁棒性失败的威胁尤其相关。
这是因为基础模型由单个模型组成，该模型可以适应许多不同的用例，这样，从模型学习的统计关联中派生的健壮性失败原则上可以跨几个不同领域以相关的方式表现出来。
如果将相同的基础模型集成到多个关键功能中，则模型中缺乏鲁棒性可能导致跨多个关键功能的相关故障或故障安全。
Misspecified目标。
使用基础模型可能会增加优化不一致但易于指定的目标的风险，通常被称为古德哈特定律[Kenton等人，2021;Goodhart 1984]。
当前这些风险的一个例子是某些推荐系统(如两极分化、媒体成瘾)的负面影响，这些推荐系统可能会优化简单的用户粘性指标，而不是更难以衡量的社会和消费者福祉组合[Burr etal . 2018;Milano et al. 2020]。
未来的机构可能会利用不可解释的基础模型来最大化简单的度量，如利润或GDP，因为这些模型有能力适应这些度量所依赖的许多不同的子问题。
然而，在更大的尺度上，优化这些代理指标，而不是为人类福利设计的更全面的目标，可能会无意中导致环境或地缘政治危害[Gabriel 2020;Creel and Hellman 2021]。
4.9.4的结论。
总之，我们认为基础模型的当前和潜在的未来涌现特性使它们成为人工智能安全领域的成熟研究对象。
我们鼓励今后在描述和预测基础模型的确切能力和风险方面开展工作;开发新方法，使基础模型与人类价值观和预期目标保持一致;各州、研究实验室和企业应协调采取积极措施，以减轻突出的风险。
4.10理论
作者:Aditi Raghunathan, Sang Michael Xie, Ananya Kumar, Niladri Chatterji, Rohan Taori, Tatsunori Hashimoto, Tengyu Ma严谨的数学理论在许多工程和科学学科(如电气工程中的信息论)中起着基础作用。
我们相信基础模型理论在指导技术决策和创新方面特别有益，因为与基础模型实验相关的巨大计算成本。
此外，理论见解有助于阐明基本的局限性，并解释令人惊讶的经验现象。
然而，尽管最近取得了许多进展，但社区目前对基础模型的理论理解有限[Arora et al. 2019b];郝晨等。2021a;魏等，2020b;;Zhang and Hashimoto 2021;Saunshi等，2020b;Dao et al. 2019;Tosh et al.， 2021;;Cai et al. 2021;Lee et al. 2020a;Zimmermann et al. 2021;Bansal et al. 2020;Wang and Isola 2020;Tsai et al. 2020;田等。2020a,b;Tripuraneni et al. 2020;Du et al. 2020]。
深度神经网络构成了基础模型的主干。
即使在经过充分研究的监督学习设置中，训练和测试场景具有相同的分布，围绕深度网络也存在许多未决问题，例如理解非凸优化、优化器的隐式正则化效果和表达性。
基础模型提出的问题远远超出了监督深度学习的范围。
理论分析基础模型的核心问题是理解为什么在一个可能存在无监督/自监督损失的分布上进行训练，会对不同的下游分布和任务产生良好的自适应性能。87
E将讨论一个直观的模块化来分析基础模型，它揭示了监督学习和基础模型、具体和核心技术问题之间的联系，以及一些有前途的理论工具来解决这些问题。
这些新的核心问题可以为基础模型提供有用的见解，并且可以与监督深度学习理论并行研究。
当我们专注于分析下游性能时，提出的模块化和工具可以证明对分析其他感兴趣的指标很有用，例如对分布转移的鲁棒性(第4.8节:鲁棒性)和安全性(第4.7节:安全性)。
回想一下，基础模型是在大量原始数据(§4.2:训练)上进行训练的，然后适应特定的任务(§4.3:适应)，因此可以自然地分解为训练和适应阶段。
我们识别它们之间的接口，并将特定于基础模型的部分从需要标准深度学习理论的部分中分离出来，这样它们就可以独立工作。
我们引入了一个模块化的分析框架，它也被隐式或显式地应用于最近的作品中，例如，Arora等人[2019b];郝晨等[2021a];Wei等[2020b];Tripuraneni et al.[2020]。
这种模块化分析的关键部分是预训练自适应接口。
我们首先描述了模块化，并讨论了为什么我们发现这种模块化很有前途，最后讨论了一些限制。
我们将把训练阶段明确地称为“预训练”，以区别于适应阶段，适应阶段也可能涉及对特定任务的几个样本进行训练。
Pretraining阶段。
基础模型的预训练通常涉及数据分布预(例如，自然文本的分布)和预训练损失函数(;)用于测量参数为∈Θ的模型的输入上的损失(例如GPT-3中的语言建模损失)。
设ppre表示来自pre的大量独立样本的经验分布。
预训练使ppre上的损失最小化，我们称之为经验预训练损失，得到模型θFM:
我们将种群分布预上的相应损失(称为种群预训练损失)作为中心概念:
（2)
基于优化的适应阶段。
我们将自适应定义为依赖于θFM的一般约束优化问题，抽象掉那些基于优化某些损失函数(如微调和提示调谐)的自适应方法(参见，例如[Houlsby等人，2019;李梁2021;Lester et al. 2021]，§4.3:适应)。
由于不同的自适应方法可以修改不同的模型参数子集，因此我们用一些Γ来表示自适应模型参数的空间。
给定一个下游任务分布任务(例如，特定领域的问答)和从任务中采样的几个经验样本ptask，我们将适应阶段建模为最小化ptask上的一些适应损失r适应。
where adapt (， θFM ?
为经验自适应损失，(，θFM)≤0是控制自适应参数复杂性的可选约束，包括自适应过程的显式正则化(如模型维数和范数)和隐式正则化。
列举了几种常用的自适应方法，并讨论了相应的自适应参数和约束条件(，θFM)≤0。
(1)线性探测:在基础模型的表示之上训练线性分类器。
这里Γ = R是维度表示上的线性分类器的集合，(，θFM)可以是的l2范数或l1范数。
(2)微调:对随机初始化的线性头部进行几步优化，其他参数均从θFM初始化开始。
这是和线性头部的连接。
这样的过程可以对应于(，θFM)≤0捕获的初始化θFM的一些隐式正则化。
确切的术语(，θFM)将取决于所使用的优化算法，并且这种优化隐式正则化的表征是一个活跃的研究领域[例如，Gunasekar等人。2017;Soudry et al. 2018;Gunasekar et al. 2018;Arora et al. 2019a;Blanc et al. 2019;Woodworth et al. 2020;魏等。2020a;郝晨等。2021b;Damian et al. 2021;Kumar et al. 2022，以及其中的参考文献]。88
(3)提示调优:优化一小组连续的任务特定向量，它们位于任务输入之前。
这里是连续提示向量，通常具有较小的维度，我们可以选择对规范进行约束。
需要注意的一个明显的限制是，该公式排除了适应方法，如情境学习[Brown et al. 2020]，其中在适应阶段没有“训练”(即最小化一些经验适应损失)。
我们将在§4.10.3中讨论这一点和其他限制:
theory-incontext。
适应阶段的两个中心量是种群适应损失
最小的适应损失
对模块化阶段进行单独分析。
现有的标准监督学习泛化理论旨在证明pre≈pre和adapt≈adapt。
专门针对深度网络解决这些问题是一个活跃的研究领域。
我们还可以利用标准的学习理论分解，通过过量泛化误差和最小自适应损失来约束最终的下游任务损失，如下所示。
adapt(任务，θFM)≤★adapt (θFM)最小适应损失
其中泛化误差捕获了适应和适应之间的接近程度。89
这些关键数量之间的分解和关系如图22所示。
如上所述，泛化和优化箭头在很大程度上归结为监督设置下的深度学习理论。
我们剩下的是基础模型的主要挑战，即理解为什么最小的适应损失*适应(θFM)可以很小，这是§4.10.2中研究的预训练种群损失的结果:
theory-interface。
Arora等人[2019b]的工作开创了对这个问题的追求，在对比学习的背景下，从上面的★adapt (θFM)到pre (θFM)，以及HaoChen等人[2021a];Tosh et al. [2020 Tosh et al.[， 2021]]放宽数据假设。
通过显式约束(，θFM)≤0来表征适应的归纳偏差可能并不总是可行的。
我们提出的模块化也适用于这些情况，但为了简化符号，我们将重点放在可以通过显式约束近似实现隐式正则化的情况上。89
更准确地说，泛化误差项是adapt (task， θFM) - adapt (task， θFM)和adapt(★task， θFM) - adapt(★，θFM) = adapt(★task， θFM) -★adapt (θFM)的和，其中★task是(5)的最小值。(6)易于使用adapt (task， θFM)≤adapt(★task， θFM)。
在该框架下(隐式或显式)包括使用语言模型进行预训练[Wei等人，2021]或使用自训练算法进行自我监督[Lee等人，2020a] [Wei等人，2020b];Cai et al. 2021]，以及多个监督任务[Tripuraneni et al. 2020;Du et al. 2020]。
4.10.2
为什么预训练-自适应界面很有趣?
如图22所示，标准监督理论之外的主要缺失环节是:
在什么条件下，较小的群体预训练损失pre (θFM)意味着较小的最小适应损失★adapt (θFM)，为什么?
导致成功的接口的条件可能取决于几个数量，如预训练和适应分布，目标和训练方法，以及模型架构。
这个问题超出了标准泛化理论的范围，但它确实将我们的范围缩小到几个特定于基础模型的重要因素，并抓住了基础模型中各种重要开放问题的本质，正如我们在下面讨论的那样。
首先，我们注意到该接口处理涉及两个不同分布的人口数量。
因此，界面成功的条件可能涉及分布的特殊属性，例如，预训练分布的多样性以及预训练和自适应数据之间的结构变化。
这使得接口的分析具有挑战性(如§4.10.4所述:
理论工具)，因为我们需要对这两个分布之间的关系做出仔细的建模假设。
然而，这提出了一种可能性，即用于分析这些接口的工具和技术可能有助于理解分布变化的影响，并预测基础模型何时可以提高鲁棒性。
其次，人口损失以及成功接口的可能条件取决于模型体系结构。
这就提出了打开神经网络黑盒子的挑战。
在一个特定分布上的一个小的预训练损失告诉我们关于中间层的性质是什么?
这样的分析也将指导我们设计新的适应方法，更仔细地利用不同的中间表示。
第三，在最小的自适应损失下，通过约束复杂度度量(，θFM) < 0，可以捕获少镜头学习或自适应的样本效率。
我们需要正式地描述这些复杂性度量(例如，通过理解适应过程的隐式正则化效应)，并进一步理解为什么小的种群预训练损失意味着低复杂度的适应参数任务。
对这个问题的一个满意的答案可能使我们能够提高下游适应的样本效率。
最后，重要的是，接口的关键组成部分是预训练和自适应损失的选择。
我们想了解如何最好地结合预训练和适应目标，以成功适应。
最能保证成功适应的预训练目标可能与预训练过程中明确最小化的目标不同——上面的界面允许在预训练分布上使用任何替代人口目标。
此外，新的替代目标可以证明在广泛的任务集合中导致良好的适应，可以阐明使基础模型成功的基本方面。
总而言之，该界面排除了泛化问题，并允许我们正式推理预训练和适应阶段的几个重要数量之间的相互作用，这些相互作用可以以重要的方式指导实践。
4.10.3
挑战:分析情境学习和其他突发行为。
GPT-3 [Brown et al. 2020]展示了上下文学习的力量，这是一种不需要任何参数优化的自适应方法。
在适应阶段，预训练的语言基础模型接受一个提示——将任务的输入-输出示例连接起来的标记序列——然后是一个测试示例，并通过对迄今为止看到的序列(提示加测试示例)进行调节，简单地生成测试示例的标签。
换句话说，没有对模型参数进行显式训练或更改。
通过简单地将示例作为输入执行，模型从不同的示例中“学习”的机制是什么?
前面的模块化并不直接适用，因为我们在适应过程中没有获得新的模型参数，而是通过执行结构设计的输入来使用基础模型的生成能力。
然而，分离无限数据预训练和有限数据预训练的想法仍然是有用的。
例如，最近的一项工作从假设无限预训练数据和足够的模型表达能力开始研究上下文学习[Xie et al. 2021c]。
这些假设将上下文学习的特征简化为分析基于上下文学习提示的预训练分布的问题，这些提示来自于与预训练数据不同的分布。
特别是Xie等人[2021c]提出，情境学习是从预训练分布中的长期连贯结构中产生的，这是一种具有连贯结构的潜变量生成模型来描述的。
更广泛地说，虽然本节中提出的模块化提供了一个很好的框架，可以获得对基础模型有用的理论见解，但一些紧急行为，如上下文学习和其他尚未发现的功能，可能需要超越模块化，例如，通过打开架构的黑盒。
理解预训练和适应阶段之间的接口需要比传统的监督学习更仔细地研究数据分布。
这是因为预训练和任务适应的分布本质上是不同的。
根据定义，基础模型是在原始数据上训练的，这些数据通常是极其多样化和任务不可知的，而适应数据则严重依赖于任务。
类似地，上下文学习是学习生成与预训练分布相似的数据的结果，因此理解上下文学习需要对预训练数据进行仔细的建模。
因此，回答围绕基础模型的核心问题需要现实的和可解释的假设，这些假设也可以进行分析。
最近的研究要么假设人口数据的某些属性，例如，HaoChen等人[2021a]中的扩展属性;Wei等[2020b]，或者总体数据是由具有某种结构的潜变量模型生成的[Saunshi等. 2020a];Wei et al. 2021;Arora et al. 2016;Lee et al. 2020a;张和桥本2020;Tosh et al. 2021]。
我们通常缺乏将基础模型的性质与总体数据分布的结构联系起来的数学工具。
HaoChen等[2021a]运用谱图理论利用种群分布中的类内连通性。
对于潜在变量模型，通过概率和解析推导更精确地表征θFM是可能的，但迄今为止仅限于相对简单的模型。
社区将从解决这个问题的更系统和通用的数学工具中受益匪浅。
定义简单的玩具案例也是非常可取的，这样理论家就可以精确地比较各种工具和分析的优势。
例如，HaoChen等人[2021a]和Wei等人[2020b]考虑了歧管的混合问题，这可能是视觉应用的一个很好的简化测试平台。
对于诸如NLP这样的离散领域，我们需要更多有趣的测试平台。
我们认为，捕获真实数据集相关属性的可处理的理论模型是将基础模型置于坚实的理论基础上的关键一步。
4.11可解释性
作者:John Hewitt*， Armin W. Thomas*， Pratyusha Kalluri, Rodrigo Castellon, Christopher D. Manning与大多数其他机器学习模型相比，基础模型的特点是训练数据和复杂性的大量增加以及不可预见能力的出现:基础模型能够完成不可预见的任务，并以不可预见的方式完成这些任务。
越来越多地采用基础模型，因此产生了对理解其行为的不断增长的愿望、需求和前所未有的挑战。
与特定任务模型相比，基础模型是在庞大且通常高度不同的数据集上进行训练的，可能跨越许多领域和模式(参见§4.2:训练)。
通过这种训练，基础模型学习了非常广泛的行为，这些行为在任务和领域之间可能会有很大的不同，正如它们适应不同类型的下游任务的能力所证明的那样，并且表现出针对每个任务的特定行为(参见§4.3:适应)。
以GPT-3为例，它被训练成一个巨大的模型来简单地预测文本中的下一个单词。
虽然这是一个非常具体和简单定义的学习任务，但它使GPT-3能够获得远远超过下一个单词预测的能力，通过将其与包含各种互联网文本的庞大训练数据集相结合。
因此，GPT-3现在可以在提供少量训练样本的情况下适应明显超出其原始训练任务范围的行为，例如简单的算术和计算机编程。
这表明，即使是回答关于基础模型的看似最简单的问题也是具有挑战性的:它具有什么功能?
此外，这些不同的功能在多大程度上依赖于不同的或共享的模型机制，类似于模型中的算法构建块，这是一个悬而未决的问题。
一方面，基础模型可以被解释为单个模型，它利用一些可推广的模型机制集来跨任务和领域很好地执行。
在这种情况下，可以通过识别和描述这些机制来全面了解它们的行为。
另一方面，基础模型对不同任务深刻适应不同行为的能力表明，它们也可以被理解为一个独立的专家模型的大集合，每个模型都针对一个特定的任务。
例如，GPT-3用于算术的模型参数似乎不太可能与用于从英语翻译成法语的参数有多大关系。
在这种情况下，对一个任务中的模型行为的解释因此不一定能提供关于其他任务中的行为的信息。
我们将其称为基础模型的一个模型-多个模型的性质(参见图23)，并认为理解基础模型在一个和多个模型之间的位置将是理解其行为的核心。
为了使这一研究领域系统化，我们提出并讨论了理解基础模型的三个层次[受Marr 1982的启发]:我们首先讨论了理解模型能够做什么的挑战和机遇，然后讨论了为什么它会输出某些行为，最后讨论了它是如何做到的。
具体来说，“什么”的问题旨在描述模型在不窥视模型内部的情况下可以执行的行为类型，“为什么”的问题旨在根据数据中的潜在原因解释模型的行为，以及“如何”的问题旨在理解产生这些行为的内部模型表示和机制。
在介绍了所有三个层次之后，我们最后讨论了基础模型的不可解释性和可解释性所导致的潜在后果。
人们普遍认为，对一项技术最简单的理解就是知道这项技术能做什么。
这个看似直截了当的问题对于基础模型来说是非常具有挑战性的，因为这些模型能够执行无数不可预见的行为和任务。
特定任务的神经网络模型被训练成在单一领域执行单一任务，例如，图像分类。
因此，他们的任务以及输入和输出领域是明确的;然而，即使对于这些模型，在给定特定输入的情况下，要确切地知道模型将做什么也是一项挑战。
例如，对于两个感知上相似的输入，模型行为可能出乎意料地大不相同[Garg和Ramakrishnan 2020;Jin et al. 2020]或相同数据的两个亚群(例如，按种族或性别分层[Hovy and Søgaard 2015;Blodgett et al. 2016;Tatman 2017;Buolamwini and Gebru 2018])。
对于基础模型来说，描述模型行为的挑战被放大了许多倍。
模型能够执行的任务空间通常很大且未知，输入和输出域通常是高维且巨大的(例如，语言或视觉)，并且模型较少局限于特定领域的行为或故障模式。
例如，考虑到GPT-3在大型语言语料库上的惊人能力，并随后发展出生成大多数功能的计算机程序片段的能力。
因此，描述基础模型行为的一个关键挑战是识别它所具有的能力。
更进一步，对于基础模型可以执行的每个任务，可能有很多或无限多的任务，当试图理解更简单的、特定于任务的模型的行为时，所有的挑战仍然存在。
描述基础模型可以执行的每个“任务”，由于它们的一个模型-多个模型的性质而变得更加复杂(参见图23)。
再次以GPT-3为例，表明它可以通过简单的提示来定制许多任务(见§4.3:适应性)。
然而，可以通过许多可能的提示来指定每个任务，提示中的细微变化可能导致模型行为的有意义的更改。
例如，对一篇电影评论进行情感分类的任务，可以通过在电影评论后面加上“她对这部电影的情感是……”来指定。或“我的总体感觉是这部电影……”;尽管这些提示似乎构成了密切相关的任务，但GPT-3对每个提示的响应准确性不同[Zhao et al. 2021]。
像这样的观察提出了关于提示特征和由此产生的模型行为之间关系的重要问题。
具体来说，对看似相似的提示的有意义的不同响应实际上可以被认为是来自同一个模型还是来自高度不同的模型机制，并且在一个任务中描述基础模型(或其适应衍生品)的行为是否真的有助于描述模型的其他可能适应的行为?
为了确定一个基础模型所具有的能力和它所缺少的能力，研究人员可以利用受控的评估。
在这里，领域专家设计已知需要特定能力的提示，然后研究模型正确响应这些提示的能力[Papadimitriou和Jurafsky 2020;Lu et al. 2021a;Kataoka et al. 2020;吴等。2021c;谢等。2021a;Koh et al. 2021]。
例如，心理语言学家设计了提示，要求语言模型在语法正确的句子和语法不准确的句子之间进行选择;知道模型是否始终更喜欢语法正确的句子而不是语法错误的句子，可以告诉我们模型是否具有识别这种不准确性所需的特定语法能力[Linzen et al. 2016]。
考虑到基础模型可能的能力的巨大范围，以及我们目前缺乏任何通用的方法来确定先验的基础模型是否具有给定的能力，像这样的定制评估是至关重要的。
它们允许探索基础模型能够的行为范围，同时需要最小的模型访问:我们只需要呈现输入并接收模型输出，并且我们不需要依赖于对模型的实现或参数的访问。
考虑到基础模型可能能够(或不能)完成的无限多的需要的和不需要的任务、子任务和行为，描述模型行为和能力将变得越来越具有挑战性和重要性。
我们认为，与其依赖少数专家来制定和测试可能的行为，不如将这些类型的分析扩展到测试更多的行为，这在一定程度上是通过向不同的社区和许多学科的专家开放这条探索线，以及通过增加这些评估的访问和规模。
4.11.2行为解释
除了描述基础模型正在做什么之外，还可以尝试通过根据数据中的潜在原因提供这些行为的解释来描述它执行某些行为的原因。
虽然当前的解释方法提供了对行为的解释，可以揭示影响模型响应的输入的质量，但它们通常需要完全访问模型才能做到这一点，并且通常在阐明任何一般模型机制的能力方面受到限制，基础模型使用这些机制来响应许多输入、任务和域。
当前的解释方法通常可以被理解为不同的模型，这些模型旨在为另一个黑箱模型的特定行为提供解释。
重要的是，这些方法与要分析其行为的模型是分开的，模型本身是不可解释的。
这种分离可能是有问题的，因为所提供的解释可能缺乏可信度[Jacovi和Goldberg 2020]，因为对行为的原因不可靠和误导[参见]。
Rudin 2019]。
更进一步，不合理的解释可能会诱使人类更加信任不合理的模型(有关人工智能信任的详细讨论，请参阅Jacovi等人[2021])。
当我们从特定于任务的模型过渡到广泛采用基础模型时，这些类型的关注就会增加，因为它们的行为要复杂得多。
目前的解释方法在很大程度上可以分为提供模型行为的局部或全局解释[Doshi-Velez和Kim 2017]。
局部解释试图解释模型对特定输入的响应，例如，通过将行为的每个输入特征归因于相关性，或通过识别与行为最相关的训练样本[Simonyan等人，2013;Bach et al. 2015;Sundararajan et al. 2017;Shrikumar et al. 2017;Springenberg et al. 2014;Zeiler and Fergus 2014;Lundberg and Lee 2017;Zintgraf et al. 2017;Fong and Vedaldi 2017;Koh and Liang 2017]。
相比之下，全局解释不依赖于特定的输入，而是旨在揭示影响模型行为的整体数据质量，例如，通过综合模型与行为最密切相关的输入[Simonyan等人，2013;Nguyen et al. 2016]。
局部和全局解释为特定任务模型的行为提供了有用的见解[例如，Li et al. 2015;Wang et al. 2015b;Lapuschkin et al. 2019;Thomas et al. 2019;Poplin et al. 2018]。
在这里，由此产生的解释通常被认为是产生行为的模型机制的启发式;例如，当模型读取手写数字“7”时，看到一个解释认为水平线非常重要，很容易产生这样的印象，即水平线是模型用来识别所有7或区分所有数字的重要特征。
然而，考虑到基础模型的一个模型-多个模型的性质，我们应该小心不要从对行为的特定解释跳到对模型行为的一般假设。
虽然目前的解释方法可能会揭示特定的行为，例如，通过识别强烈影响这些行为的数据方面，最终的解释不一定能提供对其他(甚至看似相似的)输入的模型行为的见解，更不用说其他任务和领域了。
另一种方法可能是通过以自我解释的形式利用基础模型的生成能力来完全避开这些类型的事后解释[cf. Elton 2020;Chen et al. 2018]，也就是说，通过训练这些模型不仅生成对输入的响应，而且共同生成对该响应的人类可理解的解释。
虽然目前还不清楚这种方法在未来是否会有成果，但我们有理由怀疑:语言模型和现在的基础模型，在没有任何事实基础的情况下，在产生流利、看似合理的内容方面表现得非常出色。
简单的自我生成的“解释”也可以效仿。
因此，辨别模型创造听起来合理的解释的能力与提供对其行为的真实见解的能力之间的区别是很重要的。
4.11.3模型机理表征。
对系统的深刻理解通常意味着理解系统是如何运行的:它包含哪些知识和机制，以及它们是如何组合成整体的?
如果这确实是可能的，那么描述基础模型中的表示和在其上操作的机制将是满足彻底理解这些增殖模型的愿望的核心;无论这些机制是多而具体的还是少而可概括的，它们都是基础模型在不同任务和领域中采用广泛行为的能力的核心。
为了使模型表征和机制的概念具体化，考虑GPT-3表现出的一个简单行为:它很快观察到GPT-3在提供小数字加法的例子时所做的事情，然后查询执行两个新数字的加法:在高概率下，它预测了加法的正确结果[Branwen 2020;Brockman 2020]。
当被问及为什么GPT-3会有这样的表现时，人们可以在输入中找到证据，比如它的提示对它的反应有很大的影响(这些可能是两个要加的数字，但不一定)，或者GPT-3的训练数据对它的反应有很大的影响(这些可能是加法的例子，但不一定)。
深入研究该模型，我们可以设想更深入地了解GPT-3用于添加特定一对数字的机制以及它用于添加其他任意一对数字的机制。
我们还可以设想对这些机制是否类似于“加法”的数学概念或仅仅与这个概念相关的更深层次的理解。
通过理解单个模型机制，我们可以建立对基础模型复杂行为的组合理解。
一项比数字加法稍微复杂的任务是解决数学单词问题，其中数字带有单位，问题用自然语言表示。
一旦我们理解了模型执行加法的机制(或机制)，我们就可以研究这种机制是否被用作解决文字问题的中间步骤。
如果使用了加法机制，我们已经建立了对模型如何解决单词问题的理解，我们已经增加了对基础模型概括数量和加法概念的信心(而不是另一种相关性或启发式)，而且，我们已经增加了对我们预测模型的能力的信心为什么(它关注的是输入的哪一部分)和输出是什么(两个数字的加法)。
如果没有使用加法机制，我们可能会对这是真正的加法保持健康的怀疑态度，并且我们可以研究使用了哪些表示和机制。
重要的是要意识到有许多更复杂和有关模型机制的潜在情况，例如，从名称中的字符或图像中的像素估计种族。
在基础模型中建立这种机制的证据及其使用可以支持道德或法律责任，以禁止模型执行诸如预测性警务、市场营销、贷款申请和监视等任务。
已经出现了大量的方法来研究神经网络模型的这些内部方面。
通常，这些方法将模型分离为节点(例如，神经元、层或层的一部分)，然后询问节点中捕获的表示或节点组装的机制。
有些方法是由假设驱动的:通过假设节点可能捕获某些信息(例如，一个单词的语法特征，或者一个人的种族)，人们可以探测所有节点，以量化它们提供了多少信息[Alain和Bengio 2016;Veldhoen et al. 2016;Belinkov et al. 2017;Adi et al. 2017;Conneau et al. 2018;Hewitt and Liang 2019;休伊特和曼宁2019;Voita和Titov 2020;Pimentel et al. 2020]。
其他方法建立在解释方法的基础上，它们不是识别哪些数据导致某种行为，而是试图识别哪些数据导致某个节点激活，或者哪些节点导致模型中稍后的另一个节点激活，从而揭示模型表示和机制的集合[Olah et al. 2020;Mu和Andreas 2020;Carter et al. 2019;Goh et al. 2021]。
总的来说，这些方法检查模型的内部，并为正在进行的基础模型行为的探索提供了基础。
然而，基础模型中潜在的表示和机制的数量是巨大的，特别是考虑到它们的一个模型-许多模型的性质，并且这些类型的方法通常只捕获模型内部的一小部分。
因此，扩大表征和机制的发现并阐明那些与模型行为最相关或最普遍的表征和机制是一个公开的挑战。
与解释基础模型的许多方法一样，这些类型的探索将受益于包括和支持更多样化和跨学科的研究者，以及更容易获得的、灵活的和可扩展的发现方法。
总之，我们相信基础模型的一个模型-多个模型的性质(回顾图23)为当前的可解释性研究提供了新的机会和挑战:单个基础模型有许多适应性，我们根本不知道它们共享公共机制的程度。
在机制被共享的程度上，理解基础模型可能是描述这些机制及其关系的一个容易处理的问题。
在某种程度上，机制是独立的，基础模型的每一个适应都必须被独立地分析，这导致了基础模型的任何新的适应的性质的深刻的不确定性。
4.11.4不可解释性和可解释性的影响。
最后，我们想强调的是，基础模型的广泛采用与最近许多跨学科研究人员的请求不一致，即不要将复杂的黑箱模型用于高风险决策[例如，Rudin 2019]，而是要专注于更具有内在可解释性的模型的长期开发和应用。
在这些请求中，旨在解释基础模型的工作是一把双刃剑。
大型机器学习模型，以及现在的基础模型，通常是由强大的公司和机构部署的，可解释性方面的渐进式进步可以被夸大为“道德清洗”，并继续使用模型，好像它们已经实现了可解释性，掩盖了它们仍然远远低于传统算法可解释性标准的现实。
此外，当可解释性方法通常假定易于访问模型及其实现和参数时，可解释性不仅可以作为强大机构的掩护，还可以将模型知识集中在同一个人手中。
对于那些致力于基础模型的可解释性的人来说，始终如一地询问是否正在努力使基础模型对研究人员和模型所有者来说是可解释的，还是对每个人来说都是可解释的，这是一种责任。
同时，在基础模型已经被部署的程度上，可解释性的工作提供了独特的机会，将基础模型的知识，从而权力，转移到数据化和评估的人。
解释可以促进发现模型的社会突出方面。
更根本的是，创建允许任何人解释基础模型行为的可访问方法的工作将权力转移给不同的人，创造调查模型的机会，发现对个人或其社区重要的模型方面的机会，以及有意义地同意、改进或完全反对基础模型使用的机会。
最后，对于研究人员来说，重要的是将基础模型的可解释性不仅视为一个目标，而且视为一个问题:研究可以探索和评估缺乏基础模型可解释性是否是内在的，应该深入研究和广泛了解，这是一个阻碍这些系统使用(或增加监管)的严重问题，或者未来的基础模型是否有可能维持所有人的高可解释性标准。
5社会
基础模型的社会影响，既涉及模型本身的构建，也涉及它们在开发应用程序中的作用，需要仔细检查。
具体地说，我们预计基础模型将具有广泛的社会后果，这很难理解:基础模型是不直接部署的中间资产，而是作为进一步适应的基础。
因此，对技术的社会影响进行推理的传统方法可能很复杂;对于具有明确目的的系统，社会影响更容易(但仍然很难)掌握。
在本章中，我们将讨论如何处理并开始理解模型基础模型的社会影响的复杂性。
具体来说，我们讨论(i)关于不平等(第5.1节:公平)和滥用(第5.2节:滥用)的危害，(ii)关于经济(第5.5节:经济学)和环境(第5.3节:环境)的影响，以及(iii)关于法律(第5.4节:合法性)和道德(第5.6节:道德)的更广泛的考虑。
5.1不公平和不公平
作者:Rishi Bommasani, Fereshte Khani, Esin Durmus, Faisal Ladhak, Dan Jurafsky
基础模型中存在的内在偏差是各种训练偏差源(左)的副产品，它与适应过程中引入的偏差一起，决定了用户在特定下游应用环境中所经历的外在危害(右)。
我们强调相同的基础模型是许多不同应用程序的共享基础;因此，它的偏差会传播到许多应用程序中。
此外，由于用户所经历的危害是特定适应模型的结果，因此将这些危害归因于此图中描述的各种过程和来源既重要又具有挑战性。
5.1.1介绍。
基础模型有可能产生不公平的结果:对人们的待遇是不公平的，特别是由于历史歧视的不平等分配[Hellman 2021]。
与任何人工智能系统一样，基础模型可能会产生不公平的结果，巩固权力体系，并将技术的负面影响不成比例地分配给已经被边缘化的人，从而加剧现有的不平等[Sweeney 2013;Kay et al. 2015;Buolamwini and Gebru 2018;本杰明2019;Ajunwa 2019;D'Ignazio and Klein 2020;Crawford 2021]。
在这里，我们询问与基础模型相关的与公平相关的危害，这些危害的来源是什么，以及我们如何进行干预以解决它们。
我们在这里讨论的问题与算法公平和人工智能伦理等更广泛的问题有关[Corbett-Davies and Goel 2018;Chouldechova and Roth 2020;赫尔曼2020;约翰逊2020;Fazelpour and Danks 2021]，种族和技术[Benjamin 2019;Hanna et al. 2020;Gebru 2021;Field et al. 2021]，以及社会与技术的共存[Abebe et al. 2020]。
5.1.2危害。
基础模型是中间资产，在调整之前没有指定用途;理解它们的危害需要对它们的属性和它们在构建特定任务模型中所扮演的角色进行推理。
我们描述了内在偏差90，即间接但普遍影响下游应用的基础模型的特性，以及外在危害，即在特定下游应用的背景下产生的危害[Galliers和Spärck Jones 1993]。
固有的偏见。
基础模型的特性会对下游系统造成危害。
因此，这些内在偏差可以在基础模型中直接测量，尽管危害本身只有在基础模型被适应并随后应用时才会意识到，即这些是潜在的偏差或危害[DeCamp和Lindvall 2020]。
我们专注于最广泛研究的内在偏见形式，代表性偏见，特别是考虑误读，代表性不足和代表性过高。
人们可能会被有害的刻板印象所歪曲[Bolukbasi等人，2016;Caliskan et al. 2017;Abid et al. 2021;Nadeem et al. 2021;Gehman等人，2020]或消极态度[Hutchinson等人，2020]，这可以通过下游模型传播，以加强社会中的这种错误陈述[Noble 2018;[本杰明2019]。
人们可能被低估或完全被抹去，例如，当LGBTQ+身份术语[stronger et al. 2020;Oliva等，2021;Tomasev et al. 2021]或描述非裔美国人的数据[Buolamwini and Gebru 2018;Koenecke et al. 2020;Blodgett和O'Connor 2017]在训练数据中被排除在外，下游模型将在测试时与类似的数据作斗争。
人们可能被过度代表，例如，BERT似乎默认编码了一种以英语为中心的观点[Zhou等人。2021a]，这可以放大多数人的声音，并有助于观点的同质化[Creel和Hellman 2021]或单一文化[Kleinberg和Raghavan 2021](§5.6:伦理)。
这些表征偏差适用于所有人工智能系统，但在基础模型范式中，它们的重要性大大提高。
由于相同的基础模型可以作为无数应用程序的基础，因此人员表示中的偏差会传播到许多应用程序和设置中。
此外，由于基础模型做了很多繁重的工作(与适应相比，适应通常是轻量级的)，我们预计许多经历过的危害将在很大程度上由基础模型的内部属性决定。
外在的伤害。
用户可以从下游应用程序体验特定的危害，这些下游应用程序是通过调整基础模型创建的。
这些危害可能具有代表性[Barocas et al. 2017;克劳福德2017;Blodgett et al. 2020]，例如信息检索系统对黑人女性的性化描述[Noble 2018]，默认使用男性代词的机器翻译系统对人的性别错误[Schiebinger 2013 [Schiebinger, 2014]]，或者产生有害的刻板印象[Nozza et al. 2021;Sheng et al. 2019;Abid et al. 2021]。
它们可能包含滥用，例如当基于基础模型的对话代理使用有毒内容攻击用户时[Dinan等人，2021;Gehman et al. 2020]或microaggression [Breitfeller et al. 2019;Jurgens et al. 2019]。
所有这些面向用户的行为都可能导致心理伤害或有害刻板印象的强化[Spencer等人，2016;Williams 2020]。
除了个人遭受的伤害之外，群体或亚群体也可能遭受诸如群体一级表现差异等伤害。
例如，系统可能在非裔美国英语的文本或语音上表现不佳[Blodgett和O'Connor 2017;Koenecke et al. 2020]，从种族、性别和保险状况少数群体的临床记录中错误地检测出医疗状况[Zhang et al. 2020b]，或者无法检测到肤色较深的人的脸[Wilson et al. 2019];Buolamwini and Gebru 2018]。
随着基础模型得到更广泛的应用，包括在高风险领域，这些差异可能会导致进一步的、更严重的危害。
Koenecke等人[2020]讨论了如果非裔美国英语使用者不能可靠地使用语音识别技术(例如，由于底层基础模型的不公平)，这可能意味着他们无法从某些衍生产品(例如，语音助手，辅助技术)中受益，并且如果这些技术用于进行就业面试或转录法庭诉讼程序，他们将处于不利地位。
更一般地说，描述这些群体层面的伤害(并为那些受到伤害的人争取正义)也需要人工智能社区提高对基于群体的偏见(Allport 1954)和社会群体的理解:我们指出了社会科学和其他社区在超越性别二元治疗方面的相关工作[Lindsey 2015;威斯布鲁克和萨珀斯坦2015;Richards et al. 2017;达尔文2017年;凯斯2018;Hyde et al. 2019;曹和多乌姆格尔III 2020;Dinan et al. 2020]，对种族进行更细致的处理[例如，Penner和Saperstein 2008;Freeman et al. 2011;Saperstein and Penner 2012;Saperstein et al. 2013;Penner and Saperstein 2015;Field等人。2021]，更好地处理交叉身份[例如，Crenshaw 1989;纳什2008;希内斯2011;Penner and Saperstein 2013;Ghavami and Peplau 2013;Bright et al. 2016;Buolamwini and Gebru 2018;May et al. 2019;O'Connor et al. 2019;Guo和Caliskan 2021]，以及更现代的残疾治疗[例如，Batterbury 2012;Spiel et al. 2019;Hutchinson et al. 2020]。
额外的考虑。
为了更全面地理解基础模型的危害，需要进一步记录内在偏差和外在危害;未来的工作应该阐明内在偏见和外在伤害之间的关系[Blodgett et al. 2020] [Blodgett et al.， 2021;;]Goldfarb-Tarrant et al. 2021]。
这份文件要求以学术界和行业从业人员以外的利益相关者为中心:基金会模式的不公平影响将主要由少数群体感受到，他们在学术界和行业中都没有得到充分的代表。
具体来说，对于基金会模型，它们的创建和研究可能将由那些拥有所需资源和资源的人进行，这进一步强调了以边缘化声音为中心的场所的重要性[D' Ignazio和Klein 2020，§5.6:伦理]。
特别是，当跨应用程序聚合时，特定适应模型的用户研究可以提供来自基础模型固有偏差的危害的引人注目的和个性化的文档，所有这些都以个人用户为中心。
通过这种方式，我们想象人机交互(HCI)中的方法，通过一些调整来适应基础模型中涉及的抽象，将有助于边缘化社区的声音集中(在§2.5:交互中进一步讨论)。
5.1.3来源。
为了充分表征和适当干预基础模型的危害，我们必须能够将其根源追溯到基础模型的属性和适应过程，并进一步分解为个体偏见来源的作用[Friedman and Nissenbaum 1996]。
来源追踪对于确定所经历的伤害的伦理和法律责任至关重要，尽管归因将需要新的技术研究，以突出因果关系[Pearl 2000]和影响[Koh and Liang 2017]等问题。
数据。
基于基础模型的几种类型的数据塑造了应用程序的行为和相关的外部危害:用于训练基础模型的训练数据，用于调整基础模型的适应数据，以及测试时的用户数据/交互。
对于所有这些数据源，数据的属性(例如，毒性和仇恨言论[Henderson等人，2017]，辱骂性语言[Waseem等人，2017]，微侵犯[Breitfeller等人，2019]，刻板印象[Voigt等人，2018])将在基础模型(及其改编衍生物)的偏差中表现出来。91
由于训练数据是决定基础模型和相关固有偏差的关键数据源，因此我们在这里主要关注训练数据。
目前，训练数据之间的关系，以及相关的数据实践(例如，数据管理，数据选择和数据加权)[Paullada et al. 2020;Bender et al. 2021;Rogers 2021])，基础模型获得的内在偏差仍不清楚;未来的工作迫切需要澄清这种关系。
由于基础模型通常需要巨大规模的训练数据，这不仅对其文档提出了明显的挑战[Bender等人，2021]，而且对91进行了全面的科学探索适应，其中涉及标记特定于任务的数据，标签空间选择中的偏差[Crawford 2021]以及标记该数据的注释者的偏差[Geva等人，2019;Sap等人[2019]也会造成用户所经历的外在伤害。
阐明数据偏差和模型偏差的关系，我们预计需要新的协议来解决这个规模。
建立偏差的标度定律，类似于准确度指标的标度定律[Kaplan et al. 2020;Henighan et al. 2020]，可以使小尺度的系统研究为大尺度的数据实践提供信息。
建模。
建模决策(例如，训练目标(§4.2:训练)、模型架构(§4.1:建模)、适应方法(§4.3:适应))影响基础模型及其衍生品中的偏差，从而影响所经历的外在危害。
现有的研究表明，基础模型放大了训练数据的偏差，扩展了机器学习和深度学习模型的趋势[Zhao et al. 2017;Wang et al. 2019d;Jia et al. 2020;Hashimoto等人[2018]，尽管关于模型属性是什么以及如何导致这种偏倚放大的仍不清楚。
此外，考虑到直接应用基础模型可能是不可行的(由于其规模)，压缩这些模型或使其更有效的努力也似乎会放大偏差[Hooker等人。2020;Renduchintala等。2021]。
基础模型修正社会行为并诱发社会变化，进而修正后续训练数据，反馈循环也可能加剧放大效应;这种形式的反馈效应往往会加剧其他ML应用中的不平等[Lum和Isaac 2016;Ensign et al. 2018;桥本等。2018]。
除了在培训和应用基础模型时做出的明确决策外，社区价值观[Birhane et al. 2020]和规范(第5.6节:伦理)间接和隐含地[Liu et al. 2021b]影响了构建模型的决策。
因此，结合引入基础模型的工作[例如，Brown et al. 2020]和标准基准[Friedman and Nissenbaum 1996，§4.4:评估]来测量偏差，以及对不同用户群体进行用户研究以记录所经历的伤害，是确保最佳实践积极强调对偏差和不公平的考虑的步骤。
建模者。
与所有算法系统一样，在开发或应用基础模型的决策机构中，利益相关者和边缘化社区的代表性和多样性不足本身就存在问题，并可能对这些社区造成更大的伤害。92
虽然难以记录，但现有的开发基础模型的努力表明这是一种可能性:Caswell等人[2021]证明了用于训练多语言模型的多语言数据集中对代表性较低的语言的数据处理存在缺陷，Hutchinson等人[2020]表明模型通常包含对残疾人的不良偏见。
在这两种情况下，这些偏见和危害可能已经被开发团队中更好的代表所注意到。
此外，由于最终用户可能比开发人员更多样化，并且可能更早注意到这些问题，因此允许用户反馈有助于基础模型设计(§2.5:交互)是一个重要的前进方向。
5.1.4干预措施和追索权。
解决、缓解和纠正与技术相关的不平等需要整合社会和技术方法[Abebe等人，2020]。
具体来说，对于基础模型，我们既考虑了主动方法，它改变了模型的开发和部署方式，以预防性地减少伤害，也考虑了被动方法，它对伤害做出反应，并为未来做出改变。
在其核心，基础模型的抽象使两个方面复杂化:了解基础级别的干预是否成功地减少了损害，需要在具体部署的应用程序级别进行下游观察，而在发生损害时进行追索权，需要向基础模型提供者提供反馈和责任的上游传播。
干预。
管理技术系统干预的一般原则适用于基础模型设置:确定哪些来源对偏见或伤害负有最大责任，为有针对性的行动提供所需的证据。
例如，迫切需要在设计、生产和控制技术(例如，基础模型)及其应用的团队中提高多样性[Longino 1990;哈丁2015;Nielsen et al. 2017;O'Connor et al. 2019;Hofstra et al. 2020;Katell et al. 2020]如果证明缺乏多样性与危害有关，则会进一步加剧[Caswell et al. 2021]。
此外，透明文档[例如，Gebru等人，2018;Bender and Friedman 2018;Mitchell et al. 2019]和审计[例如Raji和Buolamwini 2019]在提供干预和变革的动力方面同样至关重要[Burrell 2016;立顿2018;粗纱架2020;Raji et al. 2020;Wilson et al. 2021]。
基础模型的规模，以及其可访问性的细节，为现有的文档和审计协议引入了新的挑战，我们将在§5.6:道德中进一步讨论。
迄今为止，为减少技术的不公平影响而考虑的许多干预措施，包括在基础模型制度中考虑的干预措施，都是以数据(避免反映不公平或偏见)和建模决策(避免放大数据偏见)为中心的技术缓解方法。
在基础模型制度中特别重要的是认识到这些缓解方法可能针对管道中的不同步骤，如训练数据[例如，Lu等人，2020]、建模目标[例如，Zhao等人，2018])以及适应方法和测试时间使用[例如，Park等人，2018;Zhao et al. 2019]。
因此，不同的方法可能不仅或多或少有效，而且需要来自不同实体的行动(例如，基础模型提供者vs.应用程序开发人员)，并且或多或少强烈地影响这些模型昂贵的训练过程(例如，更改创建基础模型的过程vs.事后更改它)。
这种形式的技术干预也可能针对不同的目标:一些干预，如改变训练数据，旨在减少内在偏见。
另一方面，大多数缓解算法/机器学习公平性的工作都考虑减少模型行为方面的结果差异，即与外在伤害更直接相关的下游系统的输出。
目前，所有形式的技术缓解都受到严重限制:衡量或打击内在偏见的方法是脆弱或无效的[Gonen和Goldberg 2019;Ethayarajh et al. 2019;Bommasani et al. 2020;周等。2021b;Antoniak和Mimno 2021]，测量或对抗外部结果差异的方法可能与利益相关者的目标不一致[Saha等人，2020]，并且有证据表明某些类型的技术干预可能同时无法令人满意[Corbett-Davies和Goel 2018;Kleinberg et al. 2017]，不可能[Lechner et al. 2021]，甚至可能加剧不平等[Xu et al. 2021]。
尽管如此，我们仍然相信技术方法在解决基础模型制度中出现的危害方面仍将发挥重要作用;总的来说，我们提倡透明度，特别是考虑到技术缓解方法可能无法实现预期目标。
更广泛地说，必须谨慎地提出偏见和减轻偏见的主张，以便向具有不同专业知识的各种利益相关者清楚地传达现状(例如，基于基础模型构建的应用程序开发人员和监管技术的政策制定者;[Nissim et al. 2020])。
追索权。
不幸的是，主动干预不太可能完全解决基础模型可能产生的所有潜在危害或不平等。
当出现损害时，目前没有广泛采用(或法律要求)的框架来解决受损害方的适当追索权问题。
虽然某些协议可能存在于特定的应用程序中，但是基础模型的抽象再次引入了一个分离:损害可能部分归因于基础模型提供者和下游应用程序开发人员，但是将这一责任分配给任何一方仍然具有挑战性。
更简单地说，甚至没有适当的机制将这些危害传达给基础模型提供者(即使向应用程序开发人员提出了反馈或投诉)。
因此，需要新的规范和标准来规范应用程序开发人员和最终用户的反馈如何到达上游的基础模型提供商，如何确定对这些损害负责的实体(例如，基础模型提供商，应用程序开发人员)，以及与法律责任的关系(§5.4:合法性)。
为了在这个问题上取得进展，我们鼓励未来的工作参考其他领域中使用的实践(特别是那些具有类似抽象和多实体结构的领域)，并且我们预计引入的任何标准都可能需要合理的动态，以便它们能够与这些模型及其应用程序的快速变化的现状同步。
是5.1.5外卖。
机器学习有不公平影响的记录，其危害的大部分负担由边缘化社区承担。
基础模型给这一计算带来了新的挑战，但最终，为了使其社会影响公平，需要进行重大研究和变革，以了解它们造成的危害，并有意义地解决和纠正这些危害:
(1)基础模型的一对多性质，即在许多应用程序中使用相同的几个基础模型，意味着基础模型的内在属性渗透到许多下游应用程序中。
因此，这些模型中的有害偏见对所经历的伤害产生了巨大的影响。
(2)基础模型制度中的偏差和危害来自许多来源(例如，训练和适应数据、建模和适应决策、建模人员多样性和社区价值观)。
找出偏见和伤害的根源是干预和责任问题的根本所在;归因需要可靠地进行新的技术研究。
(3)基础模型的不公平并非不可避免，但解决这些问题需要多管齐下的方法，包括主动干预(例如，以数据为中心和以模型为中心的变化)和被动追索权(例如，反馈和问责机制)。
5.2滥用
作者:Antoine Bosselut*， Shelby Grossman*， Ben Newman在本节中，我们考虑误用基础模型——人们使用基础模型的情况(例如，生成语言)，但是他们的能力被有意地利用来对群体或个人造成伤害。
这一定义将滥用问题置于不平等(模型可能在没有恶意的情况下造成伤害;§5.1:公平性)和安全性(不良行为者利用模型中无意的能力或漏洞造成伤害;§4.7:安全性)。
下面，我们将概述基础模型如何启用新形式的误用并支持用于误用检测和缓解的新工具。
5.2.1基础模型会被滥用于有害的目的。
生成基础模型在规模(§4.2:训练)、多模态(§4.1:建模)和适应性(§4.3:适应)方面的进步将使它们被滥用，以生成高质量、廉价和个性化的内容，用于有害目的。
在本节中，我们将在恶意活动的两个示例上下文中讨论这三个维度:操纵性内容创建和骚扰。
内容质量。
与之前的人工智能方法相比，基础模型能够自动生成质量更高、看起来更人性化的内容。
它们可能会赋予造谣者权力，例如，一些国家创建内容来欺骗外国民众，而不透明地表明这些内容与某个国家有关。
目前，创建这些内容通常需要雇佣能说目标人群语言的人。
政府可能会将内容制作外包给目标国家的母语人士，但这一决定会给运营安全带来真正的风险。
基础模型将允许创建通常与人类创建的内容无法区分的内容[Kreps等人，2020;Clark et al. 2021]——事实上，它将能够为各种各样的语言做到这一点——实现了创建共鸣内容和维护操作安全性的两个目标。
除了欺骗外国人之外，基础模型生成高质量合成图像(深度伪造)或文本的能力可能被滥用来骚扰个人。
深度造假已经被用于骚扰目的。
例如，印度调查记者拉纳·阿尤布(Rana Ayyub)就被一种高质量的深度伪造技术锁定，该技术将她的脸叠加到一段色情视频中，导致她离开公共生活数月。95
因为基础模型通常是多模态的(§4.1:建模)，它们可以类似地模拟语言、动作或写作，并且可能被滥用来尴尬、恐吓和勒索受害者。96
内容创造成本。
基础模型将大大降低内容创建的成本，进一步降低恶意行为者进行有害攻击的进入门槛[Brundage et al. 2018]。
2017年，一项源自俄罗斯、针对美国人的影响行动的预算为1220万美元[DiResta et al. 2018]。
最近，俄罗斯的个人向美国自由记者支付每篇75至200美元的稿费，作为虚假信息宣传活动的一部分。97
基础模型将降低这些边际成本。
虽然GPT-3等基础模型在生成内容时可能会出现错误[Buchanan et al. 2021]，但聘请少量编辑来修复它们比直接聘请内容创作者更可行。
训练基础模型的初始成本更大(§4.5:系统)，但这些费用对于大多数国家行为体来说应该是可控的[Buchanan et al. 2021]。
除了货币成本之外，基础模型需要较少的技术技能来获得高质量的结果。
目前的工具，如视频编辑软件，可以实现可信的照片或视频深度伪造，但需要熟练的用户花费几个小时的时间来生成高质量的内容。
基础模型降低了这种使用障碍:它们的少量适应能力(§4.3:适应)为应用程序用户提供了新的交互模式(§2.5:交互)，这将允许用户快速迭代内容创建。
个性化。
基础模型将减少创建个性化内容的障碍。
例如，2016年针对美国的俄罗斯个人虚假信息包括高度定制的内容。
社交媒体帖子被精心制作，以推动有关叙利亚的叙事(例如，美国应该离开叙利亚)，这些叙事与“黑人的命也是命”活动人士产生共鸣[DiResta等人，2018](例如，建议美国应该关注美国黑人社区面临的问题，而不是叙利亚的问题)。
同样的叙述被重新包装，以引起德克萨斯州分离主义者的共鸣[DiResta等人，2021]。
这样的内容创建工作既昂贵又耗时。
基础模型将允许类似的活动，但由于适应的成本较低(§4.3:适应)，其规模较小。
除了基础模型允许演员为小众观众定制内容外，它们还允许演员为单个个体定制内容——这一功能可能被骚扰者滥用。
基于个人属性或信息的基础模型可以创建现实的个性化内容，这可能更令人尴尬，使受害者处于更危险的境地，98并导致更多成功的勒索企图。
5.2.2基础模型将是有害内容的强大检测器。
虽然基础模型的生成能力将提供大量的误用机会，但这些能力可能使它们成为有害内容的强大检测器。
虽然这些功能与检测人类和模型生成的内容同样相关，但在本节中我们将重点关注检测模型生成的内容。
首先，我们概述了当前人工检测方法在发现基础模型的有害滥用方面将面临的挑战。
然后，我们提出了基础模型的交互和多模态表示能力如何使它们成为自动检测有害内容的强大工具。
最后，我们讨论了在在线设置中部署自动检测模型以对抗潜在的基础模型误用的风险。
重新思考人为干预。
目前，恶意行为经常被发现(在社交媒体上，有时被删除)，通过搜索互联网来发现内容的来源。99
例如，虚假的社交媒体个人资料通常会从约会网站窃取个人照片，这些照片是通过反向图片搜索发现的。
同样，虚假信息网站经常使用抄袭的内容来掩盖欺骗性内容[DiResta和Grossman 2019]，这很容易通过进行网络短语搜索来识别。
基础模型将限制这些检测策略的有效性。
相对简单的虚假信息活动已经利用人工智能生成的照片100来消除通过反向图像搜索发现的可能性。
评估这些照片是否由人工智能生成的工具是可用的，但基础模型将使这项工作复杂化-对于文本和视频也是如此-挑战人工发现技术[Ippolito等人，2020;Clark et al. 2021]。
作为检测器的基础模型。
基础模型的能力使它们成为创造性内容的强大生成器，同样的能力也使它们成为模型生成内容的强大检测器。
现有的研究表明，基础模型可以用于检测来自文本生成器的虚假信息[Zellers等人，2019b] -生成统计文本伪像[Holtzman等人，2020]-并且可以使用提示问题来评估他们自己世代的毒性水平[Schick等人，2021]。
下面，我们将描述未来的基础模型将如何实现更强大的机器生成有害内容检测系统。
基础模型的交互和多模态接口的改进将为改进对基础模型滥用有害内容生成的检测提供新的机会。
当前的统计检测器必须重新训练和重新部署，以整合有关误用策略文本内容的新知识[Dinan等人，2019]。
基础模型的快速学习能力(第4.3节:适应)可能使它们能够从人类反馈中适应新的滥用策略，而基础模型最初并未训练以识别这些策略[Lee et al. 2021a]。
同时，基础模型的多模态能力将使误用生态系统更具表现力。
先前的工作已经探索了错误信息如何比真实内容在社交网络上传播得更快[Starbird等人，2018;Vosoughi等。2018]，在回顾性分析时产生可识别的特征。
基础模型的多模态能力可以让它们共同学习有害内容的表征及其在社交网络上的典型传播特征。
这些联合表示可以提供强大的工具，用于预测某些类型的自动生成内容是否表示滥用行为。
地基模型作为自动探测器的风险。
对模型生成和人为生成的有害内容的自动检测系统的改进将使这些系统在网上更加普遍，产生潜在的负面后果。
任何检测系统都会有误报的情况，人工生成的公平内容将被标记为有害的[Sap等人，2019;Xu et al. 2021]。
算法误报影响用户(或用户组)的速度可能导致下游损害(§5.1:公平性)。
基础模型的适应能力应该使系统误报更容易解决，因为模型可以在局部编辑以重新分类这些例子(§4.3:适应)。
然而，极端情况可能不会被优先考虑，在这些情况下，追索权将具有挑战性。
更广泛地说，误用检测系统的大规模部署可能会导致有害内容生成器和检测器之间的“军备竞赛”。
大多数使用基础模型的内容生成器将缺乏单独开发它们的资源，并且将使用由较大实体部署的系统。
虽然使用条款政策应概述这些系统的可接受用途(§5.6:道德)，但基础模型的部署者还需要内部检测系统来识别其产品的滥用101并减轻滥用(§5.4:合法性)。
然而，对于那些拥有资源来开发他们自己的基于基础模型的内容生成器的滥用行为者，将会有更少的控制，这将对平台施加压力，要求它们通过其分发渠道管理共享的内容。
乐观地说，内容平台包含了世界上一些资本最充足的公司。
他们的资源可能使检测器的发展超出了大多数个人滥用代理的能力。
由于大规模重复训练这些系统的高成本，这种资源优势可能会抑制个体基础模型的开发。
然而，即使没有最大的基础模型来支持它们，许多基础模型误用的实例仍然可能成功，特别是当攻击者可能利用基础模型的交互能力来快速生成可以逃避检测的内容时。
5.3环境
作者:Peter Henderson, Lauren Gillespie, Dan Jurafsky基金会模型可能会带来许多社会和环境效益，例如在法律领域(第3.2节:法律)，医疗保健(第3.1节:医疗保健)，甚至应对气候变化[Rolnick等，2019]。
但由于它们的规模，如果模型创建者不小心，它们本身就会通过增加碳排放对环境产生负面影响[Strubell et al. 2019;Lottick et al. 2019;Schwartz et al. 2019;Lacoste et al. 2019;曹等，2020;Henderson等人，2020;为了了解基础模型中可能发生此类排放的位置，我们考虑了它们的生命周期。
首先，他们接受大量数据的训练，可能长达数月的时间，通常分布在数百到数千个gpu上。
然后，它们可能被适应到新的领域，或者可能被提炼成更小的模型。
所有这些都可以被认为是训练制度的一部分。
纯粹用于研究的模型可能不会超出这些步骤。
在对模型进行调整和/或提炼之后，它们可能会被部署到生产环境中。
在这一点上，许多轮推理将通过模型运行，直到一个新的模型被训练出来，循环重复。
这些步骤中的每一步都有可能使用大量的能源，并可能导致碳排放。
基础模型在初始训练阶段可能产生大量的一次性能源成本和碳排放。
例如，在某些条件下，训练一个基于bert的模型所产生的排放量只能被种植10年的40棵树所抵消。102
如果大规模部署，基础模型可能需要大量能源来满足数百万个需求——如果使用不可再生资源，就会产生大量碳排放。
因此，训练和部署基础模型的特定设计决策对环境的影响可能是巨大的。
即使是看似微不足道的决定，比如减少模型的层数，也可能导致大规模的环境成本降低。
例如，根据Henderson等人[2020]的计算，在商业翻译服务规模上部署的稍微节能一点的翻译模型，每天可以节省78千克二氧化碳当量到12,768千克二氧化碳当量的碳排放，具体取决于所使用的能源网。
这大致相当于种植1至211棵树10年所吸收的碳，或者一年吸收0.35至57.4英亩森林所吸收的碳。104
因此，基础模型的设计、部署和部署后监控应该充分反映这些风险。
当然，在计算任何给定模型的能源使用量或碳排放量时都存在不确定性[Henderson等人，2020;曹等，2020;Patterson et al. 2021]，其他排放源目前可能比基础模型产生的排放量大得多[Mora et al. 2018]。
但是，如果基金会模式继续扩大规模并获得普及，它们很可能成为碳排放的重要贡献者。
我们的目标是为基础模型开发人员和大规模部署人员提供一个框架105，以考虑他们如何减少任何不必要的碳排放，并保持这些模型的净社会影响。
我们建议:
(1)在许多情况下，碳影响可以而且应该得到缓解。
这可以通过在低碳强度地区训练模型来实现，或者通过使用更有效的模型和硬件来实现
(2)当所有缓解机制都已用尽且不再可能缓解时，应评估其对社会的成本和效益，以确定是否以及何时应采用更大的基础模型，而不是更小、更有效的模型——同时了解到，大型基础模型的前期成本可在模型的整个生命周期内摊销(第5.3.2节);
环境成本)。
(3)应明确报告能源、计算和碳成本，以及为减轻负面影响而采取的任何努力，以便为政策制定和研究提供信息
5.3.1
在许多情况下，碳影响可以而且应该得到缓解。
训练基础模型的碳影响不同于部署它们进行推理的影响。
模型训练没有延迟要求，因此可以在云环境中相对轻松地跨能源网格移动训练。
每个能源网都有自己的碳强度，即每千瓦时能源的碳排放量。
例如，由于依赖水力发电，卡塔尔的碳强度极低，而爱沙尼亚的能源网络由于依赖页岩油(尽管这种情况正在迅速改变)而具有极高的碳强度[Henderson et al. 2020]。
最近的研究甚至表明，污染最严重的5%的发电厂贡献了所有电力排放的73% [Grant et al. 2021]。
因此，虽然训练基础模型可能是相当能源密集型的，但研究人员已经证明，通过选择碳排放最少的能源网格，可以部分减轻这些模型的碳影响[Henderson等人，2020;Lacoste et al. 2019;Patterson et al. 2021]。
碳补偿也被提议作为权宜之计，直到所有数据中心都可以使用无碳可再生电力。
这一战略包括减少一项活动的碳排放，以抵消另一项活动的碳排放。
但是大多数——如果不是全部的话——碳补偿都是严格来说比一开始就不排放二氧化碳更糟糕的解决方案[Holl and Brancalion 2020]。
一些碳补偿计划甚至会产生负面影响。
例如，对森林种植运动(通常是碳抵消的一个来源)的研究表明，它们弊大于利。
它们可能产生单一栽培(使用一种特定树种)，从而减少该地区的生物多样性，减少森林土壤中的碳储量[Heilmayr等人，2020;洪等。2020b]。
当使用碳补偿时，这可能会导致比最初从未排放过的碳更多的碳排放。
因此，在培训或部署基础模型时，我们建议预先设计尽可能少的碳排放，而不是简单地依靠碳抵消来取消排放。
当不可能在低碳地区运行时，应利用其他缓解战略，减少不必要的能源使用。
这包括:
•使用更高效的硬件，106分
•使用混合精度训练[Micikevicius et al. 2017]或量化[Gholami et al. 2021]，
•使用更高效的体系结构(例如，在普通的变压器体系结构上使用进化的变压器;或使用稀疏模型)[So et al. 2019;Patterson et al. 2021;Mostafa and Wang 2019]，
•提炼模型并使用提炼模型(例如，[Sanh et al. 2019]);
•并利用其他优化策略来降低能源成本(参见§4.5:系统中的更多讨论)。
开源项目和云计算的维护者应努力将其默认设置设置为最有效的设置，因为众所周知，“绿色默认设置”是最有效的缓解策略(见[Henderson et al. 2020]中的讨论)。
其他缓解策略可以在最近的文献中找到[Strubell等人，2019;Lacoste et al. 2019;Schwartz et al. 2019;Henderson et al. 2020]。
我们还注意到，减少和减轻能源使用也有额外的好处，使模型更容易访问那些有限的计算访问(见§5.6:道德更多的讨论)。
然而，当模型主要用于推理时，例如，部署在生产应用程序中，通常不能将其移动到低延迟应用程序的低碳密集型能源网格中。
除了使用上述缓解战略外，在这种情况下，重要的是权衡拟议的基础模型与更节能的替代方案的好处。
我们将在下一节中进一步讨论这一点。
5.3.2
在使用基础模型之前，应该评估成本和收益。
在采取尽可能多的缓解措施(或在不可能缓解的情况下)之后，至关重要的是评估基础模型所需的大小——或者是否应该使用基础模型。
这种成本效益分析应考虑:
(1)部署基础模式的社会成本和环境成本是否大于该模式的社会效益?
(2)是否有另一种计算更简单、成本更低的方法可以实现类似的社会效益(例如，更有效的基础模型，或者简单的基线)?
评估这种权衡的简化方案将模型的总体影响考虑为:()=()-()-()-()。
(7)图26表示了这个方程以及可能进入每个变量的成本和收益。
这里是模型和净社会效益，以及环境效益，以美元计。
可以通过改善医疗保健、诉诸司法、减少贫困、改善环境监测、协助生态系统保护工作等等来增加。
是能源使用产生的碳的社会成本。
这代表了作为当今货币价值释放的碳对社会的未来危害。
美国环境保护署(EPA)估计，2017年碳排放的社会成本上限为每公吨二氧化碳排放105美元(按2007年美元计算)。107
是模型的能量消耗。
例如，2021年4月，美国住宅平均能源成本约为每千瓦时0.1376美元。108
加上这个变量，可能是能源网络压力增加带来的成本。
例如，最近的一项研究表明，按平均需求标准化的每次电网中断事件的成本可能高达每平均千瓦15.9美元[Sullivan等人，2015]。109
是其他二阶环境影响的社会成本。
这可能包括:
•芯片需求和芯片生产增加带来的复合碳影响[Gupta等，2021a]。
•芯片制造的其他环境影响，如在硅谷产生的有毒废物场地，其健康影响在社会弱势群体中分布不均[Stewart等人，2014]，或台湾制造的污染与慢性健康问题有关[Tu和Lee, 2009;Lin et al. 2016]。
•气候变化的复合效应尚未包括在SCC模式中。
例如，这些影响可能包括加速沙漠化[Huang et al. 2016]，快速的生态系统变化使许多物种面临灭绝的危险[Urban 2015]，以及由于永久冻土融化而增加的碳排放[Schuur et al. 2015]。
•不必要的芯片生产能力压力。
最近的芯片短缺已经导致汽车制造业停工。110
没有证据表明对机器学习优化芯片的需求增加导致了这种短缺。111
但是这种考虑属于二阶效应，研究人员可能会权衡是否值得使用或部署一个大型模型，尽管造成这种负面影响的风险很小。112
在这一分析中，重要的是要考虑到碳的经济效益和社会成本可能在社区之间分配不均，较贫穷的社区受到气候变化的影响更大，而较富裕的社区则受益于模型[Bender et al. 2021]。113
因此，在进行公式7分析时，人们应该更广泛地考虑对社会的利益和危害，而不是针对特定的组织或国家。
在这种情况下，()可以被看作是一个分布，理想情况下，它应该在总体中均匀分布。
在分布极不均匀的情况下——例如，所有的好处都落在模型设计者身上，而所有的危害都落在永远不会从模型中受益的人群身上——设计者应该在部署模型之前花更多的精力在缓解上。
当然，在评估公式7的每个组成部分时，使用哪种方法存在一些不确定性。根据数据来源和对现象的建模选择，例如评估碳的社会成本的不同机制，对其中许多术语的经验估计可能会有多个量级的变化。
当然，可能难以用货币量化的额外外部性，将继续需要考虑。
然而，这种成本效益分析的关键结论不是等式中每一项的美元估值，而是每一项影响的存在及其相对重要性。
我们的目标是为开始考虑这些权衡提供一个高级框架。
未来的研究可能会对如何量化这些价值提供更多的指导。
最后，我们注意到这些因素也应该在模型的整个生命周期内进行评估，而不是在每次运行的基础上。
考虑一个必须为每个新任务从头开始训练的备选基线模型。
基线很可能需要昂贵的超参数搜索，才能在下游任务上达到同等的性能。
相比之下，基础模型将成本的主要部分放在初始预训练过程上，微调可能更简单，更节能。
在基础模型的整个生命周期中，它可能比基线的碳效率更高(图27)。
更有效的适应机制可以进一步改善这种摊销(见§4.3:适应)。
然而，适应的效率并不能得到保证。
一些基础模型可能永远不会比一个特定的基线更有效，即使在许多任务上分摊也是如此。
例如，不能假设具有更少参数的更小模型将转化为能源效率的改进。
由于增加的超参数调整成本或其他优化，在某些情况下，参数的数量已被证明与能源效率无关[Zhou等人。2020;Henderson et al. 2020]。
因此，基础模型开发人员应该在开始大规模培训工作之前严格评估他们的模型和适应机制的效率。
本节中的框架旨在指导读者在培训和部署他们的模型时考虑环境和社会权衡，但是在部署基础模型时还涉及其他实质性的社会正义考虑，在§5.6:伦理中讨论。
§5.5:经济学还从算法部署中更详细地讨论了社会福利的动态。
5.3.3应系统地报告碳/能源影响。
除非研究基础模型的研究人员和工程师报告其模型的计算成本、能源成本和碳成本，否则无法进行成本效益分析。
我们鼓励基金会模型的开发者、提供者和管理者报告这些指标，以及在基金会模型的制定过程中使用了哪些碳减排策略。
参见[Henderson et al. 2020];Lottick et al. 2019;Lacoste et al. 2019;Schmidt et al. 2021;Anthony等人[2020]，以获取碳影响声明的示例以及可以促进该报告的工具。
对于研究人员来说，这样的报告可以在发表时出现，但是我们也鼓励行业参与者采用透明机制来为他们部署的模型报告这些指标。114
这将有助于在工业界和学术界制定政策建议，并帮助下游用户确定碳友好型使用模式。
标准化报告还将有助于确定哪些模型对那些具有有限计算访问权限的人是可访问的(参见§5.6:关于可访问性的更多讨论)。
为了鼓励更多地报告能源和碳影响，我们建议，除其他策略外:在会议上颁发绿色徽章，要求向会议场所提交相关指标的报告，游说基金会模型的大规模部署者提供更多的透明度，以及普遍将学术界和工业界的专业规范转向这些指标的标准报告(参见§5.6关于专业规范的更多讨论)。Henderson等人[2020]对报告机制的道德和更多讨论。
5.4合法性
作者:Neel Guha, Peter Henderson, Lucia Zheng, Mark Krass, Daniel E. Ho在本节中，我们描述了美国法律如何影响、约束或促进基础模型的创建和使用。115
我们注意到，围绕算法工具的法律环境仍然不确定。
我们强调与(1)模型训练有关的问题，(2)模型预测的责任，以及(3)模型输出的保护。
虽然理解法律如何影响基础模型是至关重要的，但重要的是要认识到，法律不能成为我们评估基础模型的构建、维护和使用的唯一视角。
伦理框架是必要的，以了解法律允许的基础模型的应用可能仍然是不明智的，因为它们造成了伤害，并在§5.6:伦理和§5.1:公平中进行了更深入的讨论。
研究潜在的滥用和可能的安全问题(见§5.2:滥用和§4.7:安全)对于事前预防有害结果至关重要，而不是法律机制通常提供的事后处理。
5.4.1之前培训。
训练基础模型将需要积累大量的多模态数据，这就提出了有关数据收集和数据使用的问题。
首先，模型创建者通过网络抓取增长数据集的能力将受到法院解释服务条款的方式的制约，特别是美国计算机欺诈和滥用法案(CFAA)，该法案将“未经授权”访问服务器视为犯罪[Wajert和Rottman 2019]。
法院在这些问题上存在分歧，最近的案件试图澄清在何种情况下网络抓取可能被禁止。116
数据访问的限制性将从根本上影响从业者可用于训练基础模型的数据的多样性[Levendowski 2018]。
其次，训练集中包含的许多数据将受版权保护，并可能受到知识产权法的保护。
然而，版权法承认例外情况，即允许个人使用受版权保护的材料。117
一些学者认为，训练数据集的法律允许程度将在很大程度上取决于法院是否将模型训练过程解释为合理使用原则下的“变革性”[Lemley和Casey 2020]。
虽然什么是变革性的问题是高度依赖于上下文的，但一般规则是变革性使用是那些“添加新的东西，具有进一步的目的或不同的特征，并且不替代作品的原始使用”[Office 2021]。
最近发布的Github Copilot工具已经将这些争论带到了前台[Gershgorn 2021]。
最后，一些训练数据集可能会违反隐私法。
例如，伊利诺斯州允许个人起诉不当收集或使用生物特征数据(例如，视网膜或虹膜扫描、指纹、声纹或手或脸的几何形状扫描)。118
像欧盟这样的外国隐私法通用数据保护条例(GDPR)——如果数据集包含来自欧盟公民的信息，将影响美国模型创建者——将要求数据主体被告知数据收集的目的。
加州消费者保护隐私法案(CCPA)等法律可能会出现进一步的问题，该法案规定个人有“被遗忘的权利”，这引发了模型创建者是否需要从模型中“删除”训练数据的问题[Villaronga等人，2018;Ginart et al. 2019]。
5.4.2产出负债。
虽然基础模型本身是任务不可知的，但微调模型-或基础模型本身学习的表示-可以用于传统的预测任务。
在这些任务构成较大决策系统的组件的地方，基础模型将因此影响行动、决策或政策。
当这些造成伤害时，模型创建者和操作它们的个人可能要承担法律责任。
在物理系统(如自动驾驶汽车、电网管理、医疗诊断等)中嵌入基础模型可能会对个人造成身体伤害。
在这里，法院可能会根据侵权原则解决责任问题[Lemley和Casey 2019;[Selbst 2020]。
关键的开放问题包括用户、基础模型提供者和应用程序开发人员的责任之间的相互作用，以及法院将用于评估基础模型风险概况的标准。
在特别敏感的领域(例如医药)部署将需要监管部门的批准，并开发标准化的流程来评估安全性[Wu等人，2021g]。
根据受保护的属性(如种族、性别)对个人进行分类的精细基础模型可能会面临民权法律的挑战。
学者们注意到，基础模型导致的差别待遇索赔可能会在招聘、住房或信贷贷款的背景下提出[Gillis和spess 2019;Scherer et al. 2019]。
法院究竟将如何裁决这些问题还远不清楚。
例如，学者们注意到，法院对“歧视”的传统观点实际上会阻止机器学习从业者实施许多算法公平技术[Xiang 2021;Ho and Xiang 2020]。119
s。法律承认政府实体的特权和限制。
因此，政府实体-在地方，州或联邦层面-使用基础模型将涉及特殊考虑，除了平等保护要求。
在导致剥夺生命、自由或财产的其他情况下使用风险评估模型将引起程序正当程序索赔。120
例如，当行政机构(例如环境保护署)使用模型时，原告可能会声称这种使用违反了正当程序、合理性/非任务性和透明度的基本标准。
5.4.3对产出的法律保护。
模型输出——以及对模型负责的模型创建者——也可以得到一定的法律保护。
首先，生成模型产生的内容可能涉及言论自由问题。
法院将在多大程度上认定第一修正案对机器生成内容的保护尚不清楚。
学者们讨论了许多悬而未决的问题，包括“人工智能语音”是否受到保护[Massaro等人，2016]，或者模型输出是否实际上是人类程序员的语音[Kajbaf 2019]。
其他人注意到披露要求的可能性(类似于药品或其他物质的安全披露)，这也涉及到语音原则，在这种原则下，模型将被迫与听众分享他们的内容是机器生成的[Lamo和Calo 2019]。
这些问题可能会产生广泛的后果，影响个人是否可以使用基础模型来大规模生产语音，或者模型创建者是否可以对基础模型生成的内容负责。
其次，关于谁可能对模型输出主张所有权存在不确定性。
现有版权法不承认计算机程序为作者，因此，不为计算机程序创造的“作品”提供版权保护[Grimmelmann 2015]。
因此，学者们提倡多种方法。
一些人认为，根据具体情况，程序的人类创建者和它的人类用户都可以声称自己是程序输出的“作者”[Ginsburg和Budiardjo 2019]。
随着模型越来越多地用于“创作”过程——从艺术创作到更普通的场景，如新闻归档——关于机器生成内容所有权的争议将变得更加普遍。
虽然我们上面的分析只触及了基础模型所涉及的法律问题的表面，但这些问题的解决对于基础模型的构建、使用和部署至关重要，或者，借用Larry Lessig的短语，“代码是法律”。
5.5经济
作者:Zanele Munyikwa, Mina Lee, Erik Brynjolfsson基金会模型有潜力通过提高生产力和创新来大幅提高整体生活水平。
这些模型可以用来替代人力、增加人力，或者帮助发现新的任务和机会，这可能导致所有权和权力的集中，或者更多的分散。
在更广泛的层面上，其结果可能是由于潜在的集中化(§5.1:公平，§5.6:道德)导致不平等加剧，或者由于基础模型更容易适应广泛的应用而更广泛地共享繁荣(§1:介绍)。
所有这些方面的最终结果不仅仅是由技术或经济决定的，而是由技术专家、政策制定者、管理者、工人和其他社会成员的选择和行动决定的。
基础模型可以被认为是经济学家所说的通用技术[Bresnahan and Trajtenberg 1995]。
通用技术指的是像蒸汽机和电力这样的技术，由于它们的普及、随着时间的推移而改进，以及产生互补创新(围绕一个核心产品的一系列产品和服务)的能力，它们推动了转型和生产力增长的浪潮。
虽然基础模型目前可能还没有普及，但它们似乎已经准备好成为广泛的技术创新的基础，并且具有通用技术的关键特征。
因此，这些模型可能具有重要的经济意义。
在考虑基础模型对经济的影响时，我们将重点关注三个广泛的影响领域:生产率、工资不平等和所有权。
5.5.1生产力与创新。
基础模型可能会大大提高生产力和创新。
生产力增长是提高生活水平的主要因素之一，因为它增加了国家的财富，并解决了从贫困、医疗保健到环境和教育的一系列挑战。
生产率被定义为每单位投入的产出。121
提高生产率的一种方法是减少分母;例如，使一个公司的广告可以用更少的文案来写，或者每个文案的劳动时间更少，这就降低了投入单位的数量。
生产力也可以通过增加分子来提高，例如，通过使软件开发人员在给定的时间内编写更多的代码。
如果分子的增长足够大，这可能会导致更多的人开发软件，而不是更少[Autor 2015]。
在许多任务中，我们已经观察到机器学习系统提高了生产力。
例如，临床文档的自动完成系统将临床概念的击键负担减少了67% [Gopinath等人，2020]。
同样，基础模型影响生产力的潜力几乎跨越了每个行业和许多职业。
仅就语言而言，一项使用美国劳工部O*NET数据库对美国职业进行的分析显示，许多职业都涉及与语言相关的工作类型，可能会受到基础模型的影响。
大约13%的职业的主要任务与写作有关，这些职业的总工资(年薪乘以从事该职业的人数)超过6750亿美元。
然而，基础模型的潜在影响超出了语言。
它们还将对医学、平面设计、音乐122和许多其他任务中的诊断成像产生影响，在这些任务中，人们正在创造与其他已经存在的东西相似的东西[Winkler等人，2019;Ramesh et al. 2021]。
也许基础模型最深刻的影响(如果仍然是推测性的)是它们增强创造力和提高创新速度本身的潜力。
例如，DALL•E [Ramesh et al. 2021]可以改变插图市场，就像廉价相机彻底改变了摄影一样。
如果这些模型使人类能够开发新的方法来编写新歌和小说(§2.5:互动)，发现药物分子的变体(§3.1:医疗保健)，扩展专利(§3.2:法律)，构建创新的软件应用程序，或开发新的业务流程，那么不仅生产力水平会提高，而且生产力的增长率也会提高。
通过这种方式，基础模型具有保罗·罗默(Paul Romer)增长模型[Romer 1990]中的想法或蓝图的一些特征，甚至具有与大多数其他商品不同的元想法(关于想法的想法)的一些特征，这些特征是非竞争性的，因此可以加速增长。
值得注意的是，生产率的变化并不总是在官方统计数据中可见，因为投入和产出的许多方面很难衡量[Brynjolfsson和Collis 2019]。
因此，基础模型的收益和成本将不能被传统的生产率指标完全捕获，也不能被国内生产总值(GDP)或价格水平(整个商品和服务范围内当前价格的平均值)等相关指标完全捕获。
从历史上看，通用技术尤其如此，因为它们是一系列二次创新的催化剂，往往会改变经济中的商品和服务，甚至在几年甚至几十年的时间里改变生产和创新的性质。
5.5.2工资不平等。
即使基础模型提高了平均生产率或收入，也没有经济规律保证每个人都能从中受益。
在某种程度上，这是因为并非所有任务都会受到同样程度的影响。
更重要的是，无论生产率增长如何，基础模型对劳动力需求(以及就业和工资)的影响可能是正的，也可能是负的[Brynjolfsson和McAfee 2011;Brynjolfsson and Mitchell 2017]。
当一项技术代替人类劳动完成任务时，它往往会减少对从事这些任务的工人的需求。
这抑制了就业和工资。
然而，当一项技术补充劳动力，或促进创造新的机会或任务时，它往往会增加劳动力需求[Acemoglu和Restrepo 2019]。
即使生产力提高了，就业率也可以(而且经常)上升。
例如，飞机的发明创造了对一个全新职业的需求，航空公司飞行员。
反过来，喷气发动机的发展是对人类飞行员的补充，进一步增加了对喷气发动机的需求。
同样，基础模型对就业、工资和收入不平等的影响也会因使用方式而异。
虽然工业革命主要改变了体力劳动，但基础模型可能会改变涉及认知工作的任务，如内容创作和交流。
一般来说，由于基础模型是中介资产，通常具有强大的生成能力，我们设想它们将能够在许多创造性环境中增强人类，而不是取代人类，因为将这些模型单独用于开放式生成任务仍然存在重大限制[参见et al. 2019]。
正如我们在§2.5:交互中所描述的那样，基础模型也可以为用户可以利用的系统提供动力，以共同构建新的艺术形式或更有效地构建新应用程序的原型。
流体人机和人在环交互将需要界面设计的进步(第2.5节:交互)以及这些模型的可解释性(第4.11节:可解释性)和鲁棒性(第4.8节:鲁棒性)的根本改进，以便人类能够理解模型行为并期望模型在不同的环境中表现良好。
5.5.3集中化。
基础模型经济影响的另一个关键决定因素是谁拥有数据和模型。
特别地，推动基础模型的前沿到目前为止主要是大型公司实体的权限。
因此，数据和模型的所有权往往高度集中，导致市场集中(§5.6:道德)。
反过来，这可能导致决策权和权力的显著集中，减少那些没有所有权的人的收入和机会。
这种权力集中可能导致一种平衡，即更少的人拥有社会和经济流动性和机会，这种情况被Brynjolfsson[2022]称为“图灵陷阱”。
为了平衡这种集中化，已经有基层努力开放开源人工智能研究，如Masakhane, EleutherAI和HuggingFace，或者通过分布式训练建立基础模型。
然而，由于基础模型依赖于大量的数据和计算资源(§5.3:环境)，工业界可以训练的私有模型与向社区开放的模型之间的差距可能仍然很大。123
5.5.4其他注意事项。
这个简短的章节并不打算全面介绍基础模型的所有经济影响。
除了影响生产率、工资不平等和所有权外，基础模型还可能对工作质量和工作满意度产生重大影响。
例如，他们可能会通过自动化重复的、无趣的工作部分来增加工作满意度，或者通过加快工作速度来降低满意度，从而导致更频繁的倦怠。
正如§5.1:公平和§5.6:道德所讨论的，它们也可以放大和延续偏见，通常以意想不到的方式，或者被用作减少偏见的工具。
基础模型可以促进全球贸易和远程工作，正如早期使用机器翻译系统在这些领域产生了重大影响一样[例如，Brynjolfsson等人，2019]。
也可能存在显著的环境影响(§5.3:环境)，以及对经济中职业变化和业务转型的速度和方向的意外和未预料到的影响。
更广泛地说，考虑到基础模型的涌现能力，我们应该期待新的未知未知的出现，它们很难预测，并且可能有实质性的后续影响。124
综上所述，基础模型有望成为我们这个时代重要的通用技术。
它们有可能大幅提高生活水平，但也有加剧不平等和权力集中的风险。
这些技术的经济影响并不是预先确定的，而是取决于技术专家、政策制定者、管理人员、工人和其他利益相关者如何应对以下挑战:
•我们如何利用基础模型的潜力来提高生产率?
•我们能否开发出增强创造力和提高创新速度的模式?
•利益和控制权是局限于少数人还是广泛共享?
了解这些系统的经济潜力是引导它们朝着符合我们价值观的方向发展的第一步。
5.6规模伦理
作者:Kathleen Creel, Dallas Card, Rose E. Wang, Isabelle Levent, Alex Tamkin, Armin W. Thomas, Lauren Gillespie, Rishi Bommasani, Rob Reich基础模型的广泛采用带来了道德，社会和政治方面的挑战，以及对不平等增加的担忧，这是§5.1:公平的主题。
在本节中，我们将讨论与基础模型的应用规模相关的社会、政治和伦理风险，例如同质化和权力集中，解决这些问题的规范和释放策略，以及对基础模型开发和部署的更广泛的政治经济的关注。
5.6.1均质化和标度。
如果在各种领域中使用相同的模型而最小的适应性，则原始模型的优势，弱点，偏见和特质将被放大(§5.1:公平性)。
对任何标准化技术的广泛采用和依赖都是如此。
类似于许多汽车或飞机中使用的零件在制造过程中的失败可能会对各个部门造成广泛而严重的后果，基础模型固有的偏差或服务失败可能会向外扩散。
然而，目前基础模型的不可解释性(§4.11:可解释性)及其任务不可知论训练使得预测、理解和解决这些弱点具有挑战性。
如果基础模型被广泛采用，基础模型开发人员比标准模型开发人员承担更大的责任，因为他们在设计和部署方面的选择具有广泛的含义[Arendt 1987]。
基础模型的定义特征——它们被有效地适应多种任务的能力——是使它们可能被广泛采用于广泛的社会后果任务的原因。
与当前的分布式和多样化的决策模型相比，在多个自动化决策任务中采用同一基础模型的许多适应性意味着决策主体可能面临根植于底层基础模型的更同质的一组判断。
这种算法单一文化[Kleinberg and Raghavan 2021]可能导致对个人决策主体的一致和任意拒绝、错误分类或虐待[Gandy 2021]。
我们称之为同质化[Creel and Hellman 2021]。
例如，§4.6.2:
数据解决方案讨论了数据质量问题，这些问题会导致数据子种群上的不良行为，其中子种群可以通过对数据分层的任何过滤器产生，包括社会群体(参见§4.11.1:可解释性-行为和§4.8.1:稳健性-优势的相关讨论)。
在数据质量工具得到改进之前(§4.6.2):
数据解决方案)以及识别模型表现不佳的数据片段的能力[Chung等人，2019;Goel等人[2021]，基础模型可能始终无法为一个子群体提供准确的信息或服务(另见§4.8:鲁棒性)。
同质化有可能放大偏见;使偏见标准化，加剧不公正，而不是分配不公正;并扩大任意排除[Creel和Hellman 2021;Gandy 2021]。
例如，Zhou等人[2021a]认为BERT默认编码一个以英语为中心的相似性度量，如果在应用基础模型的上下文中应用，该度量可能是有害的。
跨领域的基础模型的应用有可能作为一种认识论和文化上的同质化力量，在多个应用领域传播一种隐含的观点，通常是一种社会主导的观点。
由于需要大量的未标记和标记数据，现有的训练语料库标准化趋势可能会在基础模型中加剧。
在某种程度上，模型在类似的数据上训练，它们可能获得类似的行为模式，偏差(§5.1.3);
公平(来源)和错误。
以前的高强度数据管理和标记工作，如ImageNet，已经标准化了训练语料库。
在这样做的过程中，他们也有标准化的错误:在ImageNet上训练的模型经常依赖于相同的“虚假线索”和“捷径”，例如使用绿草等背景纹理来预测前景对象类别，如奶牛[Geirhos等人，2020;Hendrycks等。[2021e]。
尽管它们对许多类型的分布转移(§4.8.1:鲁棒性优势)的鲁棒性增加了，但基础模型和其他大型模型不太可能学习到虚假相关性(§4.8.2:鲁棒性挑战)，因此如果在相同的数据集上训练，可能会学习到类似的错误。
由于选择公开可用的未标记数据，可能会产生类似的效果。
许多基础模型是在未标记的语料库上训练的，这些语料库是根据其便利性和可访问性而选择的，例如公共互联网数据[Caswell et al. 2021]，而不是它们的质量。
然而，公开可访问的数据，无论是标记的还是未标记的，往往被许多专有基础模型的训练语料库中的专有数据所取代，如[Marr 2017]和§4.6.1所讨论的:
data-desiderata。
因此，需要对基础模型中相似数据的训练在多大程度上使相关性同质化，以及这种同质化可能在多大程度上导致模型的适应衍生品一致失败(除非在每次适应过程中应用约束来消除这种行为，如§4.3.2所述)进行更多的研究。
adaptation-usecases)。
同质化并非不可避免。
当模型开发人员有意拓宽数据集中表示的视角范围时(§5.1.3):
公平来源)，需要对基础模型的能力进行更多的研究，以便在用于生成任务时提供多样化的视角。
例如，Sheng等人[2021]已经证明，采用特定人口群体“角色”的对话系统在衡量社会偏见时表现不同。
除了以避免偏见为目标在“人物角色”之间进行选择之外，沿着各种认知和人口统计轴多样化的“人物角色”也可用于为生成任务生成更广泛的连贯输出。
关于如何平衡产出的多样性与对个人用户的相关性和实用性，仍然存在许多悬而未决的问题。125年5.6.2
监视、排斥和权力。
基础模型的一个关键前提是，大量未标记的数据集可以与大量的计算资源相结合，以创建一个基础，从中可以衍生出用于各种应用的众多产品。
这种范式转变有可能改变社会结构和转移权力，建立或巩固模型创造者的影响力[Zimmerman 2020]。
我们将在下面讨论三个潜在的影响。
大量数据收集和监控。
虽然收集标记数据集通常需要与领域专家合作，并了解这些数据的问题和局限性，但训练基础模型中对异常大量数据的需求鼓励了一些研究人员强调数量而不是质量。126
虽然预处理可以帮助提高数据的质量[例如，Brown等人，2020]，但所涉及的规模需要自动化方法，这可能是生硬的或缺乏记录的[Dodge等人，2021]。
尽管数据保护立法(例如欧洲的GDPR)正在不断发展，但从不透明的政策[Obar和Oeldorf-Hirsch 2020]和使用“黑暗模式”(即操纵界面[Narayanan et al. 2020])到直接违反服务条款，各种可疑的做法继续被用于获取数据。
事实上，这就是明视人工智能(Clearview AI)采取的策略，该公司未经用户同意，违反平台的服务条款，从社交媒体上抓取照片，目的是开发面部分类软件。
尽管如此，该公司还是能够将这项技术出售给警察部门和其他组织，在许多情况下，州议员或部门负责人都不知情[Mac et al. 2021]。
在某种程度上，基础模型的范式增加了在任何特定领域率先拥有最大数据集的价值，这可能进一步鼓励参与者追求积极的数据收集，即使这种追求在法律上是有问题的或与用户期望相反[Nissenbaum 2009;Zuboff 2018]。
数据对基础模型的重要性也意味着已经从事广泛数据收集的组织将处于开发这种模型的有利地位，并且可能有动力保持这种优势。
如果衍生产品本身可用于收集额外数据(例如，用于监测或健康诊断应用)，基础模型的开发人员可设法确保获得此类数据的所有权。
因此，尽管基础模型范例的一个关键优势是能够生成适应的衍生品，但是基础模型的开发人员可能会寻求以一种确保数据从所有适应的衍生品流回给他们的方式来许可他们的工作。127
权力的集中。
尽管随着时间的推移，计算的绝对成本已经大大降低，但目前最大的基础模型的训练需要计算资源，这使得除了少数机构和组织之外，它们的开发超出了所有机构和组织的能力(§5.3:环境)。
因此，谁有权访问相关的计算资源和数据的问题可能会决定谁能够在未来几年产生尖端的基础模型(参见§5.5.3:经济学集中化)。
GPT-3至少在一定程度上是一个规模实验，表明可以通过扩大模型大小、数据量和训练时间来实现主要收益，而无需进行重大的建模创新。
尽管目前正在进行大量研究，以减少训练此类模型所需的资源数量(参见§4.2:training)，但OpenAI的工作表明，从更大规模的努力中仍然可以获得收益[Kaplan等人，2020]，其他组织似乎可能会在其他领域寻求遵循这条道路(例如，参见[Lieber等人，2021])。
如果规模确实是成功的关键，那么最有能力生产有竞争力的基金会模型的组织将是资源最充足的:风险投资的初创企业、已经占据主导地位的科技巨头和州政府。
这引发了对市场集中度的潜在担忧，并可能表明目前存在于国防和半导体制造业等极端资本密集型行业的垄断或寡头垄断[Carril和Duggan 2020]。
此外，这种权力集中引发了人们对目前被边缘化的个人和社区参与基金会模式开发过程的能力的担忧[Kalluri 2020]。
特别是在政府服务领域，采用基金会模式可能进一步将决策权从政府转移到企业服务提供商，并为正当程序和问责制引入额外的障碍[Citron 2008]。
然而，更多的基层努力(例如Masakhane, EleutherAI, HuggingFace)提供了令人鼓舞的替代方案，并且在如何纳入参与式或价值敏感设计方面进行了广泛的工作[Friedman and Hendry 2019;Prabhakaran and Donald Martin[2020]。
推动广泛的自动化决策。
近年来，自动化决策系统在工业和政府中的应用急剧扩大[O'Neil 2016;Engstrom et al. 2020]。
尽管对这种自动化的许多担忧并非特定于基础模型，但GPT-3等模型的生成能力，以及在基准任务上令人印象深刻的表现(例如，Devlin等人[2019])，有可能促使行政机构不那么谨慎地采用这种技术，其中许多机构缺乏理解复杂机器学习系统所需的专业知识[Calo和Citron 2021]。
因此，就基础模型的实际能力和局限性进行清楚的沟通是特别重要的。
大多数自动化决策系统将作为更广泛的社会技术系统的一部分存在，人类在其中发挥关键作用[Selbst et al. 2018]。128
因此，不能保证在标准化评估上的性能的巨大改进将转化为现实世界中期望的结果(特别是如果系统部署时没有仔细考虑或正在进行的评估)。
例如，研究表明，法官在解释风险评估系统的输出时可能会重新施加种族偏见[Albright 2019]，或者以其他方式施加他们自己的偏见[Stevenson和Doleac 2021]。
在这方面，具有适当生态有效性的持续评估[de Vries等人，2020]将是至关重要的，但可能无法阻止在没有充分证据的情况下采用潜在危险或昂贵的系统[Ferguson 2017]。
关于拒绝方法的研究正在进行:个人选择退出参与基础模型及其衍生品的方法，无论是作为数据主体还是决策主体，都不会产生影响[Benjamin 2016]。
简而言之，一旦部署基础模型，算法决策存在的问题就会在其功能中显现出来。
在某种程度上，采用基础模型加速了从人类决策到机器决策的转变，基础模型强调了对自动化的关注。
虽然这些挑战没有明显的解决方案，但重要的是要提出关于基础模型将如何影响其创建的对话的权力部分的问题;与民间社会组织、政策制定者和公民就此类系统的能力和局限性进行沟通;并努力在社会各阶层之间就采用这些模式进行更广泛的对话。
5.6.3规范。
公共政策和法律的正式规定(第5.4节:合法性)在为技术创新创造基础设施以及减轻广泛传播的技术的潜在有害影响方面发挥着至关重要的作用。
正如塔斯基吉梅毒实验与研究方案和IRB等机构的发展之间长达数十年的差距所表明的那样，保护人类受试者和利益相关者的公共政策往往落后于公众意识和对他们有害的证据[Grady 2015;斯塔克2012;卫生和福利署，1979]。
因此，社会依赖于负责任的开发和部署以及建立最佳实践的专业规范。
规范存在于建议和需求之间的连续体上。
作为一项新兴技术，负责任的基础模型开发和部署的规范还没有很好地建立起来[Crootof 2019]。
在接下来的内容中，我们将讨论部署模型的规范，因为用于研究的模型具有更广泛的纬度。
那些希望基础模型的开发人员采用某些规范的人可能会以身作则，允许他们自己的行为和陈述来推荐规范。
正如在§1.2:生态系统中所讨论的，我们认为大学和其他非营利机构在基金会模型的建模规范中扮演着重要的角色。
作为教育机构，大学处于独特的地位，可以鼓励下一代理论家和实践者考虑本报告中提出的问题，并促进研究人员和学生之间的跨学科对话[Rogers 2021]。
大学和学院也可以通过审核现有的基金会模型并公布他们的发现，建立道德审查委员会[Bernstein et al. 2021]，以及开发自己的基金会模型来促进规范的建立。
要创建和采用规范，将需要在资金结构、模型存储库、发布实践、会议提交和拨款提案要求方面制度化。129
例如，HuggingFace的界面目前鼓励发布数据和模型卡片，包括对偏见和社会影响的讨论。130
由于不是必需的，也许是因为数据质量工作相对于其重要性被低估了[Sambasivan et al. 2021]，很少有人填写。
偏见和社会影响包括在会议的伦理声明和某些形式的标准评价中(如§4.4:评价所讨论的)，但其他方面被一些研究人员视为可选考虑因素。
这种情况必须改变。
对于一些具有社会后果的用例，我们建议建立法律标准，要求适应的衍生品可以证明具有某些特性(§5.4:合法性)。
应民主决定特别关注的领域，但可能包括分配和分配政府服务、医疗诊断和监测、雇用和贷款:在所有情况下，人们的机会甚至生命都取决于经过调整的衍生产品的正常运作。
我们应该促进、制度化或要求什么样的规范?
我们在这里推荐一些，但是主要的目的是鼓励关于开发和使用基础模型的适当规范的对话。
先前的工作通常侧重于倡导文档的规范[Gebru等人，2018;Bender and Friedman 2018;Mitchell et al. 2019;Dodge et al. 2019]。
因为在下游环境中出现的许多负面社会后果最初可能是外在的或特定于用例的(§5.1:公平性)，文档和透明度对基础模型特别重要。
目前，那些改编基础模型的人没有自动机制向基础模型的开发人员报告他们的发现，这些模型记录了他们改编衍生品的偏见或其他负面特征。
在适应的衍生品中编译相关问题的多个报告可能允许模型开发团队发现跨越多个用例的模型的内在属性。
由于改编衍生品的创建者通常代表与基础模型开发者或提供者不同的实体，因此需要额外的报告结构和规范或法规才能将这种类型的反馈传递给基础模型开发者。
这种反馈也可以提供给模范审计员的一般读者，从而使审计和追索更容易获得。
公众对规范、标准的承诺，以及报告机制的创建也可以允许下游用户向基础模型提供者提交反馈。
为了实现这一点，适应的衍生品应该以一种允许受影响方追踪问题根源的方式进行一致的标记。
在实践中，重要的技术和社会障碍可能会阻碍这种跟踪，例如隐私考虑和许多基础模型的专有性质，但是没有标记是不可能的。
模型开发人员和提供者为这样的报告创建机制是很重要的。
报告机制可以通过当前平台上的类似结构来提供信息，例如GitHub上开源项目的问题跟踪。
特别是，提交的问题应该是公开的，这样其他用户就可以确定趋势，即使还没有做出改变，这样开发人员和提供商就可以对未解决的问题负责。
需要额外的机制将趋势向上升级到基础模型提供者。
关于训练数据中跟踪问题的类似建议在Dinan等人[2021]和§4.6:data中进行了讨论。Holland等人[2018]从消费者隐私中的标签讨论中得出结论，认为营养标签是一个有用的模型[Kelley等人，2009]。
营养标签既包括“原料”清单，也包括加工食品的全部营养信息。
因此，适应衍生品的模型卡(Mitchell et al. 2019)或营养标签也可以包括“原材料”清单，如所使用的训练数据和基础模型，以及适应衍生品的完整“营养成分”，如其已知的能力、弱点和偏见。
为了让数据主体和受影响方追踪危害的来源，必须报告整个管道。
然而，如果没有能力将损害的责任归因于改编的衍生品、基础模型或两者，并且一旦将损害归因于没有追索权的框架，即使成功地追踪到损害，也不太可能导致模型的变化(参见§5.1.4:
fairness-recourse)。
因此，需要进行大量的技术、政策和法律工作，以便制定框架，将数据、模型和衍生内容传达给其他专家，并最终向公众传达;归罪于对伤害的责任;并创造追索权的途径。
5.6.4发布与审计。
2019年2月，OpenAI开始了一项实验。
通过发布减少的124M参数GPT-2，无数据集，他们希望争取时间:测试偏差的时间，为误用做准备的时间，以及社会适应大型语言模型的时间[Solaiman et al. 2019]。
8个月后，当OpenAI发布完整的~ 15亿个参数版本时，测试暴露了模型的一些功能和局限性，但绝不是全部。
当今天考虑类似的问题时，发布的可能危害，主要集中在滥用(§5.2:滥用)，131必须权衡透明度的好处，没有闭门测试可以复制，即更广泛和独立的审计和访问。
审计审计人员探索当前模型的局限性，并提出修复它们的途径，以及在各种自然环境中测试模型的适应衍生品。
审计的开放获取政策允许更多和不同的研究人员调查任何模型的偏见、局限性和安全漏洞，更好地告知模型的可接受用途，并校准对模型的适当信任[Danks 2019;拜尔1986]。132
为了支持基础模型的独立审计，模型开发人员或第三方中介机构可以为审计员提供开放的API访问，包括梯度访问，并允许访问训练数据[Raji和Buolamwini 2019;Raji et al. 2020]。
用工业专有数据训练的基础模型不太可能被发布，而那些用私人数据(如在医疗环境中)训练的基础模型也不应该被发布。
为了使专有模型从独立的审计中受益，并且使模型主体从审计过程所促进的改进中受益，我们建议在分阶段发布中进行审计。
虽然阶段发布可能无法阐明所有可能的模型用例，但是扩大已覆盖用例范围的一种方法是争取中立的第三方来决定哪些个人或组织应该在阶段发布计划中获得早期访问权。
当模型开发人员决定谁应该获得阶段性访问权限时，他们就会受到偏袒、选择性分发和操纵公众对其产品看法的指控。
一个中立的“分阶段发布委员会”，或联邦审计员，可以为这些失效模式提供支持，并确保向广泛的审计员和用户提供访问权限，以获取一系列学科专业知识和社会部门。
分阶段发布委员会还可以减轻这样一种看法，即如果审计师共享不讨人喜欢的输出，他们将面临失去早期访问模型的风险，因为他们可能处于标准的分阶段发布过程中。
获取和适应。
在某种程度上，基础模型具有社会效益，模型的发布具有进一步分发模型的潜力。
像BERT和M-BERT这样的大型语言模型能够跨语言迁移，当这些模型是开源的时候，可以适应那些文本太少的语言[Wu and Dredze 2019;Wang等。2020a]。
考虑到商业提供商目前没有很好地提供服务的语言的数量，这样的好处本身可能是实质性的。
发布并不足以使基础模型的访问大众化，因为计算能力的障碍仍然使许多人无法修改甚至加载基础模型，更不用说开发自己的模型了。
然而，在这些方面，我们都看到了最近的重大技术进步。
内存技术，如零冗余优化器(zero)，允许研究人员在一个简单的设置上运行和训练非常大的模型[Rasley et al. 2020;Rajbhandari et al. 2021]。
像蒸馏这样的技术可以允许释放更小、更易于处理的模型，这些模型可以恢复其父模型的大部分性能，同时更容易训练[Li等人。2020d]。
如§5.3:环境中所讨论的，开发能源密集度较低的训练方法可以进一步扩展使用已发布模型的能力。
正如§4.5:系统中所讨论的，需要提高效率，例如硬件和软件的协同设计，以训练更大的模型，但也可以用于降低访问当前模型的价格。
相比之下，最严重的危害并不是由释放引起的。
有能力进行大规模虚假信息、网络战或有针对性的网络钓鱼的老练或机构行为者，也可能有能力创建一个类似的模型，如果这些模型都没有被释放的话。
尽管这些危害可能很重要，但不应因此对释放计算产生重大影响[Solaiman et al. 2019;Shevlane and Dafoe[2020]。
要权衡利弊的是那些资源不足的参与者，他们无法创建自己的基础模型，但可能会产生垃圾邮件或滥用，虚假评论或在测试中作弊。
发布的好处是否超过了参与者的潜在危害，这些参与者足够成熟，可以使用已发布的模型或API，但不够成熟，无法创建自己的模型或API ?
我们相信答案是肯定的。
拥有开发基础模型所需的资源和关系的研究团队数量很少。
即使是集体，我们也不可能有足够的数量或多样性来想象所有可能的有益用例或所有可能的探测，这些探测可以照亮基础模型的能力表面。
5.6.5何时不构建。
强大技术的开发和部署不像重力，是一种作用于我们的外部力量。
技术反映了人类做出的一系列选择;人类的能动性塑造了技术前沿。
因此，技术人员可以选择何时不构建、设计或部署基础模型[Zimmermann 2021]。
这个决定不必是二元的;相反，人们可以通过颠覆固有的价值观、挑战假设和塑造研究议程来拒绝以默认的方式参与[Simpson 2007]。
技术工件，包括基础模型，本质上是政治性的，因此对它们的研究具有社会政治背景，而不仅仅是技术背景。
开发人员和研究人员应该认识到他们寻求解决的问题，例如，如何扩大基础模型的规模，以及如何使其更易于计算;这些问题是如何形成的;以及他们的解决方案最终赋予谁权力[Rogaway 2016;赢家1980;Passi and Barocas 2019]。
我们应该重视那些寻求使基础模型更具可解释性、可访问性、可持续性和公平性的研究(参见§4.11:可解释性、§5.3:环境、§5.1:公平性)。
通过询问何时不构建基础模型或改编的衍生品，我们含蓄地问的不仅仅是“我们应该构建或不构建什么?”
而且，“应该在什么条件下建立模型?”
以及“什么标准和原则管理建筑?”
第一个问题源于模型观点;以下问题来自生态系统的观点(§1:引言)。
邀请你考虑拒绝建设并不等于说“什么都不做”。
这是一个邀请，让我们慎重而明智地选择什么是值得花费时间、财力、专业知识和能源去建造、设计和部署的。
归根结底，这是一个困难的道德问题，植根于背景和价值观。
在某些情况下，自适应衍生品(以及更普遍的算法和机器学习)的应用是不合适的，因为社区影响了抗议活动，或者因为自适应衍生品天真地加剧了系统性问题，而这些问题可以通过公共政策、额外资金或跨学科合作来更好地解决[Angwin et al. 2016]。
Belmont报告应用于Floridi等人的机器学习[2018]，为这个问题提供了一个可能的框架。
根据“慈善”原则(Department of Health and Welfare, 1979)，当一种适应性衍生品或研究途径可能弊大于利，甚至根本没有益处时，我们可以找出重新考虑建设的案例。
或者，在某些情况下，自适应衍生品在效率、性能和泛化方面表现更好，机器学习社区优先考虑的价值[Birhane等人，2020]，但是个人、社区或组织可能会选择优先考虑现有的解决方案，突出其他价值，如人际关系和可解释性[Benjamin 2016]。133
在这样做的过程中，他们行使了自己的自主权——正如贝尔蒙特报告中所解释的“对人的尊重”——决定这不是一个适当的环境来建立[卫生和福利部，1979]。
回答何时不构建的问题是个人责任问题，也是更广泛的专业责任问题。
不让一个人、一个团队或一个公司做某件事的决定，会招致这样的回答:“但如果我们不做这件事，别人会做的，而且他们可能会做得更糟。”
对两种模式的结果的相对危害进行简单的功利主义权衡，忽略了诚信的重要性。
是我们自己建立了错误的模型，还是别人建立了错误的模型，这一点非常重要(Williams 1973)。
个人有理由不去建造一些违背他们价值观的东西，或者他们认为不应该建造的东西[Korsgaard 2009]。
然而，这样创造的结构环境是不同的。
即使只有一家公司决定为道德上存在问题的模型构建最有效的版本，它们也为其他公司考虑类似的研究途径打开了大门;它们使不进行研究在竞争中处于不利地位[Askell等人，2019]。
因此，什么时候不建设既是一个集体问题，也是一个个人问题，要求社区遵守职业道德和责任准则。
与医疗等其他领域相比，AI/ML社区的基础设施尚不发达。
虽然像计算机协会(ACM)这样的专业机构有伦理声明，但行业和学术界都缺乏广泛使用和接受的专业誓言(例如，希波克拉底誓言或工程师的义务)，涉及部署和研究的监管机构(例如，药物的FDA)和伦理审查的官方协议(例如，涉及人体受试者的研究的IRB;[Bernstein et al. 2021])。
选择退出的能力可以在许多阶段合并到基础模型生态系统中，包括在数据生产、调整和部署期间。
随着规范转向收集越来越多的训练数据(§4.6:数据)，我们应该努力保持“对人的尊重”，[Department of Health and Welfare, 1979]强调隐私和同意是数据生命周期的一部分。
这将需要在数据管理方面进行创新，并在技术上和哲学上更具体地理解在线知情同意、记录和确保尊重同意的方式以及隐私(见§4.6:特定数据管理提案的数据;[Ohm 2014])。
尽管数据和基础模型在其应用程序中是不同的，但是数据参与者应该能够指出他们不希望如何使用他们的数据。
选择退出同意模型有利于开发人员，因为它不要求他们为每个新的、意外的用例获得同意。
因此，重要的是撤销对目前正在进行的申请所给予的毫无意义的同意的权利，但在最初给予同意时却没有。
5.6.6结论。
在本节中，我们调查了伴随广泛采用基础模型而来的一些社会风险，例如结果的同质化和权力的集中化。
基金会模型的开发人员应该采用关于基金会模型的开发、审计和发布的规范，以便在立法要求的帮助下解决这些风险，并且个人应该能够拒绝成为基金会模型的数据或决策主体而不会受到影响。
基础模型的生成和交互能力的许多含义在这里仍未被调查。
例如，§5.5:经济学讨论了创造性和设计工作自动化对经济生产力的潜在收益。
然而，由于它们的生成性，基础模型可能会取代许多人认为有意义和充实的工作，比如图形设计和写作。
我们希望，本报告的范围将有助于其他人探讨这里未涉及的伦理和社会问题。
6结论
在本报告中，我们努力全面讨论基础模型的许多最关键的方面，从它们的技术基础到它们的社会后果。
通过这种方式，我们承认采取了不同寻常的方法:我们试图澄清一个可能刚刚开始的范式的本质，而不是等待更多的展开或尘埃落定。
因此，尽管我们做出了努力，但仍有许多事情尚不清楚，我们重申，这只是范式转变的开始:基础模型才刚刚开始改变人工智能系统在世界上的构建和部署方式。
展望未来，我们认为这份文件将在引导和构建关于这些模型和人工智能新范式的对话方面发挥重要作用。
也就是说，为了确保在持久的基础上负责任地开发和部署这些模型，我们设想不同部门、机构和学科之间的合作从一开始就尤为重要。
图2。
基础模型可以集中来自各种模态的所有数据的信息。
然后，这个模型可以适应广泛的下游任务。
图3。
在对基础模型的社会影响进行推理之前，重要的是要理解它们是从数据创建到部署的更广泛的生态系统的一部分。
在这两个方面，我们都强调了人作为基础模型训练的最终数据来源的作用，以及作为任何利益和损害的下游接受者的作用。
深思熟虑的数据管理和适应应该是任何人工智能系统负责任开发的一部分。
最后，请注意调整基础模型的部署是与它们的构造分开的决定，这可以用于研究。
图5所示。
目前，世界上只有一小部分语言在基础模型中得到了表示。世界上有超过6000种语言，由于对什么是单独的语言存在固有的不确定性，估计会有所不同[Nordhoff and Hammarström 2011]。
这张地图显示了世界上的语言，每个点代表一种语言，其颜色表示顶级语系。
数据来自Glottolog [Hammarström et al. 2021]。
我们在地图上标出几种语言作为例子。
图10所示。
左图:基于机器学习的药物逆转录计划器AiZynthFinder预测的1,6-庚二烯-3,5-二酮的反应路线[Genheden et al. 2020;Yoshikawa et al. 2021]。
右图:命题逻辑中的证明树示例，其中绿色的公式表示公理。
虽然它们来自不同的领域，但两棵树在结构上是相同的。
图13所示。
一个美国民事案件的不同步骤的例子，以及基金会模式可能有所帮助的地方。
在每个阶段可能需要处理不同的方式，并需要适应新的法院或法律观点。
图14所示。
这部作品的作者之一所写的一篇虚构摘要的摘录。
法律学生被要求写摘要的典型形式包括:(1)介绍论点;(二)以说服性方式陈述法律规则;(三)对案件事实适用法律规则;(4)结束语有说服力。
这通常包括从先前的案例和当前案例的事实中检索和解释信息。
图17。
基础模型的五个关键属性:表现力——灵活地捕获和表示丰富的信息;可扩展性—有效地使用大量数据;多模态——将不同的模式和领域连接在一起;记忆容量——存储大量积累的知识;和组合性-推广到新的上下文，任务和环境。
图17。
基础模型的五个关键属性:表现力——灵活地捕获和表示丰富的信息;可扩展性—有效地使用大量数据;多模态——将不同的模式和领域连接在一起;记忆容量——存储大量积累的知识;和组合性-推广到新的上下文，任务和环境。
图18所示。
在适应过程中，一个基础模型被转换成一个已适应的模型(最下面一行)，以便反映更新的信息、期望的行为或部署约束。
图19所示。
该图显示了基于变压器的语言模型的参数数量和训练操作次数(flop)的增长(蓝色表示)，以及NVIDIA P100、V100和A100 gpu的内存容量和峰值设备吞吐量(红色表示)随时间的增长。
最先进的语言模型的增长率(每条线的斜率)(大约每年10倍)远远超过硬件计算能力的增长率(大约四年10倍)，这激发了对大量加速器之间并行性的需求，以及算法、模型、软件和硬件的协同设计，以推动进一步的进步。
参数数和训练操作数来源于相关论文[Brown et al. 2020]，内存容量和峰值吞吐量来源于GPU规格表。
图20。
基础模型为机器学习系统的安全和隐私带来的风险和机遇。
图22。
从不同数据的预训练到适应任务的下游性能，基础模型的分析涉及捕获如上所示的不同损失项之间的关系。
主要的挑战是分析突出的预训练-适应界面，这需要仔细推理除了模型架构，预训练和适应阶段的损失和数据分布之外的人口损失(§4.10.2);
theory-interface)。
泛化和优化的分析在很大程度上归结为标准监督学习中的分析。
理论公式和模块化。
挑战:适当的数据假设和数学工具。
Fig.23。
基础模型的一个模型-多个模型性质:一个核心的可解释性问题是理解基础模型在一个模型和许多模型之间的范围中的位置。
作为一个模型，行为可以通过识别和描述用于跨任务产生行为的有限数量的可推广模型机制(例如，赋予单词意义、比较数量和执行算术的机制)而变得可解释。
与许多模型一样，对一个任务中的模型行为的解释不一定能说明其他任务中的行为，因此需要对每个任务中的行为进行独立研究。
图25。
这张图显示了基础模型对操纵性和有害内容生成的影响，以及对检测的影响。
图26。
用于部署基础模型的成本效益分析的可视化。
模型的总价值可以通过首先考虑模型的净正社会效益以及任何环境效益来近似。
然后，我们减去训练和部署模型的负能源成本、训练模型所排放的碳的社会成本和二次环境影响。
如果净成本大于收益，那么基础模型开发人员和大规模部署人员应该考虑减少危害的策略。
这可能包括部署一个更有效的模型，或者根本不部署模型。
;Patterson et al. 2021;Lannelongue et al. 2021;Parcollet and Ravanelli[2021]。
解决这类排放问题势在必行:目前的预测表明，气候变化的速度比以前认为的要快[Masson-Delmotte et al. 2021]。
图27所示。
一个假设的平摊微调示例显示了基础模型(在本例中为BERT Base)比从头开始训练的变压器模型具有更低的能量成本。
我们估计了Strubell等人[2019]训练BERT的前期能量成本，以及Chaudhary等人[2020]对下游任务进行微调的成本。
我们比较了从头开始训练变压器的线性增加成本，来自Strubell等人[2019]。
如果BERT用于少于~ 80个任务，则无法收回前期的能量成本。
在此之后，BERT比从头开始训练的模型更节能。
Ramsundar et al. 2015;Camacho et al. 2018;Duran-Frigola et al. 2020;
2020molecule需要多年昂贵的湿实验室实验[Hughes et al. 2011;Schenone et al. 2013;Schneider 2018]。基础模型的生成性可以提高搜索空间和效率(参见§2.4:推理)，这不仅减少了实验数量，而且有助于发现新的更好的药物[Jin et al. 2018;You et al. 2018;Walters and Barzilay 2020;Stokes et al. 2020]。此外，通过单一基础模型同时解决相关药物发现问题(即靶点识别、疗效预测、副作用预测等)可能会改善每个问题的解决方案[Ramsundar et al. 2015;Camacho et al. 2018;Duran-Frigola et al. 2020;黄等。2021a]。
例如，基础模型在影响治疗设计方面显示出巨大潜力的一个领域是使用语言模型对蛋白质进行建模。
阿什利2016]。
例如，给定一组药物和患者基因组，基础模型可以帮助预测哪种药物最有可能以最小的副作用治疗患者
[Harrer等人。2019]也研究了治疗或候选药物的有效性和安全性。传统临床试验效率低下且成本高昂:80%的试验失败是由于无法显示有效性/安全性或患者匹配问题[Ali等人，2020;刘等。2021c]。基础模型可以在以下方面提供帮助:预测潜在的失败，并根据现有研究设计有希望的临床试验方案(例如，患者资格标准);以及根据患者个人资料自动匹配符合条件的患者，这些资料是包括电子病历、基因序列等在内的多模态数据[Harrer et al. 2019]。
[preup et al. 2000;Lange et al. 2012;Ajay et al. 2021;Yang and Nachum 2021;Schwarzer et al. 2021;Pathak et al. 2017;Haber et al. 2018]数据影响训练信号56，但随着模型的改进，训练算法本身可以自适应地寻找或构建更丰富的训练样例，以加速学习[Tamkin et al. 2021c]。Stiennon et al.[2020])。相反，人们可能会想象当前方法的更复杂版本，这些方法从原始的在线(Klyubin等人，2005;Singh等人，2005;Salge等人，2013;Shakir Mohamed 2015;Florensa等人，2017;Pathak等人，2017;Haber等人，2018)或离线[preup等人，2000;Lange等人，2012;Ajay等人，2021;Yang和Nachum 2021;Schwarzer等人，2021]交互中获取各种现实世界的能力，而不需要人工注释或任务构建。
这些方法可能使用与现有SSL算法非常相似的技术:
